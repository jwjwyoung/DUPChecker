-----------cassandra-0.3.0-final vs cassandra-0.3.0-rc1-----------
add: 6, delete: 0, change: 108, unhandled: 0 size_exceptions: 0 size_serialize: 0
-----------cassandra-0.3.0-rc1 vs cassandra-0.3.0-rc2-----------
add: 0, delete: 6, change: 97, unhandled: 0 size_exceptions: 0 size_serialize: 0
-----------cassandra-0.3.0-rc2 vs cassandra-0.3.0-rc3-----------
add: 0, delete: 0, change: 5, unhandled: 0 size_exceptions: 0 size_serialize: 0
-----------cassandra-0.3.0-rc3 vs cassandra-0.4.0-beta1-----------
    public void serialize(RangeCommand command, DataOutputStream dos) throws IOException
    {
        dos.writeUTF(command.table);
        dos.writeUTF(command.startWith);
        dos.writeUTF(command.stopAt);
        dos.writeInt(command.maxResults);
    }

+++++++++++++++++++++++
    public void serialize(RangeCommand command, DataOutputStream dos) throws IOException
    {
        dos.writeUTF(command.table);
        dos.writeUTF(command.columnFamily);
        dos.writeUTF(command.startWith);
        dos.writeUTF(command.stopAt);
        dos.writeInt(command.maxResults);
    }

    public RangeCommand deserialize(DataInputStream dis) throws IOException
    {
        return new RangeCommand(dis.readUTF(), dis.readUTF(), dis.readUTF(), dis.readInt());
+++++++++++++++++++++++
    public RangeCommand deserialize(DataInputStream dis) throws IOException
    {
        return new RangeCommand(dis.readUTF(), dis.readUTF(), dis.readUTF(), dis.readUTF(), dis.readInt());
    private ReadCommand serializeAndDeserializeReadMessage(ReadCommand rm)
    {
        ReadCommand rm2 = null;
        ReadCommandSerializer rms = ReadCommand.serializer();
        DataOutputBuffer dos = new DataOutputBuffer();
        DataInputBuffer dis = new DataInputBuffer();

        try
        {
            rms.serialize(rm, dos);
            dis.reset(dos.getData(), dos.getLength());
            rm2 = rms.deserialize(dis);
        }
        catch (IOException e)
        {
            throw new RuntimeException(e);
        }
        return rm2;
    }
    
    @Test
+++++++++++++++++++++++
    private ReadCommand serializeAndDeserializeReadMessage(ReadCommand rm) throws IOException
    {
        ReadCommandSerializer rms = ReadCommand.serializer();
        DataOutputBuffer dos = new DataOutputBuffer();
        DataInputBuffer dis = new DataInputBuffer();

        rms.serialize(rm, dos);
        dis.reset(dos.getData(), dos.getLength());
        return rms.deserialize(dis);
    }
    
    @Test
add: 53, delete: 105, change: 224, unhandled: 0 size_exceptions: 5 size_serialize: 3
-----------cassandra-0.4.0-beta1 vs cassandra-0.4.0-final-----------
add: 14, delete: 16, change: 178, unhandled: 0 size_exceptions: 9 size_serialize: 0
-----------cassandra-0.4.0-final vs cassandra-0.4.0-rc1-----------
add: 6, delete: 3, change: 24, unhandled: 0 size_exceptions: 14 size_serialize: 0
-----------cassandra-0.4.0-rc1 vs cassandra-0.4.0-rc2-----------
add: 1, delete: 6, change: 19, unhandled: 0 size_exceptions: 0 size_serialize: 0
-----------cassandra-0.4.0-rc2 vs cassandra-0.4.1-----------
add: 2, delete: 0, change: 30, unhandled: 0 size_exceptions: 14 size_serialize: 0
-----------cassandra-0.4.1 vs cassandra-0.4.2-----------
add: 1, delete: 0, change: 13, unhandled: 0 size_exceptions: 1 size_serialize: 0
-----------cassandra-0.4.2 vs cassandra-0.5.0-----------
    static ColumnSerializer serializer()
    {
        return serializer_;
    }

+++++++++++++++++++++++
    public static ColumnSerializer serializer()
    {
        return serializer_;
    }

    public void serializeForSSTable(ColumnFamily columnFamily, DataOutput dos) throws IOException
    {
        dos.writeInt(columnFamily.localDeletionTime);
        dos.writeLong(columnFamily.markedForDeleteAt);

        Collection<IColumn> columns = columnFamily.getSortedColumns();
        dos.writeInt(columns.size());
        for ( IColumn column : columns )
        {
            columnFamily.getColumnSerializer().serialize(column, dos);
        }
    }

+++++++++++++++++++++++
    public void serializeForSSTable(ColumnFamily columnFamily, DataOutput dos)
    {
        try
        {
            dos.writeInt(columnFamily.localDeletionTime);
            dos.writeLong(columnFamily.markedForDeleteAt);

            Collection<IColumn> columns = columnFamily.getSortedColumns();
            dos.writeInt(columns.size());
            for (IColumn column : columns)
            {
                columnFamily.getColumnSerializer().serialize(column, dos);
            }
        }
        catch (IOException e)
        {
            throw new RuntimeException(e);
        }
    }

    public void serializeWithIndexes(ColumnFamily columnFamily, DataOutput dos) throws IOException
    {
        ColumnIndexer.serialize(columnFamily, dos);
        serializeForSSTable(columnFamily, dos);
    }

+++++++++++++++++++++++
    public void serializeWithIndexes(ColumnFamily columnFamily, DataOutput dos)
    {
        ColumnIndexer.serialize(columnFamily, dos);
        serializeForSSTable(columnFamily, dos);
    }

    public void serialize(ColumnFamily columnFamily, DataOutput dos) throws IOException
    {
        dos.writeUTF(columnFamily.name());
        dos.writeUTF(columnFamily.type_);
        dos.writeUTF(columnFamily.getComparatorName());
        dos.writeUTF(columnFamily.getSubComparatorName());
        serializeForSSTable(columnFamily, dos);
    }

+++++++++++++++++++++++
    public void serialize(ColumnFamily columnFamily, DataOutput dos)
    {
        try
        {
            if (columnFamily == null)
            {
                dos.writeUTF(""); // not a legal CF name
                return;
            }

            dos.writeUTF(columnFamily.name());
            dos.writeUTF(columnFamily.type_);
            dos.writeUTF(columnFamily.getComparatorName());
            dos.writeUTF(columnFamily.getSubComparatorName());
        }
        catch (IOException e)
        {
            throw new RuntimeException(e);
        }
        serializeForSSTable(columnFamily, dos);
    }

    public ColumnFamily deserialize(DataInput dis) throws IOException
    {
        ColumnFamily cf = deserializeFromSSTableNoColumns(dis.readUTF(), dis.readUTF(), readComparator(dis), readComparator(dis), dis);
        int size = dis.readInt();
        IColumn column;
        for (int i = 0; i < size; ++i)
        {
            column = cf.getColumnSerializer().deserialize(dis);
            cf.addColumn(column);
        }
        return cf;
    }

+++++++++++++++++++++++
    public ColumnFamily deserialize(DataInput dis) throws IOException
    {
        String cfName = dis.readUTF();
        if (cfName.isEmpty())
            return null;
        ColumnFamily cf = deserializeFromSSTableNoColumns(cfName, dis.readUTF(), readComparator(dis), readComparator(dis), dis);
        deserializeColumns(dis, cf);
        return cf;
    }

    public static void serialize(ColumnFamily columnFamily, DataOutput dos) throws IOException
	{
        Collection<IColumn> columns = columnFamily.getSortedColumns();
        BloomFilter bf = createColumnBloomFilter(columns);                    
        /* Write out the bloom filter. */
        DataOutputBuffer bufOut = new DataOutputBuffer(); 
        BloomFilter.serializer().serialize(bf, bufOut);
        /* write the length of the serialized bloom filter. */
        dos.writeInt(bufOut.getLength());
        /* write out the serialized bytes. */
        dos.write(bufOut.getData(), 0, bufOut.getLength());

        /* Do the indexing */
        doIndexing(columnFamily.getComparator(), columns, dos);
	}
    
    /**
     * Create a bloom filter that contains the subcolumns and the columns that
     * make up this Column Family.
     * @param columns columns of the ColumnFamily
     * @return BloomFilter with the summarized information.
     */
+++++++++++++++++++++++
    public static void serialize(ColumnFamily columnFamily, DataOutput dos)
	{
        Collection<IColumn> columns = columnFamily.getSortedColumns();
        BloomFilter bf = createColumnBloomFilter(columns);                    
        /* Write out the bloom filter. */
        DataOutputBuffer bufOut = new DataOutputBuffer();
        try
        {
            BloomFilter.serializer().serialize(bf, bufOut);
            /* write the length of the serialized bloom filter. */
            dos.writeInt(bufOut.getLength());
            /* write out the serialized bytes. */
            dos.write(bufOut.getData(), 0, bufOut.getLength());

            /* Do the indexing */
            doIndexing(columnFamily.getComparator(), columns, dos);
        }
        catch (IOException e)
        {
            throw new RuntimeException(e);
        }
	}
    
    /**
     * Create a bloom filter that contains the subcolumns and the columns that
     * make up this Column Family.
     * @param columns columns of the ColumnFamily
     * @return BloomFilter with the summarized information.
     */
    public void serialize(IColumn column, DataOutput dos) throws IOException
    {
        ColumnSerializer.writeName(column.name(), dos);
        dos.writeBoolean(column.isMarkedForDelete());
        dos.writeLong(column.timestamp());
        FBUtilities.writeByteArray(column.value(), dos);
    }

+++++++++++++++++++++++
    public void serialize(IColumn column, DataOutput dos)
    {
        ColumnSerializer.writeName(column.name(), dos);
        try
        {
            dos.writeBoolean(column.isMarkedForDelete());
            dos.writeLong(column.timestamp());
            FBUtilities.writeByteArray(column.value(), dos);
        }
        catch (IOException e)
        {
            throw new RuntimeException(e);
        }
    }

    public Column deserialize(DataInput dis) throws IOException
    {
        byte[] name = ColumnSerializer.readName(dis);
        boolean delete = dis.readBoolean();
        long ts = dis.readLong();
        byte[] value = FBUtilities.readByteArray(dis);
        return new Column(name, value, ts, delete);
+++++++++++++++++++++++
    public Column deserialize(DataInput dis) throws IOException
    {
        byte[] name = ColumnSerializer.readName(dis);
        boolean delete = dis.readBoolean();
        long ts = dis.readLong();
        int length = dis.readInt();
        if (length < 0)
        {
            throw new IOException("Corrupt (negative) value length encountered");
        }
        byte[] value = new byte[length];
        if (length > 0)
        {
            dis.readFully(value);
        }
        return new Column(name, value, ts, delete);
    public void serialize(Row row, DataOutputStream dos) throws IOException
    {
        dos.writeUTF(row.getTable());
        dos.writeUTF(row.key());
        Collection<ColumnFamily> columnFamilies = row.getColumnFamilies();
        int size = columnFamilies.size();
        dos.writeInt(size);

        for (ColumnFamily cf : columnFamilies)
        {
            ColumnFamily.serializer().serialize(cf, dos);
        }
    }

+++++++++++++++++++++++
    public void serialize(Row row, DataOutputStream dos) throws IOException
    {
        dos.writeUTF(row.key);
        ColumnFamily.serializer().serialize(row.cf, dos);
    }

    public Row deserialize(DataInputStream dis) throws IOException
    {
        String table = dis.readUTF();
        String key = dis.readUTF();
        Row row = new Row(table, key);
        int size = dis.readInt();

        for (int i = 0; i < size; ++i)
        {
            ColumnFamily cf = ColumnFamily.serializer().deserialize(dis);
            row.addColumnFamily(cf);
        }
        return row;
+++++++++++++++++++++++
    public Row deserialize(DataInputStream dis) throws IOException
    {
        return new Row(dis.readUTF(), ColumnFamily.serializer().deserialize(dis));
    static SuperColumnSerializer serializer(AbstractType comparator)
    {
        return new SuperColumnSerializer(comparator);
    }

+++++++++++++++++++++++
    public static SuperColumnSerializer serializer(AbstractType comparator)
    {
        return new SuperColumnSerializer(comparator);
    }

    public void serialize(IColumn column, DataOutput dos) throws IOException
    {
    	SuperColumn superColumn = (SuperColumn)column;
        ColumnSerializer.writeName(column.name(), dos);
        dos.writeInt(superColumn.getLocalDeletionTime());
        dos.writeLong(superColumn.getMarkedForDeleteAt());

        Collection<IColumn> columns  = column.getSubColumns();
        dos.writeInt(columns.size());

        for ( IColumn subColumn : columns )
        {
            Column.serializer().serialize(subColumn, dos);
        }
    }

+++++++++++++++++++++++
    public void serialize(IColumn column, DataOutput dos)
    {
    	SuperColumn superColumn = (SuperColumn)column;
        ColumnSerializer.writeName(column.name(), dos);
        try
        {
            dos.writeInt(superColumn.getLocalDeletionTime());
            dos.writeLong(superColumn.getMarkedForDeleteAt());

            Collection<IColumn> columns = column.getSubColumns();
            dos.writeInt(columns.size());
            for (IColumn subColumn : columns)
            {
                Column.serializer().serialize(subColumn, dos);
            }
        }
        catch (IOException e)
        {
            throw new RuntimeException(e);
        }
    }

    public IColumn deserialize(DataInput dis) throws IOException
    {
        byte[] name = ColumnSerializer.readName(dis);
        SuperColumn superColumn = new SuperColumn(name, comparator);
        superColumn.markForDeleteAt(dis.readInt(), dis.readLong());

        /* read the number of columns */
        int size = dis.readInt();
        for ( int i = 0; i < size; ++i )
        {
            IColumn subColumn = Column.serializer().deserialize(dis);
            superColumn.addColumn(subColumn);
        }
        return superColumn;
+++++++++++++++++++++++
    public IColumn deserialize(DataInput dis) throws IOException
    {
        byte[] name = ColumnSerializer.readName(dis);
        SuperColumn superColumn = new SuperColumn(name, comparator);
        int localDeleteTime = dis.readInt();
        if (localDeleteTime != Integer.MIN_VALUE && localDeleteTime <= 0)
        {
            throw new IOException("Invalid localDeleteTime read: " + localDeleteTime);
        }
        superColumn.markForDeleteAt(localDeleteTime, dis.readLong());

        /* read the number of columns */
        int size = dis.readInt();
        for ( int i = 0; i < size; ++i )
        {
            IColumn subColumn = Column.serializer().deserialize(dis);
            superColumn.addColumn(subColumn);
        }
        return superColumn;
    public static void serialize(EndPoint endPoint, DataOutputStream dos) throws IOException
    {        
        dos.write(EndPoint.toBytes(endPoint));
    }
    
+++++++++++++++++++++++
    public static void serialize(InetAddress endPoint, DataOutputStream dos) throws IOException
    {        
        dos.write(endPoint.getAddress());
    }
    
    public static EndPoint deserialize(DataInputStream dis) throws IOException
    {     
        byte[] bytes = new byte[6];
        dis.readFully(bytes, 0, bytes.length);
        return EndPoint.fromBytes(bytes);       
    }
    
+++++++++++++++++++++++
    public static InetAddress deserialize(DataInputStream dis) throws IOException
    {     
        byte[] bytes = new byte[4];
        dis.readFully(bytes, 0, bytes.length);
        return InetAddress.getByAddress(bytes);
add: 55, delete: 37, change: 161, unhandled: 0 size_exceptions: 45 size_serialize: 15
-----------cassandra-0.5.0 vs cassandra-0.5.0-beta1-----------
    public void serialize(ColumnFamily columnFamily, DataOutput dos)
    {
        try
        {
            if (columnFamily == null)
            {
                dos.writeUTF(""); // not a legal CF name
                return;
            }

            dos.writeUTF(columnFamily.name());
            dos.writeUTF(columnFamily.type_);
            dos.writeUTF(columnFamily.getComparatorName());
            dos.writeUTF(columnFamily.getSubComparatorName());
        }
        catch (IOException e)
        {
            throw new RuntimeException(e);
        }
        serializeForSSTable(columnFamily, dos);
    }

+++++++++++++++++++++++
    public void serialize(ColumnFamily columnFamily, DataOutput dos)
    {
        try
        {
            dos.writeUTF(columnFamily.name());
            dos.writeUTF(columnFamily.type_);
            dos.writeUTF(columnFamily.getComparatorName());
            dos.writeUTF(columnFamily.getSubComparatorName());
        }
        catch (IOException e)
        {
            throw new RuntimeException(e);
        }
        serializeForSSTable(columnFamily, dos);
    }

    public ColumnFamily deserialize(DataInput dis) throws IOException
    {
        String cfName = dis.readUTF();
        if (cfName.isEmpty())
            return null;
        ColumnFamily cf = deserializeFromSSTableNoColumns(cfName, dis.readUTF(), readComparator(dis), readComparator(dis), dis);
        deserializeColumns(dis, cf);
        return cf;
    }

+++++++++++++++++++++++
    public ColumnFamily deserialize(DataInput dis) throws IOException
    {
        ColumnFamily cf = deserializeFromSSTableNoColumns(dis.readUTF(), dis.readUTF(), readComparator(dis), readComparator(dis), dis);
        deserializeColumns(dis, cf);
        return cf;
    }

    public void serialize(RangeSliceCommand sliceCommand, DataOutputStream dos) throws IOException
    {
        dos.writeUTF(sliceCommand.keyspace);
        dos.writeUTF(sliceCommand.column_family);
        dos.writeInt(sliceCommand.super_column == null ? 0 : sliceCommand.super_column.length);
        if (sliceCommand.super_column != null)
            dos.write(sliceCommand.super_column);

        TSerializer ser = new TSerializer(new TBinaryProtocol.Factory());
        try
        {
            byte[] serPred = ser.serialize(sliceCommand.predicate);
            dos.writeInt(serPred.length);
            dos.write(serPred);
        }
        catch (TException ex)
        {
            throw new IOException(ex);
        }

        DecoratedKey.serializer().serialize(sliceCommand.startKey, dos);
        DecoratedKey.serializer().serialize(sliceCommand.finishKey, dos);
        dos.writeInt(sliceCommand.max_keys);
    }

+++++++++++++++++++++++
    public void serialize(RangeSliceCommand sliceCommand, DataOutputStream dos) throws IOException
    {
        dos.writeUTF(sliceCommand.keyspace);
        dos.writeUTF(sliceCommand.column_family);
        dos.writeInt(sliceCommand.super_column == null ? 0 : sliceCommand.super_column.length);
        if (sliceCommand.super_column != null)
            dos.write(sliceCommand.super_column);

        TSerializer ser = new TSerializer(new TBinaryProtocol.Factory());
        try
        {
            byte[] serPred = ser.serialize(sliceCommand.predicate);
            dos.writeInt(serPred.length);
            dos.write(serPred);
        }
        catch (TException ex)
        {
            throw new IOException(ex);
        }

        dos.writeUTF(sliceCommand.start_key);
        dos.writeUTF(sliceCommand.finish_key);
        dos.writeInt(sliceCommand.max_keys);
    }

    public RangeSliceCommand deserialize(DataInputStream dis) throws IOException
    {
        String keyspace = dis.readUTF();
        String column_family = dis.readUTF();

        int scLength = dis.readInt();
        byte[] super_column = null;
        if (scLength > 0)
            super_column = readBuf(scLength, dis);

        byte[] predBytes = new byte[dis.readInt()];
        dis.readFully(predBytes);
        TDeserializer dser = new TDeserializer(new TBinaryProtocol.Factory());
        SlicePredicate pred =  new SlicePredicate();
        try
        {
            dser.deserialize(pred, predBytes);
        }
        catch (TException ex)
        {
            throw new IOException(ex);
        }

        DecoratedKey startKey = DecoratedKey.serializer().deserialize(dis);
        DecoratedKey finishKey = DecoratedKey.serializer().deserialize(dis);
        int max_keys = dis.readInt();
        return new RangeSliceCommand(keyspace,
                                     new ColumnParent(column_family, super_column),
                                     pred,
                                     startKey,
                                     finishKey,
                                     max_keys);

    }

+++++++++++++++++++++++
    public RangeSliceCommand deserialize(DataInputStream dis) throws IOException
    {
        String keyspace = dis.readUTF();
        String column_family = dis.readUTF();

        int scLength = dis.readInt();
        byte[] super_column = null;
        if (scLength > 0)
            super_column = readBuf(scLength, dis);

        byte[] predBytes = new byte[dis.readInt()];
        dis.readFully(predBytes);
        TDeserializer dser = new TDeserializer(new TBinaryProtocol.Factory());
        SlicePredicate pred =  new SlicePredicate();
        try
        {
            dser.deserialize(pred, predBytes);
        }
        catch (TException ex)
        {
            throw new IOException(ex);
        }

        String start_key = dis.readUTF();
        String finish_key = dis.readUTF();
        int max_keys = dis.readInt();
        return new RangeSliceCommand(keyspace,
                                     new ColumnParent(column_family, super_column),
                                     pred,
                                     start_key,
                                     finish_key,
                                     max_keys);

    }

    public void serialize(Row row, DataOutputStream dos) throws IOException
    {
        dos.writeUTF(row.key);
        ColumnFamily.serializer().serialize(row.cf, dos);
    }

+++++++++++++++++++++++
    public void serialize(Row row, DataOutputStream dos) throws IOException
    {
        dos.writeUTF(row.getTable());
        dos.writeUTF(row.key());
        Collection<ColumnFamily> columnFamilies = row.getColumnFamilies();
        int size = columnFamilies.size();
        dos.writeInt(size);

        for (ColumnFamily cf : columnFamilies)
        {
            ColumnFamily.serializer().serialize(cf, dos);
        }
    }

    public Row deserialize(DataInputStream dis) throws IOException
    {
        return new Row(dis.readUTF(), ColumnFamily.serializer().deserialize(dis));
+++++++++++++++++++++++
    public Row deserialize(DataInputStream dis) throws IOException
    {
        String table = dis.readUTF();
        String key = dis.readUTF();
        Row row = new Row(table, key);
        int size = dis.readInt();

        for (int i = 0; i < size; ++i)
        {
            ColumnFamily cf = ColumnFamily.serializer().deserialize(dis);
            row.addColumnFamily(cf);
        }
        return row;
add: 11, delete: 25, change: 119, unhandled: 0 size_exceptions: 11 size_serialize: 6
-----------cassandra-0.5.0-beta1 vs cassandra-0.5.0-beta2-----------
    public void serialize(ColumnFamily columnFamily, DataOutput dos)
    {
        try
        {
            dos.writeUTF(columnFamily.name());
            dos.writeUTF(columnFamily.type_);
            dos.writeUTF(columnFamily.getComparatorName());
            dos.writeUTF(columnFamily.getSubComparatorName());
        }
        catch (IOException e)
        {
            throw new RuntimeException(e);
        }
        serializeForSSTable(columnFamily, dos);
    }

+++++++++++++++++++++++
    public void serialize(ColumnFamily columnFamily, DataOutput dos)
    {
        try
        {
            if (columnFamily == null)
            {
                dos.writeUTF(""); // not a legal CF name
                return;
            }

            dos.writeUTF(columnFamily.name());
            dos.writeUTF(columnFamily.type_);
            dos.writeUTF(columnFamily.getComparatorName());
            dos.writeUTF(columnFamily.getSubComparatorName());
        }
        catch (IOException e)
        {
            throw new RuntimeException(e);
        }
        serializeForSSTable(columnFamily, dos);
    }

    public ColumnFamily deserialize(DataInput dis) throws IOException
    {
        ColumnFamily cf = deserializeFromSSTableNoColumns(dis.readUTF(), dis.readUTF(), readComparator(dis), readComparator(dis), dis);
        deserializeColumns(dis, cf);
        return cf;
    }

+++++++++++++++++++++++
    public ColumnFamily deserialize(DataInput dis) throws IOException
    {
        String cfName = dis.readUTF();
        if (cfName.isEmpty())
            return null;
        ColumnFamily cf = deserializeFromSSTableNoColumns(cfName, dis.readUTF(), readComparator(dis), readComparator(dis), dis);
        deserializeColumns(dis, cf);
        return cf;
    }

    public void serialize(RangeSliceCommand sliceCommand, DataOutputStream dos) throws IOException
    {
        dos.writeUTF(sliceCommand.keyspace);
        dos.writeUTF(sliceCommand.column_family);
        dos.writeInt(sliceCommand.super_column == null ? 0 : sliceCommand.super_column.length);
        if (sliceCommand.super_column != null)
            dos.write(sliceCommand.super_column);

        TSerializer ser = new TSerializer(new TBinaryProtocol.Factory());
        try
        {
            byte[] serPred = ser.serialize(sliceCommand.predicate);
            dos.writeInt(serPred.length);
            dos.write(serPred);
        }
        catch (TException ex)
        {
            throw new IOException(ex);
        }

        dos.writeUTF(sliceCommand.start_key);
        dos.writeUTF(sliceCommand.finish_key);
        dos.writeInt(sliceCommand.max_keys);
    }

+++++++++++++++++++++++
    public void serialize(RangeSliceCommand sliceCommand, DataOutputStream dos) throws IOException
    {
        dos.writeUTF(sliceCommand.keyspace);
        dos.writeUTF(sliceCommand.column_family);
        dos.writeInt(sliceCommand.super_column == null ? 0 : sliceCommand.super_column.length);
        if (sliceCommand.super_column != null)
            dos.write(sliceCommand.super_column);

        TSerializer ser = new TSerializer(new TBinaryProtocol.Factory());
        try
        {
            byte[] serPred = ser.serialize(sliceCommand.predicate);
            dos.writeInt(serPred.length);
            dos.write(serPred);
        }
        catch (TException ex)
        {
            throw new IOException(ex);
        }

        DecoratedKey.serializer().serialize(sliceCommand.startKey, dos);
        DecoratedKey.serializer().serialize(sliceCommand.finishKey, dos);
        dos.writeInt(sliceCommand.max_keys);
    }

    public RangeSliceCommand deserialize(DataInputStream dis) throws IOException
    {
        String keyspace = dis.readUTF();
        String column_family = dis.readUTF();

        int scLength = dis.readInt();
        byte[] super_column = null;
        if (scLength > 0)
            super_column = readBuf(scLength, dis);

        byte[] predBytes = new byte[dis.readInt()];
        dis.readFully(predBytes);
        TDeserializer dser = new TDeserializer(new TBinaryProtocol.Factory());
        SlicePredicate pred =  new SlicePredicate();
        try
        {
            dser.deserialize(pred, predBytes);
        }
        catch (TException ex)
        {
            throw new IOException(ex);
        }

        String start_key = dis.readUTF();
        String finish_key = dis.readUTF();
        int max_keys = dis.readInt();
        return new RangeSliceCommand(keyspace,
                                     new ColumnParent(column_family, super_column),
                                     pred,
                                     start_key,
                                     finish_key,
                                     max_keys);

    }

+++++++++++++++++++++++
    public RangeSliceCommand deserialize(DataInputStream dis) throws IOException
    {
        String keyspace = dis.readUTF();
        String column_family = dis.readUTF();

        int scLength = dis.readInt();
        byte[] super_column = null;
        if (scLength > 0)
            super_column = readBuf(scLength, dis);

        byte[] predBytes = new byte[dis.readInt()];
        dis.readFully(predBytes);
        TDeserializer dser = new TDeserializer(new TBinaryProtocol.Factory());
        SlicePredicate pred =  new SlicePredicate();
        try
        {
            dser.deserialize(pred, predBytes);
        }
        catch (TException ex)
        {
            throw new IOException(ex);
        }

        DecoratedKey startKey = DecoratedKey.serializer().deserialize(dis);
        DecoratedKey finishKey = DecoratedKey.serializer().deserialize(dis);
        int max_keys = dis.readInt();
        return new RangeSliceCommand(keyspace,
                                     new ColumnParent(column_family, super_column),
                                     pred,
                                     startKey,
                                     finishKey,
                                     max_keys);

    }

    public void serialize(Row row, DataOutputStream dos) throws IOException
    {
        dos.writeUTF(row.getTable());
        dos.writeUTF(row.key());
        Collection<ColumnFamily> columnFamilies = row.getColumnFamilies();
        int size = columnFamilies.size();
        dos.writeInt(size);

        for (ColumnFamily cf : columnFamilies)
        {
            ColumnFamily.serializer().serialize(cf, dos);
        }
    }

+++++++++++++++++++++++
    public void serialize(Row row, DataOutputStream dos) throws IOException
    {
        dos.writeUTF(row.key);
        ColumnFamily.serializer().serialize(row.cf, dos);
    }

    public Row deserialize(DataInputStream dis) throws IOException
    {
        String table = dis.readUTF();
        String key = dis.readUTF();
        Row row = new Row(table, key);
        int size = dis.readInt();

        for (int i = 0; i < size; ++i)
        {
            ColumnFamily cf = ColumnFamily.serializer().deserialize(dis);
            row.addColumnFamily(cf);
        }
        return row;
+++++++++++++++++++++++
    public Row deserialize(DataInputStream dis) throws IOException
    {
        return new Row(dis.readUTF(), ColumnFamily.serializer().deserialize(dis));
add: 21, delete: 10, change: 95, unhandled: 0 size_exceptions: 3 size_serialize: 6
-----------cassandra-0.5.0-beta2 vs cassandra-0.5.0-rc1-----------
add: 2, delete: 0, change: 55, unhandled: 0 size_exceptions: 0 size_serialize: 0
-----------cassandra-0.5.0-rc1 vs cassandra-0.5.0-rc2-----------
add: 0, delete: 0, change: 10, unhandled: 0 size_exceptions: 0 size_serialize: 0
-----------cassandra-0.5.0-rc2 vs cassandra-0.5.0-rc3-----------
add: 0, delete: 0, change: 2, unhandled: 0 size_exceptions: 0 size_serialize: 0
-----------cassandra-0.5.0-rc3 vs cassandra-0.5.1-----------
add: 6, delete: 2, change: 48, unhandled: 0 size_exceptions: 9 size_serialize: 0
-----------cassandra-0.5.1 vs cassandra-0.6.0-----------
    public void serializeForSSTable(ColumnFamily columnFamily, DataOutput dos)
    {
        try
        {
            dos.writeInt(columnFamily.localDeletionTime);
            dos.writeLong(columnFamily.markedForDeleteAt);

            Collection<IColumn> columns = columnFamily.getSortedColumns();
            dos.writeInt(columns.size());
            for (IColumn column : columns)
            {
                columnFamily.getColumnSerializer().serialize(column, dos);
            }
        }
        catch (IOException e)
        {
            throw new RuntimeException(e);
        }
    }

+++++++++++++++++++++++
    public void serializeForSSTable(ColumnFamily columnFamily, DataOutput dos)
    {
        try
        {
            dos.writeInt(columnFamily.localDeletionTime.get());
            dos.writeLong(columnFamily.markedForDeleteAt.get());

            Collection<IColumn> columns = columnFamily.getSortedColumns();
            dos.writeInt(columns.size());
            for (IColumn column : columns)
            {
                columnFamily.getColumnSerializer().serialize(column, dos);
            }
        }
        catch (IOException e)
        {
            throw new RuntimeException(e);
        }
    }

    public int serializedSize()
    {
    	/*
    	 * We need to keep the way we are calculating the column size in sync with the
    	 * way we are calculating the size for the column family serializer.
    	 */
    	return IColumn.UtfPrefix_ + name_.length + DBConstants.intSize_ + DBConstants.longSize_ + DBConstants.intSize_ + getSizeOfAllColumns();
    }

    /**
     * This calculates the exact size of the sub columns on the fly
     */
+++++++++++++++++++++++
    public int serializedSize()
    {
    	/*
    	 * We need to keep the way we are calculating the column size in sync with the
    	 * way we are calculating the size for the column family serializer.
    	 */
    	return IColumn.UtfPrefix_ + name_.length + DBConstants.intSize_ + DBConstants.longSize_ + DBConstants.intSize_ + size();
    }

	public static ArrayList<IndexInfo> deserializeIndex(RandomAccessFile in) throws IOException
	{
        ArrayList<IndexInfo> indexList = new ArrayList<IndexInfo>();

		int columnIndexSize = in.readInt();
        long start = in.getFilePointer();
        while (in.getFilePointer() < start + columnIndexSize)
        {
            indexList.add(IndexInfo.deserialize(in));
        }
        assert in.getFilePointer() == start + columnIndexSize;

        return indexList;
	}

    /**
     * Defreeze the bloom filter.
     *
     * @return bloom filter summarizing the column information
     * @throws java.io.IOException
     */
+++++++++++++++++++++++
	public static ArrayList<IndexInfo> deserializeIndex(FileDataInput in) throws IOException
	{
        ArrayList<IndexInfo> indexList = new ArrayList<IndexInfo>();

		int columnIndexSize = in.readInt();
        in.mark();
        while (in.bytesPastMark() < columnIndexSize)
        {
            indexList.add(IndexInfo.deserialize(in));
        }
        assert in.bytesPastMark() == columnIndexSize;

        return indexList;
	}

    /**
     * Defreeze the bloom filter.
     *
     * @return bloom filter summarizing the column information
     * @throws java.io.IOException
     */
    private ReadCommand serializeAndDeserializeReadMessage(ReadCommand rm) throws IOException
    {
        ReadCommandSerializer rms = ReadCommand.serializer();
        DataOutputBuffer dos = new DataOutputBuffer();
        DataInputBuffer dis = new DataInputBuffer();

        rms.serialize(rm, dos);
        dis.reset(dos.getData(), dos.getLength());
        return rms.deserialize(dis);
    }
    
    @Test
+++++++++++++++++++++++
    private ReadCommand serializeAndDeserializeReadMessage(ReadCommand rm) throws IOException
    {
        ReadCommandSerializer rms = ReadCommand.serializer();
        DataOutputBuffer dos = new DataOutputBuffer();
        ByteArrayInputStream bis;

        rms.serialize(rm, dos);
        bis = new ByteArrayInputStream(dos.getData(), 0, dos.getLength());
        return rms.deserialize(new DataInputStream(bis));
    }
    
    @Test
add: 100, delete: 80, change: 147, unhandled: 0 size_exceptions: 6 size_serialize: 4
-----------cassandra-0.6.0 vs cassandra-0.6.0-beta1-----------
add: 6, delete: 6, change: 128, unhandled: 0 size_exceptions: 0 size_serialize: 0
-----------cassandra-0.6.0-beta1 vs cassandra-0.6.0-beta2-----------
add: 0, delete: 0, change: 45, unhandled: 0 size_exceptions: 0 size_serialize: 0
-----------cassandra-0.6.0-beta2 vs cassandra-0.6.0-beta3-----------
add: 3, delete: 3, change: 83, unhandled: 0 size_exceptions: 0 size_serialize: 0
-----------cassandra-0.6.0-beta3 vs cassandra-0.6.0-rc1-----------
add: 3, delete: 3, change: 27, unhandled: 0 size_exceptions: 0 size_serialize: 0
-----------cassandra-0.6.0-rc1 vs cassandra-0.6.1-----------
    public static void serialize(InetAddress endPoint, DataOutputStream dos) throws IOException
    {        
        dos.write(endPoint.getAddress());
    }
    
+++++++++++++++++++++++
    public static void serialize(InetAddress endPoint, DataOutputStream dos) throws IOException
    {
        byte[] buf = endPoint.getAddress();
        dos.writeByte(buf.length);
        dos.write(buf);
    }
    
    public static InetAddress deserialize(DataInputStream dis) throws IOException
    {     
        byte[] bytes = new byte[4];
        dis.readFully(bytes, 0, bytes.length);
        return InetAddress.getByAddress(bytes);
+++++++++++++++++++++++
    public static InetAddress deserialize(DataInputStream dis) throws IOException
    {     
        byte[] bytes = new byte[dis.readByte()];
        dis.readFully(bytes, 0, bytes.length);
        return InetAddress.getByAddress(bytes);
add: 3, delete: 1, change: 19, unhandled: 0 size_exceptions: 1 size_serialize: 2
-----------cassandra-0.6.1 vs cassandra-0.6.2-----------
add: 11, delete: 2, change: 34, unhandled: 0 size_exceptions: 0 size_serialize: 0
-----------cassandra-0.6.2 vs cassandra-0.6.3-----------
add: 7, delete: 0, change: 36, unhandled: 0 size_exceptions: 3 size_serialize: 0
-----------cassandra-0.6.3 vs cassandra-0.6.4-----------
add: 0, delete: 7, change: 35, unhandled: 0 size_exceptions: 0 size_serialize: 0
-----------cassandra-0.6.4 vs cassandra-0.6.5-----------
add: 8, delete: 1, change: 22, unhandled: 0 size_exceptions: 0 size_serialize: 0
-----------cassandra-0.6.5 vs cassandra-0.6.6-----------
add: 4, delete: 5, change: 50, unhandled: 0 size_exceptions: 4 size_serialize: 0
-----------cassandra-0.6.6 vs cassandra-0.6.7-----------
add: 0, delete: 0, change: 17, unhandled: 0 size_exceptions: 1 size_serialize: 0
-----------cassandra-0.6.7 vs cassandra-0.6.8-----------
add: 0, delete: 0, change: 6, unhandled: 0 size_exceptions: 0 size_serialize: 0
-----------cassandra-0.6.8 vs cassandra-0.6.9-----------
add: 1, delete: 1, change: 167, unhandled: 0 size_exceptions: 1 size_serialize: 0
-----------cassandra-0.6.9 vs cassandra-0.6.10-----------
add: 0, delete: 1, change: 6, unhandled: 0 size_exceptions: 0 size_serialize: 0
-----------cassandra-0.6.10 vs cassandra-0.6.11-----------
add: 0, delete: 0, change: 11, unhandled: 0 size_exceptions: 0 size_serialize: 0
-----------cassandra-0.6.11 vs cassandra-0.6.12-----------
add: 0, delete: 0, change: 9, unhandled: 0 size_exceptions: 0 size_serialize: 0
-----------cassandra-0.6.12 vs cassandra-0.6.13-----------
add: 0, delete: 0, change: 6, unhandled: 0 size_exceptions: 0 size_serialize: 0
-----------cassandra-0.6.13 vs cassandra-0.7.0-----------
    public static ColumnSerializer serializer()
    {
        return serializer_;
    }

+++++++++++++++++++++++
    public static ColumnSerializer serializer()
    {
        return new ColumnSerializer();
    }

    public void serializeForSSTable(ColumnFamily columnFamily, DataOutput dos)
    {
        try
        {
            dos.writeInt(columnFamily.localDeletionTime.get());
            dos.writeLong(columnFamily.markedForDeleteAt.get());

            Collection<IColumn> columns = columnFamily.getSortedColumns();
            dos.writeInt(columns.size());
            for (IColumn column : columns)
            {
                columnFamily.getColumnSerializer().serialize(column, dos);
            }
        }
        catch (IOException e)
        {
            throw new RuntimeException(e);
        }
    }

+++++++++++++++++++++++
    public int serializeForSSTable(ColumnFamily columnFamily, DataOutput dos)
    {
        try
        {
            serializeCFInfo(columnFamily, dos);

            Collection<IColumn> columns = columnFamily.getSortedColumns();
            int count = columns.size();
            dos.writeInt(count);
            for (IColumn column : columns)
            {
                columnFamily.getColumnSerializer().serialize(column, dos);
            }
            return count;
        }
        catch (IOException e)
        {
            throw new RuntimeException(e);
        }
    }

    public void serializeWithIndexes(ColumnFamily columnFamily, DataOutput dos)
    {
        ColumnIndexer.serialize(columnFamily, dos);
        serializeForSSTable(columnFamily, dos);
    }

+++++++++++++++++++++++
    public int serializeWithIndexes(ColumnFamily columnFamily, DataOutput dos)
    {
        ColumnIndexer.serialize(columnFamily, dos);
        return serializeForSSTable(columnFamily, dos);
    }

    public void serialize(ColumnFamily columnFamily, DataOutput dos)
    {
        try
        {
            if (columnFamily == null)
            {
                dos.writeUTF(""); // not a legal CF name
                return;
            }

            dos.writeUTF(columnFamily.name());
            dos.writeUTF(columnFamily.type_);
            dos.writeUTF(columnFamily.getComparatorName());
            dos.writeUTF(columnFamily.getSubComparatorName());
        }
        catch (IOException e)
        {
            throw new RuntimeException(e);
        }
        serializeForSSTable(columnFamily, dos);
    }

+++++++++++++++++++++++
    public void serialize(ColumnFamily columnFamily, DataOutput dos)
    {
        try
        {
            if (columnFamily == null)
            {
                dos.writeBoolean(false);
                return;
            }

            dos.writeBoolean(true);
            dos.writeInt(columnFamily.id());
        }
        catch (IOException e)
        {
            throw new RuntimeException(e);
        }
        serializeForSSTable(columnFamily, dos);
    }

    public ColumnFamily deserialize(DataInput dis) throws IOException
    {
        String cfName = dis.readUTF();
        if (cfName.isEmpty())
            return null;
        ColumnFamily cf = deserializeFromSSTableNoColumns(cfName, dis.readUTF(), readComparator(dis), readComparator(dis), dis);
        deserializeColumns(dis, cf);
        return cf;
    }

+++++++++++++++++++++++
    public ColumnFamily deserialize(DataInput dis) throws IOException
    {
        if (!dis.readBoolean())
            return null;

        // create a ColumnFamily based on the cf id
        int cfId = dis.readInt();
        if (CFMetaData.getCF(cfId) == null)
            throw new UnserializableColumnFamilyException("Couldn't find cfId=" + cfId, cfId);
        ColumnFamily cf = ColumnFamily.create(cfId);
        deserializeFromSSTableNoColumns(cf, dis);
        deserializeColumns(dis, cf);
        return cf;
    }

    private void deserializeColumns(DataInput dis, ColumnFamily cf) throws IOException
    {
        int size = dis.readInt();
        for (int i = 0; i < size; ++i)
        {
            IColumn column = cf.getColumnSerializer().deserialize(dis);
            cf.addColumn(column);
        }
    }

+++++++++++++++++++++++
    public void deserializeColumns(DataInput dis, ColumnFamily cf) throws IOException
    {
        int size = dis.readInt();
        for (int i = 0; i < size; ++i)
        {
            IColumn column = cf.getColumnSerializer().deserialize(dis);
            cf.addColumn(column);
        }
    }

    public static void serialize(ColumnFamily columnFamily, DataOutput dos)
	{
        Collection<IColumn> columns = columnFamily.getSortedColumns();
        BloomFilter bf = createColumnBloomFilter(columns);                    
        /* Write out the bloom filter. */
        DataOutputBuffer bufOut = new DataOutputBuffer();
        try
        {
            BloomFilter.serializer().serialize(bf, bufOut);
            /* write the length of the serialized bloom filter. */
            dos.writeInt(bufOut.getLength());
            /* write out the serialized bytes. */
            dos.write(bufOut.getData(), 0, bufOut.getLength());

            /* Do the indexing */
            doIndexing(columnFamily.getComparator(), columns, dos);
        }
        catch (IOException e)
        {
            throw new RuntimeException(e);
        }
	}
    
    /**
     * Create a bloom filter that contains the subcolumns and the columns that
     * make up this Column Family.
     * @param columns columns of the ColumnFamily
     * @return BloomFilter with the summarized information.
     */
+++++++++++++++++++++++
    public static void serialize(IIterableColumns columns, DataOutput dos)
    {
        try
        {
            serializeInternal(columns, dos);
        }
        catch (IOException e)
        {
            throw new IOError(e);
        }
    }

    public void serialize(IColumn column, DataOutput dos)
    {
        ColumnSerializer.writeName(column.name(), dos);
        try
        {
            dos.writeBoolean(column.isMarkedForDelete());
            dos.writeLong(column.timestamp());
            FBUtilities.writeByteArray(column.value(), dos);
        }
        catch (IOException e)
        {
            throw new RuntimeException(e);
        }
    }

+++++++++++++++++++++++
    public void serialize(IColumn column, DataOutput dos)
    {
        assert column.name().remaining() > 0;
        FBUtilities.writeShortByteArray(column.name(), dos);
        try
        {
            if (column instanceof ExpiringColumn) {
              dos.writeByte(EXPIRATION_MASK);
              dos.writeInt(((ExpiringColumn) column).getTimeToLive());
              dos.writeInt(column.getLocalDeletionTime());
            } else {
              dos.writeByte((column.isMarkedForDelete()) ? DELETION_MASK : 0);
            }
            dos.writeLong(column.timestamp());
            FBUtilities.writeByteArray(column.value(), dos);
        }
        catch (IOException e)
        {
            throw new RuntimeException(e);
        }
    }

    public Column deserialize(DataInput dis) throws IOException
    {
        byte[] name = ColumnSerializer.readName(dis);
        boolean delete = dis.readBoolean();
        long ts = dis.readLong();
        int length = dis.readInt();
        if (length < 0)
        {
            throw new IOException("Corrupt (negative) value length encountered");
        }
        byte[] value = new byte[length];
        if (length > 0)
        {
            dis.readFully(value);
        }
        return new Column(name, value, ts, delete);
+++++++++++++++++++++++
    public Column deserialize(DataInput dis) throws IOException
    {
        ByteBuffer name = FBUtilities.readShortByteArray(dis);
        if (name.remaining() <= 0)
            throw new CorruptColumnException("invalid column name length " + name.remaining());

        int b = dis.readUnsignedByte();
        if ((b & EXPIRATION_MASK) != 0)
        {
            int ttl = dis.readInt();
            int expiration = dis.readInt();
            long ts = dis.readLong();
            ByteBuffer value = FBUtilities.readByteArray(dis);
            if ((int) (System.currentTimeMillis() / 1000 ) > expiration)
            {
                // the column is now expired, we can safely return a simple
                // tombstone
                ByteBuffer bytes = ByteBuffer.allocate(4);
                bytes.putInt(expiration);
                bytes.rewind();
                return new DeletedColumn(name, bytes, ts);
            }
            else
            {
                return new ExpiringColumn(name, value, ts, ttl, expiration);
            }
        }
        else
        {
            long ts = dis.readLong();
            ByteBuffer value = FBUtilities.readByteArray(dis);
            return (b & DELETION_MASK) == 0
                   ? new Column(name, value, ts)
                   : new DeletedColumn(name, value, ts);
        }
    }

    public void serialize(RangeSliceCommand sliceCommand, DataOutputStream dos) throws IOException
    {
        dos.writeUTF(sliceCommand.keyspace);
        dos.writeUTF(sliceCommand.column_family);
        dos.writeInt(sliceCommand.super_column == null ? 0 : sliceCommand.super_column.length);
        if (sliceCommand.super_column != null)
            dos.write(sliceCommand.super_column);

        TSerializer ser = new TSerializer(new TBinaryProtocol.Factory());
        FBUtilities.serialize(ser, sliceCommand.predicate, dos);
        AbstractBounds.serializer().serialize(sliceCommand.range, dos);
        dos.writeInt(sliceCommand.max_keys);
    }

+++++++++++++++++++++++
    public void serialize(RangeSliceCommand sliceCommand, DataOutputStream dos) throws IOException
    {
        dos.writeUTF(sliceCommand.keyspace);
        dos.writeUTF(sliceCommand.column_family);
        ByteBuffer sc = sliceCommand.super_column;
        dos.writeInt(sc == null ? 0 : sc.remaining());
        if (sc != null)
            dos.write(sc.array(), sc.position() + sc.arrayOffset(), sc.remaining());

        TSerializer ser = new TSerializer(new TBinaryProtocol.Factory());
        FBUtilities.serialize(ser, sliceCommand.predicate, dos);
        AbstractBounds.serializer().serialize(sliceCommand.range, dos);
        dos.writeInt(sliceCommand.max_keys);
    }

    public RangeSliceCommand deserialize(DataInputStream dis) throws IOException
    {
        String keyspace = dis.readUTF();
        String column_family = dis.readUTF();

        int scLength = dis.readInt();
        byte[] super_column = null;
        if (scLength > 0)
            super_column = readBuf(scLength, dis);

        TDeserializer dser = new TDeserializer(new TBinaryProtocol.Factory());
        SlicePredicate pred = new SlicePredicate();
        FBUtilities.deserialize(dser, pred, dis);

        AbstractBounds range = AbstractBounds.serializer().deserialize(dis);
        int max_keys = dis.readInt();
        return new RangeSliceCommand(keyspace, column_family, super_column, pred, range, max_keys);
    }

+++++++++++++++++++++++
    public RangeSliceCommand deserialize(DataInputStream dis) throws IOException
    {
        String keyspace = dis.readUTF();
        String column_family = dis.readUTF();

        int scLength = dis.readInt();
        ByteBuffer super_column = null;
        if (scLength > 0)
            super_column = ByteBuffer.wrap(readBuf(scLength, dis));

        TDeserializer dser = new TDeserializer(new TBinaryProtocol.Factory());
        SlicePredicate pred = new SlicePredicate();
        FBUtilities.deserialize(dser, pred, dis);

        AbstractBounds range = AbstractBounds.serializer().deserialize(dis);
        int max_keys = dis.readInt();
        return new RangeSliceCommand(keyspace, column_family, super_column, pred, range, max_keys);
    }

    public void serialize(Row row, DataOutputStream dos) throws IOException
    {
        dos.writeUTF(row.key);
        ColumnFamily.serializer().serialize(row.cf, dos);
    }

+++++++++++++++++++++++
    public void serialize(Row row, DataOutputStream dos) throws IOException
    {
        FBUtilities.writeShortByteArray(row.key.key, dos);
        ColumnFamily.serializer().serialize(row.cf, dos);
    }

    public Row deserialize(DataInputStream dis) throws IOException
    {
        return new Row(dis.readUTF(), ColumnFamily.serializer().deserialize(dis));
+++++++++++++++++++++++
    public Row deserialize(DataInputStream dis) throws IOException
    {
        return new Row(StorageService.getPartitioner().decorateKey(FBUtilities.readShortByteArray(dis)),
                       ColumnFamily.serializer().deserialize(dis));
    public void serialize(ReadCommand rm, DataOutputStream dos) throws IOException
    {
        SliceByNamesReadCommand realRM = (SliceByNamesReadCommand)rm;
        dos.writeBoolean(realRM.isDigestQuery());
        dos.writeUTF(realRM.table);
        dos.writeUTF(realRM.key);
        realRM.queryPath.serialize(dos);
        dos.writeInt(realRM.columnNames.size());
        if (realRM.columnNames.size() > 0)
        {
            for (byte[] cName : realRM.columnNames)
            {
                ColumnSerializer.writeName(cName, dos);
            }
        }
    }

    @Override
+++++++++++++++++++++++
    public void serialize(ReadCommand rm, DataOutputStream dos) throws IOException
    {
        SliceByNamesReadCommand realRM = (SliceByNamesReadCommand)rm;
        dos.writeBoolean(realRM.isDigestQuery());
        dos.writeUTF(realRM.table);
        FBUtilities.writeShortByteArray(realRM.key, dos);
        realRM.queryPath.serialize(dos);
        dos.writeInt(realRM.columnNames.size());
        if (realRM.columnNames.size() > 0)
        {
            for (ByteBuffer cName : realRM.columnNames)
            {
                FBUtilities.writeShortByteArray(cName, dos);
            }
        }
    }

    @Override
    public ReadCommand deserialize(DataInputStream dis) throws IOException
    {
        boolean isDigest = dis.readBoolean();
        String table = dis.readUTF();
        String key = dis.readUTF();
        QueryPath columnParent = QueryPath.deserialize(dis);

        int size = dis.readInt();
        List<byte[]> columns = new ArrayList<byte[]>();
        for (int i = 0; i < size; ++i)
        {
            columns.add(ColumnSerializer.readName(dis));
        }
        SliceByNamesReadCommand rm = new SliceByNamesReadCommand(table, key, columnParent, columns);
        rm.setDigestQuery(isDigest);
        return rm;
+++++++++++++++++++++++
    public ReadCommand deserialize(DataInputStream dis) throws IOException
    {
        boolean isDigest = dis.readBoolean();
        String table = dis.readUTF();
        ByteBuffer key = FBUtilities.readShortByteArray(dis);
        QueryPath columnParent = QueryPath.deserialize(dis);

        int size = dis.readInt();
        List<ByteBuffer> columns = new ArrayList<ByteBuffer>();
        for (int i = 0; i < size; ++i)
        {
            columns.add(FBUtilities.readShortByteArray(dis));
        }
        SliceByNamesReadCommand rm = new SliceByNamesReadCommand(table, key, columnParent, columns);
        rm.setDigestQuery(isDigest);
        return rm;
    public void serialize(ReadCommand rm, DataOutputStream dos) throws IOException
    {
        SliceFromReadCommand realRM = (SliceFromReadCommand)rm;
        dos.writeBoolean(realRM.isDigestQuery());
        dos.writeUTF(realRM.table);
        dos.writeUTF(realRM.key);
        realRM.queryPath.serialize(dos);
        ColumnSerializer.writeName(realRM.start, dos);
        ColumnSerializer.writeName(realRM.finish, dos);
        dos.writeBoolean(realRM.reversed);
        dos.writeInt(realRM.count);
    }

    @Override
+++++++++++++++++++++++
    public void serialize(ReadCommand rm, DataOutputStream dos) throws IOException
    {
        SliceFromReadCommand realRM = (SliceFromReadCommand)rm;
        dos.writeBoolean(realRM.isDigestQuery());
        dos.writeUTF(realRM.table);
        FBUtilities.writeShortByteArray(realRM.key, dos);
        realRM.queryPath.serialize(dos);
        FBUtilities.writeShortByteArray(realRM.start, dos);
        FBUtilities.writeShortByteArray(realRM.finish, dos);
        dos.writeBoolean(realRM.reversed);
        dos.writeInt(realRM.count);
    }

    @Override
    public ReadCommand deserialize(DataInputStream dis) throws IOException
    {
        boolean isDigest = dis.readBoolean();
        SliceFromReadCommand rm = new SliceFromReadCommand(dis.readUTF(),
                                                           dis.readUTF(),
                                                           QueryPath.deserialize(dis),
                                                           ColumnSerializer.readName(dis),
                                                           ColumnSerializer.readName(dis),
                                                           dis.readBoolean(), 
                                                           dis.readInt());
        rm.setDigestQuery(isDigest);
        return rm;
+++++++++++++++++++++++
    public ReadCommand deserialize(DataInputStream dis) throws IOException
    {
        boolean isDigest = dis.readBoolean();
        SliceFromReadCommand rm = new SliceFromReadCommand(dis.readUTF(),
                                                           FBUtilities.readShortByteArray(dis),
                                                           QueryPath.deserialize(dis),
                                                           FBUtilities.readShortByteArray(dis),
                                                           FBUtilities.readShortByteArray(dis),
                                                           dis.readBoolean(),
                                                           dis.readInt());
        rm.setDigestQuery(isDigest);
        return rm;
    public int serializedSize()
    {
    	/*
    	 * We need to keep the way we are calculating the column size in sync with the
    	 * way we are calculating the size for the column family serializer.
    	 */
    	return IColumn.UtfPrefix_ + name_.length + DBConstants.intSize_ + DBConstants.longSize_ + DBConstants.intSize_ + size();
    }

+++++++++++++++++++++++
    public int serializedSize()
    {
    	/*
    	 * We need to keep the way we are calculating the column size in sync with the
    	 * way we are calculating the size for the column family serializer.
    	 */
      return DBConstants.shortSize_ + name_.remaining() + DBConstants.intSize_ + DBConstants.longSize_ + DBConstants.intSize_ + size();
    }

    public void serialize(IColumn column, DataOutput dos)
    {
    	SuperColumn superColumn = (SuperColumn)column;
        ColumnSerializer.writeName(column.name(), dos);
        try
        {
            dos.writeInt(superColumn.getLocalDeletionTime());
            dos.writeLong(superColumn.getMarkedForDeleteAt());

            Collection<IColumn> columns = column.getSubColumns();
            dos.writeInt(columns.size());
            for (IColumn subColumn : columns)
            {
                Column.serializer().serialize(subColumn, dos);
            }
        }
        catch (IOException e)
        {
            throw new RuntimeException(e);
        }
    }

+++++++++++++++++++++++
    public void serialize(IColumn column, DataOutput dos)
    {
    	SuperColumn superColumn = (SuperColumn)column;
        FBUtilities.writeShortByteArray(column.name(), dos);
        try
        {
            dos.writeInt(superColumn.getLocalDeletionTime());
            dos.writeLong(superColumn.getMarkedForDeleteAt());

            Collection<IColumn> columns = column.getSubColumns();
            dos.writeInt(columns.size());
            for (IColumn subColumn : columns)
            {
                Column.serializer().serialize(subColumn, dos);
            }
        }
        catch (IOException e)
        {
            throw new RuntimeException(e);
        }
    }

    public IColumn deserialize(DataInput dis) throws IOException
    {
        byte[] name = ColumnSerializer.readName(dis);
        SuperColumn superColumn = new SuperColumn(name, comparator);
        int localDeleteTime = dis.readInt();
        if (localDeleteTime != Integer.MIN_VALUE && localDeleteTime <= 0)
        {
            throw new IOException("Invalid localDeleteTime read: " + localDeleteTime);
        }
        superColumn.markForDeleteAt(localDeleteTime, dis.readLong());

        /* read the number of columns */
        int size = dis.readInt();
        for ( int i = 0; i < size; ++i )
        {
            IColumn subColumn = Column.serializer().deserialize(dis);
            superColumn.addColumn(subColumn);
        }
        return superColumn;
+++++++++++++++++++++++
    public IColumn deserialize(DataInput dis) throws IOException
    {
        ByteBuffer name = FBUtilities.readShortByteArray(dis);
        SuperColumn superColumn = new SuperColumn(name, comparator);
        int localDeleteTime = dis.readInt();
        if (localDeleteTime != Integer.MIN_VALUE && localDeleteTime <= 0)
        {
            throw new IOException("Invalid localDeleteTime read: " + localDeleteTime);
        }
        superColumn.markForDeleteAt(localDeleteTime, dis.readLong());

        /* read the number of columns */
        int size = dis.readInt();
        for ( int i = 0; i < size; ++i )
        {
            IColumn subColumn = Column.serializer().deserialize(dis);
            superColumn.addColumn(subColumn);
        }
        return superColumn;
    public void serialize(DataOutputStream dos) throws IOException
    {
        assert !"".equals(columnFamilyName);
        assert superColumnName == null || superColumnName.length > 0;
        assert columnName == null || columnName.length > 0;
        dos.writeUTF(columnFamilyName == null ? "" : columnFamilyName);
        ColumnSerializer.writeName(superColumnName == null ? ArrayUtils.EMPTY_BYTE_ARRAY : superColumnName, dos);
        ColumnSerializer.writeName(columnName == null ? ArrayUtils.EMPTY_BYTE_ARRAY : columnName, dos);
    }

+++++++++++++++++++++++
    public void serialize(DataOutputStream dos) throws IOException
    {
        assert !"".equals(columnFamilyName);
        assert superColumnName == null || superColumnName.remaining() > 0;
        assert columnName == null || columnName.remaining() > 0;
        dos.writeUTF(columnFamilyName == null ? "" : columnFamilyName);
        FBUtilities.writeShortByteArray(superColumnName == null ? FBUtilities.EMPTY_BYTE_BUFFER : superColumnName, dos);
        FBUtilities.writeShortByteArray(columnName == null ? FBUtilities.EMPTY_BYTE_BUFFER : columnName, dos);
    }

    public static QueryPath deserialize(DataInputStream din) throws IOException
    {
        String cfName = din.readUTF();
        byte[] scName = ColumnSerializer.readName(din);
        byte[] cName = ColumnSerializer.readName(din);
        return new QueryPath(cfName.isEmpty() ? null : cfName, scName.length == 0 ? null : scName, cName.length == 0 ? null : cName);
+++++++++++++++++++++++
    public static QueryPath deserialize(DataInputStream din) throws IOException
    {
        String cfName = din.readUTF();
        ByteBuffer scName = FBUtilities.readShortByteArray(din);
        ByteBuffer cName = FBUtilities.readShortByteArray(din);
        return new QueryPath(cfName.isEmpty() ? null : cfName, 
                             scName.remaining() == 0 ? null : scName, 
                             cName.remaining() == 0 ? null : cName);
add: 176, delete: 49, change: 241, unhandled: 0 size_exceptions: 56 size_serialize: 22
-----------cassandra-0.7.0 vs cassandra-0.7.0-beta1-----------
    public static ColumnSerializer serializer()
    {
        return new ColumnSerializer();
    }

+++++++++++++++++++++++
    public static ColumnSerializer serializer(ClockType clockType)
    {
        return new ColumnSerializer(clockType);
    }

    public void serializeCFInfo(ColumnFamily columnFamily, DataOutput dos) throws IOException
    {
        dos.writeInt(columnFamily.localDeletionTime.get());
        dos.writeLong(columnFamily.markedForDeleteAt.get());
    }

+++++++++++++++++++++++
    public void serializeCFInfo(ColumnFamily columnFamily, DataOutput dos) throws IOException
    {
        dos.writeInt(columnFamily.localDeletionTime.get());
        IClock _markedForDeleteAt = columnFamily.markedForDeleteAt.get();
        columnFamily.getClockType().serializer().serialize(_markedForDeleteAt, dos);
    }

    public ColumnFamily deserializeFromSSTableNoColumns(ColumnFamily cf, DataInput input) throws IOException
    {        
        cf.delete(input.readInt(), input.readLong());
        return cf;
+++++++++++++++++++++++
    public ColumnFamily deserializeFromSSTableNoColumns(ColumnFamily cf, DataInput input) throws IOException
    {        
        cf.delete(input.readInt(), cf.getClockType().serializer().deserialize(input));
        return cf;
    }

    public void serialize(IColumn column, DataOutput dos)
    {
        assert column.name().remaining() > 0;
        FBUtilities.writeShortByteArray(column.name(), dos);
        try
        {
            if (column instanceof ExpiringColumn) {
              dos.writeByte(EXPIRATION_MASK);
              dos.writeInt(((ExpiringColumn) column).getTimeToLive());
              dos.writeInt(column.getLocalDeletionTime());
            } else {
              dos.writeByte((column.isMarkedForDelete()) ? DELETION_MASK : 0);
            }
            dos.writeLong(column.timestamp());
            FBUtilities.writeByteArray(column.value(), dos);
        }
        catch (IOException e)
        {
            throw new RuntimeException(e);
        }
    }

+++++++++++++++++++++++
    public void serialize(IColumn column, DataOutput dos)
    {
        FBUtilities.writeShortByteArray(column.name(), dos);
        try
        {
            if (column instanceof ExpiringColumn) {
              dos.writeByte(EXPIRATION_MASK);
              dos.writeInt(((ExpiringColumn) column).getTimeToLive());
              dos.writeInt(column.getLocalDeletionTime());
            } else {
              dos.writeByte((column.isMarkedForDelete()) ? DELETION_MASK : 0);
            }
            clockType.serializer().serialize(column.clock(), dos);
            FBUtilities.writeByteArray(column.value(), dos);
        }
        catch (IOException e)
        {
            throw new RuntimeException(e);
        }
    }

    public Column deserialize(DataInput dis) throws IOException
    {
        ByteBuffer name = FBUtilities.readShortByteArray(dis);
        if (name.remaining() <= 0)
            throw new CorruptColumnException("invalid column name length " + name.remaining());

        int b = dis.readUnsignedByte();
        if ((b & EXPIRATION_MASK) != 0)
        {
            int ttl = dis.readInt();
            int expiration = dis.readInt();
            long ts = dis.readLong();
            ByteBuffer value = FBUtilities.readByteArray(dis);
            if ((int) (System.currentTimeMillis() / 1000 ) > expiration)
            {
                // the column is now expired, we can safely return a simple
                // tombstone
                ByteBuffer bytes = ByteBuffer.allocate(4);
                bytes.putInt(expiration);
                bytes.rewind();
                return new DeletedColumn(name, bytes, ts);
            }
            else
            {
                return new ExpiringColumn(name, value, ts, ttl, expiration);
            }
        }
        else
        {
            long ts = dis.readLong();
            ByteBuffer value = FBUtilities.readByteArray(dis);
            return (b & DELETION_MASK) == 0
                   ? new Column(name, value, ts)
                   : new DeletedColumn(name, value, ts);
        }
    }

+++++++++++++++++++++++
    public Column deserialize(DataInput dis) throws IOException
    {
        byte[] name = FBUtilities.readShortByteArray(dis);
        int b = dis.readUnsignedByte();
        if (FBUtilities.testBitUsingBitMask(b, EXPIRATION_MASK))
        {
            int ttl = dis.readInt();
            int expiration = dis.readInt();
            IClock clock = clockType.serializer().deserialize(dis);
            byte[] value = FBUtilities.readByteArray(dis);
            if ((int) (System.currentTimeMillis() / 1000 ) > expiration)
            {
                // the column is now expired, we can safely return a simple
                // tombstone
                ByteBuffer bytes = ByteBuffer.allocate(4);
                bytes.putInt(expiration);
                return new DeletedColumn(name, bytes.array(), clock);
            }
            else
            {
                return new ExpiringColumn(name, value, clock, ttl, expiration);
            }
        }
        else
        {
            boolean delete = FBUtilities.testBitUsingBitMask(b, DELETION_MASK);
            IClock clock = clockType.serializer().deserialize(dis);
            byte[] value = FBUtilities.readByteArray(dis);
            if (FBUtilities.testBitUsingBitMask(b, DELETION_MASK)) {
                return new DeletedColumn(name, value, clock);
            } else {
                return new Column(name, value, clock);
            }
        }
    public void serialize(RangeSliceCommand sliceCommand, DataOutputStream dos) throws IOException
    {
        dos.writeUTF(sliceCommand.keyspace);
        dos.writeUTF(sliceCommand.column_family);
        ByteBuffer sc = sliceCommand.super_column;
        dos.writeInt(sc == null ? 0 : sc.remaining());
        if (sc != null)
            dos.write(sc.array(), sc.position() + sc.arrayOffset(), sc.remaining());

        TSerializer ser = new TSerializer(new TBinaryProtocol.Factory());
        FBUtilities.serialize(ser, sliceCommand.predicate, dos);
        AbstractBounds.serializer().serialize(sliceCommand.range, dos);
        dos.writeInt(sliceCommand.max_keys);
    }

+++++++++++++++++++++++
    public void serialize(RangeSliceCommand sliceCommand, DataOutputStream dos) throws IOException
    {
        dos.writeUTF(sliceCommand.keyspace);
        dos.writeUTF(sliceCommand.column_family);
        dos.writeInt(sliceCommand.super_column == null ? 0 : sliceCommand.super_column.length);
        if (sliceCommand.super_column != null)
            dos.write(sliceCommand.super_column);

        TSerializer ser = new TSerializer(new TBinaryProtocol.Factory());
        FBUtilities.serialize(ser, sliceCommand.predicate, dos);
        AbstractBounds.serializer().serialize(sliceCommand.range, dos);
        dos.writeInt(sliceCommand.max_keys);
    }

    public RangeSliceCommand deserialize(DataInputStream dis) throws IOException
    {
        String keyspace = dis.readUTF();
        String column_family = dis.readUTF();

        int scLength = dis.readInt();
        ByteBuffer super_column = null;
        if (scLength > 0)
            super_column = ByteBuffer.wrap(readBuf(scLength, dis));

        TDeserializer dser = new TDeserializer(new TBinaryProtocol.Factory());
        SlicePredicate pred = new SlicePredicate();
        FBUtilities.deserialize(dser, pred, dis);

        AbstractBounds range = AbstractBounds.serializer().deserialize(dis);
        int max_keys = dis.readInt();
        return new RangeSliceCommand(keyspace, column_family, super_column, pred, range, max_keys);
    }

+++++++++++++++++++++++
    public RangeSliceCommand deserialize(DataInputStream dis) throws IOException
    {
        String keyspace = dis.readUTF();
        String column_family = dis.readUTF();

        int scLength = dis.readInt();
        byte[] super_column = null;
        if (scLength > 0)
            super_column = readBuf(scLength, dis);

        TDeserializer dser = new TDeserializer(new TBinaryProtocol.Factory());
        SlicePredicate pred = new SlicePredicate();
        FBUtilities.deserialize(dser, pred, dis);

        AbstractBounds range = AbstractBounds.serializer().deserialize(dis);
        int max_keys = dis.readInt();
        return new RangeSliceCommand(keyspace, column_family, super_column, pred, range, max_keys);
    }

    public void serialize(ReadCommand rm, DataOutputStream dos) throws IOException
    {
        SliceByNamesReadCommand realRM = (SliceByNamesReadCommand)rm;
        dos.writeBoolean(realRM.isDigestQuery());
        dos.writeUTF(realRM.table);
        FBUtilities.writeShortByteArray(realRM.key, dos);
        realRM.queryPath.serialize(dos);
        dos.writeInt(realRM.columnNames.size());
        if (realRM.columnNames.size() > 0)
        {
            for (ByteBuffer cName : realRM.columnNames)
            {
                FBUtilities.writeShortByteArray(cName, dos);
            }
        }
    }

    @Override
+++++++++++++++++++++++
    public void serialize(ReadCommand rm, DataOutputStream dos) throws IOException
    {
        SliceByNamesReadCommand realRM = (SliceByNamesReadCommand)rm;
        dos.writeBoolean(realRM.isDigestQuery());
        dos.writeUTF(realRM.table);
        FBUtilities.writeShortByteArray(realRM.key, dos);
        realRM.queryPath.serialize(dos);
        dos.writeInt(realRM.columnNames.size());
        if (realRM.columnNames.size() > 0)
        {
            for (byte[] cName : realRM.columnNames)
            {
                FBUtilities.writeShortByteArray(cName, dos);
            }
        }
    }

    @Override
    public ReadCommand deserialize(DataInputStream dis) throws IOException
    {
        boolean isDigest = dis.readBoolean();
        String table = dis.readUTF();
        ByteBuffer key = FBUtilities.readShortByteArray(dis);
        QueryPath columnParent = QueryPath.deserialize(dis);

        int size = dis.readInt();
        List<ByteBuffer> columns = new ArrayList<ByteBuffer>();
        for (int i = 0; i < size; ++i)
        {
            columns.add(FBUtilities.readShortByteArray(dis));
        }
        SliceByNamesReadCommand rm = new SliceByNamesReadCommand(table, key, columnParent, columns);
        rm.setDigestQuery(isDigest);
        return rm;
+++++++++++++++++++++++
    public ReadCommand deserialize(DataInputStream dis) throws IOException
    {
        boolean isDigest = dis.readBoolean();
        String table = dis.readUTF();
        byte[] key = FBUtilities.readShortByteArray(dis);
        QueryPath columnParent = QueryPath.deserialize(dis);

        int size = dis.readInt();
        List<byte[]> columns = new ArrayList<byte[]>();
        for (int i = 0; i < size; ++i)
        {
            columns.add(FBUtilities.readShortByteArray(dis));
        }
        SliceByNamesReadCommand rm = new SliceByNamesReadCommand(table, key, columnParent, columns);
        rm.setDigestQuery(isDigest);
        return rm;
    public void serialize(ReadCommand rm, DataOutputStream dos) throws IOException
    {
        SliceFromReadCommand realRM = (SliceFromReadCommand)rm;
        dos.writeBoolean(realRM.isDigestQuery());
        dos.writeUTF(realRM.table);
        FBUtilities.writeShortByteArray(realRM.key, dos);
        realRM.queryPath.serialize(dos);
        FBUtilities.writeShortByteArray(realRM.start, dos);
        FBUtilities.writeShortByteArray(realRM.finish, dos);
        dos.writeBoolean(realRM.reversed);
        dos.writeInt(realRM.count);
    }

    @Override
+++++++++++++++++++++++
    public void serialize(ReadCommand rm, DataOutputStream dos) throws IOException
    {
        SliceFromReadCommand realRM = (SliceFromReadCommand)rm;
        dos.writeBoolean(realRM.isDigestQuery());
        dos.writeUTF(realRM.table);
        FBUtilities.writeShortByteArray(realRM.key, dos);
        realRM.queryPath.serialize(dos);
        FBUtilities.writeShortByteArray(realRM.start, dos);
        FBUtilities.writeShortByteArray(realRM.finish, dos);
        ByteArrayListSerializer.serialize(realRM.bitmasks, dos);
        dos.writeBoolean(realRM.reversed);
        dos.writeInt(realRM.count);
    }

    @Override
    public ReadCommand deserialize(DataInputStream dis) throws IOException
    {
        boolean isDigest = dis.readBoolean();
        SliceFromReadCommand rm = new SliceFromReadCommand(dis.readUTF(),
                                                           FBUtilities.readShortByteArray(dis),
                                                           QueryPath.deserialize(dis),
                                                           FBUtilities.readShortByteArray(dis),
                                                           FBUtilities.readShortByteArray(dis),
                                                           dis.readBoolean(),
                                                           dis.readInt());
        rm.setDigestQuery(isDigest);
        return rm;
+++++++++++++++++++++++
    public ReadCommand deserialize(DataInputStream dis) throws IOException
    {
        boolean isDigest = dis.readBoolean();
        SliceFromReadCommand rm = new SliceFromReadCommand(dis.readUTF(),
                                                           FBUtilities.readShortByteArray(dis),
                                                           QueryPath.deserialize(dis),
                                                           FBUtilities.readShortByteArray(dis),
                                                           FBUtilities.readShortByteArray(dis),
                                                           ByteArrayListSerializer.deserialize(dis),
                                                           dis.readBoolean(), 
                                                           dis.readInt());
        rm.setDigestQuery(isDigest);
        return rm;
    public static SuperColumnSerializer serializer(AbstractType comparator)
    {
        return new SuperColumnSerializer(comparator);
    }

+++++++++++++++++++++++
    public static SuperColumnSerializer serializer(AbstractType comparator, ClockType clockType, AbstractReconciler reconciler)
    {
        return new SuperColumnSerializer(comparator, clockType, reconciler);
    }

    public int serializedSize()
    {
    	/*
    	 * We need to keep the way we are calculating the column size in sync with the
    	 * way we are calculating the size for the column family serializer.
    	 */
      return DBConstants.shortSize_ + name_.remaining() + DBConstants.intSize_ + DBConstants.longSize_ + DBConstants.intSize_ + size();
    }

+++++++++++++++++++++++
    public int serializedSize()
    {
    	/*
    	 * We need to keep the way we are calculating the column size in sync with the
    	 * way we are calculating the size for the column family serializer.
    	 */
      IClock _markedForDeleteAt = markedForDeleteAt.get();
      return DBConstants.shortSize_ + name_.length + DBConstants.intSize_ + _markedForDeleteAt.size() + DBConstants.intSize_ + size();
    }

    public void serialize(IColumn column, DataOutput dos)
    {
    	SuperColumn superColumn = (SuperColumn)column;
        FBUtilities.writeShortByteArray(column.name(), dos);
        try
        {
            dos.writeInt(superColumn.getLocalDeletionTime());
            dos.writeLong(superColumn.getMarkedForDeleteAt());

            Collection<IColumn> columns = column.getSubColumns();
            dos.writeInt(columns.size());
            for (IColumn subColumn : columns)
            {
                Column.serializer().serialize(subColumn, dos);
            }
        }
        catch (IOException e)
        {
            throw new RuntimeException(e);
        }
    }

+++++++++++++++++++++++
    public void serialize(IColumn column, DataOutput dos)
    {
    	SuperColumn superColumn = (SuperColumn)column;
        FBUtilities.writeShortByteArray(column.name(), dos);
        try
        {
            dos.writeInt(superColumn.getLocalDeletionTime());
            IClock _markedForDeleteAt = superColumn.getMarkedForDeleteAt();
            clockType.serializer().serialize(_markedForDeleteAt, dos);

            Collection<IColumn> columns = column.getSubColumns();
            dos.writeInt(columns.size());
            for (IColumn subColumn : columns)
            {
                Column.serializer(clockType).serialize(subColumn, dos);
            }
        }
        catch (IOException e)
        {
            throw new RuntimeException(e);
        }
    }

    public IColumn deserialize(DataInput dis) throws IOException
    {
        ByteBuffer name = FBUtilities.readShortByteArray(dis);
        SuperColumn superColumn = new SuperColumn(name, comparator);
        int localDeleteTime = dis.readInt();
        if (localDeleteTime != Integer.MIN_VALUE && localDeleteTime <= 0)
        {
            throw new IOException("Invalid localDeleteTime read: " + localDeleteTime);
        }
        superColumn.markForDeleteAt(localDeleteTime, dis.readLong());

        /* read the number of columns */
        int size = dis.readInt();
        for ( int i = 0; i < size; ++i )
        {
            IColumn subColumn = Column.serializer().deserialize(dis);
            superColumn.addColumn(subColumn);
        }
        return superColumn;
+++++++++++++++++++++++
    public IColumn deserialize(DataInput dis) throws IOException
    {
        byte[] name = FBUtilities.readShortByteArray(dis);
        SuperColumn superColumn = new SuperColumn(name, comparator, clockType, reconciler);
        int localDeleteTime = dis.readInt();
        if (localDeleteTime != Integer.MIN_VALUE && localDeleteTime <= 0)
        {
            throw new IOException("Invalid localDeleteTime read: " + localDeleteTime);
        }
        superColumn.markForDeleteAt(localDeleteTime, clockType.serializer().deserialize(dis));

        /* read the number of columns */
        int size = dis.readInt();
        for ( int i = 0; i < size; ++i )
        {
            IColumn subColumn = Column.serializer(clockType).deserialize(dis);
            superColumn.addColumn(subColumn);
        }
        return superColumn;
    public void serialize(DataOutputStream dos) throws IOException
    {
        assert !"".equals(columnFamilyName);
        assert superColumnName == null || superColumnName.remaining() > 0;
        assert columnName == null || columnName.remaining() > 0;
        dos.writeUTF(columnFamilyName == null ? "" : columnFamilyName);
        FBUtilities.writeShortByteArray(superColumnName == null ? FBUtilities.EMPTY_BYTE_BUFFER : superColumnName, dos);
        FBUtilities.writeShortByteArray(columnName == null ? FBUtilities.EMPTY_BYTE_BUFFER : columnName, dos);
    }

+++++++++++++++++++++++
    public void serialize(DataOutputStream dos) throws IOException
    {
        assert !"".equals(columnFamilyName);
        assert superColumnName == null || superColumnName.length > 0;
        assert columnName == null || columnName.length > 0;
        dos.writeUTF(columnFamilyName == null ? "" : columnFamilyName);
        FBUtilities.writeShortByteArray(superColumnName == null ? ArrayUtils.EMPTY_BYTE_ARRAY : superColumnName, dos);
        FBUtilities.writeShortByteArray(columnName == null ? ArrayUtils.EMPTY_BYTE_ARRAY : columnName, dos);
    }

    public static QueryPath deserialize(DataInputStream din) throws IOException
    {
        String cfName = din.readUTF();
        ByteBuffer scName = FBUtilities.readShortByteArray(din);
        ByteBuffer cName = FBUtilities.readShortByteArray(din);
        return new QueryPath(cfName.isEmpty() ? null : cfName, 
                             scName.remaining() == 0 ? null : scName, 
                             cName.remaining() == 0 ? null : cName);
+++++++++++++++++++++++
    public static QueryPath deserialize(DataInputStream din) throws IOException
    {
        String cfName = din.readUTF();
        byte[] scName = FBUtilities.readShortByteArray(din);
        byte[] cName = FBUtilities.readShortByteArray(din);
        return new QueryPath(cfName.isEmpty() ? null : cfName, scName.length == 0 ? null : scName, cName.length == 0 ? null : cName);
    public ByteBuffer serialize() throws IOException
    {
        // super deflate
        org.apache.cassandra.db.migration.avro.Migration mi = new org.apache.cassandra.db.migration.avro.Migration();
        mi.old_version = new org.apache.cassandra.utils.avro.UUID();
        mi.old_version.bytes(UUIDGen.decompose(lastVersion));
        mi.new_version = new org.apache.cassandra.utils.avro.UUID();
        mi.new_version.bytes(UUIDGen.decompose(newVersion));
        mi.classname = new org.apache.avro.util.Utf8(this.getClass().getName());
        // TODO: Avro RowMutation serialization?
        DataOutputBuffer dob = new DataOutputBuffer();
        try
        {
            RowMutation.serializer().serialize(rm, dob);
        }
        catch (IOException e)
        {
            throw new RuntimeException(e);
        }
        mi.row_mutation = ByteBuffer.wrap(dob.asByteArray());

        // sub deflate
        this.subdeflate(mi);

        // serialize
        return SerDeUtils.serializeWithSchema(mi);
    }

+++++++++++++++++++++++
    public byte[] serialize() throws IOException
    {
        // super deflate
        org.apache.cassandra.db.migration.avro.Migration mi = new org.apache.cassandra.db.migration.avro.Migration();
        mi.old_version = new org.apache.cassandra.utils.avro.UUID();
        mi.old_version.bytes(UUIDGen.decompose(lastVersion));
        mi.new_version = new org.apache.cassandra.utils.avro.UUID();
        mi.new_version.bytes(UUIDGen.decompose(newVersion));
        mi.classname = new org.apache.avro.util.Utf8(this.getClass().getName());
        // TODO: Avro RowMutation serialization?
        DataOutputBuffer dob = new DataOutputBuffer();
        try
        {
            RowMutation.serializer().serialize(rm, dob);
        }
        catch (IOException e)
        {
            throw new RuntimeException(e);
        }
        mi.row_mutation = ByteBuffer.wrap(dob.asByteArray());

        // sub deflate
        this.subdeflate(mi);

        // serialize
        return SerDeUtils.serializeWithSchema(mi);
    }

    public static Migration deserialize(ByteBuffer bytes) throws IOException
    {
        // deserialize
        org.apache.cassandra.db.migration.avro.Migration mi = SerDeUtils.deserializeWithSchema(bytes, new org.apache.cassandra.db.migration.avro.Migration());

        // create an instance of the migration subclass
        Migration migration;
        try
        {
            Class migrationClass = Class.forName(mi.classname.toString());
            Constructor migrationConstructor = migrationClass.getDeclaredConstructor();
            migrationConstructor.setAccessible(true);
            migration = (Migration)migrationConstructor.newInstance();
        }
        catch (Exception e)
        {
            throw new RuntimeException("Invalid migration class: " + mi.classname.toString(), e);
        }
        
        // super inflate
        migration.lastVersion = UUIDGen.getUUID(ByteBuffer.wrap(mi.old_version.bytes()));
        migration.newVersion = UUIDGen.getUUID(ByteBuffer.wrap(mi.new_version.bytes()));
        try
        {
            migration.rm = RowMutation.serializer().deserialize(SerDeUtils.createDataInputStream(mi.row_mutation));
        }
        catch (IOException e)
        {
            throw new RuntimeException(e);
        }
        // sub inflate
        migration.subinflate(mi);
        return migration;
    }
    
    /** load serialized migrations. */
+++++++++++++++++++++++
    public static Migration deserialize(byte[] bytes) throws IOException
    {
        // deserialize
        org.apache.cassandra.db.migration.avro.Migration mi = SerDeUtils.deserializeWithSchema(bytes, new org.apache.cassandra.db.migration.avro.Migration());

        // create an instance of the migration subclass
        Migration migration;
        try
        {
            Class migrationClass = Class.forName(mi.classname.toString());
            Constructor migrationConstructor = migrationClass.getDeclaredConstructor();
            migrationConstructor.setAccessible(true);
            migration = (Migration)migrationConstructor.newInstance();
        }
        catch (Exception e)
        {
            throw new RuntimeException("Invalid migration class: " + mi.classname.toString(), e);
        }
        
        // super inflate
        migration.lastVersion = UUIDGen.makeType1UUID(mi.old_version.bytes());
        migration.newVersion = UUIDGen.makeType1UUID(mi.new_version.bytes());
        try
        {
            migration.rm = RowMutation.serializer().deserialize(SerDeUtils.createDataInputStream(mi.row_mutation));
        }
        catch (IOException e)
        {
            throw new RuntimeException(e);
        }
        // sub inflate
        migration.subinflate(mi);
        return migration;
    }
    
    /** load serialized migrations. */
    public void serialize(EndpointState epState, DataOutputStream dos) throws IOException
    {
        /* serialize the HeartBeatState */
        HeartBeatState hbState = epState.getHeartBeatState();
        HeartBeatState.serializer().serialize(hbState, dos);

        /* serialize the map of ApplicationState objects */
        int size = epState.applicationState_.size();
        dos.writeInt(size);
        for (Map.Entry<ApplicationState, VersionedValue> entry : epState.applicationState_.entrySet())
        {
            VersionedValue value = entry.getValue();
            if (value != null)
            {
                dos.writeInt(entry.getKey().ordinal());
                VersionedValue.serializer.serialize(value, dos);
            }
        }
    }

+++++++++++++++++++++++
    public void serialize(EndpointState epState, DataOutputStream dos) throws IOException
    {
        /* serialize the HeartBeatState */
        HeartBeatState hbState = epState.getHeartBeatState();
        HeartBeatState.serializer().serialize(hbState, dos);

        /* serialize the map of ApplicationState objects */
        int size = epState.applicationState_.size();
        dos.writeInt(size);
        for (String key : epState.applicationState_.keySet())
        {
            ApplicationState appState = epState.applicationState_.get(key);
            if (appState != null)
            {
                dos.writeUTF(key);
                ApplicationState.serializer().serialize(appState, dos);
            }
        }
    }

    public EndpointState deserialize(DataInputStream dis) throws IOException
    {
        HeartBeatState hbState = HeartBeatState.serializer().deserialize(dis);
        EndpointState epState = new EndpointState(hbState);

        int appStateSize = dis.readInt();
        for ( int i = 0; i < appStateSize; ++i )
        {
            if ( dis.available() == 0 )
            {
                break;
            }

            int key = dis.readInt();
            VersionedValue value = VersionedValue.serializer.deserialize(dis);
            epState.addApplicationState(Gossiper.STATES[key], value);
        }
        return epState;
+++++++++++++++++++++++
    public EndpointState deserialize(DataInputStream dis) throws IOException
    {
        HeartBeatState hbState = HeartBeatState.serializer().deserialize(dis);
        EndpointState epState = new EndpointState(hbState);

        int appStateSize = dis.readInt();
        for ( int i = 0; i < appStateSize; ++i )
        {
            if ( dis.available() == 0 )
            {
                break;
            }

            String key = dis.readUTF();
            ApplicationState appState = ApplicationState.serializer().deserialize(dis);            
            epState.addApplicationState(key, appState);            
        }
        return epState;
    public static <T extends SpecificRecord> ByteBuffer serializeWithSchema(T o) throws IOException
    {
        OutputBuffer buff = new OutputBuffer();
        BinaryEncoder enc = new BinaryEncoder(buff);
        enc.writeString(new Utf8(o.getSchema().toString()));
        SpecificDatumWriter<T> writer = new SpecificDatumWriter<T>(o.getSchema());
        writer.write(o, enc);
        enc.flush();
        return ByteBuffer.wrap(buff.asByteArray());
    }

    /**
     * @return a DataInputStream wrapping the given buffer.
     */
+++++++++++++++++++++++
    public static <T extends SpecificRecord> byte[] serializeWithSchema(T o) throws IOException
    {
        OutputBuffer buff = new OutputBuffer();
        BinaryEncoder enc = new BinaryEncoder(buff);
        enc.writeString(new Utf8(o.getSchema().toString()));
        SpecificDatumWriter<T> writer = new SpecificDatumWriter<T>(o.getSchema());
        writer.write(o, enc);
        enc.flush();
        return buff.asByteArray();
    }

    /**
     * @return a DataInputStream wrapping the given buffer.
     */
    public static <T extends SpecificRecord> ByteBuffer serialize(T o) throws IOException
    {
        OutputBuffer buff = new OutputBuffer();
        BinaryEncoder enc = new BinaryEncoder(buff);
        SpecificDatumWriter<T> writer = new SpecificDatumWriter<T>(o.getSchema());
        writer.write(o, enc);
        enc.flush();
        return ByteBuffer.wrap(buff.asByteArray());
    }

	/**
     * Deserializes a single object as stored along with its Schema by serialize(T). NB: See warnings on serialize(T).
     * @param ob An empty object to deserialize into (must not be null).
     * @param bytes Array to deserialize from
     * @throws IOException
     */
+++++++++++++++++++++++
    public static <T extends SpecificRecord> byte[] serialize(T o) throws IOException
    {
        OutputBuffer buff = new OutputBuffer();
        BinaryEncoder enc = new BinaryEncoder(buff);
        SpecificDatumWriter<T> writer = new SpecificDatumWriter<T>(o.getSchema());
        writer.write(o, enc);
        enc.flush();
        return buff.asByteArray();
    }

	/**
     * Deserializes a single object as stored along with its Schema by serialize(T). NB: See warnings on serialize(T).
     * @param ob An empty object to deserialize into (must not be null).
     * @param bytes Array to deserialize from
     * @throws IOException
     */
    public static <T extends SpecificRecord> T deserialize(Schema writer, ByteBuffer bytes, T ob) throws IOException
    {
        BinaryDecoder dec = DIRECT_DECODERS.createBinaryDecoder(bytes.array(),bytes.position()+bytes.arrayOffset(),bytes.remaining(), null);
        SpecificDatumReader<T> reader = new SpecificDatumReader<T>(writer);
        reader.setExpected(ob.getSchema());
        return reader.read(ob, dec);
    }

	/**
     * Serializes a single object.
     * @param o Object to serialize
     */
+++++++++++++++++++++++
    public static <T extends SpecificRecord> T deserialize(Schema writer, byte[] bytes, T ob) throws IOException
    {
        BinaryDecoder dec = DIRECT_DECODERS.createBinaryDecoder(bytes, null);
        SpecificDatumReader<T> reader = new SpecificDatumReader<T>(writer);
        reader.setExpected(ob.getSchema());
        return reader.read(ob, dec);
    }

	/**
     * Serializes a single object.
     * @param o Object to serialize
     */
    public static <T extends SpecificRecord> T deserializeWithSchema(ByteBuffer bytes, T ob) throws IOException
    {
        BinaryDecoder dec = DIRECT_DECODERS.createBinaryDecoder(bytes.array(),bytes.position()+bytes.arrayOffset(), bytes.remaining(), null);
        Schema writer = Schema.parse(dec.readString(new Utf8()).toString());
        SpecificDatumReader<T> reader = new SpecificDatumReader<T>(writer);
        reader.setExpected(ob.getSchema());
        return reader.read(ob, dec);
    }

	/**
     * Serializes a single object along with its Schema. NB: For performance critical areas, it is <b>much</b>
     * more efficient to store the Schema independently.
     * @param o Object to serialize
     */
+++++++++++++++++++++++
    public static <T extends SpecificRecord> T deserializeWithSchema(byte[] bytes, T ob) throws IOException
    {
        BinaryDecoder dec = DIRECT_DECODERS.createBinaryDecoder(bytes, null);
        Schema writer = Schema.parse(dec.readString(new Utf8()).toString());
        SpecificDatumReader<T> reader = new SpecificDatumReader<T>(writer);
        reader.setExpected(ob.getSchema());
        return new SpecificDatumReader<T>(writer).read(ob, dec);
    }

	/**
     * Serializes a single object along with its Schema. NB: For performance critical areas, it is <b>much</b>
     * more efficient to store the Schema independently.
     * @param o Object to serialize
     */
add: 34, delete: 52, change: 351, unhandled: 0 size_exceptions: 34 size_serialize: 25
-----------cassandra-0.7.0-beta1 vs cassandra-0.7.0-beta2-----------
    public Column deserialize(DataInput dis) throws IOException
    {
        byte[] name = FBUtilities.readShortByteArray(dis);
        int b = dis.readUnsignedByte();
        if (FBUtilities.testBitUsingBitMask(b, EXPIRATION_MASK))
        {
            int ttl = dis.readInt();
            int expiration = dis.readInt();
            IClock clock = clockType.serializer().deserialize(dis);
            byte[] value = FBUtilities.readByteArray(dis);
            if ((int) (System.currentTimeMillis() / 1000 ) > expiration)
            {
                // the column is now expired, we can safely return a simple
                // tombstone
                ByteBuffer bytes = ByteBuffer.allocate(4);
                bytes.putInt(expiration);
                return new DeletedColumn(name, bytes.array(), clock);
            }
            else
            {
                return new ExpiringColumn(name, value, clock, ttl, expiration);
            }
        }
        else
        {
            boolean delete = FBUtilities.testBitUsingBitMask(b, DELETION_MASK);
            IClock clock = clockType.serializer().deserialize(dis);
            byte[] value = FBUtilities.readByteArray(dis);
            if (FBUtilities.testBitUsingBitMask(b, DELETION_MASK)) {
                return new DeletedColumn(name, value, clock);
            } else {
                return new Column(name, value, clock);
            }
        }
+++++++++++++++++++++++
    public Column deserialize(DataInput dis) throws IOException
    {
        byte[] name = FBUtilities.readShortByteArray(dis);
        int b = dis.readUnsignedByte();
        if ((b & EXPIRATION_MASK) != 0)
        {
            int ttl = dis.readInt();
            int expiration = dis.readInt();
            IClock clock = clockType.serializer().deserialize(dis);
            byte[] value = FBUtilities.readByteArray(dis);
            if ((int) (System.currentTimeMillis() / 1000 ) > expiration)
            {
                // the column is now expired, we can safely return a simple
                // tombstone
                ByteBuffer bytes = ByteBuffer.allocate(4);
                bytes.putInt(expiration);
                return new DeletedColumn(name, bytes.array(), clock);
            }
            else
            {
                return new ExpiringColumn(name, value, clock, ttl, expiration);
            }
        }
        else
        {
            boolean delete = (b & DELETION_MASK) != 0;
            IClock clock = clockType.serializer().deserialize(dis);
            byte[] value = FBUtilities.readByteArray(dis);
            if ((b & DELETION_MASK) != 0) {
                return new DeletedColumn(name, value, clock);
            } else {
                return new Column(name, value, clock);
            }
        }
    public void serialize(ReadCommand rm, DataOutputStream dos) throws IOException
    {
        SliceFromReadCommand realRM = (SliceFromReadCommand)rm;
        dos.writeBoolean(realRM.isDigestQuery());
        dos.writeUTF(realRM.table);
        FBUtilities.writeShortByteArray(realRM.key, dos);
        realRM.queryPath.serialize(dos);
        FBUtilities.writeShortByteArray(realRM.start, dos);
        FBUtilities.writeShortByteArray(realRM.finish, dos);
        ByteArrayListSerializer.serialize(realRM.bitmasks, dos);
        dos.writeBoolean(realRM.reversed);
        dos.writeInt(realRM.count);
    }

    @Override
+++++++++++++++++++++++
    public void serialize(ReadCommand rm, DataOutputStream dos) throws IOException
    {
        SliceFromReadCommand realRM = (SliceFromReadCommand)rm;
        dos.writeBoolean(realRM.isDigestQuery());
        dos.writeUTF(realRM.table);
        FBUtilities.writeShortByteArray(realRM.key, dos);
        realRM.queryPath.serialize(dos);
        FBUtilities.writeShortByteArray(realRM.start, dos);
        FBUtilities.writeShortByteArray(realRM.finish, dos);
        dos.writeBoolean(realRM.reversed);
        dos.writeInt(realRM.count);
    }

    @Override
    public ReadCommand deserialize(DataInputStream dis) throws IOException
    {
        boolean isDigest = dis.readBoolean();
        SliceFromReadCommand rm = new SliceFromReadCommand(dis.readUTF(),
                                                           FBUtilities.readShortByteArray(dis),
                                                           QueryPath.deserialize(dis),
                                                           FBUtilities.readShortByteArray(dis),
                                                           FBUtilities.readShortByteArray(dis),
                                                           ByteArrayListSerializer.deserialize(dis),
                                                           dis.readBoolean(), 
                                                           dis.readInt());
        rm.setDigestQuery(isDigest);
        return rm;
+++++++++++++++++++++++
    public ReadCommand deserialize(DataInputStream dis) throws IOException
    {
        boolean isDigest = dis.readBoolean();
        SliceFromReadCommand rm = new SliceFromReadCommand(dis.readUTF(),
                                                           FBUtilities.readShortByteArray(dis),
                                                           QueryPath.deserialize(dis),
                                                           FBUtilities.readShortByteArray(dis),
                                                           FBUtilities.readShortByteArray(dis),
                                                           dis.readBoolean(),
                                                           dis.readInt());
        rm.setDigestQuery(isDigest);
        return rm;
    public void serialize(EndpointState epState, DataOutputStream dos) throws IOException
    {
        /* serialize the HeartBeatState */
        HeartBeatState hbState = epState.getHeartBeatState();
        HeartBeatState.serializer().serialize(hbState, dos);

        /* serialize the map of ApplicationState objects */
        int size = epState.applicationState_.size();
        dos.writeInt(size);
        for (String key : epState.applicationState_.keySet())
        {
            ApplicationState appState = epState.applicationState_.get(key);
            if (appState != null)
            {
                dos.writeUTF(key);
                ApplicationState.serializer().serialize(appState, dos);
            }
        }
    }

+++++++++++++++++++++++
    public void serialize(EndpointState epState, DataOutputStream dos) throws IOException
    {
        /* serialize the HeartBeatState */
        HeartBeatState hbState = epState.getHeartBeatState();
        HeartBeatState.serializer().serialize(hbState, dos);

        /* serialize the map of ApplicationState objects */
        int size = epState.applicationState_.size();
        dos.writeInt(size);
        for (Map.Entry<ApplicationState, VersionedValue> entry : epState.applicationState_.entrySet())
        {
            VersionedValue value = entry.getValue();
            if (value != null)
            {
                dos.writeInt(entry.getKey().ordinal());
                VersionedValue.serializer.serialize(value, dos);
            }
        }
    }

    public EndpointState deserialize(DataInputStream dis) throws IOException
    {
        HeartBeatState hbState = HeartBeatState.serializer().deserialize(dis);
        EndpointState epState = new EndpointState(hbState);

        int appStateSize = dis.readInt();
        for ( int i = 0; i < appStateSize; ++i )
        {
            if ( dis.available() == 0 )
            {
                break;
            }

            String key = dis.readUTF();
            ApplicationState appState = ApplicationState.serializer().deserialize(dis);            
            epState.addApplicationState(key, appState);            
        }
        return epState;
+++++++++++++++++++++++
    public EndpointState deserialize(DataInputStream dis) throws IOException
    {
        HeartBeatState hbState = HeartBeatState.serializer().deserialize(dis);
        EndpointState epState = new EndpointState(hbState);

        int appStateSize = dis.readInt();
        for ( int i = 0; i < appStateSize; ++i )
        {
            if ( dis.available() == 0 )
            {
                break;
            }

            int key = dis.readInt();
            VersionedValue value = VersionedValue.serializer.deserialize(dis);
            epState.addApplicationState(Gossiper.STATES[key], value);
        }
        return epState;
add: 38, delete: 25, change: 211, unhandled: 0 size_exceptions: 9 size_serialize: 5
-----------cassandra-0.7.0-beta2 vs cassandra-0.7.0-beta3-----------
    public static ColumnSerializer serializer(ClockType clockType)
    {
        return new ColumnSerializer(clockType);
    }

+++++++++++++++++++++++
    public static ColumnSerializer serializer()
    {
        return new ColumnSerializer();
    }

    public void serializeCFInfo(ColumnFamily columnFamily, DataOutput dos) throws IOException
    {
        dos.writeInt(columnFamily.localDeletionTime.get());
        IClock _markedForDeleteAt = columnFamily.markedForDeleteAt.get();
        columnFamily.getClockType().serializer().serialize(_markedForDeleteAt, dos);
    }

+++++++++++++++++++++++
    public void serializeCFInfo(ColumnFamily columnFamily, DataOutput dos) throws IOException
    {
        dos.writeInt(columnFamily.localDeletionTime.get());
        dos.writeLong(columnFamily.markedForDeleteAt.get());
    }

    public ColumnFamily deserializeFromSSTableNoColumns(ColumnFamily cf, DataInput input) throws IOException
    {        
        cf.delete(input.readInt(), cf.getClockType().serializer().deserialize(input));
        return cf;
+++++++++++++++++++++++
    public ColumnFamily deserializeFromSSTableNoColumns(ColumnFamily cf, DataInput input) throws IOException
    {        
        cf.delete(input.readInt(), input.readLong());
        return cf;
    public void serialize(IColumn column, DataOutput dos)
    {
        FBUtilities.writeShortByteArray(column.name(), dos);
        try
        {
            if (column instanceof ExpiringColumn) {
              dos.writeByte(EXPIRATION_MASK);
              dos.writeInt(((ExpiringColumn) column).getTimeToLive());
              dos.writeInt(column.getLocalDeletionTime());
            } else {
              dos.writeByte((column.isMarkedForDelete()) ? DELETION_MASK : 0);
            }
            clockType.serializer().serialize(column.clock(), dos);
            FBUtilities.writeByteArray(column.value(), dos);
        }
        catch (IOException e)
        {
            throw new RuntimeException(e);
        }
    }

+++++++++++++++++++++++
    public void serialize(IColumn column, DataOutput dos)
    {
        FBUtilities.writeShortByteArray(column.name(), dos);
        try
        {
            if (column instanceof ExpiringColumn) {
              dos.writeByte(EXPIRATION_MASK);
              dos.writeInt(((ExpiringColumn) column).getTimeToLive());
              dos.writeInt(column.getLocalDeletionTime());
            } else {
              dos.writeByte((column.isMarkedForDelete()) ? DELETION_MASK : 0);
            }
            dos.writeLong(column.timestamp());
            FBUtilities.writeByteArray(column.value(), dos);
        }
        catch (IOException e)
        {
            throw new RuntimeException(e);
        }
    }

    public Column deserialize(DataInput dis) throws IOException
    {
        byte[] name = FBUtilities.readShortByteArray(dis);
        int b = dis.readUnsignedByte();
        if ((b & EXPIRATION_MASK) != 0)
        {
            int ttl = dis.readInt();
            int expiration = dis.readInt();
            IClock clock = clockType.serializer().deserialize(dis);
            byte[] value = FBUtilities.readByteArray(dis);
            if ((int) (System.currentTimeMillis() / 1000 ) > expiration)
            {
                // the column is now expired, we can safely return a simple
                // tombstone
                ByteBuffer bytes = ByteBuffer.allocate(4);
                bytes.putInt(expiration);
                return new DeletedColumn(name, bytes.array(), clock);
            }
            else
            {
                return new ExpiringColumn(name, value, clock, ttl, expiration);
            }
        }
        else
        {
            boolean delete = (b & DELETION_MASK) != 0;
            IClock clock = clockType.serializer().deserialize(dis);
            byte[] value = FBUtilities.readByteArray(dis);
            if ((b & DELETION_MASK) != 0) {
                return new DeletedColumn(name, value, clock);
            } else {
                return new Column(name, value, clock);
            }
        }
+++++++++++++++++++++++
    public Column deserialize(DataInput dis) throws IOException
    {
        ByteBuffer name = FBUtilities.readShortByteArray(dis);
        int b = dis.readUnsignedByte();
        if ((b & EXPIRATION_MASK) != 0)
        {
            int ttl = dis.readInt();
            int expiration = dis.readInt();
            long ts = dis.readLong();
            ByteBuffer value = FBUtilities.readByteArray(dis);
            if ((int) (System.currentTimeMillis() / 1000 ) > expiration)
            {
                // the column is now expired, we can safely return a simple
                // tombstone
                ByteBuffer bytes = ByteBuffer.allocate(4);
                bytes.putInt(expiration);
                bytes.rewind();
                return new DeletedColumn(name, bytes, ts);
            }
            else
            {
                return new ExpiringColumn(name, value, ts, ttl, expiration);
            }
        }
        else
        {
            boolean delete = (b & DELETION_MASK) != 0;
            long ts = dis.readLong();
            ByteBuffer value = FBUtilities.readByteArray(dis);
            if ((b & DELETION_MASK) != 0) {
                return new DeletedColumn(name, value, ts);
            } else {
                return new Column(name, value, ts);
            }
        }
    public void serialize(RangeSliceCommand sliceCommand, DataOutputStream dos) throws IOException
    {
        dos.writeUTF(sliceCommand.keyspace);
        dos.writeUTF(sliceCommand.column_family);
        dos.writeInt(sliceCommand.super_column == null ? 0 : sliceCommand.super_column.length);
        if (sliceCommand.super_column != null)
            dos.write(sliceCommand.super_column);

        TSerializer ser = new TSerializer(new TBinaryProtocol.Factory());
        FBUtilities.serialize(ser, sliceCommand.predicate, dos);
        AbstractBounds.serializer().serialize(sliceCommand.range, dos);
        dos.writeInt(sliceCommand.max_keys);
    }

+++++++++++++++++++++++
    public void serialize(RangeSliceCommand sliceCommand, DataOutputStream dos) throws IOException
    {
        dos.writeUTF(sliceCommand.keyspace);
        dos.writeUTF(sliceCommand.column_family);
        ByteBuffer sc = sliceCommand.super_column;
        dos.writeInt(sc == null ? 0 : sc.remaining());
        if (sc != null)
            dos.write(sc.array(), sc.position() + sc.arrayOffset(), sc.remaining());

        TSerializer ser = new TSerializer(new TBinaryProtocol.Factory());
        FBUtilities.serialize(ser, sliceCommand.predicate, dos);
        AbstractBounds.serializer().serialize(sliceCommand.range, dos);
        dos.writeInt(sliceCommand.max_keys);
    }

    public RangeSliceCommand deserialize(DataInputStream dis) throws IOException
    {
        String keyspace = dis.readUTF();
        String column_family = dis.readUTF();

        int scLength = dis.readInt();
        byte[] super_column = null;
        if (scLength > 0)
            super_column = readBuf(scLength, dis);

        TDeserializer dser = new TDeserializer(new TBinaryProtocol.Factory());
        SlicePredicate pred = new SlicePredicate();
        FBUtilities.deserialize(dser, pred, dis);

        AbstractBounds range = AbstractBounds.serializer().deserialize(dis);
        int max_keys = dis.readInt();
        return new RangeSliceCommand(keyspace, column_family, super_column, pred, range, max_keys);
    }

+++++++++++++++++++++++
    public RangeSliceCommand deserialize(DataInputStream dis) throws IOException
    {
        String keyspace = dis.readUTF();
        String column_family = dis.readUTF();

        int scLength = dis.readInt();
        ByteBuffer super_column = null;
        if (scLength > 0)
            super_column = ByteBuffer.wrap(readBuf(scLength, dis));

        TDeserializer dser = new TDeserializer(new TBinaryProtocol.Factory());
        SlicePredicate pred = new SlicePredicate();
        FBUtilities.deserialize(dser, pred, dis);

        AbstractBounds range = AbstractBounds.serializer().deserialize(dis);
        int max_keys = dis.readInt();
        return new RangeSliceCommand(keyspace, column_family, super_column, pred, range, max_keys);
    }

    public void serialize(ReadCommand rm, DataOutputStream dos) throws IOException
    {
        SliceByNamesReadCommand realRM = (SliceByNamesReadCommand)rm;
        dos.writeBoolean(realRM.isDigestQuery());
        dos.writeUTF(realRM.table);
        FBUtilities.writeShortByteArray(realRM.key, dos);
        realRM.queryPath.serialize(dos);
        dos.writeInt(realRM.columnNames.size());
        if (realRM.columnNames.size() > 0)
        {
            for (byte[] cName : realRM.columnNames)
            {
                FBUtilities.writeShortByteArray(cName, dos);
            }
        }
    }

    @Override
+++++++++++++++++++++++
    public void serialize(ReadCommand rm, DataOutputStream dos) throws IOException
    {
        SliceByNamesReadCommand realRM = (SliceByNamesReadCommand)rm;
        dos.writeBoolean(realRM.isDigestQuery());
        dos.writeUTF(realRM.table);
        FBUtilities.writeShortByteArray(realRM.key, dos);
        realRM.queryPath.serialize(dos);
        dos.writeInt(realRM.columnNames.size());
        if (realRM.columnNames.size() > 0)
        {
            for (ByteBuffer cName : realRM.columnNames)
            {
                FBUtilities.writeShortByteArray(cName, dos);
            }
        }
    }

    @Override
    public ReadCommand deserialize(DataInputStream dis) throws IOException
    {
        boolean isDigest = dis.readBoolean();
        String table = dis.readUTF();
        byte[] key = FBUtilities.readShortByteArray(dis);
        QueryPath columnParent = QueryPath.deserialize(dis);

        int size = dis.readInt();
        List<byte[]> columns = new ArrayList<byte[]>();
        for (int i = 0; i < size; ++i)
        {
            columns.add(FBUtilities.readShortByteArray(dis));
        }
        SliceByNamesReadCommand rm = new SliceByNamesReadCommand(table, key, columnParent, columns);
        rm.setDigestQuery(isDigest);
        return rm;
+++++++++++++++++++++++
    public ReadCommand deserialize(DataInputStream dis) throws IOException
    {
        boolean isDigest = dis.readBoolean();
        String table = dis.readUTF();
        ByteBuffer key = FBUtilities.readShortByteArray(dis);
        QueryPath columnParent = QueryPath.deserialize(dis);

        int size = dis.readInt();
        List<ByteBuffer> columns = new ArrayList<ByteBuffer>();
        for (int i = 0; i < size; ++i)
        {
            columns.add(FBUtilities.readShortByteArray(dis));
        }
        SliceByNamesReadCommand rm = new SliceByNamesReadCommand(table, key, columnParent, columns);
        rm.setDigestQuery(isDigest);
        return rm;
    public static SuperColumnSerializer serializer(AbstractType comparator, ClockType clockType, AbstractReconciler reconciler)
    {
        return new SuperColumnSerializer(comparator, clockType, reconciler);
    }

+++++++++++++++++++++++
    public static SuperColumnSerializer serializer(AbstractType comparator)
    {
        return new SuperColumnSerializer(comparator);
    }

    public int serializedSize()
    {
    	/*
    	 * We need to keep the way we are calculating the column size in sync with the
    	 * way we are calculating the size for the column family serializer.
    	 */
      IClock _markedForDeleteAt = markedForDeleteAt.get();
      return DBConstants.shortSize_ + name_.length + DBConstants.intSize_ + _markedForDeleteAt.size() + DBConstants.intSize_ + size();
    }

+++++++++++++++++++++++
    public int serializedSize()
    {
    	/*
    	 * We need to keep the way we are calculating the column size in sync with the
    	 * way we are calculating the size for the column family serializer.
    	 */
      return DBConstants.shortSize_ + name_.remaining() + DBConstants.intSize_ + DBConstants.longSize_ + DBConstants.intSize_ + size();
    }

    public void serialize(IColumn column, DataOutput dos)
    {
    	SuperColumn superColumn = (SuperColumn)column;
        FBUtilities.writeShortByteArray(column.name(), dos);
        try
        {
            dos.writeInt(superColumn.getLocalDeletionTime());
            IClock _markedForDeleteAt = superColumn.getMarkedForDeleteAt();
            clockType.serializer().serialize(_markedForDeleteAt, dos);

            Collection<IColumn> columns = column.getSubColumns();
            dos.writeInt(columns.size());
            for (IColumn subColumn : columns)
            {
                Column.serializer(clockType).serialize(subColumn, dos);
            }
        }
        catch (IOException e)
        {
            throw new RuntimeException(e);
        }
    }

+++++++++++++++++++++++
    public void serialize(IColumn column, DataOutput dos)
    {
    	SuperColumn superColumn = (SuperColumn)column;
        FBUtilities.writeShortByteArray(column.name(), dos);
        try
        {
            dos.writeInt(superColumn.getLocalDeletionTime());
            dos.writeLong(superColumn.getMarkedForDeleteAt());

            Collection<IColumn> columns = column.getSubColumns();
            dos.writeInt(columns.size());
            for (IColumn subColumn : columns)
            {
                Column.serializer().serialize(subColumn, dos);
            }
        }
        catch (IOException e)
        {
            throw new RuntimeException(e);
        }
    }

    public IColumn deserialize(DataInput dis) throws IOException
    {
        byte[] name = FBUtilities.readShortByteArray(dis);
        SuperColumn superColumn = new SuperColumn(name, comparator, clockType, reconciler);
        int localDeleteTime = dis.readInt();
        if (localDeleteTime != Integer.MIN_VALUE && localDeleteTime <= 0)
        {
            throw new IOException("Invalid localDeleteTime read: " + localDeleteTime);
        }
        superColumn.markForDeleteAt(localDeleteTime, clockType.serializer().deserialize(dis));

        /* read the number of columns */
        int size = dis.readInt();
        for ( int i = 0; i < size; ++i )
        {
            IColumn subColumn = Column.serializer(clockType).deserialize(dis);
            superColumn.addColumn(subColumn);
        }
        return superColumn;
+++++++++++++++++++++++
    public IColumn deserialize(DataInput dis) throws IOException
    {
        ByteBuffer name = FBUtilities.readShortByteArray(dis);
        SuperColumn superColumn = new SuperColumn(name, comparator);
        int localDeleteTime = dis.readInt();
        if (localDeleteTime != Integer.MIN_VALUE && localDeleteTime <= 0)
        {
            throw new IOException("Invalid localDeleteTime read: " + localDeleteTime);
        }
        superColumn.markForDeleteAt(localDeleteTime, dis.readLong());

        /* read the number of columns */
        int size = dis.readInt();
        for ( int i = 0; i < size; ++i )
        {
            IColumn subColumn = Column.serializer().deserialize(dis);
            superColumn.addColumn(subColumn);
        }
        return superColumn;
    public void serialize(DataOutputStream dos) throws IOException
    {
        assert !"".equals(columnFamilyName);
        assert superColumnName == null || superColumnName.length > 0;
        assert columnName == null || columnName.length > 0;
        dos.writeUTF(columnFamilyName == null ? "" : columnFamilyName);
        FBUtilities.writeShortByteArray(superColumnName == null ? ArrayUtils.EMPTY_BYTE_ARRAY : superColumnName, dos);
        FBUtilities.writeShortByteArray(columnName == null ? ArrayUtils.EMPTY_BYTE_ARRAY : columnName, dos);
    }

+++++++++++++++++++++++
    public void serialize(DataOutputStream dos) throws IOException
    {
        assert !"".equals(columnFamilyName);
        assert superColumnName == null || superColumnName.remaining() > 0;
        assert columnName == null || columnName.remaining() > 0;
        dos.writeUTF(columnFamilyName == null ? "" : columnFamilyName);
        FBUtilities.writeShortByteArray(superColumnName == null ? FBUtilities.EMPTY_BYTE_BUFFER : superColumnName, dos);
        FBUtilities.writeShortByteArray(columnName == null ? FBUtilities.EMPTY_BYTE_BUFFER : columnName, dos);
    }

    public static QueryPath deserialize(DataInputStream din) throws IOException
    {
        String cfName = din.readUTF();
        byte[] scName = FBUtilities.readShortByteArray(din);
        byte[] cName = FBUtilities.readShortByteArray(din);
        return new QueryPath(cfName.isEmpty() ? null : cfName, scName.length == 0 ? null : scName, cName.length == 0 ? null : cName);
+++++++++++++++++++++++
    public static QueryPath deserialize(DataInputStream din) throws IOException
    {
        String cfName = din.readUTF();
        ByteBuffer scName = FBUtilities.readShortByteArray(din);
        ByteBuffer cName = FBUtilities.readShortByteArray(din);
        return new QueryPath(cfName.isEmpty() ? null : cfName, 
                             scName.remaining() == 0 ? null : scName, 
                             cName.remaining() == 0 ? null : cName);
    public byte[] serialize() throws IOException
    {
        // super deflate
        org.apache.cassandra.db.migration.avro.Migration mi = new org.apache.cassandra.db.migration.avro.Migration();
        mi.old_version = new org.apache.cassandra.utils.avro.UUID();
        mi.old_version.bytes(UUIDGen.decompose(lastVersion));
        mi.new_version = new org.apache.cassandra.utils.avro.UUID();
        mi.new_version.bytes(UUIDGen.decompose(newVersion));
        mi.classname = new org.apache.avro.util.Utf8(this.getClass().getName());
        // TODO: Avro RowMutation serialization?
        DataOutputBuffer dob = new DataOutputBuffer();
        try
        {
            RowMutation.serializer().serialize(rm, dob);
        }
        catch (IOException e)
        {
            throw new RuntimeException(e);
        }
        mi.row_mutation = ByteBuffer.wrap(dob.asByteArray());

        // sub deflate
        this.subdeflate(mi);

        // serialize
        return SerDeUtils.serializeWithSchema(mi);
    }

+++++++++++++++++++++++
    public ByteBuffer serialize() throws IOException
    {
        // super deflate
        org.apache.cassandra.db.migration.avro.Migration mi = new org.apache.cassandra.db.migration.avro.Migration();
        mi.old_version = new org.apache.cassandra.utils.avro.UUID();
        mi.old_version.bytes(UUIDGen.decompose(lastVersion));
        mi.new_version = new org.apache.cassandra.utils.avro.UUID();
        mi.new_version.bytes(UUIDGen.decompose(newVersion));
        mi.classname = new org.apache.avro.util.Utf8(this.getClass().getName());
        // TODO: Avro RowMutation serialization?
        DataOutputBuffer dob = new DataOutputBuffer();
        try
        {
            RowMutation.serializer().serialize(rm, dob);
        }
        catch (IOException e)
        {
            throw new RuntimeException(e);
        }
        mi.row_mutation = ByteBuffer.wrap(dob.asByteArray());

        // sub deflate
        this.subdeflate(mi);

        // serialize
        return SerDeUtils.serializeWithSchema(mi);
    }

    public static Migration deserialize(byte[] bytes) throws IOException
    {
        // deserialize
        org.apache.cassandra.db.migration.avro.Migration mi = SerDeUtils.deserializeWithSchema(bytes, new org.apache.cassandra.db.migration.avro.Migration());

        // create an instance of the migration subclass
        Migration migration;
        try
        {
            Class migrationClass = Class.forName(mi.classname.toString());
            Constructor migrationConstructor = migrationClass.getDeclaredConstructor();
            migrationConstructor.setAccessible(true);
            migration = (Migration)migrationConstructor.newInstance();
        }
        catch (Exception e)
        {
            throw new RuntimeException("Invalid migration class: " + mi.classname.toString(), e);
        }
        
        // super inflate
        migration.lastVersion = UUIDGen.makeType1UUID(mi.old_version.bytes());
        migration.newVersion = UUIDGen.makeType1UUID(mi.new_version.bytes());
        try
        {
            migration.rm = RowMutation.serializer().deserialize(SerDeUtils.createDataInputStream(mi.row_mutation));
        }
        catch (IOException e)
        {
            throw new RuntimeException(e);
        }
        // sub inflate
        migration.subinflate(mi);
        return migration;
    }
    
    /** load serialized migrations. */
+++++++++++++++++++++++
    public static Migration deserialize(ByteBuffer bytes) throws IOException
    {
        // deserialize
        org.apache.cassandra.db.migration.avro.Migration mi = SerDeUtils.deserializeWithSchema(bytes, new org.apache.cassandra.db.migration.avro.Migration());

        // create an instance of the migration subclass
        Migration migration;
        try
        {
            Class migrationClass = Class.forName(mi.classname.toString());
            Constructor migrationConstructor = migrationClass.getDeclaredConstructor();
            migrationConstructor.setAccessible(true);
            migration = (Migration)migrationConstructor.newInstance();
        }
        catch (Exception e)
        {
            throw new RuntimeException("Invalid migration class: " + mi.classname.toString(), e);
        }
        
        // super inflate
        migration.lastVersion = UUIDGen.makeType1UUID(ByteBuffer.wrap(mi.old_version.bytes()));
        migration.newVersion = UUIDGen.makeType1UUID(ByteBuffer.wrap(mi.new_version.bytes()));
        try
        {
            migration.rm = RowMutation.serializer().deserialize(SerDeUtils.createDataInputStream(mi.row_mutation));
        }
        catch (IOException e)
        {
            throw new RuntimeException(e);
        }
        // sub inflate
        migration.subinflate(mi);
        return migration;
    }
    
    /** load serialized migrations. */
    public static <T extends SpecificRecord> byte[] serializeWithSchema(T o) throws IOException
    {
        OutputBuffer buff = new OutputBuffer();
        BinaryEncoder enc = new BinaryEncoder(buff);
        enc.writeString(new Utf8(o.getSchema().toString()));
        SpecificDatumWriter<T> writer = new SpecificDatumWriter<T>(o.getSchema());
        writer.write(o, enc);
        enc.flush();
        return buff.asByteArray();
    }

    /**
     * @return a DataInputStream wrapping the given buffer.
     */
+++++++++++++++++++++++
    public static <T extends SpecificRecord> ByteBuffer serializeWithSchema(T o) throws IOException
    {
        OutputBuffer buff = new OutputBuffer();
        BinaryEncoder enc = new BinaryEncoder(buff);
        enc.writeString(new Utf8(o.getSchema().toString()));
        SpecificDatumWriter<T> writer = new SpecificDatumWriter<T>(o.getSchema());
        writer.write(o, enc);
        enc.flush();
        return ByteBuffer.wrap(buff.asByteArray());
    }

    /**
     * @return a DataInputStream wrapping the given buffer.
     */
    public static <T extends SpecificRecord> byte[] serialize(T o) throws IOException
    {
        OutputBuffer buff = new OutputBuffer();
        BinaryEncoder enc = new BinaryEncoder(buff);
        SpecificDatumWriter<T> writer = new SpecificDatumWriter<T>(o.getSchema());
        writer.write(o, enc);
        enc.flush();
        return buff.asByteArray();
    }

	/**
     * Deserializes a single object as stored along with its Schema by serialize(T). NB: See warnings on serialize(T).
     * @param ob An empty object to deserialize into (must not be null).
     * @param bytes Array to deserialize from
     * @throws IOException
     */
+++++++++++++++++++++++
    public static <T extends SpecificRecord> ByteBuffer serialize(T o) throws IOException
    {
        OutputBuffer buff = new OutputBuffer();
        BinaryEncoder enc = new BinaryEncoder(buff);
        SpecificDatumWriter<T> writer = new SpecificDatumWriter<T>(o.getSchema());
        writer.write(o, enc);
        enc.flush();
        return ByteBuffer.wrap(buff.asByteArray());
    }

	/**
     * Deserializes a single object as stored along with its Schema by serialize(T). NB: See warnings on serialize(T).
     * @param ob An empty object to deserialize into (must not be null).
     * @param bytes Array to deserialize from
     * @throws IOException
     */
    public static <T extends SpecificRecord> T deserialize(Schema writer, byte[] bytes, T ob) throws IOException
    {
        BinaryDecoder dec = DIRECT_DECODERS.createBinaryDecoder(bytes, null);
        SpecificDatumReader<T> reader = new SpecificDatumReader<T>(writer);
        reader.setExpected(ob.getSchema());
        return reader.read(ob, dec);
    }

	/**
     * Serializes a single object.
     * @param o Object to serialize
     */
+++++++++++++++++++++++
    public static <T extends SpecificRecord> T deserialize(Schema writer, ByteBuffer bytes, T ob) throws IOException
    {
        BinaryDecoder dec = DIRECT_DECODERS.createBinaryDecoder(bytes.array(),bytes.position()+bytes.arrayOffset(),bytes.remaining(), null);
        SpecificDatumReader<T> reader = new SpecificDatumReader<T>(writer);
        reader.setExpected(ob.getSchema());
        return reader.read(ob, dec);
    }

	/**
     * Serializes a single object.
     * @param o Object to serialize
     */
    public static <T extends SpecificRecord> T deserializeWithSchema(byte[] bytes, T ob) throws IOException
    {
        BinaryDecoder dec = DIRECT_DECODERS.createBinaryDecoder(bytes, null);
        Schema writer = Schema.parse(dec.readString(new Utf8()).toString());
        SpecificDatumReader<T> reader = new SpecificDatumReader<T>(writer);
        reader.setExpected(ob.getSchema());
        return new SpecificDatumReader<T>(writer).read(ob, dec);
    }

	/**
     * Serializes a single object along with its Schema. NB: For performance critical areas, it is <b>much</b>
     * more efficient to store the Schema independently.
     * @param o Object to serialize
     */
+++++++++++++++++++++++
    public static <T extends SpecificRecord> T deserializeWithSchema(ByteBuffer bytes, T ob) throws IOException
    {
        BinaryDecoder dec = DIRECT_DECODERS.createBinaryDecoder(bytes.array(),bytes.position()+bytes.arrayOffset(), bytes.remaining(), null);
        Schema writer = Schema.parse(dec.readString(new Utf8()).toString());
        SpecificDatumReader<T> reader = new SpecificDatumReader<T>(writer);
        reader.setExpected(ob.getSchema());
        return reader.read(ob, dec);
    }

	/**
     * Serializes a single object along with its Schema. NB: For performance critical areas, it is <b>much</b>
     * more efficient to store the Schema independently.
     * @param o Object to serialize
     */
add: 8, delete: 10, change: 263, unhandled: 0 size_exceptions: 25 size_serialize: 21
-----------cassandra-0.7.0-beta3 vs cassandra-0.7.0-rc1-----------
    public void serialize(IColumn column, DataOutput dos)
    {
        FBUtilities.writeShortByteArray(column.name(), dos);
        try
        {
            if (column instanceof ExpiringColumn) {
              dos.writeByte(EXPIRATION_MASK);
              dos.writeInt(((ExpiringColumn) column).getTimeToLive());
              dos.writeInt(column.getLocalDeletionTime());
            } else {
              dos.writeByte((column.isMarkedForDelete()) ? DELETION_MASK : 0);
            }
            dos.writeLong(column.timestamp());
            FBUtilities.writeByteArray(column.value(), dos);
        }
        catch (IOException e)
        {
            throw new RuntimeException(e);
        }
    }

+++++++++++++++++++++++
    public void serialize(IColumn column, DataOutput dos)
    {
        assert column.name().remaining() > 0;
        FBUtilities.writeShortByteArray(column.name(), dos);
        try
        {
            if (column instanceof ExpiringColumn) {
              dos.writeByte(EXPIRATION_MASK);
              dos.writeInt(((ExpiringColumn) column).getTimeToLive());
              dos.writeInt(column.getLocalDeletionTime());
            } else {
              dos.writeByte((column.isMarkedForDelete()) ? DELETION_MASK : 0);
            }
            dos.writeLong(column.timestamp());
            FBUtilities.writeByteArray(column.value(), dos);
        }
        catch (IOException e)
        {
            throw new RuntimeException(e);
        }
    }

    public Column deserialize(DataInput dis) throws IOException
    {
        ByteBuffer name = FBUtilities.readShortByteArray(dis);
        int b = dis.readUnsignedByte();
        if ((b & EXPIRATION_MASK) != 0)
        {
            int ttl = dis.readInt();
            int expiration = dis.readInt();
            long ts = dis.readLong();
            ByteBuffer value = FBUtilities.readByteArray(dis);
            if ((int) (System.currentTimeMillis() / 1000 ) > expiration)
            {
                // the column is now expired, we can safely return a simple
                // tombstone
                ByteBuffer bytes = ByteBuffer.allocate(4);
                bytes.putInt(expiration);
                bytes.rewind();
                return new DeletedColumn(name, bytes, ts);
            }
            else
            {
                return new ExpiringColumn(name, value, ts, ttl, expiration);
            }
        }
        else
        {
            boolean delete = (b & DELETION_MASK) != 0;
            long ts = dis.readLong();
            ByteBuffer value = FBUtilities.readByteArray(dis);
            if ((b & DELETION_MASK) != 0) {
                return new DeletedColumn(name, value, ts);
            } else {
                return new Column(name, value, ts);
            }
        }
+++++++++++++++++++++++
    public Column deserialize(DataInput dis) throws IOException
    {
        ByteBuffer name = FBUtilities.readShortByteArray(dis);
        if (name.remaining() <= 0)
            throw new CorruptColumnException("invalid column name length " + name.remaining());

        int b = dis.readUnsignedByte();
        if ((b & EXPIRATION_MASK) != 0)
        {
            int ttl = dis.readInt();
            int expiration = dis.readInt();
            long ts = dis.readLong();
            ByteBuffer value = FBUtilities.readByteArray(dis);
            if ((int) (System.currentTimeMillis() / 1000 ) > expiration)
            {
                // the column is now expired, we can safely return a simple
                // tombstone
                ByteBuffer bytes = ByteBuffer.allocate(4);
                bytes.putInt(expiration);
                bytes.rewind();
                return new DeletedColumn(name, bytes, ts);
            }
            else
            {
                return new ExpiringColumn(name, value, ts, ttl, expiration);
            }
        }
        else
        {
            boolean delete = (b & DELETION_MASK) != 0;
            long ts = dis.readLong();
            ByteBuffer value = FBUtilities.readByteArray(dis);
            if ((b & DELETION_MASK) != 0) {
                return new DeletedColumn(name, value, ts);
            } else {
                return new Column(name, value, ts);
            }
        }
    }

    public static Migration deserialize(ByteBuffer bytes) throws IOException
    {
        // deserialize
        org.apache.cassandra.db.migration.avro.Migration mi = SerDeUtils.deserializeWithSchema(bytes, new org.apache.cassandra.db.migration.avro.Migration());

        // create an instance of the migration subclass
        Migration migration;
        try
        {
            Class migrationClass = Class.forName(mi.classname.toString());
            Constructor migrationConstructor = migrationClass.getDeclaredConstructor();
            migrationConstructor.setAccessible(true);
            migration = (Migration)migrationConstructor.newInstance();
        }
        catch (Exception e)
        {
            throw new RuntimeException("Invalid migration class: " + mi.classname.toString(), e);
        }
        
        // super inflate
        migration.lastVersion = UUIDGen.makeType1UUID(ByteBuffer.wrap(mi.old_version.bytes()));
        migration.newVersion = UUIDGen.makeType1UUID(ByteBuffer.wrap(mi.new_version.bytes()));
        try
        {
            migration.rm = RowMutation.serializer().deserialize(SerDeUtils.createDataInputStream(mi.row_mutation));
        }
        catch (IOException e)
        {
            throw new RuntimeException(e);
        }
        // sub inflate
        migration.subinflate(mi);
        return migration;
    }
    
    /** load serialized migrations. */
+++++++++++++++++++++++
    public static Migration deserialize(ByteBuffer bytes) throws IOException
    {
        // deserialize
        org.apache.cassandra.db.migration.avro.Migration mi = SerDeUtils.deserializeWithSchema(bytes, new org.apache.cassandra.db.migration.avro.Migration());

        // create an instance of the migration subclass
        Migration migration;
        try
        {
            Class migrationClass = Class.forName(mi.classname.toString());
            Constructor migrationConstructor = migrationClass.getDeclaredConstructor();
            migrationConstructor.setAccessible(true);
            migration = (Migration)migrationConstructor.newInstance();
        }
        catch (Exception e)
        {
            throw new RuntimeException("Invalid migration class: " + mi.classname.toString(), e);
        }
        
        // super inflate
        migration.lastVersion = UUIDGen.getUUID(ByteBuffer.wrap(mi.old_version.bytes()));
        migration.newVersion = UUIDGen.getUUID(ByteBuffer.wrap(mi.new_version.bytes()));
        try
        {
            migration.rm = RowMutation.serializer().deserialize(SerDeUtils.createDataInputStream(mi.row_mutation));
        }
        catch (IOException e)
        {
            throw new RuntimeException(e);
        }
        // sub inflate
        migration.subinflate(mi);
        return migration;
    }
    
    /** load serialized migrations. */
add: 3, delete: 0, change: 98, unhandled: 0 size_exceptions: 4 size_serialize: 3
-----------cassandra-0.7.0-rc1 vs cassandra-0.7.0-rc2-----------
add: 1, delete: 0, change: 88, unhandled: 0 size_exceptions: 0 size_serialize: 0
-----------cassandra-0.7.0-rc2 vs cassandra-0.7.0-rc3-----------
    public Column deserialize(DataInput dis) throws IOException
    {
        ByteBuffer name = FBUtilities.readShortByteArray(dis);
        if (name.remaining() <= 0)
            throw new CorruptColumnException("invalid column name length " + name.remaining());

        int b = dis.readUnsignedByte();
        if ((b & EXPIRATION_MASK) != 0)
        {
            int ttl = dis.readInt();
            int expiration = dis.readInt();
            long ts = dis.readLong();
            ByteBuffer value = FBUtilities.readByteArray(dis);
            if ((int) (System.currentTimeMillis() / 1000 ) > expiration)
            {
                // the column is now expired, we can safely return a simple
                // tombstone
                ByteBuffer bytes = ByteBuffer.allocate(4);
                bytes.putInt(expiration);
                bytes.rewind();
                return new DeletedColumn(name, bytes, ts);
            }
            else
            {
                return new ExpiringColumn(name, value, ts, ttl, expiration);
            }
        }
        else
        {
            boolean delete = (b & DELETION_MASK) != 0;
            long ts = dis.readLong();
            ByteBuffer value = FBUtilities.readByteArray(dis);
            if ((b & DELETION_MASK) != 0) {
                return new DeletedColumn(name, value, ts);
            } else {
                return new Column(name, value, ts);
            }
        }
    }

+++++++++++++++++++++++
    public Column deserialize(DataInput dis) throws IOException
    {
        ByteBuffer name = FBUtilities.readShortByteArray(dis);
        if (name.remaining() <= 0)
            throw new CorruptColumnException("invalid column name length " + name.remaining());

        int b = dis.readUnsignedByte();
        if ((b & EXPIRATION_MASK) != 0)
        {
            int ttl = dis.readInt();
            int expiration = dis.readInt();
            long ts = dis.readLong();
            ByteBuffer value = FBUtilities.readByteArray(dis);
            if ((int) (System.currentTimeMillis() / 1000 ) > expiration)
            {
                // the column is now expired, we can safely return a simple
                // tombstone
                ByteBuffer bytes = ByteBuffer.allocate(4);
                bytes.putInt(expiration);
                bytes.rewind();
                return new DeletedColumn(name, bytes, ts);
            }
            else
            {
                return new ExpiringColumn(name, value, ts, ttl, expiration);
            }
        }
        else
        {
            long ts = dis.readLong();
            ByteBuffer value = FBUtilities.readByteArray(dis);
            return (b & DELETION_MASK) == 0
                   ? new Column(name, value, ts)
                   : new DeletedColumn(name, value, ts);
        }
    }

add: 0, delete: 0, change: 68, unhandled: 0 size_exceptions: 1 size_serialize: 1
-----------cassandra-0.7.0-rc3 vs cassandra-0.7.0-rc4-----------
add: 3, delete: 0, change: 206, unhandled: 0 size_exceptions: 0 size_serialize: 0
-----------cassandra-0.7.0-rc4 vs cassandra-0.7.1-----------
    public void serialize(IColumn column, DataOutput dos)
    {
        assert column.name().remaining() > 0;
        FBUtilities.writeShortByteArray(column.name(), dos);
        try
        {
            if (column instanceof ExpiringColumn) {
              dos.writeByte(EXPIRATION_MASK);
              dos.writeInt(((ExpiringColumn) column).getTimeToLive());
              dos.writeInt(column.getLocalDeletionTime());
            } else {
              dos.writeByte((column.isMarkedForDelete()) ? DELETION_MASK : 0);
            }
            dos.writeLong(column.timestamp());
            FBUtilities.writeByteArray(column.value(), dos);
        }
        catch (IOException e)
        {
            throw new RuntimeException(e);
        }
    }

+++++++++++++++++++++++
    public void serialize(IColumn column, DataOutput dos)
    {
        assert column.name().remaining() > 0;
        ByteBufferUtil.writeWithShortLength(column.name(), dos);
        try
        {
            if (column instanceof ExpiringColumn) {
              dos.writeByte(EXPIRATION_MASK);
              dos.writeInt(((ExpiringColumn) column).getTimeToLive());
              dos.writeInt(column.getLocalDeletionTime());
            } else {
              dos.writeByte((column.isMarkedForDelete()) ? DELETION_MASK : 0);
            }
            dos.writeLong(column.timestamp());
            ByteBufferUtil.writeWithLength(column.value(), dos);
        }
        catch (IOException e)
        {
            throw new RuntimeException(e);
        }
    }

    public Column deserialize(DataInput dis) throws IOException
    {
        ByteBuffer name = FBUtilities.readShortByteArray(dis);
        if (name.remaining() <= 0)
            throw new CorruptColumnException("invalid column name length " + name.remaining());

        int b = dis.readUnsignedByte();
        if ((b & EXPIRATION_MASK) != 0)
        {
            int ttl = dis.readInt();
            int expiration = dis.readInt();
            long ts = dis.readLong();
            ByteBuffer value = FBUtilities.readByteArray(dis);
            if ((int) (System.currentTimeMillis() / 1000 ) > expiration)
            {
                // the column is now expired, we can safely return a simple
                // tombstone
                ByteBuffer bytes = ByteBuffer.allocate(4);
                bytes.putInt(expiration);
                bytes.rewind();
                return new DeletedColumn(name, bytes, ts);
            }
            else
            {
                return new ExpiringColumn(name, value, ts, ttl, expiration);
            }
        }
        else
        {
            long ts = dis.readLong();
            ByteBuffer value = FBUtilities.readByteArray(dis);
            return (b & DELETION_MASK) == 0
                   ? new Column(name, value, ts)
                   : new DeletedColumn(name, value, ts);
        }
    }

+++++++++++++++++++++++
    public Column deserialize(DataInput dis) throws IOException
    {
        ByteBuffer name = ByteBufferUtil.readWithShortLength(dis);
        if (name.remaining() <= 0)
            throw new CorruptColumnException("invalid column name length " + name.remaining());

        int b = dis.readUnsignedByte();
        if ((b & EXPIRATION_MASK) != 0)
        {
            int ttl = dis.readInt();
            int expiration = dis.readInt();
            long ts = dis.readLong();
            ByteBuffer value = ByteBufferUtil.readWithLength(dis);
            if ((int) (System.currentTimeMillis() / 1000 ) > expiration)
            {
                // the column is now expired, we can safely return a simple
                // tombstone
                ByteBuffer bytes = ByteBuffer.allocate(4);
                bytes.putInt(expiration);
                bytes.rewind();
                return new DeletedColumn(name, bytes, ts);
            }
            else
            {
                return new ExpiringColumn(name, value, ts, ttl, expiration);
            }
        }
        else
        {
            long ts = dis.readLong();
            ByteBuffer value = ByteBufferUtil.readWithLength(dis);
            return (b & DELETION_MASK) == 0
                   ? new Column(name, value, ts)
                   : new DeletedColumn(name, value, ts);
        }
    }

    public void serialize(RangeSliceCommand sliceCommand, DataOutputStream dos) throws IOException
    {
        dos.writeUTF(sliceCommand.keyspace);
        dos.writeUTF(sliceCommand.column_family);
        ByteBuffer sc = sliceCommand.super_column;
        dos.writeInt(sc == null ? 0 : sc.remaining());
        if (sc != null)
            dos.write(sc.array(), sc.position() + sc.arrayOffset(), sc.remaining());

        TSerializer ser = new TSerializer(new TBinaryProtocol.Factory());
        FBUtilities.serialize(ser, sliceCommand.predicate, dos);
        AbstractBounds.serializer().serialize(sliceCommand.range, dos);
        dos.writeInt(sliceCommand.max_keys);
    }

+++++++++++++++++++++++
    public void serialize(RangeSliceCommand sliceCommand, DataOutputStream dos) throws IOException
    {
        dos.writeUTF(sliceCommand.keyspace);
        dos.writeUTF(sliceCommand.column_family);
        ByteBuffer sc = sliceCommand.super_column;
        dos.writeInt(sc == null ? 0 : sc.remaining());
        if (sc != null)
            ByteBufferUtil.write(sc, dos);

        TSerializer ser = new TSerializer(new TBinaryProtocol.Factory());
        FBUtilities.serialize(ser, sliceCommand.predicate, dos);
        AbstractBounds.serializer().serialize(sliceCommand.range, dos);
        dos.writeInt(sliceCommand.max_keys);
    }

    public void serialize(Row row, DataOutputStream dos) throws IOException
    {
        FBUtilities.writeShortByteArray(row.key.key, dos);
        ColumnFamily.serializer().serialize(row.cf, dos);
    }

+++++++++++++++++++++++
    public void serialize(Row row, DataOutputStream dos) throws IOException
    {
        ByteBufferUtil.writeWithShortLength(row.key.key, dos);
        ColumnFamily.serializer().serialize(row.cf, dos);
    }

    public Row deserialize(DataInputStream dis) throws IOException
    {
        return new Row(StorageService.getPartitioner().decorateKey(FBUtilities.readShortByteArray(dis)),
                       ColumnFamily.serializer().deserialize(dis));
+++++++++++++++++++++++
    public Row deserialize(DataInputStream dis) throws IOException
    {
        return new Row(StorageService.getPartitioner().decorateKey(ByteBufferUtil.readWithShortLength(dis)),
                       ColumnFamily.serializer().deserialize(dis));
    public void serialize(ReadCommand rm, DataOutputStream dos) throws IOException
    {
        SliceByNamesReadCommand realRM = (SliceByNamesReadCommand)rm;
        dos.writeBoolean(realRM.isDigestQuery());
        dos.writeUTF(realRM.table);
        FBUtilities.writeShortByteArray(realRM.key, dos);
        realRM.queryPath.serialize(dos);
        dos.writeInt(realRM.columnNames.size());
        if (realRM.columnNames.size() > 0)
        {
            for (ByteBuffer cName : realRM.columnNames)
            {
                FBUtilities.writeShortByteArray(cName, dos);
            }
        }
    }

    @Override
+++++++++++++++++++++++
    public void serialize(ReadCommand rm, DataOutputStream dos) throws IOException
    {
        SliceByNamesReadCommand realRM = (SliceByNamesReadCommand)rm;
        dos.writeBoolean(realRM.isDigestQuery());
        dos.writeUTF(realRM.table);
        ByteBufferUtil.writeWithShortLength(realRM.key, dos);
        realRM.queryPath.serialize(dos);
        dos.writeInt(realRM.columnNames.size());
        if (realRM.columnNames.size() > 0)
        {
            for (ByteBuffer cName : realRM.columnNames)
            {
                ByteBufferUtil.writeWithShortLength(cName, dos);
            }
        }
    }

    @Override
    public ReadCommand deserialize(DataInputStream dis) throws IOException
    {
        boolean isDigest = dis.readBoolean();
        String table = dis.readUTF();
        ByteBuffer key = FBUtilities.readShortByteArray(dis);
        QueryPath columnParent = QueryPath.deserialize(dis);

        int size = dis.readInt();
        List<ByteBuffer> columns = new ArrayList<ByteBuffer>();
        for (int i = 0; i < size; ++i)
        {
            columns.add(FBUtilities.readShortByteArray(dis));
        }
        SliceByNamesReadCommand rm = new SliceByNamesReadCommand(table, key, columnParent, columns);
        rm.setDigestQuery(isDigest);
        return rm;
+++++++++++++++++++++++
    public ReadCommand deserialize(DataInputStream dis) throws IOException
    {
        boolean isDigest = dis.readBoolean();
        String table = dis.readUTF();
        ByteBuffer key = ByteBufferUtil.readWithShortLength(dis);
        QueryPath columnParent = QueryPath.deserialize(dis);

        int size = dis.readInt();
        List<ByteBuffer> columns = new ArrayList<ByteBuffer>();
        for (int i = 0; i < size; ++i)
        {
            columns.add(ByteBufferUtil.readWithShortLength(dis));
        }
        SliceByNamesReadCommand rm = new SliceByNamesReadCommand(table, key, columnParent, columns);
        rm.setDigestQuery(isDigest);
        return rm;
    public void serialize(ReadCommand rm, DataOutputStream dos) throws IOException
    {
        SliceFromReadCommand realRM = (SliceFromReadCommand)rm;
        dos.writeBoolean(realRM.isDigestQuery());
        dos.writeUTF(realRM.table);
        FBUtilities.writeShortByteArray(realRM.key, dos);
        realRM.queryPath.serialize(dos);
        FBUtilities.writeShortByteArray(realRM.start, dos);
        FBUtilities.writeShortByteArray(realRM.finish, dos);
        dos.writeBoolean(realRM.reversed);
        dos.writeInt(realRM.count);
    }

    @Override
+++++++++++++++++++++++
    public void serialize(ReadCommand rm, DataOutputStream dos) throws IOException
    {
        SliceFromReadCommand realRM = (SliceFromReadCommand)rm;
        dos.writeBoolean(realRM.isDigestQuery());
        dos.writeUTF(realRM.table);
        ByteBufferUtil.writeWithShortLength(realRM.key, dos);
        realRM.queryPath.serialize(dos);
        ByteBufferUtil.writeWithShortLength(realRM.start, dos);
        ByteBufferUtil.writeWithShortLength(realRM.finish, dos);
        dos.writeBoolean(realRM.reversed);
        dos.writeInt(realRM.count);
    }

    @Override
    public ReadCommand deserialize(DataInputStream dis) throws IOException
    {
        boolean isDigest = dis.readBoolean();
        SliceFromReadCommand rm = new SliceFromReadCommand(dis.readUTF(),
                                                           FBUtilities.readShortByteArray(dis),
                                                           QueryPath.deserialize(dis),
                                                           FBUtilities.readShortByteArray(dis),
                                                           FBUtilities.readShortByteArray(dis),
                                                           dis.readBoolean(),
                                                           dis.readInt());
        rm.setDigestQuery(isDigest);
        return rm;
+++++++++++++++++++++++
    public ReadCommand deserialize(DataInputStream dis) throws IOException
    {
        boolean isDigest = dis.readBoolean();
        SliceFromReadCommand rm = new SliceFromReadCommand(dis.readUTF(),
                                                           ByteBufferUtil.readWithShortLength(dis),
                                                           QueryPath.deserialize(dis),
                                                           ByteBufferUtil.readWithShortLength(dis),
                                                           ByteBufferUtil.readWithShortLength(dis),
                                                           dis.readBoolean(),
                                                           dis.readInt());
        rm.setDigestQuery(isDigest);
        return rm;
    public void serialize(IColumn column, DataOutput dos)
    {
    	SuperColumn superColumn = (SuperColumn)column;
        FBUtilities.writeShortByteArray(column.name(), dos);
        try
        {
            dos.writeInt(superColumn.getLocalDeletionTime());
            dos.writeLong(superColumn.getMarkedForDeleteAt());

            Collection<IColumn> columns = column.getSubColumns();
            dos.writeInt(columns.size());
            for (IColumn subColumn : columns)
            {
                Column.serializer().serialize(subColumn, dos);
            }
        }
        catch (IOException e)
        {
            throw new RuntimeException(e);
        }
    }

+++++++++++++++++++++++
    public void serialize(IColumn column, DataOutput dos)
    {
        SuperColumn superColumn = (SuperColumn)column;
        ByteBufferUtil.writeWithShortLength(column.name(), dos);
        try
        {
            dos.writeInt(superColumn.getLocalDeletionTime());
            dos.writeLong(superColumn.getMarkedForDeleteAt());

            Collection<IColumn> columns = column.getSubColumns();
            dos.writeInt(columns.size());
            for (IColumn subColumn : columns)
            {
                Column.serializer().serialize(subColumn, dos);
            }
        }
        catch (IOException e)
        {
            throw new RuntimeException(e);
        }
    }

    public IColumn deserialize(DataInput dis) throws IOException
    {
        ByteBuffer name = FBUtilities.readShortByteArray(dis);
        SuperColumn superColumn = new SuperColumn(name, comparator);
        int localDeleteTime = dis.readInt();
        if (localDeleteTime != Integer.MIN_VALUE && localDeleteTime <= 0)
        {
            throw new IOException("Invalid localDeleteTime read: " + localDeleteTime);
        }
        superColumn.markForDeleteAt(localDeleteTime, dis.readLong());

        /* read the number of columns */
        int size = dis.readInt();
        for ( int i = 0; i < size; ++i )
        {
            IColumn subColumn = Column.serializer().deserialize(dis);
            superColumn.addColumn(subColumn);
        }
        return superColumn;
+++++++++++++++++++++++
    public IColumn deserialize(DataInput dis) throws IOException
    {
        ByteBuffer name = ByteBufferUtil.readWithShortLength(dis);
        int localDeleteTime = dis.readInt();
        if (localDeleteTime != Integer.MIN_VALUE && localDeleteTime <= 0)
        {
            throw new IOException("Invalid localDeleteTime read: " + localDeleteTime);
        }
        long markedForDeleteAt = dis.readLong();

        /* read the number of columns */
        int size = dis.readInt();
        ColumnSerializer serializer = Column.serializer();
        ColumnSortedMap preSortedMap = new ColumnSortedMap(comparator, serializer, dis, size);
        SuperColumn superColumn = new SuperColumn(name, new ConcurrentSkipListMap<ByteBuffer,IColumn>(preSortedMap));
        if (localDeleteTime != Integer.MIN_VALUE && localDeleteTime <= 0)
        {
            throw new IOException("Invalid localDeleteTime read: " + localDeleteTime);
        }
        superColumn.markForDeleteAt(localDeleteTime, markedForDeleteAt);
        return superColumn;
    public void serialize(DataOutputStream dos) throws IOException
    {
        assert !"".equals(columnFamilyName);
        assert superColumnName == null || superColumnName.remaining() > 0;
        assert columnName == null || columnName.remaining() > 0;
        dos.writeUTF(columnFamilyName == null ? "" : columnFamilyName);
        FBUtilities.writeShortByteArray(superColumnName == null ? FBUtilities.EMPTY_BYTE_BUFFER : superColumnName, dos);
        FBUtilities.writeShortByteArray(columnName == null ? FBUtilities.EMPTY_BYTE_BUFFER : columnName, dos);
    }

+++++++++++++++++++++++
    public void serialize(DataOutputStream dos) throws IOException
    {
        assert !"".equals(columnFamilyName);
        assert superColumnName == null || superColumnName.remaining() > 0;
        assert columnName == null || columnName.remaining() > 0;
        dos.writeUTF(columnFamilyName == null ? "" : columnFamilyName);
        ByteBufferUtil.writeWithShortLength(superColumnName == null ? ByteBufferUtil.EMPTY_BYTE_BUFFER : superColumnName, dos);
        ByteBufferUtil.writeWithShortLength(columnName == null ? ByteBufferUtil.EMPTY_BYTE_BUFFER : columnName, dos);
    }

    public static QueryPath deserialize(DataInputStream din) throws IOException
    {
        String cfName = din.readUTF();
        ByteBuffer scName = FBUtilities.readShortByteArray(din);
        ByteBuffer cName = FBUtilities.readShortByteArray(din);
        return new QueryPath(cfName.isEmpty() ? null : cfName, 
                             scName.remaining() == 0 ? null : scName, 
                             cName.remaining() == 0 ? null : cName);
+++++++++++++++++++++++
    public static QueryPath deserialize(DataInputStream din) throws IOException
    {
        String cfName = din.readUTF();
        ByteBuffer scName = ByteBufferUtil.readWithShortLength(din);
        ByteBuffer cName = ByteBufferUtil.readWithShortLength(din);
        return new QueryPath(cfName.isEmpty() ? null : cfName, 
                             scName.remaining() == 0 ? null : scName, 
                             cName.remaining() == 0 ? null : cName);
    public static <T extends SpecificRecord> T deserialize(Schema writer, ByteBuffer bytes, T ob) throws IOException
    {
        BinaryDecoder dec = DIRECT_DECODERS.createBinaryDecoder(bytes.array(),bytes.position()+bytes.arrayOffset(),bytes.remaining(), null);
        SpecificDatumReader<T> reader = new SpecificDatumReader<T>(writer);
        reader.setExpected(ob.getSchema());
        return reader.read(ob, dec);
    }

	/**
     * Serializes a single object.
     * @param o Object to serialize
     */
+++++++++++++++++++++++
    public static <T extends SpecificRecord> T deserialize(Schema writer, ByteBuffer bytes, T ob) throws IOException
    {
        BinaryDecoder dec = DIRECT_DECODERS.createBinaryDecoder(ByteBufferUtil.getArray(bytes), null);
        SpecificDatumReader<T> reader = new SpecificDatumReader<T>(writer);
        reader.setExpected(ob.getSchema());
        return reader.read(ob, dec);
    }

	/**
     * Serializes a single object.
     * @param o Object to serialize
     */
    public static <T extends SpecificRecord> T deserializeWithSchema(ByteBuffer bytes, T ob) throws IOException
    {
        BinaryDecoder dec = DIRECT_DECODERS.createBinaryDecoder(bytes.array(),bytes.position()+bytes.arrayOffset(), bytes.remaining(), null);
        Schema writer = Schema.parse(dec.readString(new Utf8()).toString());
        SpecificDatumReader<T> reader = new SpecificDatumReader<T>(writer);
        reader.setExpected(ob.getSchema());
        return reader.read(ob, dec);
    }

	/**
     * Serializes a single object along with its Schema. NB: For performance critical areas, it is <b>much</b>
     * more efficient to store the Schema independently.
     * @param o Object to serialize
     */
+++++++++++++++++++++++
    public static <T extends SpecificRecord> T deserializeWithSchema(ByteBuffer bytes, T ob) throws IOException
    {
        BinaryDecoder dec = DIRECT_DECODERS.createBinaryDecoder(ByteBufferUtil.getArray(bytes), null);
        Schema writer = Schema.parse(dec.readString(new Utf8()).toString());
        SpecificDatumReader<T> reader = new SpecificDatumReader<T>(writer);
        reader.setExpected(ob.getSchema());
        return reader.read(ob, dec);
    }

	/**
     * Serializes a single object along with its Schema. NB: For performance critical areas, it is <b>much</b>
     * more efficient to store the Schema independently.
     * @param o Object to serialize
     */
	public static ArrayList<IndexInfo> deserializeIndex(FileDataInput in) throws IOException
	{
		int columnIndexSize = in.readInt();
        if (columnIndexSize == 0)
            return null;
        ArrayList<IndexInfo> indexList = new ArrayList<IndexInfo>();
        FileMark mark = in.mark();
        while (in.bytesPastMark(mark) < columnIndexSize)
        {
            indexList.add(IndexInfo.deserialize(in));
        }
        assert in.bytesPastMark(mark) == columnIndexSize;

        return indexList;
	}

    /**
     * Defreeze the bloom filter.
     *
     * @return bloom filter summarizing the column information
     * @throws java.io.IOException
     */
+++++++++++++++++++++++
	public static ArrayList<IndexInfo> deserializeIndex(FileDataInput in) throws IOException
	{
		int columnIndexSize = in.readInt();
        if (columnIndexSize == 0)
            return null;
        ArrayList<IndexInfo> indexList = new ArrayList<IndexInfo>();
        FileMark mark = in.mark();
        while (in.bytesPastMark(mark) < columnIndexSize)
        {
            indexList.add(IndexInfo.deserialize(in));
        }
        assert in.bytesPastMark(mark) == columnIndexSize;

        return indexList;
	}

    /**
     * De-freeze the bloom filter.
     *
     * @param file - source file
     * @param useOldBuffer - do we need to reuse old buffer?
     *
     * @return bloom filter summarizing the column information
     * @throws java.io.IOException if an I/O error occurs.
     */
add: 43, delete: 9, change: 227, unhandled: 0 size_exceptions: 15 size_serialize: 16
-----------cassandra-0.7.1 vs cassandra-0.7.2-----------
add: 2, delete: 0, change: 8, unhandled: 0 size_exceptions: 0 size_serialize: 0
-----------cassandra-0.7.2 vs cassandra-0.7.3-----------
add: 11, delete: 3, change: 93, unhandled: 0 size_exceptions: 11 size_serialize: 0
-----------cassandra-0.7.3 vs cassandra-0.7.4-----------
add: 1, delete: 1, change: 60, unhandled: 0 size_exceptions: 1 size_serialize: 0
-----------cassandra-0.7.4 vs cassandra-0.7.5-----------
    public Column deserialize(DataInput dis) throws IOException
    {
        ByteBuffer name = ByteBufferUtil.readWithShortLength(dis);
        if (name.remaining() <= 0)
            throw new CorruptColumnException("invalid column name length " + name.remaining());

        int b = dis.readUnsignedByte();
        if ((b & EXPIRATION_MASK) != 0)
        {
            int ttl = dis.readInt();
            int expiration = dis.readInt();
            long ts = dis.readLong();
            ByteBuffer value = ByteBufferUtil.readWithLength(dis);
            if ((int) (System.currentTimeMillis() / 1000 ) > expiration)
            {
                // the column is now expired, we can safely return a simple
                // tombstone
                ByteBuffer bytes = ByteBuffer.allocate(4);
                bytes.putInt(expiration);
                bytes.rewind();
                return new DeletedColumn(name, bytes, ts);
            }
            else
            {
                return new ExpiringColumn(name, value, ts, ttl, expiration);
            }
        }
        else
        {
            long ts = dis.readLong();
            ByteBuffer value = ByteBufferUtil.readWithLength(dis);
            return (b & DELETION_MASK) == 0
                   ? new Column(name, value, ts)
                   : new DeletedColumn(name, value, ts);
        }
    }

+++++++++++++++++++++++
    public Column deserialize(DataInput dis, int expireBefore) throws IOException
    {
        ByteBuffer name = ByteBufferUtil.readWithShortLength(dis);
        if (name.remaining() <= 0)
            throw new CorruptColumnException("invalid column name length " + name.remaining());

        int b = dis.readUnsignedByte();
        if ((b & EXPIRATION_MASK) != 0)
        {
            int ttl = dis.readInt();
            int expiration = dis.readInt();
            long ts = dis.readLong();
            ByteBuffer value = ByteBufferUtil.readWithLength(dis);
            if (expiration < expireBefore)
            {
                // the column is now expired, we can safely return a simple
                // tombstone
                ByteBuffer bytes = ByteBuffer.allocate(4);
                bytes.putInt(expiration);
                bytes.rewind();
                return new DeletedColumn(name, bytes, ts);
            }
            else
            {
                return new ExpiringColumn(name, value, ts, ttl, expiration);
            }
        }
        else
        {
            long ts = dis.readLong();
            ByteBuffer value = ByteBufferUtil.readWithLength(dis);
            return (b & DELETION_MASK) == 0
                   ? new Column(name, value, ts)
                   : new DeletedColumn(name, value, ts);
        }
    }

    public IColumn deserialize(DataInput dis) throws IOException
    {
        ByteBuffer name = ByteBufferUtil.readWithShortLength(dis);
        int localDeleteTime = dis.readInt();
        if (localDeleteTime != Integer.MIN_VALUE && localDeleteTime <= 0)
        {
            throw new IOException("Invalid localDeleteTime read: " + localDeleteTime);
        }
        long markedForDeleteAt = dis.readLong();

        /* read the number of columns */
        int size = dis.readInt();
        ColumnSerializer serializer = Column.serializer();
        ColumnSortedMap preSortedMap = new ColumnSortedMap(comparator, serializer, dis, size);
        SuperColumn superColumn = new SuperColumn(name, new ConcurrentSkipListMap<ByteBuffer,IColumn>(preSortedMap));
        if (localDeleteTime != Integer.MIN_VALUE && localDeleteTime <= 0)
        {
            throw new IOException("Invalid localDeleteTime read: " + localDeleteTime);
        }
        superColumn.markForDeleteAt(localDeleteTime, markedForDeleteAt);
        return superColumn;
+++++++++++++++++++++++
    public IColumn deserialize(DataInput dis, int expireBefore) throws IOException
    {
        ByteBuffer name = ByteBufferUtil.readWithShortLength(dis);
        int localDeleteTime = dis.readInt();
        if (localDeleteTime != Integer.MIN_VALUE && localDeleteTime <= 0)
        {
            throw new IOException("Invalid localDeleteTime read: " + localDeleteTime);
        }
        long markedForDeleteAt = dis.readLong();

        /* read the number of columns */
        int size = dis.readInt();
        ColumnSerializer serializer = Column.serializer();
        ColumnSortedMap preSortedMap = new ColumnSortedMap(comparator, serializer, dis, size, expireBefore);
        SuperColumn superColumn = new SuperColumn(name, new ConcurrentSkipListMap<ByteBuffer,IColumn>(preSortedMap));
        if (localDeleteTime != Integer.MIN_VALUE && localDeleteTime <= 0)
        {
            throw new IOException("Invalid localDeleteTime read: " + localDeleteTime);
        }
        superColumn.markForDeleteAt(localDeleteTime, markedForDeleteAt);
        return superColumn;
    private IColumn deserializeNext()
    {
        try
        {
            count++;
            return serializer.deserialize(dis);
        }
        catch (IOException e)
        {
            throw new IOError(e);
        }
    }

+++++++++++++++++++++++
    private IColumn deserializeNext()
    {
        try
        {
            count++;
            return serializer.deserialize(dis, expireBefore);
        }
        catch (IOException e)
        {
            throw new IOError(e);
        }
    }

add: 1, delete: 0, change: 138, unhandled: 0 size_exceptions: 2 size_serialize: 3
-----------cassandra-0.7.5 vs cassandra-0.7.6-----------
add: 1, delete: 1, change: 36, unhandled: 0 size_exceptions: 1 size_serialize: 0
-----------cassandra-0.7.6 vs cassandra-0.7.6-2-----------
add: 0, delete: 0, change: 0, unhandled: 0 size_exceptions: 0 size_serialize: 0
-----------cassandra-0.7.6-2 vs cassandra-0.7.7-----------
    public static ColumnSerializer serializer()
    {
        return new ColumnSerializer();
    }

+++++++++++++++++++++++
    public static ColumnSerializer serializer()
    {
        return serializer;
    }

    public static SuperColumnSerializer serializer(AbstractType comparator)
    {
        return new SuperColumnSerializer(comparator);
    }

+++++++++++++++++++++++
    public static SuperColumnSerializer serializer(AbstractType comparator)
    {
        SuperColumnSerializer serializer = serializers.get(comparator);
        if (serializer == null)
        {
            serializer = new SuperColumnSerializer(comparator);
            serializers.put(comparator, serializer);
        }
        return serializer;
    }

add: 1, delete: 0, change: 51, unhandled: 0 size_exceptions: 1 size_serialize: 2
-----------cassandra-0.7.7 vs cassandra-0.7.8-----------
add: 0, delete: 0, change: 11, unhandled: 0 size_exceptions: 0 size_serialize: 0
-----------cassandra-0.7.8 vs cassandra-0.7.9-----------
add: 0, delete: 0, change: 10, unhandled: 0 size_exceptions: 0 size_serialize: 0
-----------cassandra-0.7.9 vs cassandra-0.7.10-----------
    public static void serializeInternal(IIterableColumns columns, DataOutput dos) throws IOException
    {
        int columnCount = columns.getEstimatedColumnCount();

        BloomFilter bf = BloomFilter.getFilter(columnCount, 4);

        if (columnCount == 0)
        {
            writeEmptyHeader(dos, bf);
            return;
        }

        // update bloom filter and create a list of IndexInfo objects marking the first and last column
        // in each block of ColumnIndexSize
        List<IndexHelper.IndexInfo> indexList = new ArrayList<IndexHelper.IndexInfo>();
        int endPosition = 0, startPosition = -1;
        int indexSizeInBytes = 0;
        IColumn lastColumn = null, firstColumn = null;
        for (IColumn column : columns)
        {
            bf.add(column.name());

            if (firstColumn == null)
            {
                firstColumn = column;
                startPosition = endPosition;
            }
            endPosition += column.serializedSize();
            /* if we hit the column index size that we have to index after, go ahead and index it. */
            if (endPosition - startPosition >= DatabaseDescriptor.getColumnIndexSize())
            {
                IndexHelper.IndexInfo cIndexInfo = new IndexHelper.IndexInfo(firstColumn.name(), column.name(), startPosition, endPosition - startPosition);
                indexList.add(cIndexInfo);
                indexSizeInBytes += cIndexInfo.serializedSize();
                firstColumn = null;
            }

            lastColumn = column;
        }

        // all columns were GC'd after all
        if (lastColumn == null)
        {
            writeEmptyHeader(dos, bf);
            return;
        }

        // the last column may have fallen on an index boundary already.  if not, index it explicitly.
        if (indexList.isEmpty() || columns.getComparator().compare(indexList.get(indexList.size() - 1).lastName, lastColumn.name()) != 0)
        {
            IndexHelper.IndexInfo cIndexInfo = new IndexHelper.IndexInfo(firstColumn.name(), lastColumn.name(), startPosition, endPosition - startPosition);
            indexList.add(cIndexInfo);
            indexSizeInBytes += cIndexInfo.serializedSize();
        }

        /* Write out the bloom filter. */
        writeBloomFilter(dos, bf);

        // write the index.  we should always have at least one computed index block, but we only write it out if there is more than that.
        assert indexSizeInBytes > 0;
        if (indexList.size() > 1)
        {
            dos.writeInt(indexSizeInBytes);
            for (IndexHelper.IndexInfo cIndexInfo : indexList)
            {
                cIndexInfo.serialize(dos);
            }
        }
        else
        {
            dos.writeInt(0);
        }
	}

+++++++++++++++++++++++
    public static void serializeInternal(IIterableColumns columns, DataOutput dos) throws IOException
    {
        int columnCount = columns.getEstimatedColumnCount();

        BloomFilter bf = BloomFilter.getFilter(columnCount, 4);

        if (columnCount == 0)
        {
            writeEmptyHeader(dos, bf);
            return;
        }

        // update bloom filter and create a list of IndexInfo objects marking the first and last column
        // in each block of ColumnIndexSize
        List<IndexHelper.IndexInfo> indexList = new ArrayList<IndexHelper.IndexInfo>();
        long endPosition = 0, startPosition = -1;
        int indexSizeInBytes = 0;
        IColumn lastColumn = null, firstColumn = null;
        for (IColumn column : columns)
        {
            bf.add(column.name());

            if (firstColumn == null)
            {
                firstColumn = column;
                startPosition = endPosition;
            }
            endPosition += column.serializedSize();
            /* if we hit the column index size that we have to index after, go ahead and index it. */
            if (endPosition - startPosition >= DatabaseDescriptor.getColumnIndexSize())
            {
                IndexHelper.IndexInfo cIndexInfo = new IndexHelper.IndexInfo(firstColumn.name(), column.name(), startPosition, endPosition - startPosition);
                indexList.add(cIndexInfo);
                indexSizeInBytes += cIndexInfo.serializedSize();
                firstColumn = null;
            }

            lastColumn = column;
        }

        // all columns were GC'd after all
        if (lastColumn == null)
        {
            writeEmptyHeader(dos, bf);
            return;
        }

        // the last column may have fallen on an index boundary already.  if not, index it explicitly.
        if (indexList.isEmpty() || columns.getComparator().compare(indexList.get(indexList.size() - 1).lastName, lastColumn.name()) != 0)
        {
            IndexHelper.IndexInfo cIndexInfo = new IndexHelper.IndexInfo(firstColumn.name(), lastColumn.name(), startPosition, endPosition - startPosition);
            indexList.add(cIndexInfo);
            indexSizeInBytes += cIndexInfo.serializedSize();
        }

        /* Write out the bloom filter. */
        writeBloomFilter(dos, bf);

        // write the index.  we should always have at least one computed index block, but we only write it out if there is more than that.
        assert indexSizeInBytes > 0;
        if (indexList.size() > 1)
        {
            dos.writeInt(indexSizeInBytes);
            for (IndexHelper.IndexInfo cIndexInfo : indexList)
            {
                cIndexInfo.serialize(dos);
            }
        }
        else
        {
            dos.writeInt(0);
        }
	}

add: 0, delete: 0, change: 4, unhandled: 0 size_exceptions: 0 size_serialize: 1
-----------cassandra-0.7.10 vs cassandra-0.8.0-----------
    public static ColumnSerializer serializer()
    {
        return serializer;
    }

+++++++++++++++++++++++
    public static ColumnSerializer serializer()
    {
        return new ColumnSerializer();
    }

    public ColumnFamily deserialize(DataInput dis) throws IOException
    {
        if (!dis.readBoolean())
            return null;

        // create a ColumnFamily based on the cf id
        int cfId = dis.readInt();
        if (CFMetaData.getCF(cfId) == null)
            throw new UnserializableColumnFamilyException("Couldn't find cfId=" + cfId, cfId);
        ColumnFamily cf = ColumnFamily.create(cfId);
        deserializeFromSSTableNoColumns(cf, dis);
        deserializeColumns(dis, cf);
        return cf;
    }

+++++++++++++++++++++++
    public ColumnFamily deserialize(DataInput dis, boolean intern, boolean fromRemote) throws IOException
    {
        if (!dis.readBoolean())
            return null;

        // create a ColumnFamily based on the cf id
        int cfId = dis.readInt();
        if (CFMetaData.getCF(cfId) == null)
            throw new UnserializableColumnFamilyException("Couldn't find cfId=" + cfId, cfId);
        ColumnFamily cf = ColumnFamily.create(cfId);
        deserializeFromSSTableNoColumns(cf, dis);
        deserializeColumns(dis, cf, intern, fromRemote);
        return cf;
    }

    public void deserializeColumns(DataInput dis, ColumnFamily cf) throws IOException
    {
        int size = dis.readInt();
        for (int i = 0; i < size; ++i)
        {
            IColumn column = cf.getColumnSerializer().deserialize(dis);
            cf.addColumn(column);
        }
    }

+++++++++++++++++++++++
    public void deserializeColumns(DataInput dis, ColumnFamily cf, boolean intern, boolean fromRemote) throws IOException
    {
        int size = dis.readInt();
        ColumnFamilyStore interner = intern ? Table.open(CFMetaData.getCF(cf.id()).left).getColumnFamilyStore(cf.id()) : null;
        for (int i = 0; i < size; ++i)
        {
            IColumn column = cf.getColumnSerializer().deserialize(dis, interner, fromRemote, (int) (System.currentTimeMillis() / 1000));
            cf.addColumn(column);
        }
    }

    public static void serializeInternal(IIterableColumns columns, DataOutput dos) throws IOException
    {
        int columnCount = columns.getEstimatedColumnCount();

        BloomFilter bf = BloomFilter.getFilter(columnCount, 4);

        if (columnCount == 0)
        {
            writeEmptyHeader(dos, bf);
            return;
        }

        // update bloom filter and create a list of IndexInfo objects marking the first and last column
        // in each block of ColumnIndexSize
        List<IndexHelper.IndexInfo> indexList = new ArrayList<IndexHelper.IndexInfo>();
        long endPosition = 0, startPosition = -1;
        int indexSizeInBytes = 0;
        IColumn lastColumn = null, firstColumn = null;
        for (IColumn column : columns)
        {
            bf.add(column.name());

            if (firstColumn == null)
            {
                firstColumn = column;
                startPosition = endPosition;
            }
            endPosition += column.serializedSize();
            /* if we hit the column index size that we have to index after, go ahead and index it. */
            if (endPosition - startPosition >= DatabaseDescriptor.getColumnIndexSize())
            {
                IndexHelper.IndexInfo cIndexInfo = new IndexHelper.IndexInfo(firstColumn.name(), column.name(), startPosition, endPosition - startPosition);
                indexList.add(cIndexInfo);
                indexSizeInBytes += cIndexInfo.serializedSize();
                firstColumn = null;
            }

            lastColumn = column;
        }

        // all columns were GC'd after all
        if (lastColumn == null)
        {
            writeEmptyHeader(dos, bf);
            return;
        }

        // the last column may have fallen on an index boundary already.  if not, index it explicitly.
        if (indexList.isEmpty() || columns.getComparator().compare(indexList.get(indexList.size() - 1).lastName, lastColumn.name()) != 0)
        {
            IndexHelper.IndexInfo cIndexInfo = new IndexHelper.IndexInfo(firstColumn.name(), lastColumn.name(), startPosition, endPosition - startPosition);
            indexList.add(cIndexInfo);
            indexSizeInBytes += cIndexInfo.serializedSize();
        }

        /* Write out the bloom filter. */
        writeBloomFilter(dos, bf);

        // write the index.  we should always have at least one computed index block, but we only write it out if there is more than that.
        assert indexSizeInBytes > 0;
        if (indexList.size() > 1)
        {
            dos.writeInt(indexSizeInBytes);
            for (IndexHelper.IndexInfo cIndexInfo : indexList)
            {
                cIndexInfo.serialize(dos);
            }
        }
        else
        {
            dos.writeInt(0);
        }
	}

+++++++++++++++++++++++
    public static void serializeInternal(IIterableColumns columns, DataOutput dos) throws IOException
    {
        int columnCount = columns.getEstimatedColumnCount();

        BloomFilter bf = BloomFilter.getFilter(columnCount, 4);

        if (columnCount == 0)
        {
            writeEmptyHeader(dos, bf);
            return;
        }

        // update bloom filter and create a list of IndexInfo objects marking the first and last column
        // in each block of ColumnIndexSize
        List<IndexHelper.IndexInfo> indexList = new ArrayList<IndexHelper.IndexInfo>();
        int endPosition = 0, startPosition = -1;
        int indexSizeInBytes = 0;
        IColumn lastColumn = null, firstColumn = null;
        for (IColumn column : columns)
        {
            bf.add(column.name());

            if (firstColumn == null)
            {
                firstColumn = column;
                startPosition = endPosition;
            }
            endPosition += column.serializedSize();
            /* if we hit the column index size that we have to index after, go ahead and index it. */
            if (endPosition - startPosition >= DatabaseDescriptor.getColumnIndexSize())
            {
                IndexHelper.IndexInfo cIndexInfo = new IndexHelper.IndexInfo(firstColumn.name(), column.name(), startPosition, endPosition - startPosition);
                indexList.add(cIndexInfo);
                indexSizeInBytes += cIndexInfo.serializedSize();
                firstColumn = null;
            }

            lastColumn = column;
        }

        // all columns were GC'd after all
        if (lastColumn == null)
        {
            writeEmptyHeader(dos, bf);
            return;
        }

        // the last column may have fallen on an index boundary already.  if not, index it explicitly.
        if (indexList.isEmpty() || columns.getComparator().compare(indexList.get(indexList.size() - 1).lastName, lastColumn.name()) != 0)
        {
            IndexHelper.IndexInfo cIndexInfo = new IndexHelper.IndexInfo(firstColumn.name(), lastColumn.name(), startPosition, endPosition - startPosition);
            indexList.add(cIndexInfo);
            indexSizeInBytes += cIndexInfo.serializedSize();
        }

        /* Write out the bloom filter. */
        writeBloomFilter(dos, bf);

        // write the index.  we should always have at least one computed index block, but we only write it out if there is more than that.
        assert indexSizeInBytes > 0;
        if (indexList.size() > 1)
        {
            dos.writeInt(indexSizeInBytes);
            for (IndexHelper.IndexInfo cIndexInfo : indexList)
            {
                cIndexInfo.serialize(dos);
            }
        }
        else
        {
            dos.writeInt(0);
        }
	}

    public void serialize(IColumn column, DataOutput dos)
    {
        assert column.name().remaining() > 0;
        ByteBufferUtil.writeWithShortLength(column.name(), dos);
        try
        {
            if (column instanceof ExpiringColumn) {
              dos.writeByte(EXPIRATION_MASK);
              dos.writeInt(((ExpiringColumn) column).getTimeToLive());
              dos.writeInt(column.getLocalDeletionTime());
            } else {
              dos.writeByte((column.isMarkedForDelete()) ? DELETION_MASK : 0);
            }
            dos.writeLong(column.timestamp());
            ByteBufferUtil.writeWithLength(column.value(), dos);
        }
        catch (IOException e)
        {
            throw new RuntimeException(e);
        }
    }

+++++++++++++++++++++++
    public void serialize(IColumn column, DataOutput dos)
    {
        assert column.name().remaining() > 0;
        ByteBufferUtil.writeWithShortLength(column.name(), dos);
        try
        {
            dos.writeByte(column.serializationFlags());
            if (column instanceof CounterColumn)
            {
                dos.writeLong(((CounterColumn)column).timestampOfLastDelete());
            }
            else if (column instanceof ExpiringColumn)
            {
                dos.writeInt(((ExpiringColumn) column).getTimeToLive());
                dos.writeInt(column.getLocalDeletionTime());
            }
            dos.writeLong(column.timestamp());
            ByteBufferUtil.writeWithLength(column.value(), dos);
        }
        catch (IOException e)
        {
            throw new RuntimeException(e);
        }
    }

    public Column deserialize(DataInput dis, int expireBefore) throws IOException
    {
        ByteBuffer name = ByteBufferUtil.readWithShortLength(dis);
        if (name.remaining() <= 0)
            throw new CorruptColumnException("invalid column name length " + name.remaining());

        int b = dis.readUnsignedByte();
        if ((b & EXPIRATION_MASK) != 0)
        {
            int ttl = dis.readInt();
            int expiration = dis.readInt();
            long ts = dis.readLong();
            ByteBuffer value = ByteBufferUtil.readWithLength(dis);
            if (expiration < expireBefore)
            {
                // the column is now expired, we can safely return a simple
                // tombstone
                ByteBuffer bytes = ByteBuffer.allocate(4);
                bytes.putInt(expiration);
                bytes.rewind();
                return new DeletedColumn(name, bytes, ts);
            }
            else
            {
                return new ExpiringColumn(name, value, ts, ttl, expiration);
            }
        }
        else
        {
            long ts = dis.readLong();
            ByteBuffer value = ByteBufferUtil.readWithLength(dis);
            return (b & DELETION_MASK) == 0
                   ? new Column(name, value, ts)
                   : new DeletedColumn(name, value, ts);
        }
    }

+++++++++++++++++++++++
    public Column deserialize(DataInput dis, ColumnFamilyStore interner, boolean fromRemote, int expireBefore) throws IOException
    {
        ByteBuffer name = ByteBufferUtil.readWithShortLength(dis);
        if (name.remaining() <= 0)
            throw new CorruptColumnException("invalid column name length " + name.remaining());
        if (interner != null)
            name = interner.maybeIntern(name);

        int b = dis.readUnsignedByte();
        if ((b & COUNTER_MASK) != 0)
        {
            long timestampOfLastDelete = dis.readLong();
            long ts = dis.readLong();
            ByteBuffer value = ByteBufferUtil.readWithLength(dis);
            if (fromRemote)
                value = CounterContext.instance().clearAllDelta(value);
            return new CounterColumn(name, value, ts, timestampOfLastDelete);
        }
        else if ((b & EXPIRATION_MASK) != 0)
        {
            int ttl = dis.readInt();
            int expiration = dis.readInt();
            long ts = dis.readLong();
            ByteBuffer value = ByteBufferUtil.readWithLength(dis);
            if (expiration < expireBefore)
            {
                // the column is now expired, we can safely return a simple
                // tombstone
                ByteBuffer bytes = ByteBuffer.allocate(4);
                bytes.putInt(expiration);
                bytes.rewind();
                return new DeletedColumn(name, bytes, ts);
            }
            else
            {
                return new ExpiringColumn(name, value, ts, ttl, expiration);
            }
        }
        else
        {
            long ts = dis.readLong();
            ByteBuffer value = ByteBufferUtil.readWithLength(dis);
            return (b & COUNTER_UPDATE_MASK) != 0
                   ? new CounterUpdateColumn(name, value, ts)
                   : ((b & DELETION_MASK) == 0
                      ? new Column(name, value, ts)
                      : new DeletedColumn(name, value, ts));
        }
    }

    public void serialize(RangeSliceCommand sliceCommand, DataOutputStream dos) throws IOException
    {
        dos.writeUTF(sliceCommand.keyspace);
        dos.writeUTF(sliceCommand.column_family);
        ByteBuffer sc = sliceCommand.super_column;
        dos.writeInt(sc == null ? 0 : sc.remaining());
        if (sc != null)
            ByteBufferUtil.write(sc, dos);

        TSerializer ser = new TSerializer(new TBinaryProtocol.Factory());
        FBUtilities.serialize(ser, sliceCommand.predicate, dos);
        AbstractBounds.serializer().serialize(sliceCommand.range, dos);
        dos.writeInt(sliceCommand.max_keys);
    }

+++++++++++++++++++++++
    public void serialize(RangeSliceCommand sliceCommand, DataOutputStream dos, int version) throws IOException
    {
        dos.writeUTF(sliceCommand.keyspace);
        dos.writeUTF(sliceCommand.column_family);
        ByteBuffer sc = sliceCommand.super_column;
        dos.writeInt(sc == null ? 0 : sc.remaining());
        if (sc != null)
            ByteBufferUtil.write(sc, dos);

        TSerializer ser = new TSerializer(new TBinaryProtocol.Factory());
        FBUtilities.serialize(ser, sliceCommand.predicate, dos);
        AbstractBounds.serializer().serialize(sliceCommand.range, dos);
        dos.writeInt(sliceCommand.max_keys);
    }

    public RangeSliceCommand deserialize(DataInputStream dis) throws IOException
    {
        String keyspace = dis.readUTF();
        String column_family = dis.readUTF();

        int scLength = dis.readInt();
        ByteBuffer super_column = null;
        if (scLength > 0)
            super_column = ByteBuffer.wrap(readBuf(scLength, dis));

        TDeserializer dser = new TDeserializer(new TBinaryProtocol.Factory());
        SlicePredicate pred = new SlicePredicate();
        FBUtilities.deserialize(dser, pred, dis);

        AbstractBounds range = AbstractBounds.serializer().deserialize(dis);
        int max_keys = dis.readInt();
        return new RangeSliceCommand(keyspace, column_family, super_column, pred, range, max_keys);
    }

+++++++++++++++++++++++
    public RangeSliceCommand deserialize(DataInputStream dis, int version) throws IOException
    {
        String keyspace = dis.readUTF();
        String column_family = dis.readUTF();

        int scLength = dis.readInt();
        ByteBuffer super_column = null;
        if (scLength > 0)
            super_column = ByteBuffer.wrap(readBuf(scLength, dis));

        TDeserializer dser = new TDeserializer(new TBinaryProtocol.Factory());
        SlicePredicate pred = new SlicePredicate();
        FBUtilities.deserialize(dser, pred, dis);

        AbstractBounds range = AbstractBounds.serializer().deserialize(dis);
        int max_keys = dis.readInt();
        return new RangeSliceCommand(keyspace, column_family, super_column, pred, range, max_keys);
    }

    public void serialize(Row row, DataOutputStream dos) throws IOException
    {
        ByteBufferUtil.writeWithShortLength(row.key.key, dos);
        ColumnFamily.serializer().serialize(row.cf, dos);
    }

+++++++++++++++++++++++
    public void serialize(Row row, DataOutputStream dos, int version) throws IOException
    {
        ByteBufferUtil.writeWithShortLength(row.key.key, dos);
        ColumnFamily.serializer().serialize(row.cf, dos);
    }

    public Row deserialize(DataInputStream dis) throws IOException
    {
        return new Row(StorageService.getPartitioner().decorateKey(ByteBufferUtil.readWithShortLength(dis)),
                       ColumnFamily.serializer().deserialize(dis));
+++++++++++++++++++++++
    public Row deserialize(DataInputStream dis, int version) throws IOException
    {
        return deserialize(dis, version, false);
    static RowSerializer serializer()
    {
        return serializer;
    }

+++++++++++++++++++++++
    public static RowSerializer serializer()
    {
        return serializer;
    }

    public void serialize(ReadCommand rm, DataOutputStream dos) throws IOException
    {
        SliceByNamesReadCommand realRM = (SliceByNamesReadCommand)rm;
        dos.writeBoolean(realRM.isDigestQuery());
        dos.writeUTF(realRM.table);
        ByteBufferUtil.writeWithShortLength(realRM.key, dos);
        realRM.queryPath.serialize(dos);
        dos.writeInt(realRM.columnNames.size());
        if (realRM.columnNames.size() > 0)
        {
            for (ByteBuffer cName : realRM.columnNames)
            {
                ByteBufferUtil.writeWithShortLength(cName, dos);
            }
        }
    }

    @Override
+++++++++++++++++++++++
    public void serialize(ReadCommand rm, DataOutputStream dos, int version) throws IOException
    {
        SliceByNamesReadCommand realRM = (SliceByNamesReadCommand)rm;
        dos.writeBoolean(realRM.isDigestQuery());
        dos.writeUTF(realRM.table);
        ByteBufferUtil.writeWithShortLength(realRM.key, dos);
        realRM.queryPath.serialize(dos);
        dos.writeInt(realRM.columnNames.size());
        if (realRM.columnNames.size() > 0)
        {
            for (ByteBuffer cName : realRM.columnNames)
            {
                ByteBufferUtil.writeWithShortLength(cName, dos);
            }
        }
    }

    @Override
    public ReadCommand deserialize(DataInputStream dis) throws IOException
    {
        boolean isDigest = dis.readBoolean();
        String table = dis.readUTF();
        ByteBuffer key = ByteBufferUtil.readWithShortLength(dis);
        QueryPath columnParent = QueryPath.deserialize(dis);

        int size = dis.readInt();
        List<ByteBuffer> columns = new ArrayList<ByteBuffer>();
        for (int i = 0; i < size; ++i)
        {
            columns.add(ByteBufferUtil.readWithShortLength(dis));
        }
        SliceByNamesReadCommand rm = new SliceByNamesReadCommand(table, key, columnParent, columns);
        rm.setDigestQuery(isDigest);
        return rm;
+++++++++++++++++++++++
    public ReadCommand deserialize(DataInputStream dis, int version) throws IOException
    {
        boolean isDigest = dis.readBoolean();
        String table = dis.readUTF();
        ByteBuffer key = ByteBufferUtil.readWithShortLength(dis);
        QueryPath columnParent = QueryPath.deserialize(dis);

        int size = dis.readInt();
        List<ByteBuffer> columns = new ArrayList<ByteBuffer>();
        for (int i = 0; i < size; ++i)
        {
            columns.add(ByteBufferUtil.readWithShortLength(dis));
        }
        SliceByNamesReadCommand rm = new SliceByNamesReadCommand(table, key, columnParent, columns);
        rm.setDigestQuery(isDigest);
        return rm;
    public void serialize(ReadCommand rm, DataOutputStream dos) throws IOException
    {
        SliceFromReadCommand realRM = (SliceFromReadCommand)rm;
        dos.writeBoolean(realRM.isDigestQuery());
        dos.writeUTF(realRM.table);
        ByteBufferUtil.writeWithShortLength(realRM.key, dos);
        realRM.queryPath.serialize(dos);
        ByteBufferUtil.writeWithShortLength(realRM.start, dos);
        ByteBufferUtil.writeWithShortLength(realRM.finish, dos);
        dos.writeBoolean(realRM.reversed);
        dos.writeInt(realRM.count);
    }

    @Override
+++++++++++++++++++++++
    public void serialize(ReadCommand rm, DataOutputStream dos, int version) throws IOException
    {
        SliceFromReadCommand realRM = (SliceFromReadCommand)rm;
        dos.writeBoolean(realRM.isDigestQuery());
        dos.writeUTF(realRM.table);
        ByteBufferUtil.writeWithShortLength(realRM.key, dos);
        realRM.queryPath.serialize(dos);
        ByteBufferUtil.writeWithShortLength(realRM.start, dos);
        ByteBufferUtil.writeWithShortLength(realRM.finish, dos);
        dos.writeBoolean(realRM.reversed);
        dos.writeInt(realRM.count);
    }

    @Override
    public ReadCommand deserialize(DataInputStream dis) throws IOException
    {
        boolean isDigest = dis.readBoolean();
        SliceFromReadCommand rm = new SliceFromReadCommand(dis.readUTF(),
                                                           ByteBufferUtil.readWithShortLength(dis),
                                                           QueryPath.deserialize(dis),
                                                           ByteBufferUtil.readWithShortLength(dis),
                                                           ByteBufferUtil.readWithShortLength(dis),
                                                           dis.readBoolean(),
                                                           dis.readInt());
        rm.setDigestQuery(isDigest);
        return rm;
+++++++++++++++++++++++
    public ReadCommand deserialize(DataInputStream dis, int version) throws IOException
    {
        boolean isDigest = dis.readBoolean();
        SliceFromReadCommand rm = new SliceFromReadCommand(dis.readUTF(),
                                                           ByteBufferUtil.readWithShortLength(dis),
                                                           QueryPath.deserialize(dis),
                                                           ByteBufferUtil.readWithShortLength(dis),
                                                           ByteBufferUtil.readWithShortLength(dis),
                                                           dis.readBoolean(),
                                                           dis.readInt());
        rm.setDigestQuery(isDigest);
        return rm;
    public static SuperColumnSerializer serializer(AbstractType comparator)
    {
        SuperColumnSerializer serializer = serializers.get(comparator);
        if (serializer == null)
        {
            serializer = new SuperColumnSerializer(comparator);
            serializers.put(comparator, serializer);
        }
        return serializer;
    }

+++++++++++++++++++++++
    public static SuperColumnSerializer serializer(AbstractType comparator)
    {
        return new SuperColumnSerializer(comparator);
    }

    public IColumn deserialize(DataInput dis, int expireBefore) throws IOException
    {
        ByteBuffer name = ByteBufferUtil.readWithShortLength(dis);
        int localDeleteTime = dis.readInt();
        if (localDeleteTime != Integer.MIN_VALUE && localDeleteTime <= 0)
        {
            throw new IOException("Invalid localDeleteTime read: " + localDeleteTime);
        }
        long markedForDeleteAt = dis.readLong();

        /* read the number of columns */
        int size = dis.readInt();
        ColumnSerializer serializer = Column.serializer();
        ColumnSortedMap preSortedMap = new ColumnSortedMap(comparator, serializer, dis, size, expireBefore);
        SuperColumn superColumn = new SuperColumn(name, new ConcurrentSkipListMap<ByteBuffer,IColumn>(preSortedMap));
        if (localDeleteTime != Integer.MIN_VALUE && localDeleteTime <= 0)
        {
            throw new IOException("Invalid localDeleteTime read: " + localDeleteTime);
        }
        superColumn.markForDeleteAt(localDeleteTime, markedForDeleteAt);
        return superColumn;
+++++++++++++++++++++++
    public IColumn deserialize(DataInput dis, ColumnFamilyStore interner, boolean fromRemote, int expireBefore) throws IOException
    {
        ByteBuffer name = ByteBufferUtil.readWithShortLength(dis);
        int localDeleteTime = dis.readInt();
        if (localDeleteTime != Integer.MIN_VALUE && localDeleteTime <= 0)
        {
            throw new IOException("Invalid localDeleteTime read: " + localDeleteTime);
        }
        long markedForDeleteAt = dis.readLong();

        /* read the number of columns */
        int size = dis.readInt();
        ColumnSerializer serializer = Column.serializer();
        ColumnSortedMap preSortedMap = new ColumnSortedMap(comparator, serializer, dis, interner, size, fromRemote, expireBefore);
        SuperColumn superColumn = new SuperColumn(name, new ConcurrentSkipListMap<ByteBuffer,IColumn>(preSortedMap));
        if (localDeleteTime != Integer.MIN_VALUE && localDeleteTime <= 0)
        {
            throw new IOException("Invalid localDeleteTime read: " + localDeleteTime);
        }
        superColumn.markForDeleteAt(localDeleteTime, markedForDeleteAt);
        return superColumn;
    public ByteBuffer serialize() throws IOException
    {
        // super deflate
        org.apache.cassandra.db.migration.avro.Migration mi = new org.apache.cassandra.db.migration.avro.Migration();
        mi.old_version = new org.apache.cassandra.utils.avro.UUID();
        mi.old_version.bytes(UUIDGen.decompose(lastVersion));
        mi.new_version = new org.apache.cassandra.utils.avro.UUID();
        mi.new_version.bytes(UUIDGen.decompose(newVersion));
        mi.classname = new org.apache.avro.util.Utf8(this.getClass().getName());
        // TODO: Avro RowMutation serialization?
        DataOutputBuffer dob = new DataOutputBuffer();
        try
        {
            RowMutation.serializer().serialize(rm, dob);
        }
        catch (IOException e)
        {
            throw new RuntimeException(e);
        }
        mi.row_mutation = ByteBuffer.wrap(dob.asByteArray());

        // sub deflate
        this.subdeflate(mi);

        // serialize
        return SerDeUtils.serializeWithSchema(mi);
    }

+++++++++++++++++++++++
    public ByteBuffer serialize() throws IOException
    {
        // super deflate
        org.apache.cassandra.db.migration.avro.Migration mi = new org.apache.cassandra.db.migration.avro.Migration();
        mi.old_version = new org.apache.cassandra.utils.avro.UUID();
        mi.old_version.bytes(UUIDGen.decompose(lastVersion));
        mi.new_version = new org.apache.cassandra.utils.avro.UUID();
        mi.new_version.bytes(UUIDGen.decompose(newVersion));
        mi.classname = new org.apache.avro.util.Utf8(this.getClass().getName());
        // TODO: Avro RowMutation serialization?
        DataOutputBuffer dob = new DataOutputBuffer();
        try
        {
            RowMutation.serializer().serialize(rm, dob, MessagingService.version_);
        }
        catch (IOException e)
        {
            throw new RuntimeException(e);
        }
        mi.row_mutation = ByteBuffer.wrap(dob.asByteArray());

        // sub deflate
        this.subdeflate(mi);

        // serialize
        return SerDeUtils.serializeWithSchema(mi);
    }

    public static Migration deserialize(ByteBuffer bytes) throws IOException
    {
        // deserialize
        org.apache.cassandra.db.migration.avro.Migration mi = SerDeUtils.deserializeWithSchema(bytes, new org.apache.cassandra.db.migration.avro.Migration());

        // create an instance of the migration subclass
        Migration migration;
        try
        {
            Class migrationClass = Class.forName(mi.classname.toString());
            Constructor migrationConstructor = migrationClass.getDeclaredConstructor();
            migrationConstructor.setAccessible(true);
            migration = (Migration)migrationConstructor.newInstance();
        }
        catch (Exception e)
        {
            throw new RuntimeException("Invalid migration class: " + mi.classname.toString(), e);
        }
        
        // super inflate
        migration.lastVersion = UUIDGen.getUUID(ByteBuffer.wrap(mi.old_version.bytes()));
        migration.newVersion = UUIDGen.getUUID(ByteBuffer.wrap(mi.new_version.bytes()));
        try
        {
            migration.rm = RowMutation.serializer().deserialize(SerDeUtils.createDataInputStream(mi.row_mutation));
        }
        catch (IOException e)
        {
            throw new RuntimeException(e);
        }
        // sub inflate
        migration.subinflate(mi);
        return migration;
    }
    
    /** load serialized migrations. */
+++++++++++++++++++++++
    public static Migration deserialize(ByteBuffer bytes, int version) throws IOException
    {
        // deserialize
        org.apache.cassandra.db.migration.avro.Migration mi = SerDeUtils.deserializeWithSchema(bytes, new org.apache.cassandra.db.migration.avro.Migration());

        // create an instance of the migration subclass
        Migration migration;
        try
        {
            Class migrationClass = Class.forName(mi.classname.toString());
            Constructor migrationConstructor = migrationClass.getDeclaredConstructor();
            migrationConstructor.setAccessible(true);
            migration = (Migration)migrationConstructor.newInstance();
        }
        catch (Exception e)
        {
            throw new RuntimeException("Invalid migration class: " + mi.classname.toString(), e);
        }
        
        // super inflate
        migration.lastVersion = UUIDGen.getUUID(ByteBuffer.wrap(mi.old_version.bytes()));
        migration.newVersion = UUIDGen.getUUID(ByteBuffer.wrap(mi.new_version.bytes()));
        try
        {
            migration.rm = RowMutation.serializer().deserialize(SerDeUtils.createDataInputStream(mi.row_mutation), version);
        }
        catch (IOException e)
        {
            throw new RuntimeException(e);
        }
        // sub inflate
        migration.subinflate(mi);
        return migration;
    }
    
    /** load serialized migrations. */
    public static ICompactSerializer<EndpointState> serializer()
    {
        return serializer_;
    }
    
+++++++++++++++++++++++
    public static ICompactSerializer<EndpointState> serializer()
    {
        return serializer;
    }
    
    public void serialize(EndpointState epState, DataOutputStream dos) throws IOException
    {
        /* serialize the HeartBeatState */
        HeartBeatState hbState = epState.getHeartBeatState();
        HeartBeatState.serializer().serialize(hbState, dos);

        /* serialize the map of ApplicationState objects */
        int size = epState.applicationState_.size();
        dos.writeInt(size);
        for (Map.Entry<ApplicationState, VersionedValue> entry : epState.applicationState_.entrySet())
        {
            VersionedValue value = entry.getValue();
            if (value != null)
            {
                dos.writeInt(entry.getKey().ordinal());
                VersionedValue.serializer.serialize(value, dos);
            }
        }
    }

+++++++++++++++++++++++
    public void serialize(EndpointState epState, DataOutputStream dos, int version) throws IOException
    {
        /* serialize the HeartBeatState */
        HeartBeatState hbState = epState.getHeartBeatState();
        HeartBeatState.serializer().serialize(hbState, dos, version);

        /* serialize the map of ApplicationState objects */
        int size = epState.applicationState.size();
        dos.writeInt(size);
        for (Map.Entry<ApplicationState, VersionedValue> entry : epState.applicationState.entrySet())
        {
            VersionedValue value = entry.getValue();
            if (value != null)
            {
                dos.writeInt(entry.getKey().ordinal());
                VersionedValue.serializer.serialize(value, dos, version);
            }
        }
    }

    public EndpointState deserialize(DataInputStream dis) throws IOException
    {
        HeartBeatState hbState = HeartBeatState.serializer().deserialize(dis);
        EndpointState epState = new EndpointState(hbState);

        int appStateSize = dis.readInt();
        for ( int i = 0; i < appStateSize; ++i )
        {
            if ( dis.available() == 0 )
            {
                break;
            }

            int key = dis.readInt();
            VersionedValue value = VersionedValue.serializer.deserialize(dis);
            epState.addApplicationState(Gossiper.STATES[key], value);
        }
        return epState;
+++++++++++++++++++++++
    public EndpointState deserialize(DataInputStream dis, int version) throws IOException
    {
        HeartBeatState hbState = HeartBeatState.serializer().deserialize(dis, version);
        EndpointState epState = new EndpointState(hbState);

        int appStateSize = dis.readInt();
        for ( int i = 0; i < appStateSize; ++i )
        {
            if ( dis.available() == 0 )
            {
                break;
            }

            int key = dis.readInt();
            VersionedValue value = VersionedValue.serializer.deserialize(dis, version);
            epState.addApplicationState(Gossiper.STATES[key], value);
        }
        return epState;
    private IColumn deserializeNext()
    {
        try
        {
            count++;
            return serializer.deserialize(dis, expireBefore);
        }
        catch (IOException e)
        {
            throw new IOError(e);
        }
    }

+++++++++++++++++++++++
    private IColumn deserializeNext()
    {
        try
        {
            count++;
            return serializer.deserialize(dis, interner, fromRemote, expireBefore);
        }
        catch (IOException e)
        {
            throw new IOError(e);
        }
    }

    public static ICompactSerializer<BloomFilter> serializer()
    {
        return serializer_;
    }

+++++++++++++++++++++++
    public static ICompactSerializer2<BloomFilter> serializer()
    {
        return serializer_;
    }

    public void serialize(BloomFilter bf, DataOutputStream dos) throws IOException
    {
        long[] bits = bf.bitset.getBits();
        int bitLength = bits.length;

        dos.writeInt(bf.getHashCount());
        dos.writeInt(bitLength);

        for (int i = 0; i < bitLength; i++)
            dos.writeLong(bits[i]);
        dos.flush();
    }

+++++++++++++++++++++++
    public void serialize(BloomFilter bf, DataOutput dos) throws IOException
    {
        long[] bits = bf.bitset.getBits();
        int bitLength = bits.length;

        dos.writeInt(bf.getHashCount());
        dos.writeInt(bitLength);

        for (int i = 0; i < bitLength; i++)
            dos.writeLong(bits[i]);
    }

    public BloomFilter deserialize(DataInputStream dis) throws IOException
    {
        int hashes = dis.readInt();
        int bitLength = dis.readInt();
        long[] bits = new long[bitLength];
        for (int i = 0; i < bitLength; i++)
            bits[i] = dis.readLong();
        OpenBitSet bs = new OpenBitSet(bits, bitLength);
        return new BloomFilter(hashes, bs);
+++++++++++++++++++++++
    public BloomFilter deserialize(DataInput dis) throws IOException
    {
        int hashes = dis.readInt();
        int bitLength = dis.readInt();
        long[] bits = new long[bitLength];
        for (int i = 0; i < bitLength; i++)
            bits[i] = dis.readLong();
        OpenBitSet bs = new OpenBitSet(bits, bitLength);
        return new BloomFilter(hashes, bs);
    ICompactSerializer tserializer()
    {
        return serializer_;
    }

+++++++++++++++++++++++
    ICompactSerializer<LegacyBloomFilter> tserializer()
    {
        return serializer_;
    }

    public void serialize(LegacyBloomFilter bf, DataOutputStream dos)
            throws IOException
    {
        dos.writeInt(bf.getHashCount());
        ObjectOutputStream oos = new ObjectOutputStream(dos);
        oos.writeObject(bf.getBitSet());
        oos.flush();
    }

+++++++++++++++++++++++
    public void serialize(LegacyBloomFilter bf, DataOutputStream dos, int version)
            throws IOException
    {
        throw new UnsupportedOperationException("Shouldn't be serializing legacy bloom filters");
//        dos.writeInt(bf.getHashCount());
//        ObjectOutputStream oos = new ObjectOutputStream(dos);
//        oos.writeObject(bf.getBitSet());
//        oos.flush();
    }

    public LegacyBloomFilter deserialize(DataInputStream dis) throws IOException
    {
        int hashes = dis.readInt();
        ObjectInputStream ois = new ObjectInputStream(dis);
        try
        {
          BitSet bs = (BitSet) ois.readObject();
          return new LegacyBloomFilter(hashes, bs);
        } catch (ClassNotFoundException e)
        {
          throw new RuntimeException(e);
        }
+++++++++++++++++++++++
    public LegacyBloomFilter deserialize(DataInputStream dis, int version) throws IOException
    {
        int hashes = dis.readInt();
        ObjectInputStream ois = new ObjectInputStream(dis);
        try
        {
          BitSet bs = (BitSet) ois.readObject();
          return new LegacyBloomFilter(hashes, bs);
        } catch (ClassNotFoundException e)
        {
          throw new RuntimeException(e);
        }
    private ReadCommand serializeAndDeserializeReadMessage(ReadCommand rm) throws IOException
    {
        ReadCommandSerializer rms = ReadCommand.serializer();
        DataOutputBuffer dos = new DataOutputBuffer();
        ByteArrayInputStream bis;

        rms.serialize(rm, dos);
        bis = new ByteArrayInputStream(dos.getData(), 0, dos.getLength());
        return rms.deserialize(new DataInputStream(bis));
    }
    
    @Test
+++++++++++++++++++++++
    private ReadCommand serializeAndDeserializeReadMessage(ReadCommand rm) throws IOException
    {
        ReadCommandSerializer rms = ReadCommand.serializer();
        DataOutputBuffer dos = new DataOutputBuffer();
        ByteArrayInputStream bis;

        rms.serialize(rm, dos, MessagingService.version_);
        bis = new ByteArrayInputStream(dos.getData(), 0, dos.getLength());
        return rms.deserialize(new DataInputStream(bis), MessagingService.version_);
    }
    
    @Test
add: 114, delete: 42, change: 283, unhandled: 0 size_exceptions: 29 size_serialize: 30
-----------cassandra-0.8.0 vs cassandra-0.8.0-beta1-----------
add: 6, delete: 5, change: 146, unhandled: 0 size_exceptions: 1 size_serialize: 0
-----------cassandra-0.8.0-beta1 vs cassandra-0.8.0-beta2-----------
add: 0, delete: 0, change: 70, unhandled: 0 size_exceptions: 0 size_serialize: 0
-----------cassandra-0.8.0-beta2 vs cassandra-0.8.0-rc1-----------
add: 4, delete: 4, change: 68, unhandled: 0 size_exceptions: 1 size_serialize: 0
-----------cassandra-0.8.0-rc1 vs cassandra-0.8.1-----------
    public static ColumnSerializer serializer()
    {
        return new ColumnSerializer();
    }

+++++++++++++++++++++++
    public static ColumnSerializer serializer()
    {
        return serializer;
    }

    public static SuperColumnSerializer serializer(AbstractType comparator)
    {
        return new SuperColumnSerializer(comparator);
    }

+++++++++++++++++++++++
    public static SuperColumnSerializer serializer(AbstractType comparator)
    {
        SuperColumnSerializer serializer = serializers.get(comparator);
        if (serializer == null)
        {
            serializer = new SuperColumnSerializer(comparator);
            serializers.put(comparator, serializer);
        }
        return serializer;
    }

add: 40, delete: 36, change: 155, unhandled: 0 size_exceptions: 3 size_serialize: 2
-----------cassandra-0.8.1 vs cassandra-0.8.2-----------
add: 3, delete: 0, change: 69, unhandled: 0 size_exceptions: 3 size_serialize: 0
-----------cassandra-0.8.2 vs cassandra-0.8.3-----------
add: 6, delete: 0, change: 46, unhandled: 0 size_exceptions: 0 size_serialize: 0
-----------cassandra-0.8.3 vs cassandra-0.8.4-----------
add: 0, delete: 0, change: 26, unhandled: 0 size_exceptions: 0 size_serialize: 0
-----------cassandra-0.8.4 vs cassandra-0.8.5-----------
add: 2, delete: 0, change: 55, unhandled: 0 size_exceptions: 5 size_serialize: 0
-----------cassandra-0.8.5 vs cassandra-0.8.6-----------
add: 0, delete: 0, change: 53, unhandled: 0 size_exceptions: 0 size_serialize: 0
-----------cassandra-0.8.6 vs cassandra-0.8.7-----------
add: 1, delete: 0, change: 26, unhandled: 0 size_exceptions: 0 size_serialize: 0
-----------cassandra-0.8.7 vs cassandra-0.8.8-----------
    public static void serializeInternal(IIterableColumns columns, DataOutput dos) throws IOException
    {
        int columnCount = columns.getEstimatedColumnCount();

        BloomFilter bf = BloomFilter.getFilter(columnCount, 4);

        if (columnCount == 0)
        {
            writeEmptyHeader(dos, bf);
            return;
        }

        // update bloom filter and create a list of IndexInfo objects marking the first and last column
        // in each block of ColumnIndexSize
        List<IndexHelper.IndexInfo> indexList = new ArrayList<IndexHelper.IndexInfo>();
        int endPosition = 0, startPosition = -1;
        int indexSizeInBytes = 0;
        IColumn lastColumn = null, firstColumn = null;
        for (IColumn column : columns)
        {
            bf.add(column.name());

            if (firstColumn == null)
            {
                firstColumn = column;
                startPosition = endPosition;
            }
            endPosition += column.serializedSize();
            /* if we hit the column index size that we have to index after, go ahead and index it. */
            if (endPosition - startPosition >= DatabaseDescriptor.getColumnIndexSize())
            {
                IndexHelper.IndexInfo cIndexInfo = new IndexHelper.IndexInfo(firstColumn.name(), column.name(), startPosition, endPosition - startPosition);
                indexList.add(cIndexInfo);
                indexSizeInBytes += cIndexInfo.serializedSize();
                firstColumn = null;
            }

            lastColumn = column;
        }

        // all columns were GC'd after all
        if (lastColumn == null)
        {
            writeEmptyHeader(dos, bf);
            return;
        }

        // the last column may have fallen on an index boundary already.  if not, index it explicitly.
        if (indexList.isEmpty() || columns.getComparator().compare(indexList.get(indexList.size() - 1).lastName, lastColumn.name()) != 0)
        {
            IndexHelper.IndexInfo cIndexInfo = new IndexHelper.IndexInfo(firstColumn.name(), lastColumn.name(), startPosition, endPosition - startPosition);
            indexList.add(cIndexInfo);
            indexSizeInBytes += cIndexInfo.serializedSize();
        }

        /* Write out the bloom filter. */
        writeBloomFilter(dos, bf);

        // write the index.  we should always have at least one computed index block, but we only write it out if there is more than that.
        assert indexSizeInBytes > 0;
        if (indexList.size() > 1)
        {
            dos.writeInt(indexSizeInBytes);
            for (IndexHelper.IndexInfo cIndexInfo : indexList)
            {
                cIndexInfo.serialize(dos);
            }
        }
        else
        {
            dos.writeInt(0);
        }
	}

+++++++++++++++++++++++
    public static void serializeInternal(IIterableColumns columns, DataOutput dos) throws IOException
    {
        int columnCount = columns.getEstimatedColumnCount();

        BloomFilter bf = BloomFilter.getFilter(columnCount, 4);

        if (columnCount == 0)
        {
            writeEmptyHeader(dos, bf);
            return;
        }

        // update bloom filter and create a list of IndexInfo objects marking the first and last column
        // in each block of ColumnIndexSize
        List<IndexHelper.IndexInfo> indexList = new ArrayList<IndexHelper.IndexInfo>();
        long endPosition = 0, startPosition = -1;
        int indexSizeInBytes = 0;
        IColumn lastColumn = null, firstColumn = null;
        for (IColumn column : columns)
        {
            bf.add(column.name());

            if (firstColumn == null)
            {
                firstColumn = column;
                startPosition = endPosition;
            }
            endPosition += column.serializedSize();
            /* if we hit the column index size that we have to index after, go ahead and index it. */
            if (endPosition - startPosition >= DatabaseDescriptor.getColumnIndexSize())
            {
                IndexHelper.IndexInfo cIndexInfo = new IndexHelper.IndexInfo(firstColumn.name(), column.name(), startPosition, endPosition - startPosition);
                indexList.add(cIndexInfo);
                indexSizeInBytes += cIndexInfo.serializedSize();
                firstColumn = null;
            }

            lastColumn = column;
        }

        // all columns were GC'd after all
        if (lastColumn == null)
        {
            writeEmptyHeader(dos, bf);
            return;
        }

        // the last column may have fallen on an index boundary already.  if not, index it explicitly.
        if (indexList.isEmpty() || columns.getComparator().compare(indexList.get(indexList.size() - 1).lastName, lastColumn.name()) != 0)
        {
            IndexHelper.IndexInfo cIndexInfo = new IndexHelper.IndexInfo(firstColumn.name(), lastColumn.name(), startPosition, endPosition - startPosition);
            indexList.add(cIndexInfo);
            indexSizeInBytes += cIndexInfo.serializedSize();
        }

        /* Write out the bloom filter. */
        writeBloomFilter(dos, bf);

        // write the index.  we should always have at least one computed index block, but we only write it out if there is more than that.
        assert indexSizeInBytes > 0;
        if (indexList.size() > 1)
        {
            dos.writeInt(indexSizeInBytes);
            for (IndexHelper.IndexInfo cIndexInfo : indexList)
            {
                cIndexInfo.serialize(dos);
            }
        }
        else
        {
            dos.writeInt(0);
        }
	}

    public IColumn deserialize(DataInput dis, ColumnFamilyStore interner, boolean fromRemote, int expireBefore) throws IOException
    {
        ByteBuffer name = ByteBufferUtil.readWithShortLength(dis);
        int localDeleteTime = dis.readInt();
        if (localDeleteTime != Integer.MIN_VALUE && localDeleteTime <= 0)
        {
            throw new IOException("Invalid localDeleteTime read: " + localDeleteTime);
        }
        long markedForDeleteAt = dis.readLong();

        /* read the number of columns */
        int size = dis.readInt();
        ColumnSerializer serializer = Column.serializer();
        ColumnSortedMap preSortedMap = new ColumnSortedMap(comparator, serializer, dis, interner, size, fromRemote, expireBefore);
        SuperColumn superColumn = new SuperColumn(name, new ConcurrentSkipListMap<ByteBuffer,IColumn>(preSortedMap));
        if (localDeleteTime != Integer.MIN_VALUE && localDeleteTime <= 0)
        {
            throw new IOException("Invalid localDeleteTime read: " + localDeleteTime);
        }
        superColumn.markForDeleteAt(localDeleteTime, markedForDeleteAt);
        return superColumn;
+++++++++++++++++++++++
    public IColumn deserialize(DataInput dis, ColumnFamilyStore interner, boolean fromRemote, int expireBefore) throws IOException
    {
        ByteBuffer name = ByteBufferUtil.readWithShortLength(dis);
        int localDeleteTime = dis.readInt();
        if (localDeleteTime != Integer.MIN_VALUE && localDeleteTime <= 0)
        {
            throw new IOException("Invalid localDeleteTime read: " + localDeleteTime);
        }
        long markedForDeleteAt = dis.readLong();

        /* read the number of columns */
        int size = dis.readInt();
        ColumnSerializer serializer = Column.serializer();
        ColumnSortedMap preSortedMap = new ColumnSortedMap(comparator, serializer, dis, interner, size, fromRemote, expireBefore);
        SuperColumn superColumn = new SuperColumn(name, new ConcurrentSkipListMap<ByteBuffer,IColumn>(preSortedMap));
        superColumn.markForDeleteAt(localDeleteTime, markedForDeleteAt);
        return superColumn;
add: 0, delete: 2, change: 54, unhandled: 0 size_exceptions: 1 size_serialize: 2
-----------cassandra-0.8.8 vs cassandra-0.8.9-----------
add: 0, delete: 0, change: 10, unhandled: 0 size_exceptions: 0 size_serialize: 0
-----------cassandra-0.8.9 vs cassandra-0.8.10-----------
add: 0, delete: 0, change: 14, unhandled: 0 size_exceptions: 0 size_serialize: 0
-----------cassandra-0.8.10 vs cassandra-1.0.0-----------
    public long serializedSize()
    {
        int size = boolSize_ // bool
                 + intSize_ // id
                 + intSize_ // local deletion time
                 + longSize_ // client deltion time
                 + intSize_; // column count
        for (IColumn column : columns.values())
            size += column.serializedSize();
        return size;
    }

    /**
     * Goes over all columns and check the fields are valid (as far as we can
     * tell).
     * This is used to detect corruption after deserialization.
     */
+++++++++++++++++++++++
    public long serializedSize()
    {
        return boolSize // nullness bool
               + intSize // id
               + serializedSizeForSSTable();
    }

    public void serializeCFInfo(ColumnFamily columnFamily, DataOutput dos) throws IOException
    {
        dos.writeInt(columnFamily.localDeletionTime.get());
        dos.writeLong(columnFamily.markedForDeleteAt.get());
    }

+++++++++++++++++++++++
    public void serializeCFInfo(ColumnFamily columnFamily, DataOutput dos) throws IOException
    {
        dos.writeInt(columnFamily.getLocalDeletionTime());
        dos.writeLong(columnFamily.getMarkedForDeleteAt());
    }

    public int serializeWithIndexes(ColumnFamily columnFamily, DataOutput dos)
    {
        ColumnIndexer.serialize(columnFamily, dos);
        return serializeForSSTable(columnFamily, dos);
    }

+++++++++++++++++++++++
    public int serializeWithIndexes(ColumnFamily columnFamily, ColumnIndexer.RowHeader index, DataOutput dos)
    {
        ColumnIndexer.serialize(index, dos);
        return serializeForSSTable(columnFamily, dos);
    }

    public ColumnFamily deserialize(DataInput dis, boolean intern, boolean fromRemote) throws IOException
    {
        if (!dis.readBoolean())
            return null;

        // create a ColumnFamily based on the cf id
        int cfId = dis.readInt();
        if (CFMetaData.getCF(cfId) == null)
            throw new UnserializableColumnFamilyException("Couldn't find cfId=" + cfId, cfId);
        ColumnFamily cf = ColumnFamily.create(cfId);
        deserializeFromSSTableNoColumns(cf, dis);
        deserializeColumns(dis, cf, intern, fromRemote);
        return cf;
    }

+++++++++++++++++++++++
    public ColumnFamily deserialize(DataInput dis, boolean fromRemote, ISortedColumns.Factory factory) throws IOException
    {
        if (!dis.readBoolean())
            return null;

        // create a ColumnFamily based on the cf id
        int cfId = dis.readInt();
        if (Schema.instance.getCF(cfId) == null)
            throw new UnserializableColumnFamilyException("Couldn't find cfId=" + cfId, cfId);
        ColumnFamily cf = ColumnFamily.create(cfId, factory);
        deserializeFromSSTableNoColumns(cf, dis);
        deserializeColumns(dis, cf, fromRemote);
        return cf;
    }

    public void deserializeColumns(DataInput dis, ColumnFamily cf, boolean intern, boolean fromRemote) throws IOException
    {
        int size = dis.readInt();
        ColumnFamilyStore interner = intern ? Table.open(CFMetaData.getCF(cf.id()).left).getColumnFamilyStore(cf.id()) : null;
        for (int i = 0; i < size; ++i)
        {
            IColumn column = cf.getColumnSerializer().deserialize(dis, interner, fromRemote, (int) (System.currentTimeMillis() / 1000));
            cf.addColumn(column);
        }
    }

+++++++++++++++++++++++
    public void deserializeColumns(DataInput dis, ColumnFamily cf, int size, boolean fromRemote) throws IOException
    {
        for (int i = 0; i < size; ++i)
        {
            IColumn column = cf.getColumnSerializer().deserialize(dis, fromRemote, (int) (System.currentTimeMillis() / 1000));
            cf.addColumn(column);
        }
    }

    public static void serialize(IIterableColumns columns, DataOutput dos)
    {
        try
        {
            serializeInternal(columns, dos);
        }
        catch (IOException e)
        {
            throw new IOError(e);
        }
    }

+++++++++++++++++++++++
    public static RowHeader serialize(IIterableColumns columns)
    {
        int columnCount = columns.getEstimatedColumnCount();

        BloomFilter bf = BloomFilter.getFilter(columnCount, 4);

        if (columnCount == 0)
            return new RowHeader(bf, Collections.<IndexHelper.IndexInfo>emptyList());

        // update bloom filter and create a list of IndexInfo objects marking the first and last column
        // in each block of ColumnIndexSize
        List<IndexHelper.IndexInfo> indexList = new ArrayList<IndexHelper.IndexInfo>();
        int endPosition = 0, startPosition = -1;
        IColumn lastColumn = null, firstColumn = null;

        for (IColumn column : columns)
        {
            bf.add(column.name());

            if (firstColumn == null)
            {
                firstColumn = column;
                startPosition = endPosition;
            }

            endPosition += column.serializedSize();

            // if we hit the column index size that we have to index after, go ahead and index it.
            if (endPosition - startPosition >= DatabaseDescriptor.getColumnIndexSize())
            {
                IndexHelper.IndexInfo cIndexInfo = new IndexHelper.IndexInfo(firstColumn.name(), column.name(), startPosition, endPosition - startPosition);
                indexList.add(cIndexInfo);
                firstColumn = null;
            }

            lastColumn = column;
        }

        // all columns were GC'd after all
        if (lastColumn == null)
            return new RowHeader(bf, Collections.<IndexHelper.IndexInfo>emptyList());

        // the last column may have fallen on an index boundary already.  if not, index it explicitly.
        if (indexList.isEmpty() || columns.getComparator().compare(indexList.get(indexList.size() - 1).lastName, lastColumn.name()) != 0)
        {
            IndexHelper.IndexInfo cIndexInfo = new IndexHelper.IndexInfo(firstColumn.name(), lastColumn.name(), startPosition, endPosition - startPosition);
            indexList.add(cIndexInfo);
        }

        // we should always have at least one computed index block, but we only write it out if there is more than that.
        assert indexList.size() > 0;
        return new RowHeader(bf, indexList);
    }

    public Column deserialize(DataInput dis, ColumnFamilyStore interner, boolean fromRemote, int expireBefore) throws IOException
    {
        ByteBuffer name = ByteBufferUtil.readWithShortLength(dis);
        if (name.remaining() <= 0)
            throw new CorruptColumnException("invalid column name length " + name.remaining());
        if (interner != null)
            name = interner.maybeIntern(name);

        int b = dis.readUnsignedByte();
        if ((b & COUNTER_MASK) != 0)
        {
            long timestampOfLastDelete = dis.readLong();
            long ts = dis.readLong();
            ByteBuffer value = ByteBufferUtil.readWithLength(dis);
            if (fromRemote)
                value = CounterContext.instance().clearAllDelta(value);
            return new CounterColumn(name, value, ts, timestampOfLastDelete);
        }
        else if ((b & EXPIRATION_MASK) != 0)
        {
            int ttl = dis.readInt();
            int expiration = dis.readInt();
            long ts = dis.readLong();
            ByteBuffer value = ByteBufferUtil.readWithLength(dis);
            if (expiration < expireBefore)
            {
                // the column is now expired, we can safely return a simple
                // tombstone
                ByteBuffer bytes = ByteBuffer.allocate(4);
                bytes.putInt(expiration);
                bytes.rewind();
                return new DeletedColumn(name, bytes, ts);
            }
            else
            {
                return new ExpiringColumn(name, value, ts, ttl, expiration);
            }
        }
        else
        {
            long ts = dis.readLong();
            ByteBuffer value = ByteBufferUtil.readWithLength(dis);
            return (b & COUNTER_UPDATE_MASK) != 0
                   ? new CounterUpdateColumn(name, value, ts)
                   : ((b & DELETION_MASK) == 0
                      ? new Column(name, value, ts)
                      : new DeletedColumn(name, value, ts));
        }
    }

+++++++++++++++++++++++
    public Column deserialize(DataInput dis, boolean fromRemote, int expireBefore) throws IOException
    {
        ByteBuffer name = ByteBufferUtil.readWithShortLength(dis);
        if (name.remaining() <= 0)
            throw new CorruptColumnException("invalid column name length " + name.remaining());

        int b = dis.readUnsignedByte();
        if ((b & COUNTER_MASK) != 0)
        {
            long timestampOfLastDelete = dis.readLong();
            long ts = dis.readLong();
            ByteBuffer value = ByteBufferUtil.readWithLength(dis);
            return CounterColumn.create(name, value, ts, timestampOfLastDelete, fromRemote);
        }
        else if ((b & EXPIRATION_MASK) != 0)
        {
            int ttl = dis.readInt();
            int expiration = dis.readInt();
            long ts = dis.readLong();
            ByteBuffer value = ByteBufferUtil.readWithLength(dis);
            return ExpiringColumn.create(name, value, ts, ttl, expiration, expireBefore);
        }
        else
        {
            long ts = dis.readLong();
            ByteBuffer value = ByteBufferUtil.readWithLength(dis);
            return (b & COUNTER_UPDATE_MASK) != 0
                   ? new CounterUpdateColumn(name, value, ts)
                   : ((b & DELETION_MASK) == 0
                      ? new Column(name, value, ts)
                      : new DeletedColumn(name, value, ts));
        }
    }

    public int serializedSize()
    {
    	/*
    	 * We need to keep the way we are calculating the column size in sync with the
    	 * way we are calculating the size for the column family serializer.
    	 */
      return DBConstants.shortSize_ + name_.remaining() + DBConstants.intSize_ + DBConstants.longSize_ + DBConstants.intSize_ + size();
    }

+++++++++++++++++++++++
    public int serializedSize()
    {
    	/*
    	 * We need to keep the way we are calculating the column size in sync with the
    	 * way we are calculating the size for the column family serializer.
    	 */
      return DBConstants.shortSize + name.remaining() + DBConstants.intSize + DBConstants.longSize + DBConstants.intSize + size();
    }

    public IColumn deserialize(DataInput dis, ColumnFamilyStore interner, boolean fromRemote, int expireBefore) throws IOException
    {
        ByteBuffer name = ByteBufferUtil.readWithShortLength(dis);
        int localDeleteTime = dis.readInt();
        if (localDeleteTime != Integer.MIN_VALUE && localDeleteTime <= 0)
        {
            throw new IOException("Invalid localDeleteTime read: " + localDeleteTime);
        }
        long markedForDeleteAt = dis.readLong();

        /* read the number of columns */
        int size = dis.readInt();
        ColumnSerializer serializer = Column.serializer();
        ColumnSortedMap preSortedMap = new ColumnSortedMap(comparator, serializer, dis, interner, size, fromRemote, expireBefore);
        SuperColumn superColumn = new SuperColumn(name, new ConcurrentSkipListMap<ByteBuffer,IColumn>(preSortedMap));
        superColumn.markForDeleteAt(localDeleteTime, markedForDeleteAt);
        return superColumn;
+++++++++++++++++++++++
    public IColumn deserialize(DataInput dis, boolean fromRemote, int expireBefore) throws IOException
    {
        ByteBuffer name = ByteBufferUtil.readWithShortLength(dis);
        int localDeleteTime = dis.readInt();
        if (localDeleteTime != Integer.MIN_VALUE && localDeleteTime <= 0)
        {
            throw new IOException("Invalid localDeleteTime read: " + localDeleteTime);
        }
        long markedForDeleteAt = dis.readLong();

        /* read the number of columns */
        int size = dis.readInt();
        ColumnSerializer serializer = Column.serializer();
        ColumnSortedMap preSortedMap = new ColumnSortedMap(comparator, serializer, dis, size, fromRemote, expireBefore);
        SuperColumn superColumn = new SuperColumn(name, ThreadSafeSortedColumns.factory().fromSorted(preSortedMap, false));
        if (localDeleteTime != Integer.MIN_VALUE && localDeleteTime <= 0)
        {
            throw new IOException("Invalid localDeleteTime read: " + localDeleteTime);
        }
        superColumn.delete(localDeleteTime, markedForDeleteAt);
        return superColumn;
    public ByteBuffer serialize() throws IOException
    {
        // super deflate
        org.apache.cassandra.db.migration.avro.Migration mi = new org.apache.cassandra.db.migration.avro.Migration();
        mi.old_version = new org.apache.cassandra.utils.avro.UUID();
        mi.old_version.bytes(UUIDGen.decompose(lastVersion));
        mi.new_version = new org.apache.cassandra.utils.avro.UUID();
        mi.new_version.bytes(UUIDGen.decompose(newVersion));
        mi.classname = new org.apache.avro.util.Utf8(this.getClass().getName());
        // TODO: Avro RowMutation serialization?
        DataOutputBuffer dob = new DataOutputBuffer();
        try
        {
            RowMutation.serializer().serialize(rm, dob, MessagingService.version_);
        }
        catch (IOException e)
        {
            throw new RuntimeException(e);
        }
        mi.row_mutation = ByteBuffer.wrap(dob.asByteArray());

        // sub deflate
        this.subdeflate(mi);

        // serialize
        return SerDeUtils.serializeWithSchema(mi);
    }

+++++++++++++++++++++++
    public ByteBuffer serialize() throws IOException
    {
        // super deflate
        org.apache.cassandra.db.migration.avro.Migration mi = new org.apache.cassandra.db.migration.avro.Migration();
        mi.old_version = new org.apache.cassandra.utils.avro.UUID();
        mi.old_version.bytes(UUIDGen.decompose(lastVersion));
        mi.new_version = new org.apache.cassandra.utils.avro.UUID();
        mi.new_version.bytes(UUIDGen.decompose(newVersion));
        mi.classname = new org.apache.avro.util.Utf8(this.getClass().getName());
        // TODO: Avro RowMutation serialization?
        DataOutputBuffer dob = new DataOutputBuffer();
        try
        {
            RowMutation.serializer().serialize(rm, dob, MessagingService.version_);
        }
        catch (IOException e)
        {
            throw new RuntimeException(e);
        }
        mi.row_mutation = ByteBuffer.wrap(dob.getData(), 0, dob.getLength());

        // sub deflate
        this.subdeflate(mi);

        // serialize
        return SerDeUtils.serializeWithSchema(mi);
    }

    public static Migration deserialize(ByteBuffer bytes, int version) throws IOException
    {
        // deserialize
        org.apache.cassandra.db.migration.avro.Migration mi = SerDeUtils.deserializeWithSchema(bytes, new org.apache.cassandra.db.migration.avro.Migration());

        // create an instance of the migration subclass
        Migration migration;
        try
        {
            Class migrationClass = Class.forName(mi.classname.toString());
            Constructor migrationConstructor = migrationClass.getDeclaredConstructor();
            migrationConstructor.setAccessible(true);
            migration = (Migration)migrationConstructor.newInstance();
        }
        catch (Exception e)
        {
            throw new RuntimeException("Invalid migration class: " + mi.classname.toString(), e);
        }
        
        // super inflate
        migration.lastVersion = UUIDGen.getUUID(ByteBuffer.wrap(mi.old_version.bytes()));
        migration.newVersion = UUIDGen.getUUID(ByteBuffer.wrap(mi.new_version.bytes()));
        try
        {
            migration.rm = RowMutation.serializer().deserialize(SerDeUtils.createDataInputStream(mi.row_mutation), version);
        }
        catch (IOException e)
        {
            throw new RuntimeException(e);
        }
        // sub inflate
        migration.subinflate(mi);
        return migration;
    }
    
    /** load serialized migrations. */
+++++++++++++++++++++++
    public static Migration deserialize(ByteBuffer bytes, int version) throws IOException
    {
        // deserialize
        org.apache.cassandra.db.migration.avro.Migration mi = SerDeUtils.deserializeWithSchema(bytes, new org.apache.cassandra.db.migration.avro.Migration());

        // create an instance of the migration subclass
        Migration migration;
        try
        {
            Class<?> migrationClass = Class.forName(mi.classname.toString());
            Constructor<?> migrationConstructor = migrationClass.getDeclaredConstructor();
            migrationConstructor.setAccessible(true);
            migration = (Migration)migrationConstructor.newInstance();
        }
        catch (Exception e)
        {
            throw new RuntimeException("Invalid migration class: " + mi.classname.toString(), e);
        }
        
        // super inflate
        migration.lastVersion = UUIDGen.getUUID(ByteBuffer.wrap(mi.old_version.bytes()));
        migration.newVersion = UUIDGen.getUUID(ByteBuffer.wrap(mi.new_version.bytes()));
        try
        {
            migration.rm = RowMutation.serializer().deserialize(SerDeUtils.createDataInputStream(mi.row_mutation), version);
        }
        catch (IOException e)
        {
            throw new RuntimeException(e);
        }
        // sub inflate
        migration.subinflate(mi);
        return migration;
    }
    
    /** load serialized migrations. */
    private IColumn deserializeNext()
    {
        try
        {
            count++;
            return serializer.deserialize(dis, interner, fromRemote, expireBefore);
        }
        catch (IOException e)
        {
            throw new IOError(e);
        }
    }

+++++++++++++++++++++++
    private IColumn deserializeNext()
    {
        try
        {
            count++;
            return serializer.deserialize(dis, fromRemote, expireBefore);
        }
        catch (IOException e)
        {
            throw new IOError(e);
        }
    }

    protected void testSerializeDeserialize(ColumnDefinition cd) throws Exception
    {
        ColumnDefinition newCd = ColumnDefinition.inflate(cd.deflate());
        assert cd != newCd;
        assert cd.hashCode() == newCd.hashCode();
        assert cd.equals(newCd);
+++++++++++++++++++++++
    protected void testSerializeDeserialize(ColumnDefinition cd) throws Exception
    {
        ColumnDefinition newCd = ColumnDefinition.fromAvro(cd.toAvro());
        assert cd != newCd;
        assert cd.hashCode() == newCd.hashCode();
        assert cd.equals(newCd);
add: 103, delete: 25, change: 304, unhandled: 0 size_exceptions: 14 size_serialize: 13
-----------cassandra-1.0.0 vs cassandra-1.0.0-beta1-----------
    public synchronized void serialize()
    {
        File manifestFile = tryGetManifest(cfs);
        if (manifestFile == null)
            manifestFile = new File(new File(DatabaseDescriptor.getAllDataFileLocations()[0], cfs.table.name), cfs.columnFamily + ".json");
        File oldFile = new File(manifestFile.getPath().replace(".json", "-old.json"));
        File tmpFile = new File(manifestFile.getPath().replace(".json", "-tmp.json"));

        JsonFactory f = new JsonFactory();
        try
        {
            JsonGenerator g = f.createJsonGenerator(tmpFile, JsonEncoding.UTF8);
            g.useDefaultPrettyPrinter();
            g.writeStartObject();
            g.writeArrayFieldStart("generations");
            for (int level = 0; level < generations.length; level++)
            {
                g.writeStartObject();
                g.writeNumberField("generation", level);
                g.writeArrayFieldStart("members");
                for (SSTableReader ssTableReader : generations[level])
                    g.writeNumber(ssTableReader.descriptor.generation);
                g.writeEndArray(); // members

                g.writeEndObject(); // generation
            }
            g.writeEndArray(); // for field generations
            g.writeEndObject(); // write global object
            g.close();

            if (oldFile.exists() && manifestFile.exists())
                FileUtils.deleteWithConfirm(oldFile);
            if (manifestFile.exists())
                FileUtils.renameWithConfirm(manifestFile, oldFile);
            assert tmpFile.exists();
            FileUtils.renameWithConfirm(tmpFile, manifestFile);
            logger.debug("Saved manifest {}", manifestFile);
        }
        catch (IOException e)
        {
            throw new IOError(e);
        }
    }

+++++++++++++++++++++++
    public synchronized void serialize()
    {
        String dataFileLocation = getDataFilePrefix(cfs);
        String tempManifestFileName = dataFileLocation + cfs.getColumnFamilyName() + "-" + "tmp.json";
        String manifestFileName = dataFileLocation + cfs.getColumnFamilyName() + ".json";
        String oldManifestFileName = dataFileLocation + cfs.getColumnFamilyName() + "-" + "old.json";

        File tmpManifest = new File(tempManifestFileName);

        JsonFactory f = new JsonFactory();

        try
        {
            JsonGenerator g = f.createJsonGenerator(tmpManifest, JsonEncoding.UTF8);
            g.useDefaultPrettyPrinter();
            g.writeStartObject();
            g.writeArrayFieldStart("generations");
            for (int level = 0; level < generations.length; level++)
            {
                g.writeStartObject();
                g.writeNumberField("generation", level);
                g.writeArrayFieldStart("members");
                for (SSTableReader ssTableReader : generations[level])
                    g.writeNumber(ssTableReader.descriptor.generation);
                g.writeEndArray(); // members

                g.writeEndObject(); // generation
            }
            g.writeEndArray(); // for field generations
            g.writeEndObject(); // write global object
            g.close();
        }
        catch (IOException e)
        {
            e.printStackTrace();
        }
        File oldFile = new File(oldManifestFileName);
        if (oldFile.exists())
            oldFile.delete();
        File currentManifest = new File(manifestFileName);
        if (currentManifest.exists())
            currentManifest.renameTo(new File(oldManifestFileName));
        if (tmpManifest.exists())
            tmpManifest.renameTo(new File(manifestFileName));
    }

add: 4, delete: 4, change: 79, unhandled: 0 size_exceptions: 2 size_serialize: 1
-----------cassandra-1.0.0-beta1 vs cassandra-1.0.0-rc1-----------
    public synchronized void serialize()
    {
        String dataFileLocation = getDataFilePrefix(cfs);
        String tempManifestFileName = dataFileLocation + cfs.getColumnFamilyName() + "-" + "tmp.json";
        String manifestFileName = dataFileLocation + cfs.getColumnFamilyName() + ".json";
        String oldManifestFileName = dataFileLocation + cfs.getColumnFamilyName() + "-" + "old.json";

        File tmpManifest = new File(tempManifestFileName);

        JsonFactory f = new JsonFactory();

        try
        {
            JsonGenerator g = f.createJsonGenerator(tmpManifest, JsonEncoding.UTF8);
            g.useDefaultPrettyPrinter();
            g.writeStartObject();
            g.writeArrayFieldStart("generations");
            for (int level = 0; level < generations.length; level++)
            {
                g.writeStartObject();
                g.writeNumberField("generation", level);
                g.writeArrayFieldStart("members");
                for (SSTableReader ssTableReader : generations[level])
                    g.writeNumber(ssTableReader.descriptor.generation);
                g.writeEndArray(); // members

                g.writeEndObject(); // generation
            }
            g.writeEndArray(); // for field generations
            g.writeEndObject(); // write global object
            g.close();
        }
        catch (IOException e)
        {
            e.printStackTrace();
        }
        File oldFile = new File(oldManifestFileName);
        if (oldFile.exists())
            oldFile.delete();
        File currentManifest = new File(manifestFileName);
        if (currentManifest.exists())
            currentManifest.renameTo(new File(oldManifestFileName));
        if (tmpManifest.exists())
            tmpManifest.renameTo(new File(manifestFileName));
    }

+++++++++++++++++++++++
    public synchronized void serialize()
    {
        File manifestFile = tryGetManifest(cfs);
        if (manifestFile == null)
            manifestFile = new File(new File(DatabaseDescriptor.getAllDataFileLocations()[0], cfs.table.name), cfs.columnFamily + ".json");
        File oldFile = new File(manifestFile.getPath().replace(".json", "-old.json"));
        File tmpFile = new File(manifestFile.getPath().replace(".json", "-tmp.json"));

        JsonFactory f = new JsonFactory();
        try
        {
            JsonGenerator g = f.createJsonGenerator(tmpFile, JsonEncoding.UTF8);
            g.useDefaultPrettyPrinter();
            g.writeStartObject();
            g.writeArrayFieldStart("generations");
            for (int level = 0; level < generations.length; level++)
            {
                g.writeStartObject();
                g.writeNumberField("generation", level);
                g.writeArrayFieldStart("members");
                for (SSTableReader ssTableReader : generations[level])
                    g.writeNumber(ssTableReader.descriptor.generation);
                g.writeEndArray(); // members

                g.writeEndObject(); // generation
            }
            g.writeEndArray(); // for field generations
            g.writeEndObject(); // write global object
            g.close();

            if (oldFile.exists() && manifestFile.exists())
                FileUtils.deleteWithConfirm(oldFile);
            if (manifestFile.exists())
                FileUtils.renameWithConfirm(manifestFile, oldFile);
            assert tmpFile.exists();
            FileUtils.renameWithConfirm(tmpFile, manifestFile);
            logger.debug("Saved manifest {}", manifestFile);
        }
        catch (IOException e)
        {
            throw new IOError(e);
        }
    }

add: 1, delete: 0, change: 44, unhandled: 0 size_exceptions: 1 size_serialize: 1
-----------cassandra-1.0.0-rc1 vs cassandra-1.0.0-rc2-----------
add: 1, delete: 0, change: 16, unhandled: 0 size_exceptions: 0 size_serialize: 0
-----------cassandra-1.0.0-rc2 vs cassandra-1.0.1-----------
    public int serializeForSSTable(ColumnFamily columnFamily, DataOutput dos)
    {
        try
        {
            serializeCFInfo(columnFamily, dos);

            Collection<IColumn> columns = columnFamily.getSortedColumns();
            int count = columns.size();
            dos.writeInt(count);
            for (IColumn column : columns)
            {
                columnFamily.getColumnSerializer().serialize(column, dos);
            }
            return count;
        }
        catch (IOException e)
        {
            throw new RuntimeException(e);
        }
    }

+++++++++++++++++++++++
    public int serializeForSSTable(ColumnFamily columnFamily, DataOutput dos)
    {
        try
        {
            serializeCFInfo(columnFamily, dos);

            Collection<IColumn> columns = columnFamily.getSortedColumns();
            int count = columns.size();
            dos.writeInt(count);
            int i = 0;
            for (IColumn column : columns)
            {
                columnFamily.getColumnSerializer().serialize(column, dos);
                i++;
            }
            assert count == i: "CF size changed during serialization: was " + count + " initially but " + i + " written";
            return count;
        }
        catch (IOException e)
        {
            throw new RuntimeException(e);
        }
    }

    public long serializedSize(ColumnFamily cf)
    {
        return cf.serializedSize();
    }

    /**
     * Observes columns in a single row, without adding them to the column family.
     */
+++++++++++++++++++++++
    public long serializedSize(ColumnFamily cf)
    {
        return cf == null ? DBConstants.boolSize : cf.serializedSize();
    }

    /**
     * Observes columns in a single row, without adding them to the column family.
     */
    public static RowHeader serialize(IIterableColumns columns)
    {
        int columnCount = columns.getEstimatedColumnCount();

        BloomFilter bf = BloomFilter.getFilter(columnCount, 4);

        if (columnCount == 0)
            return new RowHeader(bf, Collections.<IndexHelper.IndexInfo>emptyList());

        // update bloom filter and create a list of IndexInfo objects marking the first and last column
        // in each block of ColumnIndexSize
        List<IndexHelper.IndexInfo> indexList = new ArrayList<IndexHelper.IndexInfo>();
        int endPosition = 0, startPosition = -1;
        IColumn lastColumn = null, firstColumn = null;

        for (IColumn column : columns)
        {
            bf.add(column.name());

            if (firstColumn == null)
            {
                firstColumn = column;
                startPosition = endPosition;
            }

            endPosition += column.serializedSize();

            // if we hit the column index size that we have to index after, go ahead and index it.
            if (endPosition - startPosition >= DatabaseDescriptor.getColumnIndexSize())
            {
                IndexHelper.IndexInfo cIndexInfo = new IndexHelper.IndexInfo(firstColumn.name(), column.name(), startPosition, endPosition - startPosition);
                indexList.add(cIndexInfo);
                firstColumn = null;
            }

            lastColumn = column;
        }

        // all columns were GC'd after all
        if (lastColumn == null)
            return new RowHeader(bf, Collections.<IndexHelper.IndexInfo>emptyList());

        // the last column may have fallen on an index boundary already.  if not, index it explicitly.
        if (indexList.isEmpty() || columns.getComparator().compare(indexList.get(indexList.size() - 1).lastName, lastColumn.name()) != 0)
        {
            IndexHelper.IndexInfo cIndexInfo = new IndexHelper.IndexInfo(firstColumn.name(), lastColumn.name(), startPosition, endPosition - startPosition);
            indexList.add(cIndexInfo);
        }

        // we should always have at least one computed index block, but we only write it out if there is more than that.
        assert indexList.size() > 0;
        return new RowHeader(bf, indexList);
    }

+++++++++++++++++++++++
    public static RowHeader serialize(IIterableColumns columns)
    {
        int columnCount = columns.getEstimatedColumnCount();

        BloomFilter bf = BloomFilter.getFilter(columnCount, 4);

        if (columnCount == 0)
            return new RowHeader(bf, Collections.<IndexHelper.IndexInfo>emptyList());

        // update bloom filter and create a list of IndexInfo objects marking the first and last column
        // in each block of ColumnIndexSize
        List<IndexHelper.IndexInfo> indexList = new ArrayList<IndexHelper.IndexInfo>();
        long endPosition = 0, startPosition = -1;
        IColumn lastColumn = null, firstColumn = null;

        for (IColumn column : columns)
        {
            bf.add(column.name());

            if (firstColumn == null)
            {
                firstColumn = column;
                startPosition = endPosition;
            }

            endPosition += column.serializedSize();

            // if we hit the column index size that we have to index after, go ahead and index it.
            if (endPosition - startPosition >= DatabaseDescriptor.getColumnIndexSize())
            {
                IndexHelper.IndexInfo cIndexInfo = new IndexHelper.IndexInfo(firstColumn.name(), column.name(), startPosition, endPosition - startPosition);
                indexList.add(cIndexInfo);
                firstColumn = null;
            }

            lastColumn = column;
        }

        // all columns were GC'd after all
        if (lastColumn == null)
            return new RowHeader(bf, Collections.<IndexHelper.IndexInfo>emptyList());

        // the last column may have fallen on an index boundary already.  if not, index it explicitly.
        if (indexList.isEmpty() || columns.getComparator().compare(indexList.get(indexList.size() - 1).lastName, lastColumn.name()) != 0)
        {
            IndexHelper.IndexInfo cIndexInfo = new IndexHelper.IndexInfo(firstColumn.name(), lastColumn.name(), startPosition, endPosition - startPosition);
            indexList.add(cIndexInfo);
        }

        // we should always have at least one computed index block, but we only write it out if there is more than that.
        assert indexList.size() > 0;
        return new RowHeader(bf, indexList);
    }

    public Column deserialize(DataInput dis, boolean fromRemote, int expireBefore) throws IOException
    {
        ByteBuffer name = ByteBufferUtil.readWithShortLength(dis);
        if (name.remaining() <= 0)
            throw new CorruptColumnException("invalid column name length " + name.remaining());

        int b = dis.readUnsignedByte();
        if ((b & COUNTER_MASK) != 0)
        {
            long timestampOfLastDelete = dis.readLong();
            long ts = dis.readLong();
            ByteBuffer value = ByteBufferUtil.readWithLength(dis);
            return CounterColumn.create(name, value, ts, timestampOfLastDelete, fromRemote);
        }
        else if ((b & EXPIRATION_MASK) != 0)
        {
            int ttl = dis.readInt();
            int expiration = dis.readInt();
            long ts = dis.readLong();
            ByteBuffer value = ByteBufferUtil.readWithLength(dis);
            return ExpiringColumn.create(name, value, ts, ttl, expiration, expireBefore);
        }
        else
        {
            long ts = dis.readLong();
            ByteBuffer value = ByteBufferUtil.readWithLength(dis);
            return (b & COUNTER_UPDATE_MASK) != 0
                   ? new CounterUpdateColumn(name, value, ts)
                   : ((b & DELETION_MASK) == 0
                      ? new Column(name, value, ts)
                      : new DeletedColumn(name, value, ts));
        }
    }

+++++++++++++++++++++++
    public Column deserialize(DataInput dis, boolean fromRemote, int expireBefore) throws IOException
    {
        ByteBuffer name = ByteBufferUtil.readWithShortLength(dis);
        if (name.remaining() <= 0)
        {
            String format = "invalid column name length %d%s";
            String details = "";
            if (dis instanceof FileDataInput)
            {
                FileDataInput fdis = (FileDataInput)dis;
                details = String.format(" (%s, %d bytes remaining)", fdis.getPath(), fdis.bytesRemaining());
            }
            throw new CorruptColumnException(String.format(format, name.remaining(), details));
        }

        int b = dis.readUnsignedByte();
        if ((b & COUNTER_MASK) != 0)
        {
            long timestampOfLastDelete = dis.readLong();
            long ts = dis.readLong();
            ByteBuffer value = ByteBufferUtil.readWithLength(dis);
            return CounterColumn.create(name, value, ts, timestampOfLastDelete, fromRemote);
        }
        else if ((b & EXPIRATION_MASK) != 0)
        {
            int ttl = dis.readInt();
            int expiration = dis.readInt();
            long ts = dis.readLong();
            ByteBuffer value = ByteBufferUtil.readWithLength(dis);
            return ExpiringColumn.create(name, value, ts, ttl, expiration, expireBefore);
        }
        else
        {
            long ts = dis.readLong();
            ByteBuffer value = ByteBufferUtil.readWithLength(dis);
            return (b & COUNTER_UPDATE_MASK) != 0
                   ? new CounterUpdateColumn(name, value, ts)
                   : ((b & DELETION_MASK) == 0
                      ? new Column(name, value, ts)
                      : new DeletedColumn(name, value, ts));
        }
    }

    public void serialize(CounterMutation cm, DataOutputStream dos, int version) throws IOException
    {
        RowMutation.serializer().serialize(cm.rowMutation(), dos, version);
        dos.writeUTF(cm.consistency().name());
    }

+++++++++++++++++++++++
    public void serialize(CounterMutation cm, DataOutput dos, int version) throws IOException
    {
        RowMutation.serializer().serialize(cm.rowMutation(), dos, version);
        dos.writeUTF(cm.consistency().name());
    }

    public CounterMutation deserialize(DataInputStream dis, int version) throws IOException
    {
        RowMutation rm = RowMutation.serializer().deserialize(dis, version);
        ConsistencyLevel consistency = Enum.valueOf(ConsistencyLevel.class, dis.readUTF());
        return new CounterMutation(rm, consistency);
+++++++++++++++++++++++
    public CounterMutation deserialize(DataInput dis, int version) throws IOException
    {
        RowMutation rm = RowMutation.serializer().deserialize(dis, version);
        ConsistencyLevel consistency = Enum.valueOf(ConsistencyLevel.class, dis.readUTF());
        return new CounterMutation(rm, consistency);
    }

    public void serialize(RangeSliceCommand sliceCommand, DataOutputStream dos, int version) throws IOException
    {
        dos.writeUTF(sliceCommand.keyspace);
        dos.writeUTF(sliceCommand.column_family);
        ByteBuffer sc = sliceCommand.super_column;
        dos.writeInt(sc == null ? 0 : sc.remaining());
        if (sc != null)
            ByteBufferUtil.write(sc, dos);

        TSerializer ser = new TSerializer(new TBinaryProtocol.Factory());
        FBUtilities.serialize(ser, sliceCommand.predicate, dos);
        AbstractBounds.serializer().serialize(sliceCommand.range, dos);
        dos.writeInt(sliceCommand.max_keys);
    }

+++++++++++++++++++++++
    public void serialize(RangeSliceCommand sliceCommand, DataOutput dos, int version) throws IOException
    {
        dos.writeUTF(sliceCommand.keyspace);
        dos.writeUTF(sliceCommand.column_family);
        ByteBuffer sc = sliceCommand.super_column;
        dos.writeInt(sc == null ? 0 : sc.remaining());
        if (sc != null)
            ByteBufferUtil.write(sc, dos);

        TSerializer ser = new TSerializer(new TBinaryProtocol.Factory());
        FBUtilities.serialize(ser, sliceCommand.predicate, dos);
        AbstractBounds.serializer().serialize(sliceCommand.range, dos);
        dos.writeInt(sliceCommand.max_keys);
    }

    public RangeSliceCommand deserialize(DataInputStream dis, int version) throws IOException
    {
        String keyspace = dis.readUTF();
        String column_family = dis.readUTF();

        int scLength = dis.readInt();
        ByteBuffer super_column = null;
        if (scLength > 0)
            super_column = ByteBuffer.wrap(readBuf(scLength, dis));

        TDeserializer dser = new TDeserializer(new TBinaryProtocol.Factory());
        SlicePredicate pred = new SlicePredicate();
        FBUtilities.deserialize(dser, pred, dis);

        AbstractBounds range = AbstractBounds.serializer().deserialize(dis);
        int max_keys = dis.readInt();
        return new RangeSliceCommand(keyspace, column_family, super_column, pred, range, max_keys);
    }

+++++++++++++++++++++++
    public RangeSliceCommand deserialize(DataInput dis, int version) throws IOException
    {
        String keyspace = dis.readUTF();
        String column_family = dis.readUTF();

        int scLength = dis.readInt();
        ByteBuffer super_column = null;
        if (scLength > 0)
        {
            byte[] buf = new byte[scLength];
            dis.readFully(buf);
            super_column = ByteBuffer.wrap(buf);
        }

        TDeserializer dser = new TDeserializer(new TBinaryProtocol.Factory());
        SlicePredicate pred = new SlicePredicate();
        FBUtilities.deserialize(dser, pred, dis);

        AbstractBounds range = AbstractBounds.serializer().deserialize(dis);
        int max_keys = dis.readInt();
        return new RangeSliceCommand(keyspace, column_family, super_column, pred, range, max_keys);
    }

    public void serialize(ReadCommand rm, DataOutputStream dos, int version) throws IOException
    {
        SliceByNamesReadCommand realRM = (SliceByNamesReadCommand)rm;
        dos.writeBoolean(realRM.isDigestQuery());
        dos.writeUTF(realRM.table);
        ByteBufferUtil.writeWithShortLength(realRM.key, dos);
        realRM.queryPath.serialize(dos);
        dos.writeInt(realRM.columnNames.size());
        if (realRM.columnNames.size() > 0)
        {
            for (ByteBuffer cName : realRM.columnNames)
            {
                ByteBufferUtil.writeWithShortLength(cName, dos);
            }
        }
    }

    @Override
+++++++++++++++++++++++
    public void serialize(ReadCommand cmd, DataOutput dos, int version) throws IOException
    {
        SliceByNamesReadCommand command = (SliceByNamesReadCommand) cmd;
        dos.writeBoolean(command.isDigestQuery());
        dos.writeUTF(command.table);
        ByteBufferUtil.writeWithShortLength(command.key, dos);
        command.queryPath.serialize(dos);
        dos.writeInt(command.columnNames.size());
        if (!command.columnNames.isEmpty())
        {
            for (ByteBuffer cName : command.columnNames)
            {
                ByteBufferUtil.writeWithShortLength(cName, dos);
            }
        }
    }

    public ReadCommand deserialize(DataInputStream dis, int version) throws IOException
    {
        boolean isDigest = dis.readBoolean();
        String table = dis.readUTF();
        ByteBuffer key = ByteBufferUtil.readWithShortLength(dis);
        QueryPath columnParent = QueryPath.deserialize(dis);

        int size = dis.readInt();
        List<ByteBuffer> columns = new ArrayList<ByteBuffer>();
        for (int i = 0; i < size; ++i)
        {
            columns.add(ByteBufferUtil.readWithShortLength(dis));
        }
        SliceByNamesReadCommand rm = new SliceByNamesReadCommand(table, key, columnParent, columns);
        rm.setDigestQuery(isDigest);
        return rm;
+++++++++++++++++++++++
    public SliceByNamesReadCommand deserialize(DataInput dis, int version) throws IOException
    {
        boolean isDigest = dis.readBoolean();
        String table = dis.readUTF();
        ByteBuffer key = ByteBufferUtil.readWithShortLength(dis);
        QueryPath columnParent = QueryPath.deserialize(dis);

        int size = dis.readInt();
        List<ByteBuffer> columns = new ArrayList<ByteBuffer>();
        for (int i = 0; i < size; ++i)
        {
            columns.add(ByteBufferUtil.readWithShortLength(dis));
        }
        SliceByNamesReadCommand command = new SliceByNamesReadCommand(table, key, columnParent, columns);
        command.setDigestQuery(isDigest);
        return command;
    }

    public void serialize(ReadCommand rm, DataOutputStream dos, int version) throws IOException
    {
        SliceFromReadCommand realRM = (SliceFromReadCommand)rm;
        dos.writeBoolean(realRM.isDigestQuery());
        dos.writeUTF(realRM.table);
        ByteBufferUtil.writeWithShortLength(realRM.key, dos);
        realRM.queryPath.serialize(dos);
        ByteBufferUtil.writeWithShortLength(realRM.start, dos);
        ByteBufferUtil.writeWithShortLength(realRM.finish, dos);
        dos.writeBoolean(realRM.reversed);
        dos.writeInt(realRM.count);
    }

    @Override
+++++++++++++++++++++++
    public void serialize(ReadCommand rm, DataOutput dos, int version) throws IOException
    {
        SliceFromReadCommand realRM = (SliceFromReadCommand)rm;
        dos.writeBoolean(realRM.isDigestQuery());
        dos.writeUTF(realRM.table);
        ByteBufferUtil.writeWithShortLength(realRM.key, dos);
        realRM.queryPath.serialize(dos);
        ByteBufferUtil.writeWithShortLength(realRM.start, dos);
        ByteBufferUtil.writeWithShortLength(realRM.finish, dos);
        dos.writeBoolean(realRM.reversed);
        dos.writeInt(realRM.count);
    }

    public ReadCommand deserialize(DataInputStream dis, int version) throws IOException
    {
        boolean isDigest = dis.readBoolean();
        SliceFromReadCommand rm = new SliceFromReadCommand(dis.readUTF(),
                                                           ByteBufferUtil.readWithShortLength(dis),
                                                           QueryPath.deserialize(dis),
                                                           ByteBufferUtil.readWithShortLength(dis),
                                                           ByteBufferUtil.readWithShortLength(dis),
                                                           dis.readBoolean(),
                                                           dis.readInt());
        rm.setDigestQuery(isDigest);
        return rm;
+++++++++++++++++++++++
    public ReadCommand deserialize(DataInput dis, int version) throws IOException
    {
        boolean isDigest = dis.readBoolean();
        SliceFromReadCommand rm = new SliceFromReadCommand(dis.readUTF(),
                                                           ByteBufferUtil.readWithShortLength(dis),
                                                           QueryPath.deserialize(dis),
                                                           ByteBufferUtil.readWithShortLength(dis),
                                                           ByteBufferUtil.readWithShortLength(dis),
                                                           dis.readBoolean(),
                                                           dis.readInt());
        rm.setDigestQuery(isDigest);
        return rm;
    }

    public synchronized void serialize()
    {
        File manifestFile = tryGetManifest(cfs);
        if (manifestFile == null)
            manifestFile = new File(new File(DatabaseDescriptor.getAllDataFileLocations()[0], cfs.table.name), cfs.columnFamily + ".json");
        File oldFile = new File(manifestFile.getPath().replace(".json", "-old.json"));
        File tmpFile = new File(manifestFile.getPath().replace(".json", "-tmp.json"));

        JsonFactory f = new JsonFactory();
        try
        {
            JsonGenerator g = f.createJsonGenerator(tmpFile, JsonEncoding.UTF8);
            g.useDefaultPrettyPrinter();
            g.writeStartObject();
            g.writeArrayFieldStart("generations");
            for (int level = 0; level < generations.length; level++)
            {
                g.writeStartObject();
                g.writeNumberField("generation", level);
                g.writeArrayFieldStart("members");
                for (SSTableReader ssTableReader : generations[level])
                    g.writeNumber(ssTableReader.descriptor.generation);
                g.writeEndArray(); // members

                g.writeEndObject(); // generation
            }
            g.writeEndArray(); // for field generations
            g.writeEndObject(); // write global object
            g.close();

            if (oldFile.exists() && manifestFile.exists())
                FileUtils.deleteWithConfirm(oldFile);
            if (manifestFile.exists())
                FileUtils.renameWithConfirm(manifestFile, oldFile);
            assert tmpFile.exists();
            FileUtils.renameWithConfirm(tmpFile, manifestFile);
            logger.debug("Saved manifest {}", manifestFile);
        }
        catch (IOException e)
        {
            throw new IOError(e);
        }
    }

+++++++++++++++++++++++
    public synchronized void serialize()
    {
        File manifestFile = tryGetManifest(cfs);
        if (manifestFile == null)
            manifestFile = new File(new File(DatabaseDescriptor.getAllDataFileLocations()[0], cfs.table.name), cfs.columnFamily + ".json");
        File oldFile = new File(manifestFile.getPath().replace(EXTENSION, "-old.json"));
        File tmpFile = new File(manifestFile.getPath().replace(EXTENSION, "-tmp.json"));

        JsonFactory f = new JsonFactory();
        try
        {
            JsonGenerator g = f.createJsonGenerator(tmpFile, JsonEncoding.UTF8);
            g.useDefaultPrettyPrinter();
            g.writeStartObject();
            g.writeArrayFieldStart("generations");
            for (int level = 0; level < generations.length; level++)
            {
                g.writeStartObject();
                g.writeNumberField("generation", level);
                g.writeArrayFieldStart("members");
                for (SSTableReader ssTableReader : generations[level])
                    g.writeNumber(ssTableReader.descriptor.generation);
                g.writeEndArray(); // members

                g.writeEndObject(); // generation
            }
            g.writeEndArray(); // for field generations
            g.writeEndObject(); // write global object
            g.close();

            if (oldFile.exists() && manifestFile.exists())
                FileUtils.deleteWithConfirm(oldFile);
            if (manifestFile.exists())
                FileUtils.renameWithConfirm(manifestFile, oldFile);
            assert tmpFile.exists();
            FileUtils.renameWithConfirm(tmpFile, manifestFile);
            logger.debug("Saved manifest {}", manifestFile);
        }
        catch (IOException e)
        {
            throw new IOError(e);
        }
    }

    public void serialize(DataOutputStream dos) throws IOException
    {
        assert !"".equals(columnFamilyName);
        assert superColumnName == null || superColumnName.remaining() > 0;
        assert columnName == null || columnName.remaining() > 0;
        dos.writeUTF(columnFamilyName == null ? "" : columnFamilyName);
        ByteBufferUtil.writeWithShortLength(superColumnName == null ? ByteBufferUtil.EMPTY_BYTE_BUFFER : superColumnName, dos);
        ByteBufferUtil.writeWithShortLength(columnName == null ? ByteBufferUtil.EMPTY_BYTE_BUFFER : columnName, dos);
    }

+++++++++++++++++++++++
    public void serialize(DataOutput dos) throws IOException
    {
        assert !"".equals(columnFamilyName);
        assert superColumnName == null || superColumnName.remaining() > 0;
        assert columnName == null || columnName.remaining() > 0;
        dos.writeUTF(columnFamilyName == null ? "" : columnFamilyName);
        ByteBufferUtil.writeWithShortLength(superColumnName == null ? ByteBufferUtil.EMPTY_BYTE_BUFFER : superColumnName, dos);
        ByteBufferUtil.writeWithShortLength(columnName == null ? ByteBufferUtil.EMPTY_BYTE_BUFFER : columnName, dos);
    }

    public static QueryPath deserialize(DataInputStream din) throws IOException
    {
        String cfName = din.readUTF();
        ByteBuffer scName = ByteBufferUtil.readWithShortLength(din);
        ByteBuffer cName = ByteBufferUtil.readWithShortLength(din);
        return new QueryPath(cfName.isEmpty() ? null : cfName, 
                             scName.remaining() == 0 ? null : scName, 
                             cName.remaining() == 0 ? null : cName);
+++++++++++++++++++++++
    public static QueryPath deserialize(DataInput din) throws IOException
    {
        String cfName = din.readUTF();
        ByteBuffer scName = ByteBufferUtil.readWithShortLength(din);
        ByteBuffer cName = ByteBufferUtil.readWithShortLength(din);
        return new QueryPath(cfName.isEmpty() ? null : cfName, 
                             scName.remaining() == 0 ? null : scName, 
                             cName.remaining() == 0 ? null : cName);
    }

    public static ICompactSerializer2<AbstractBounds> serializer()
    {
        return serializer;
    }

+++++++++++++++++++++++
    public static AbstractBoundsSerializer serializer()
    {
        return serializer;
    }

    public static ICompactSerializer<EndpointState> serializer()
    {
        return serializer;
    }
    
+++++++++++++++++++++++
    public static IVersionedSerializer<EndpointState> serializer()
    {
        return serializer;
    }
    
    public void serialize(EndpointState epState, DataOutputStream dos, int version) throws IOException
    {
        /* serialize the HeartBeatState */
        HeartBeatState hbState = epState.getHeartBeatState();
        HeartBeatState.serializer().serialize(hbState, dos, version);

        /* serialize the map of ApplicationState objects */
        int size = epState.applicationState.size();
        dos.writeInt(size);
        for (Map.Entry<ApplicationState, VersionedValue> entry : epState.applicationState.entrySet())
        {
            VersionedValue value = entry.getValue();
            if (value != null)
            {
                dos.writeInt(entry.getKey().ordinal());
                VersionedValue.serializer.serialize(value, dos, version);
            }
        }
    }

+++++++++++++++++++++++
    public void serialize(EndpointState epState, DataOutput dos, int version) throws IOException
    {
        /* serialize the HeartBeatState */
        HeartBeatState hbState = epState.getHeartBeatState();
        HeartBeatState.serializer().serialize(hbState, dos, version);

        /* serialize the map of ApplicationState objects */
        int size = epState.applicationState.size();
        dos.writeInt(size);
        for (Map.Entry<ApplicationState, VersionedValue> entry : epState.applicationState.entrySet())
        {
            VersionedValue value = entry.getValue();
            dos.writeInt(entry.getKey().ordinal());
            VersionedValue.serializer.serialize(value, dos, version);
        }
    }

    public EndpointState deserialize(DataInputStream dis, int version) throws IOException
    {
        HeartBeatState hbState = HeartBeatState.serializer().deserialize(dis, version);
        EndpointState epState = new EndpointState(hbState);

        int appStateSize = dis.readInt();
        for ( int i = 0; i < appStateSize; ++i )
        {
            if ( dis.available() == 0 )
            {
                break;
            }

            int key = dis.readInt();
            VersionedValue value = VersionedValue.serializer.deserialize(dis, version);
            epState.addApplicationState(Gossiper.STATES[key], value);
        }
        return epState;
+++++++++++++++++++++++
    public EndpointState deserialize(DataInput dis, int version) throws IOException
    {
        HeartBeatState hbState = HeartBeatState.serializer().deserialize(dis, version);
        EndpointState epState = new EndpointState(hbState);

        int appStateSize = dis.readInt();
        for ( int i = 0; i < appStateSize; ++i )
        {
            int key = dis.readInt();
            VersionedValue value = VersionedValue.serializer.deserialize(dis, version);
            epState.addApplicationState(Gossiper.STATES[key], value);
        }
        return epState;
    }

    public static void serialize(InetAddress endpoint, DataOutputStream dos) throws IOException
    {
        byte[] buf = endpoint.getAddress();
        dos.writeByte(buf.length);
        dos.write(buf);
    }
    
+++++++++++++++++++++++
    public static void serialize(InetAddress endpoint, DataOutput dos) throws IOException
    {
        byte[] buf = endpoint.getAddress();
        dos.writeByte(buf.length);
        dos.write(buf);
    }
    
    public static InetAddress deserialize(DataInputStream dis) throws IOException
    {     
        byte[] bytes = new byte[dis.readByte()];
        dis.readFully(bytes, 0, bytes.length);
        return InetAddress.getByAddress(bytes);
    }

+++++++++++++++++++++++
    public static InetAddress deserialize(DataInput dis) throws IOException
    {     
        byte[] bytes = new byte[dis.readByte()];
        dis.readFully(bytes, 0, bytes.length);
        return InetAddress.getByAddress(bytes);
    }

    public int serializedSize()
    {
        return BloomFilterSerializer.serializedSize(this);
+++++++++++++++++++++++
    public long serializedSize()
    {
        return serializer_.serializedSize(this);
    public static ICompactSerializer2<BloomFilter> serializer()
    {
        return serializer_;
    }

+++++++++++++++++++++++
    public static BloomFilterSerializer serializer()
    {
        return serializer_;
    }

    public void serialize(BloomFilter bf, DataOutput dos) throws IOException
    {
        long[] bits = bf.bitset.getBits();
        int bitLength = bits.length;

        dos.writeInt(bf.getHashCount());
        dos.writeInt(bitLength);

        for (int i = 0; i < bitLength; i++)
            dos.writeLong(bits[i]);
    }

+++++++++++++++++++++++
    public void serialize(BloomFilter bf, DataOutput dos) throws IOException
    {
        int bitLength = bf.bitset.getNumWords();
        int pageSize = bf.bitset.getPageSize();
        int pageCount = bf.bitset.getPageCount();

        dos.writeInt(bf.getHashCount());
        dos.writeInt(bitLength);

        for (int p = 0; p < pageCount; p++)
        {
            long[] bits = bf.bitset.getPage(p);
            for (int i = 0; i < pageSize && bitLength-- > 0; i++)
                dos.writeLong(bits[i]);
        }
    }

    public static int serializedSize(BloomFilter bf)
    {
        return DBConstants.intSize // hash count
               + DBConstants.intSize // length
               + bf.bitset.getBits().length * DBConstants.longSize; // buckets
+++++++++++++++++++++++
    public long serializedSize(BloomFilter bf)
    {
        return DBConstants.intSize // hash count
               + DBConstants.intSize // length
               + bf.bitset.getNumWords() * DBConstants.longSize; // buckets
    public BloomFilter deserialize(DataInput dis) throws IOException
    {
        int hashes = dis.readInt();
        int bitLength = dis.readInt();
        long[] bits = new long[bitLength];
        for (int i = 0; i < bitLength; i++)
            bits[i] = dis.readLong();
        OpenBitSet bs = new OpenBitSet(bits, bitLength);
        return new BloomFilter(hashes, bs);
    }

    /**
     * Calculates a serialized size of the given Bloom Filter
     * @see this.serialize(BloomFilter, DataOutput)
     *
     * @param bf Bloom filter to calculate serialized size
     *
     * @return serialized size of the given bloom filter
     */
+++++++++++++++++++++++
    public BloomFilter deserialize(DataInput dis) throws IOException
    {
        int hashes = dis.readInt();
        long bitLength = dis.readInt();
        OpenBitSet bs = new OpenBitSet(bitLength << 6);
        int pageSize = bs.getPageSize();
        int pageCount = bs.getPageCount();

        for (int p = 0; p < pageCount; p++)
        {
            long[] bits = bs.getPage(p);
            for (int i = 0; i < pageSize && bitLength-- > 0; i++)
                bits[i] = dis.readLong();
        }

        return new BloomFilter(hashes, bs);
    }

    /**
     * Calculates a serialized size of the given Bloom Filter
     * @see this.serialize(BloomFilter, DataOutput)
     *
     * @param bf Bloom filter to calculate serialized size
     *
     * @return serialized size of the given bloom filter
     */
    public static ICompactSerializer<LegacyBloomFilter> serializer()
    {
        return serializer_;
    }

+++++++++++++++++++++++
    public static LegacyBloomFilterSerializer serializer()
    {
        return serializer_;
    }

    public void serialize(LegacyBloomFilter bf, DataOutputStream dos, int version)
            throws IOException
    {
        throw new UnsupportedOperationException("Shouldn't be serializing legacy bloom filters");
//        dos.writeInt(bf.getHashCount());
//        ObjectOutputStream oos = new ObjectOutputStream(dos);
//        oos.writeObject(bf.getBitSet());
//        oos.flush();
    }

+++++++++++++++++++++++
    public void serialize(LegacyBloomFilter bf, DataOutput dos)
            throws IOException
    {
        throw new UnsupportedOperationException("Shouldn't be serializing legacy bloom filters");
//        dos.writeInt(bf.getHashCount());
//        ObjectOutputStream oos = new ObjectOutputStream(dos);
//        oos.writeObject(bf.getBitSet());
//        oos.flush();
    }

    public LegacyBloomFilter deserialize(DataInputStream dis, int version) throws IOException
    {
        int hashes = dis.readInt();
        ObjectInputStream ois = new ObjectInputStream(dis);
        try
        {
          BitSet bs = (BitSet) ois.readObject();
          return new LegacyBloomFilter(hashes, bs);
        } catch (ClassNotFoundException e)
        {
          throw new RuntimeException(e);
        }
+++++++++++++++++++++++
    public LegacyBloomFilter deserialize(DataInputStream dis) throws IOException
    {
        int hashes = dis.readInt();
        ObjectInputStream ois = new ObjectInputStream(dis);
        try
        {
          BitSet bs = (BitSet) ois.readObject();
          return new LegacyBloomFilter(hashes, bs);
        } catch (ClassNotFoundException e)
        {
          throw new RuntimeException(e);
        }
    }

    public void serialize(Message t, DataOutputStream dos, int version) throws IOException
    {
        assert t.getVersion() == version : "internode protocol version mismatch"; // indicates programmer error.
        Header.serializer().serialize( t.header_, dos, version);
        byte[] bytes = t.getMessageBody();
        dos.writeInt(bytes.length);
        dos.write(bytes);
    }

+++++++++++++++++++++++
    public void serialize(Message t, DataOutput dos, int version) throws IOException
    {
        assert t.getVersion() == version : "internode protocol version mismatch"; // indicates programmer error.
        Header.serializer().serialize( t.header_, dos, version);
        byte[] bytes = t.getMessageBody();
        dos.writeInt(bytes.length);
        dos.write(bytes);
    }

    public Message deserialize(DataInputStream dis, int version) throws IOException
    {
        Header header = Header.serializer().deserialize(dis, version);
        int size = dis.readInt();
        byte[] bytes = new byte[size];
        dis.readFully(bytes);
        return new Message(header, bytes, version);
+++++++++++++++++++++++
    public Message deserialize(DataInput dis, int version) throws IOException
    {
        Header header = Header.serializer().deserialize(dis, version);
        int size = dis.readInt();
        byte[] bytes = new byte[size];
        dis.readFully(bytes);
        return new Message(header, bytes, version);
    }

add: 8, delete: 9, change: 163, unhandled: 0 size_exceptions: 1 size_serialize: 31
-----------cassandra-1.0.1 vs cassandra-1.0.2-----------
add: 3, delete: 0, change: 40, unhandled: 0 size_exceptions: 0 size_serialize: 0
-----------cassandra-1.0.2 vs cassandra-1.0.3-----------
    public ColumnFamily deserialize(DataInput dis, boolean fromRemote, ISortedColumns.Factory factory) throws IOException
    {
        if (!dis.readBoolean())
            return null;

        // create a ColumnFamily based on the cf id
        int cfId = dis.readInt();
        if (Schema.instance.getCF(cfId) == null)
            throw new UnserializableColumnFamilyException("Couldn't find cfId=" + cfId, cfId);
        ColumnFamily cf = ColumnFamily.create(cfId, factory);
        deserializeFromSSTableNoColumns(cf, dis);
        deserializeColumns(dis, cf, fromRemote);
        return cf;
    }

+++++++++++++++++++++++
    public ColumnFamily deserialize(DataInput dis, IColumnSerializer.Flag flag, ISortedColumns.Factory factory) throws IOException
    {
        if (!dis.readBoolean())
            return null;

        // create a ColumnFamily based on the cf id
        int cfId = dis.readInt();
        if (Schema.instance.getCF(cfId) == null)
            throw new UnserializableColumnFamilyException("Couldn't find cfId=" + cfId, cfId);
        ColumnFamily cf = ColumnFamily.create(cfId, factory);
        deserializeFromSSTableNoColumns(cf, dis);
        deserializeColumns(dis, cf, flag);
        return cf;
    }

    public void deserializeColumns(DataInput dis, ColumnFamily cf, int size, boolean fromRemote) throws IOException
    {
        for (int i = 0; i < size; ++i)
        {
            IColumn column = cf.getColumnSerializer().deserialize(dis, fromRemote, (int) (System.currentTimeMillis() / 1000));
            cf.addColumn(column);
        }
    }

+++++++++++++++++++++++
    public void deserializeColumns(DataInput dis, ColumnFamily cf, int size, IColumnSerializer.Flag flag) throws IOException
    {
        for (int i = 0; i < size; ++i)
        {
            IColumn column = cf.getColumnSerializer().deserialize(dis, flag, (int) (System.currentTimeMillis() / 1000));
            cf.addColumn(column);
        }
    }

    public Column deserialize(DataInput dis, boolean fromRemote, int expireBefore) throws IOException
    {
        ByteBuffer name = ByteBufferUtil.readWithShortLength(dis);
        if (name.remaining() <= 0)
        {
            String format = "invalid column name length %d%s";
            String details = "";
            if (dis instanceof FileDataInput)
            {
                FileDataInput fdis = (FileDataInput)dis;
                details = String.format(" (%s, %d bytes remaining)", fdis.getPath(), fdis.bytesRemaining());
            }
            throw new CorruptColumnException(String.format(format, name.remaining(), details));
        }

        int b = dis.readUnsignedByte();
        if ((b & COUNTER_MASK) != 0)
        {
            long timestampOfLastDelete = dis.readLong();
            long ts = dis.readLong();
            ByteBuffer value = ByteBufferUtil.readWithLength(dis);
            return CounterColumn.create(name, value, ts, timestampOfLastDelete, fromRemote);
        }
        else if ((b & EXPIRATION_MASK) != 0)
        {
            int ttl = dis.readInt();
            int expiration = dis.readInt();
            long ts = dis.readLong();
            ByteBuffer value = ByteBufferUtil.readWithLength(dis);
            return ExpiringColumn.create(name, value, ts, ttl, expiration, expireBefore);
        }
        else
        {
            long ts = dis.readLong();
            ByteBuffer value = ByteBufferUtil.readWithLength(dis);
            return (b & COUNTER_UPDATE_MASK) != 0
                   ? new CounterUpdateColumn(name, value, ts)
                   : ((b & DELETION_MASK) == 0
                      ? new Column(name, value, ts)
                      : new DeletedColumn(name, value, ts));
        }
    }

+++++++++++++++++++++++
    public Column deserialize(DataInput dis, IColumnSerializer.Flag flag, int expireBefore) throws IOException
    {
        ByteBuffer name = ByteBufferUtil.readWithShortLength(dis);
        if (name.remaining() <= 0)
        {
            String format = "invalid column name length %d%s";
            String details = "";
            if (dis instanceof FileDataInput)
            {
                FileDataInput fdis = (FileDataInput)dis;
                details = String.format(" (%s, %d bytes remaining)", fdis.getPath(), fdis.bytesRemaining());
            }
            throw new CorruptColumnException(String.format(format, name.remaining(), details));
        }

        int b = dis.readUnsignedByte();
        if ((b & COUNTER_MASK) != 0)
        {
            long timestampOfLastDelete = dis.readLong();
            long ts = dis.readLong();
            ByteBuffer value = ByteBufferUtil.readWithLength(dis);
            return CounterColumn.create(name, value, ts, timestampOfLastDelete, flag);
        }
        else if ((b & EXPIRATION_MASK) != 0)
        {
            int ttl = dis.readInt();
            int expiration = dis.readInt();
            long ts = dis.readLong();
            ByteBuffer value = ByteBufferUtil.readWithLength(dis);
            return ExpiringColumn.create(name, value, ts, ttl, expiration, expireBefore, flag);
        }
        else
        {
            long ts = dis.readLong();
            ByteBuffer value = ByteBufferUtil.readWithLength(dis);
            return (b & COUNTER_UPDATE_MASK) != 0
                   ? new CounterUpdateColumn(name, value, ts)
                   : ((b & DELETION_MASK) == 0
                      ? new Column(name, value, ts)
                      : new DeletedColumn(name, value, ts));
        }
    }

    public IColumn deserialize(DataInput dis, boolean fromRemote, int expireBefore) throws IOException
    {
        ByteBuffer name = ByteBufferUtil.readWithShortLength(dis);
        int localDeleteTime = dis.readInt();
        if (localDeleteTime != Integer.MIN_VALUE && localDeleteTime <= 0)
        {
            throw new IOException("Invalid localDeleteTime read: " + localDeleteTime);
        }
        long markedForDeleteAt = dis.readLong();

        /* read the number of columns */
        int size = dis.readInt();
        ColumnSerializer serializer = Column.serializer();
        ColumnSortedMap preSortedMap = new ColumnSortedMap(comparator, serializer, dis, size, fromRemote, expireBefore);
        SuperColumn superColumn = new SuperColumn(name, ThreadSafeSortedColumns.factory().fromSorted(preSortedMap, false));
        if (localDeleteTime != Integer.MIN_VALUE && localDeleteTime <= 0)
        {
            throw new IOException("Invalid localDeleteTime read: " + localDeleteTime);
        }
        superColumn.delete(localDeleteTime, markedForDeleteAt);
        return superColumn;
    }

+++++++++++++++++++++++
    public IColumn deserialize(DataInput dis, IColumnSerializer.Flag flag, int expireBefore) throws IOException
    {
        ByteBuffer name = ByteBufferUtil.readWithShortLength(dis);
        int localDeleteTime = dis.readInt();
        if (localDeleteTime != Integer.MIN_VALUE && localDeleteTime <= 0)
        {
            throw new IOException("Invalid localDeleteTime read: " + localDeleteTime);
        }
        long markedForDeleteAt = dis.readLong();

        /* read the number of columns */
        int size = dis.readInt();
        ColumnSerializer serializer = Column.serializer();
        ColumnSortedMap preSortedMap = new ColumnSortedMap(comparator, serializer, dis, size, flag, expireBefore);
        SuperColumn superColumn = new SuperColumn(name, ThreadSafeSortedColumns.factory().fromSorted(preSortedMap, false));
        superColumn.delete(localDeleteTime, markedForDeleteAt);
        return superColumn;
    }

    private IColumn deserializeNext()
    {
        try
        {
            count++;
            return serializer.deserialize(dis, fromRemote, expireBefore);
        }
        catch (IOException e)
        {
            throw new IOError(e);
        }
    }

+++++++++++++++++++++++
    private IColumn deserializeNext()
    {
        try
        {
            count++;
            return serializer.deserialize(dis, flag, expireBefore);
        }
        catch (IOException e)
        {
            throw new IOError(e);
        }
    }

add: 0, delete: 0, change: 66, unhandled: 0 size_exceptions: 0 size_serialize: 5
-----------cassandra-1.0.3 vs cassandra-1.0.4-----------
add: 0, delete: 0, change: 48, unhandled: 0 size_exceptions: 0 size_serialize: 0
-----------cassandra-1.0.4 vs cassandra-1.0.5-----------
add: 0, delete: 0, change: 5, unhandled: 0 size_exceptions: 0 size_serialize: 0
-----------cassandra-1.0.5 vs cassandra-1.0.6-----------
add: 0, delete: 7, change: 55, unhandled: 0 size_exceptions: 0 size_serialize: 0
-----------cassandra-1.0.6 vs cassandra-1.0.7-----------
add: 0, delete: 1, change: 64, unhandled: 0 size_exceptions: 3 size_serialize: 0
-----------cassandra-1.0.7 vs cassandra-1.0.8-----------
add: 1, delete: 0, change: 50, unhandled: 0 size_exceptions: 2 size_serialize: 0
-----------cassandra-1.0.8 vs cassandra-1.0.9-----------
    public ColumnFamily deserialize(DataInput dis, IColumnSerializer.Flag flag, ISortedColumns.Factory factory) throws IOException
    {
        if (!dis.readBoolean())
            return null;

        // create a ColumnFamily based on the cf id
        int cfId = dis.readInt();
        if (Schema.instance.getCF(cfId) == null)
            throw new UnserializableColumnFamilyException("Couldn't find cfId=" + cfId, cfId);
        ColumnFamily cf = ColumnFamily.create(cfId, factory);
        deserializeFromSSTableNoColumns(cf, dis);
        deserializeColumns(dis, cf, flag);
        return cf;
    }

+++++++++++++++++++++++
    public ColumnFamily deserialize(DataInput dis, IColumnSerializer.Flag flag, ISortedColumns.Factory factory) throws IOException
    {
        if (!dis.readBoolean())
            return null;

        // create a ColumnFamily based on the cf id
        int cfId = dis.readInt();
        if (Schema.instance.getCF(cfId) == null)
            throw new UnknownColumnFamilyException("Couldn't find cfId=" + cfId, cfId);
        ColumnFamily cf = ColumnFamily.create(cfId, factory);
        deserializeFromSSTableNoColumns(cf, dis);
        deserializeColumns(dis, cf, flag);
        return cf;
    }

    public void serialize(Message t, DataOutput dos, int version) throws IOException
    {
        assert t.getVersion() == version : "internode protocol version mismatch"; // indicates programmer error.
        Header.serializer().serialize( t.header_, dos, version);
        byte[] bytes = t.getMessageBody();
        dos.writeInt(bytes.length);
        dos.write(bytes);
    }

+++++++++++++++++++++++
    public void serialize(Message t, DataOutput dos, int version) throws IOException
    {
        Header.serializer().serialize( t.header_, dos, version);
        byte[] bytes = t.getMessageBody();
        dos.writeInt(bytes.length);
        dos.write(bytes);
    }

add: 1, delete: 3, change: 41, unhandled: 0 size_exceptions: 0 size_serialize: 2
-----------cassandra-1.0.9 vs cassandra-1.0.10-----------
add: 1, delete: 0, change: 25, unhandled: 0 size_exceptions: 0 size_serialize: 0
-----------cassandra-1.0.10 vs cassandra-1.0.11-----------
add: 1, delete: 0, change: 29, unhandled: 0 size_exceptions: 2 size_serialize: 0
-----------cassandra-1.0.11 vs cassandra-1.0.12-----------
add: 0, delete: 0, change: 5, unhandled: 0 size_exceptions: 0 size_serialize: 0
-----------cassandra-1.0.12 vs cassandra-1.1.0-----------
    public void serialize(RangeSliceCommand sliceCommand, DataOutput dos, int version) throws IOException
    {
        dos.writeUTF(sliceCommand.keyspace);
        dos.writeUTF(sliceCommand.column_family);
        ByteBuffer sc = sliceCommand.super_column;
        dos.writeInt(sc == null ? 0 : sc.remaining());
        if (sc != null)
            ByteBufferUtil.write(sc, dos);

        TSerializer ser = new TSerializer(new TBinaryProtocol.Factory());
        FBUtilities.serialize(ser, sliceCommand.predicate, dos);
        AbstractBounds.serializer().serialize(sliceCommand.range, dos);
        dos.writeInt(sliceCommand.max_keys);
    }

+++++++++++++++++++++++
    public void serialize(RangeSliceCommand sliceCommand, DataOutput dos, int version) throws IOException
    {
        dos.writeUTF(sliceCommand.keyspace);
        dos.writeUTF(sliceCommand.column_family);
        ByteBuffer sc = sliceCommand.super_column;
        dos.writeInt(sc == null ? 0 : sc.remaining());
        if (sc != null)
            ByteBufferUtil.write(sc, dos);

        TSerializer ser = new TSerializer(new TBinaryProtocol.Factory());
        FBUtilities.serialize(ser, sliceCommand.predicate, dos);

        if (version >= MessagingService.VERSION_11)
        {
            if (sliceCommand.row_filter == null)
            {
                dos.writeInt(0);
            }
            else
            {
                dos.writeInt(sliceCommand.row_filter.size());
                for (IndexExpression expr : sliceCommand.row_filter)
                    FBUtilities.serialize(ser, expr, dos);
            }
        }
        AbstractBounds.serializer().serialize(sliceCommand.range, dos, version);
        dos.writeInt(sliceCommand.maxResults);
        if (version >= MessagingService.VERSION_11)
        {
            dos.writeBoolean(sliceCommand.maxIsColumns);
            dos.writeBoolean(sliceCommand.isPaging);
        }
    }

    public RangeSliceCommand deserialize(DataInput dis, int version) throws IOException
    {
        String keyspace = dis.readUTF();
        String column_family = dis.readUTF();

        int scLength = dis.readInt();
        ByteBuffer super_column = null;
        if (scLength > 0)
        {
            byte[] buf = new byte[scLength];
            dis.readFully(buf);
            super_column = ByteBuffer.wrap(buf);
        }

        TDeserializer dser = new TDeserializer(new TBinaryProtocol.Factory());
        SlicePredicate pred = new SlicePredicate();
        FBUtilities.deserialize(dser, pred, dis);

        AbstractBounds range = AbstractBounds.serializer().deserialize(dis);
        int max_keys = dis.readInt();
        return new RangeSliceCommand(keyspace, column_family, super_column, pred, range, max_keys);
    }

+++++++++++++++++++++++
    public RangeSliceCommand deserialize(DataInput dis, int version) throws IOException
    {
        String keyspace = dis.readUTF();
        String columnFamily = dis.readUTF();

        int scLength = dis.readInt();
        ByteBuffer superColumn = null;
        if (scLength > 0)
        {
            byte[] buf = new byte[scLength];
            dis.readFully(buf);
            superColumn = ByteBuffer.wrap(buf);
        }

        TDeserializer dser = new TDeserializer(new TBinaryProtocol.Factory());
        SlicePredicate pred = new SlicePredicate();
        FBUtilities.deserialize(dser, pred, dis);

        List<IndexExpression> rowFilter = null;
        if (version >= MessagingService.VERSION_11)
        {
            int filterCount = dis.readInt();
            rowFilter = new ArrayList<IndexExpression>(filterCount);
            for (int i = 0; i < filterCount; i++)
            {
                IndexExpression expr = new IndexExpression();
                FBUtilities.deserialize(dser, expr, dis);
                rowFilter.add(expr);
            }
        }
        AbstractBounds<RowPosition> range = AbstractBounds.serializer().deserialize(dis, version).toRowBounds();

        int maxResults = dis.readInt();
        boolean maxIsColumns = false;
        boolean isPaging = false;
        if (version >= MessagingService.VERSION_11)
        {
            maxIsColumns = dis.readBoolean();
            isPaging = dis.readBoolean();
        }
        return new RangeSliceCommand(keyspace, columnFamily, superColumn, pred, range, rowFilter, maxResults, maxIsColumns, isPaging);
    }

    public static SuperColumnSerializer serializer(AbstractType comparator)
    {
        SuperColumnSerializer serializer = serializers.get(comparator);
        if (serializer == null)
        {
            serializer = new SuperColumnSerializer(comparator);
            serializers.put(comparator, serializer);
        }
        return serializer;
    }

+++++++++++++++++++++++
    public static SuperColumnSerializer serializer(AbstractType<?> comparator)
    {
        SuperColumnSerializer serializer = serializers.get(comparator);
        if (serializer == null)
        {
            serializer = new SuperColumnSerializer(comparator);
            serializers.put(comparator, serializer);
        }
        return serializer;
    }

    public IColumn deserialize(DataInput dis, IColumnSerializer.Flag flag, int expireBefore) throws IOException
    {
        ByteBuffer name = ByteBufferUtil.readWithShortLength(dis);
        int localDeleteTime = dis.readInt();
        if (localDeleteTime != Integer.MIN_VALUE && localDeleteTime <= 0)
        {
            throw new IOException("Invalid localDeleteTime read: " + localDeleteTime);
        }
        long markedForDeleteAt = dis.readLong();

        /* read the number of columns */
        int size = dis.readInt();
        ColumnSerializer serializer = Column.serializer();
        ColumnSortedMap preSortedMap = new ColumnSortedMap(comparator, serializer, dis, size, flag, expireBefore);
        SuperColumn superColumn = new SuperColumn(name, ThreadSafeSortedColumns.factory().fromSorted(preSortedMap, false));
        superColumn.delete(localDeleteTime, markedForDeleteAt);
        return superColumn;
    }

+++++++++++++++++++++++
    public IColumn deserialize(DataInput dis, IColumnSerializer.Flag flag, int expireBefore) throws IOException
    {
        ByteBuffer name = ByteBufferUtil.readWithShortLength(dis);
        int localDeleteTime = dis.readInt();
        if (localDeleteTime != Integer.MIN_VALUE && localDeleteTime <= 0)
        {
            throw new IOException("Invalid localDeleteTime read: " + localDeleteTime);
        }
        long markedForDeleteAt = dis.readLong();

        /* read the number of columns */
        int size = dis.readInt();
        ColumnSerializer serializer = Column.serializer();
        ColumnSortedMap preSortedMap = new ColumnSortedMap(comparator, serializer, dis, size, flag, expireBefore);
        SuperColumn superColumn = new SuperColumn(name, AtomicSortedColumns.factory().fromSorted(preSortedMap, false));
        superColumn.delete(localDeleteTime, markedForDeleteAt);
        return superColumn;
    }

    public synchronized void serialize()
    {
        File manifestFile = tryGetManifest(cfs);
        if (manifestFile == null)
            manifestFile = new File(new File(DatabaseDescriptor.getAllDataFileLocations()[0], cfs.table.name), cfs.columnFamily + ".json");
        File oldFile = new File(manifestFile.getPath().replace(EXTENSION, "-old.json"));
        File tmpFile = new File(manifestFile.getPath().replace(EXTENSION, "-tmp.json"));

        JsonFactory f = new JsonFactory();
        try
        {
            JsonGenerator g = f.createJsonGenerator(tmpFile, JsonEncoding.UTF8);
            g.useDefaultPrettyPrinter();
            g.writeStartObject();
            g.writeArrayFieldStart("generations");
            for (int level = 0; level < generations.length; level++)
            {
                g.writeStartObject();
                g.writeNumberField("generation", level);
                g.writeArrayFieldStart("members");
                for (SSTableReader ssTableReader : generations[level])
                    g.writeNumber(ssTableReader.descriptor.generation);
                g.writeEndArray(); // members

                g.writeEndObject(); // generation
            }
            g.writeEndArray(); // for field generations
            g.writeEndObject(); // write global object
            g.close();

            if (oldFile.exists() && manifestFile.exists())
                FileUtils.deleteWithConfirm(oldFile);
            if (manifestFile.exists())
                FileUtils.renameWithConfirm(manifestFile, oldFile);
            assert tmpFile.exists();
            FileUtils.renameWithConfirm(tmpFile, manifestFile);
            logger.debug("Saved manifest {}", manifestFile);
        }
        catch (IOException e)
        {
            throw new IOError(e);
        }
    }

+++++++++++++++++++++++
    public synchronized void serialize()
    {
        File manifestFile = cfs.directories.getOrCreateLeveledManifest();
        File oldFile = new File(manifestFile.getPath().replace(EXTENSION, "-old.json"));
        File tmpFile = new File(manifestFile.getPath().replace(EXTENSION, "-tmp.json"));

        JsonFactory f = new JsonFactory();
        try
        {
            JsonGenerator g = f.createJsonGenerator(tmpFile, JsonEncoding.UTF8);
            g.useDefaultPrettyPrinter();
            g.writeStartObject();
            g.writeArrayFieldStart("generations");
            for (int level = 0; level < generations.length; level++)
            {
                g.writeStartObject();
                g.writeNumberField("generation", level);
                g.writeArrayFieldStart("members");
                for (SSTableReader ssTableReader : generations[level])
                    g.writeNumber(ssTableReader.descriptor.generation);
                g.writeEndArray(); // members

                g.writeEndObject(); // generation
            }
            g.writeEndArray(); // for field generations
            g.writeEndObject(); // write global object
            g.close();

            if (oldFile.exists() && manifestFile.exists())
                FileUtils.deleteWithConfirm(oldFile);
            if (manifestFile.exists())
                FileUtils.renameWithConfirm(manifestFile, oldFile);
            assert tmpFile.exists();
            FileUtils.renameWithConfirm(tmpFile, manifestFile);
            logger.debug("Saved manifest {}", manifestFile);
        }
        catch (IOException e)
        {
            throw new IOError(e);
        }
    }

    @Override
    protected void testSerializeDeserialize(ColumnDefinition cd) throws Exception
    {
        ColumnDefinition newCd = ColumnDefinition.fromAvro(cd.toAvro());
        assert cd != newCd;
        assert cd.hashCode() == newCd.hashCode();
        assert cd.equals(newCd);
+++++++++++++++++++++++
    protected void testSerializeDeserialize(ColumnDefinition cd) throws Exception
    {
        ColumnDefinition newCd = ColumnDefinition.fromThrift(cd.toThrift());
        assert cd != newCd;
        assert cd.hashCode() == newCd.hashCode();
        assert cd.equals(newCd);
add: 76, delete: 22, change: 566, unhandled: 0 size_exceptions: 15 size_serialize: 6
-----------cassandra-1.1.0 vs cassandra-1.1.0-beta1-----------
    public static Map<String, CFMetaData> deserializeColumnFamilies(Row row)
    {
        if (row.cf == null)
            return Collections.emptyMap();

        Map<String, CFMetaData> cfms = new HashMap<String, CFMetaData>();
        UntypedResultSet results = QueryProcessor.resultify("SELECT * FROM system.schema_columnfamilies", row);
        for (UntypedResultSet.Row result : results)
        {
            CFMetaData cfm = CFMetaData.fromSchema(result);
            cfms.put(cfm.cfName, cfm);
        }

        for (CFMetaData cfm : cfms.values())
        {
            Row columnRow = ColumnDefinition.readSchema(cfm.ksName, cfm.cfName);
            for (ColumnDefinition cd : ColumnDefinition.fromSchema(columnRow, cfm))
                cfm.column_metadata.put(cd.name, cd);
        }

        return cfms;
+++++++++++++++++++++++
    public static Map<String, CfDef> deserializeColumnFamilies(ColumnFamily serializedColumnFamilies)
    {
        Map<String, CfDef> cfs = new HashMap<String, CfDef>();

        if (serializedColumnFamilies == null)
            return cfs;

        AbstractType<?> comparator = serializedColumnFamilies.getComparator();

        for (IColumn column : serializedColumnFamilies.getSortedColumns())
        {
            if (column == null || column.isMarkedForDelete())
                continue;

            String[] attr = comparator.getString(column.name()).split(":");
            assert attr.length == 2;

            CfDef cfDef = cfs.get(attr[0]);

            if (cfDef == null)
            {
                cfDef = new CfDef();
                cfs.put(attr[0], cfDef);
            }

            CfDef._Fields field = CfDef._Fields.findByName(attr[1]);
            cfDef.setFieldValue(field, deserializeValue(column.value(), getValueClass(CfDef.class, field.getFieldName())));
        }

        for (CfDef cfDef : cfs.values())
        {
            for (ColumnDef columnDef : ColumnDefinition.fromSchema(ColumnDefinition.readSchema(cfDef.keyspace, cfDef.name)))
                cfDef.addToColumn_metadata(columnDef);
        }

        return cfs;
    public ColumnFamily deserialize(DataInput dis, IColumnSerializer.Flag flag, ISortedColumns.Factory factory) throws IOException
    {
        if (!dis.readBoolean())
            return null;

        // create a ColumnFamily based on the cf id
        int cfId = dis.readInt();
        if (Schema.instance.getCF(cfId) == null)
            throw new UnknownColumnFamilyException("Couldn't find cfId=" + cfId, cfId);
        ColumnFamily cf = ColumnFamily.create(cfId, factory);
        deserializeFromSSTableNoColumns(cf, dis);
        deserializeColumns(dis, cf, flag);
        return cf;
    }

+++++++++++++++++++++++
    public ColumnFamily deserialize(DataInput dis, IColumnSerializer.Flag flag, ISortedColumns.Factory factory) throws IOException
    {
        if (!dis.readBoolean())
            return null;

        // create a ColumnFamily based on the cf id
        int cfId = dis.readInt();
        if (Schema.instance.getCF(cfId) == null)
            throw new UnserializableColumnFamilyException("Couldn't find cfId=" + cfId, cfId);
        ColumnFamily cf = ColumnFamily.create(cfId, factory);
        deserializeFromSSTableNoColumns(cf, dis);
        deserializeColumns(dis, cf, flag);
        return cf;
    }

    private static Row serializedColumnFamilies(DecoratedKey ksNameKey)
    {
        ColumnFamilyStore cfsStore = SystemTable.schemaCFS(SystemTable.SCHEMA_COLUMNFAMILIES_CF);
        return new Row(ksNameKey, cfsStore.getColumnFamily(QueryFilter.getIdentityFilter(ksNameKey, new QueryPath(SystemTable.SCHEMA_COLUMNFAMILIES_CF))));
    }

    /**
     * Loads a version of keyspace definitions from storage (using old SCHEMA_CF as a data source)
     * Note: If definitions where found in SCHEMA_CF this method would load them into new schema handling table KEYSPACE_CF
     *
     * @param version The version of the latest migration.
     *
     * @return Collection of found keyspace definitions
     *
     * @throws IOException if failed to read SCHEMA_CF or failed to deserialize Avro schema
     */
+++++++++++++++++++++++
    private static ColumnFamily serializedColumnFamilies(DecoratedKey ksNameKey)
    {
        ColumnFamilyStore cfsStore = SystemTable.schemaCFS(SystemTable.SCHEMA_COLUMNFAMILIES_CF);
        return cfsStore.getColumnFamily(QueryFilter.getIdentityFilter(ksNameKey, new QueryPath(SystemTable.SCHEMA_COLUMNFAMILIES_CF)));
    }

    /**
     * Loads a version of keyspace definitions from storage (using old SCHEMA_CF as a data source)
     * Note: If definitions where found in SCHEMA_CF this method would load them into new schema handling table KEYSPACE_CF
     *
     * @param version The version of the latest migration.
     *
     * @return Collection of found keyspace definitions
     *
     * @throws IOException if failed to read SCHEMA_CF or failed to deserialize Avro schema
     */
    public void serialize(RangeSliceCommand sliceCommand, DataOutput dos, int version) throws IOException
    {
        dos.writeUTF(sliceCommand.keyspace);
        dos.writeUTF(sliceCommand.column_family);
        ByteBuffer sc = sliceCommand.super_column;
        dos.writeInt(sc == null ? 0 : sc.remaining());
        if (sc != null)
            ByteBufferUtil.write(sc, dos);

        TSerializer ser = new TSerializer(new TBinaryProtocol.Factory());
        FBUtilities.serialize(ser, sliceCommand.predicate, dos);

        if (version >= MessagingService.VERSION_11)
        {
            if (sliceCommand.row_filter == null)
            {
                dos.writeInt(0);
            }
            else
            {
                dos.writeInt(sliceCommand.row_filter.size());
                for (IndexExpression expr : sliceCommand.row_filter)
                    FBUtilities.serialize(ser, expr, dos);
            }
        }
        AbstractBounds.serializer().serialize(sliceCommand.range, dos, version);
        dos.writeInt(sliceCommand.maxResults);
        if (version >= MessagingService.VERSION_11)
        {
            dos.writeBoolean(sliceCommand.maxIsColumns);
            dos.writeBoolean(sliceCommand.isPaging);
        }
    }

+++++++++++++++++++++++
    public void serialize(RangeSliceCommand sliceCommand, DataOutput dos, int version) throws IOException
    {
        dos.writeUTF(sliceCommand.keyspace);
        dos.writeUTF(sliceCommand.column_family);
        ByteBuffer sc = sliceCommand.super_column;
        dos.writeInt(sc == null ? 0 : sc.remaining());
        if (sc != null)
            ByteBufferUtil.write(sc, dos);

        TSerializer ser = new TSerializer(new TBinaryProtocol.Factory());
        FBUtilities.serialize(ser, sliceCommand.predicate, dos);

        if (version >= MessagingService.VERSION_11)
        {
            if (sliceCommand.row_filter == null)
            {
                dos.writeInt(0);
            }
            else
            {
                dos.writeInt(sliceCommand.row_filter.size());
                for (IndexExpression expr : sliceCommand.row_filter)
                    FBUtilities.serialize(ser, expr, dos);
            }
        }
        AbstractBounds.serializer().serialize(sliceCommand.range, dos, version);
        dos.writeInt(sliceCommand.maxResults);
        if (version >= MessagingService.VERSION_11)
        {
            dos.writeBoolean(sliceCommand.maxIsColumns);
        }
    }

    public RangeSliceCommand deserialize(DataInput dis, int version) throws IOException
    {
        String keyspace = dis.readUTF();
        String columnFamily = dis.readUTF();

        int scLength = dis.readInt();
        ByteBuffer superColumn = null;
        if (scLength > 0)
        {
            byte[] buf = new byte[scLength];
            dis.readFully(buf);
            superColumn = ByteBuffer.wrap(buf);
        }

        TDeserializer dser = new TDeserializer(new TBinaryProtocol.Factory());
        SlicePredicate pred = new SlicePredicate();
        FBUtilities.deserialize(dser, pred, dis);

        List<IndexExpression> rowFilter = null;
        if (version >= MessagingService.VERSION_11)
        {
            int filterCount = dis.readInt();
            rowFilter = new ArrayList<IndexExpression>(filterCount);
            for (int i = 0; i < filterCount; i++)
            {
                IndexExpression expr = new IndexExpression();
                FBUtilities.deserialize(dser, expr, dis);
                rowFilter.add(expr);
            }
        }
        AbstractBounds<RowPosition> range = AbstractBounds.serializer().deserialize(dis, version).toRowBounds();

        int maxResults = dis.readInt();
        boolean maxIsColumns = false;
        boolean isPaging = false;
        if (version >= MessagingService.VERSION_11)
        {
            maxIsColumns = dis.readBoolean();
            isPaging = dis.readBoolean();
        }
        return new RangeSliceCommand(keyspace, columnFamily, superColumn, pred, range, rowFilter, maxResults, maxIsColumns, isPaging);
    }

+++++++++++++++++++++++
    public RangeSliceCommand deserialize(DataInput dis, int version) throws IOException
    {
        String keyspace = dis.readUTF();
        String columnFamily = dis.readUTF();

        int scLength = dis.readInt();
        ByteBuffer superColumn = null;
        if (scLength > 0)
        {
            byte[] buf = new byte[scLength];
            dis.readFully(buf);
            superColumn = ByteBuffer.wrap(buf);
        }

        TDeserializer dser = new TDeserializer(new TBinaryProtocol.Factory());
        SlicePredicate pred = new SlicePredicate();
        FBUtilities.deserialize(dser, pred, dis);

        List<IndexExpression> rowFilter = null;
        if (version >= MessagingService.VERSION_11)
        {
            int filterCount = dis.readInt();
            rowFilter = new ArrayList<IndexExpression>(filterCount);
            for (int i = 0; i < filterCount; i++)
            {
                IndexExpression expr = new IndexExpression();
                FBUtilities.deserialize(dser, expr, dis);
                rowFilter.add(expr);
            }
        }
        AbstractBounds<RowPosition> range = AbstractBounds.serializer().deserialize(dis, version).toRowBounds();

        int maxResults = dis.readInt();
        boolean maxIsColumns = false;
        if (version >= MessagingService.VERSION_11)
        {
            maxIsColumns = dis.readBoolean();
        }
        return new RangeSliceCommand(keyspace, columnFamily, superColumn, pred, range, rowFilter, maxResults, maxIsColumns);
    }

    public static Collection<RowMutation> deserializeMigrationMessage(byte[] data, int version) throws IOException
    {
        Collection<RowMutation> schema = new ArrayList<RowMutation>();
        DataInputStream in = new DataInputStream(new FastByteArrayInputStream(data));

        int count = in.readInt();

        for (int i = 0; i < count; i++)
            schema.add(RowMutation.serializer().deserialize(in, version));

        return schema;
    }

    /**
     * Clear all locally stored schema information and reset schema to initial state.
     * Called by user (via JMX) who wants to get rid of schema disagreement.
     *
     * @throws IOException if schema tables truncation fails
     */
+++++++++++++++++++++++
    public static Collection<RowMutation> deserializeMigrationMessage(byte[] data, int version) throws IOException
    {
        if (version < MessagingService.VERSION_11)
            throw new IOException("Can't accept schema migrations from Cassandra versions previous to 1.1, please update first.");

        Collection<RowMutation> schema = new ArrayList<RowMutation>();
        DataInputStream in = new DataInputStream(new FastByteArrayInputStream(data));

        int count = in.readInt();

        for (int i = 0; i < count; i++)
            schema.add(RowMutation.serializer().deserialize(in, version));

        return schema;
    public void serialize(Message t, DataOutput dos, int version) throws IOException
    {
        Header.serializer().serialize( t.header_, dos, version);
        byte[] bytes = t.getMessageBody();
        dos.writeInt(bytes.length);
        dos.write(bytes);
    }

+++++++++++++++++++++++
    public void serialize(Message t, DataOutput dos, int version) throws IOException
    {
        assert t.getVersion() == version : "internode protocol version mismatch"; // indicates programmer error.
        Header.serializer().serialize( t.header_, dos, version);
        byte[] bytes = t.getMessageBody();
        dos.writeInt(bytes.length);
        dos.write(bytes);
    }

add: 13, delete: 11, change: 533, unhandled: 0 size_exceptions: 4 size_serialize: 7
-----------cassandra-1.1.0-beta1 vs cassandra-1.1.0-beta2-----------
    public static Map<String, CfDef> deserializeColumnFamilies(ColumnFamily serializedColumnFamilies)
    {
        Map<String, CfDef> cfs = new HashMap<String, CfDef>();

        if (serializedColumnFamilies == null)
            return cfs;

        AbstractType<?> comparator = serializedColumnFamilies.getComparator();

        for (IColumn column : serializedColumnFamilies.getSortedColumns())
        {
            if (column == null || column.isMarkedForDelete())
                continue;

            String[] attr = comparator.getString(column.name()).split(":");
            assert attr.length == 2;

            CfDef cfDef = cfs.get(attr[0]);

            if (cfDef == null)
            {
                cfDef = new CfDef();
                cfs.put(attr[0], cfDef);
            }

            CfDef._Fields field = CfDef._Fields.findByName(attr[1]);
            cfDef.setFieldValue(field, deserializeValue(column.value(), getValueClass(CfDef.class, field.getFieldName())));
        }

        for (CfDef cfDef : cfs.values())
        {
            for (ColumnDef columnDef : ColumnDefinition.fromSchema(ColumnDefinition.readSchema(cfDef.keyspace, cfDef.name)))
                cfDef.addToColumn_metadata(columnDef);
        }

        return cfs;
+++++++++++++++++++++++
    public static Map<String, CFMetaData> deserializeColumnFamilies(Row row)
    {
        if (row.cf == null)
            return Collections.emptyMap();

        Map<String, CFMetaData> cfms = new HashMap<String, CFMetaData>();
        UntypedResultSet results = QueryProcessor.resultify("SELECT * FROM system.schema_columnfamilies", row);
        for (UntypedResultSet.Row result : results)
        {
            CFMetaData cfm = CFMetaData.fromSchema(result);
            cfms.put(cfm.cfName, cfm);
        }

        for (CFMetaData cfm : cfms.values())
        {
            Row columnRow = ColumnDefinition.readSchema(cfm.ksName, cfm.cfName);
            for (ColumnDefinition cd : ColumnDefinition.fromSchema(columnRow, cfm.getColumnDefinitionComparator()))
                cfm.column_metadata.put(cd.name, cd);
        }

        return cfms;
    }

    public ColumnFamily deserialize(DataInput dis, IColumnSerializer.Flag flag, ISortedColumns.Factory factory) throws IOException
    {
        if (!dis.readBoolean())
            return null;

        // create a ColumnFamily based on the cf id
        int cfId = dis.readInt();
        if (Schema.instance.getCF(cfId) == null)
            throw new UnserializableColumnFamilyException("Couldn't find cfId=" + cfId, cfId);
        ColumnFamily cf = ColumnFamily.create(cfId, factory);
        deserializeFromSSTableNoColumns(cf, dis);
        deserializeColumns(dis, cf, flag);
        return cf;
    }

+++++++++++++++++++++++
    public ColumnFamily deserialize(DataInput dis, IColumnSerializer.Flag flag, ISortedColumns.Factory factory) throws IOException
    {
        if (!dis.readBoolean())
            return null;

        // create a ColumnFamily based on the cf id
        int cfId = dis.readInt();
        if (Schema.instance.getCF(cfId) == null)
            throw new UnknownColumnFamilyException("Couldn't find cfId=" + cfId, cfId);
        ColumnFamily cf = ColumnFamily.create(cfId, factory);
        deserializeFromSSTableNoColumns(cf, dis);
        deserializeColumns(dis, cf, flag);
        return cf;
    }

    private static ColumnFamily serializedColumnFamilies(DecoratedKey ksNameKey)
    {
        ColumnFamilyStore cfsStore = SystemTable.schemaCFS(SystemTable.SCHEMA_COLUMNFAMILIES_CF);
        return cfsStore.getColumnFamily(QueryFilter.getIdentityFilter(ksNameKey, new QueryPath(SystemTable.SCHEMA_COLUMNFAMILIES_CF)));
    }

    /**
     * Loads a version of keyspace definitions from storage (using old SCHEMA_CF as a data source)
     * Note: If definitions where found in SCHEMA_CF this method would load them into new schema handling table KEYSPACE_CF
     *
     * @param version The version of the latest migration.
     *
     * @return Collection of found keyspace definitions
     *
     * @throws IOException if failed to read SCHEMA_CF or failed to deserialize Avro schema
     */
+++++++++++++++++++++++
    private static Row serializedColumnFamilies(DecoratedKey ksNameKey)
    {
        ColumnFamilyStore cfsStore = SystemTable.schemaCFS(SystemTable.SCHEMA_COLUMNFAMILIES_CF);
        return new Row(ksNameKey, cfsStore.getColumnFamily(QueryFilter.getIdentityFilter(ksNameKey, new QueryPath(SystemTable.SCHEMA_COLUMNFAMILIES_CF))));
    }

    /**
     * Loads a version of keyspace definitions from storage (using old SCHEMA_CF as a data source)
     * Note: If definitions where found in SCHEMA_CF this method would load them into new schema handling table KEYSPACE_CF
     *
     * @param version The version of the latest migration.
     *
     * @return Collection of found keyspace definitions
     *
     * @throws IOException if failed to read SCHEMA_CF or failed to deserialize Avro schema
     */
    public static Collection<RowMutation> deserializeMigrationMessage(byte[] data, int version) throws IOException
    {
        if (version < MessagingService.VERSION_11)
            throw new IOException("Can't accept schema migrations from Cassandra versions previous to 1.1, please update first.");

        Collection<RowMutation> schema = new ArrayList<RowMutation>();
        DataInputStream in = new DataInputStream(new FastByteArrayInputStream(data));

        int count = in.readInt();

        for (int i = 0; i < count; i++)
            schema.add(RowMutation.serializer().deserialize(in, version));

        return schema;
+++++++++++++++++++++++
    public static Collection<RowMutation> deserializeMigrationMessage(byte[] data, int version) throws IOException
    {
        Collection<RowMutation> schema = new ArrayList<RowMutation>();
        DataInputStream in = new DataInputStream(new FastByteArrayInputStream(data));

        int count = in.readInt();

        for (int i = 0; i < count; i++)
            schema.add(RowMutation.serializer().deserialize(in, version));

        return schema;
    }

    /**
     * Clear all locally stored schema information and reset schema to initial state.
     * Called by user (via JMX) who wants to get rid of schema disagreement.
     *
     * @throws IOException if schema tables truncation fails
     */
add: 9, delete: 13, change: 514, unhandled: 0 size_exceptions: 3 size_serialize: 4
-----------cassandra-1.1.0-beta2 vs cassandra-1.1.0-rc1-----------
    public static Map<String, CFMetaData> deserializeColumnFamilies(Row row)
    {
        if (row.cf == null)
            return Collections.emptyMap();

        Map<String, CFMetaData> cfms = new HashMap<String, CFMetaData>();
        UntypedResultSet results = QueryProcessor.resultify("SELECT * FROM system.schema_columnfamilies", row);
        for (UntypedResultSet.Row result : results)
        {
            CFMetaData cfm = CFMetaData.fromSchema(result);
            cfms.put(cfm.cfName, cfm);
        }

        for (CFMetaData cfm : cfms.values())
        {
            Row columnRow = ColumnDefinition.readSchema(cfm.ksName, cfm.cfName);
            for (ColumnDefinition cd : ColumnDefinition.fromSchema(columnRow, cfm.getColumnDefinitionComparator()))
                cfm.column_metadata.put(cd.name, cd);
        }

        return cfms;
    }

+++++++++++++++++++++++
    public static Map<String, CFMetaData> deserializeColumnFamilies(Row row)
    {
        if (row.cf == null)
            return Collections.emptyMap();

        Map<String, CFMetaData> cfms = new HashMap<String, CFMetaData>();
        UntypedResultSet results = QueryProcessor.resultify("SELECT * FROM system.schema_columnfamilies", row);
        for (UntypedResultSet.Row result : results)
        {
            CFMetaData cfm = CFMetaData.fromSchema(result);
            cfms.put(cfm.cfName, cfm);
        }

        for (CFMetaData cfm : cfms.values())
        {
            Row columnRow = ColumnDefinition.readSchema(cfm.ksName, cfm.cfName);
            for (ColumnDefinition cd : ColumnDefinition.fromSchema(columnRow, cfm))
                cfm.column_metadata.put(cd.name, cd);
        }

        return cfms;
    public void serialize(RangeSliceCommand sliceCommand, DataOutput dos, int version) throws IOException
    {
        dos.writeUTF(sliceCommand.keyspace);
        dos.writeUTF(sliceCommand.column_family);
        ByteBuffer sc = sliceCommand.super_column;
        dos.writeInt(sc == null ? 0 : sc.remaining());
        if (sc != null)
            ByteBufferUtil.write(sc, dos);

        TSerializer ser = new TSerializer(new TBinaryProtocol.Factory());
        FBUtilities.serialize(ser, sliceCommand.predicate, dos);

        if (version >= MessagingService.VERSION_11)
        {
            if (sliceCommand.row_filter == null)
            {
                dos.writeInt(0);
            }
            else
            {
                dos.writeInt(sliceCommand.row_filter.size());
                for (IndexExpression expr : sliceCommand.row_filter)
                    FBUtilities.serialize(ser, expr, dos);
            }
        }
        AbstractBounds.serializer().serialize(sliceCommand.range, dos, version);
        dos.writeInt(sliceCommand.maxResults);
        if (version >= MessagingService.VERSION_11)
        {
            dos.writeBoolean(sliceCommand.maxIsColumns);
        }
    }

+++++++++++++++++++++++
    public void serialize(RangeSliceCommand sliceCommand, DataOutput dos, int version) throws IOException
    {
        dos.writeUTF(sliceCommand.keyspace);
        dos.writeUTF(sliceCommand.column_family);
        ByteBuffer sc = sliceCommand.super_column;
        dos.writeInt(sc == null ? 0 : sc.remaining());
        if (sc != null)
            ByteBufferUtil.write(sc, dos);

        TSerializer ser = new TSerializer(new TBinaryProtocol.Factory());
        FBUtilities.serialize(ser, sliceCommand.predicate, dos);

        if (version >= MessagingService.VERSION_11)
        {
            if (sliceCommand.row_filter == null)
            {
                dos.writeInt(0);
            }
            else
            {
                dos.writeInt(sliceCommand.row_filter.size());
                for (IndexExpression expr : sliceCommand.row_filter)
                    FBUtilities.serialize(ser, expr, dos);
            }
        }
        AbstractBounds.serializer().serialize(sliceCommand.range, dos, version);
        dos.writeInt(sliceCommand.maxResults);
        if (version >= MessagingService.VERSION_11)
        {
            dos.writeBoolean(sliceCommand.maxIsColumns);
            dos.writeBoolean(sliceCommand.isPaging);
        }
    }

    public RangeSliceCommand deserialize(DataInput dis, int version) throws IOException
    {
        String keyspace = dis.readUTF();
        String columnFamily = dis.readUTF();

        int scLength = dis.readInt();
        ByteBuffer superColumn = null;
        if (scLength > 0)
        {
            byte[] buf = new byte[scLength];
            dis.readFully(buf);
            superColumn = ByteBuffer.wrap(buf);
        }

        TDeserializer dser = new TDeserializer(new TBinaryProtocol.Factory());
        SlicePredicate pred = new SlicePredicate();
        FBUtilities.deserialize(dser, pred, dis);

        List<IndexExpression> rowFilter = null;
        if (version >= MessagingService.VERSION_11)
        {
            int filterCount = dis.readInt();
            rowFilter = new ArrayList<IndexExpression>(filterCount);
            for (int i = 0; i < filterCount; i++)
            {
                IndexExpression expr = new IndexExpression();
                FBUtilities.deserialize(dser, expr, dis);
                rowFilter.add(expr);
            }
        }
        AbstractBounds<RowPosition> range = AbstractBounds.serializer().deserialize(dis, version).toRowBounds();

        int maxResults = dis.readInt();
        boolean maxIsColumns = false;
        if (version >= MessagingService.VERSION_11)
        {
            maxIsColumns = dis.readBoolean();
        }
        return new RangeSliceCommand(keyspace, columnFamily, superColumn, pred, range, rowFilter, maxResults, maxIsColumns);
    }

+++++++++++++++++++++++
    public RangeSliceCommand deserialize(DataInput dis, int version) throws IOException
    {
        String keyspace = dis.readUTF();
        String columnFamily = dis.readUTF();

        int scLength = dis.readInt();
        ByteBuffer superColumn = null;
        if (scLength > 0)
        {
            byte[] buf = new byte[scLength];
            dis.readFully(buf);
            superColumn = ByteBuffer.wrap(buf);
        }

        TDeserializer dser = new TDeserializer(new TBinaryProtocol.Factory());
        SlicePredicate pred = new SlicePredicate();
        FBUtilities.deserialize(dser, pred, dis);

        List<IndexExpression> rowFilter = null;
        if (version >= MessagingService.VERSION_11)
        {
            int filterCount = dis.readInt();
            rowFilter = new ArrayList<IndexExpression>(filterCount);
            for (int i = 0; i < filterCount; i++)
            {
                IndexExpression expr = new IndexExpression();
                FBUtilities.deserialize(dser, expr, dis);
                rowFilter.add(expr);
            }
        }
        AbstractBounds<RowPosition> range = AbstractBounds.serializer().deserialize(dis, version).toRowBounds();

        int maxResults = dis.readInt();
        boolean maxIsColumns = false;
        boolean isPaging = false;
        if (version >= MessagingService.VERSION_11)
        {
            maxIsColumns = dis.readBoolean();
            isPaging = dis.readBoolean();
        }
        return new RangeSliceCommand(keyspace, columnFamily, superColumn, pred, range, rowFilter, maxResults, maxIsColumns, isPaging);
    }

    public void serialize(Message t, DataOutput dos, int version) throws IOException
    {
        assert t.getVersion() == version : "internode protocol version mismatch"; // indicates programmer error.
        Header.serializer().serialize( t.header_, dos, version);
        byte[] bytes = t.getMessageBody();
        dos.writeInt(bytes.length);
        dos.write(bytes);
    }

+++++++++++++++++++++++
    public void serialize(Message t, DataOutput dos, int version) throws IOException
    {
        Header.serializer().serialize( t.header_, dos, version);
        byte[] bytes = t.getMessageBody();
        dos.writeInt(bytes.length);
        dos.write(bytes);
    }

add: 2, delete: 0, change: 90, unhandled: 0 size_exceptions: 1 size_serialize: 4
-----------cassandra-1.1.0-rc1 vs cassandra-1.1.1-----------
    public int serializedSize()
    {
        return right.remaining();
    }

+++++++++++++++++++++++
    public int serializedSize()
    {
        return key.length + DBConstants.intSize;
    }

    public int serializedSize()
    {
        return key.remaining();
    }

    @Override
+++++++++++++++++++++++
    public int serializedSize()
    {
        return key.length + DBConstants.intSize;
    }

    @Override
    public SliceByNamesReadCommand deserialize(DataInput dis, int version) throws IOException
    {
        boolean isDigest = dis.readBoolean();
        String table = dis.readUTF();
        ByteBuffer key = ByteBufferUtil.readWithShortLength(dis);
        QueryPath columnParent = QueryPath.deserialize(dis);

        int size = dis.readInt();
        List<ByteBuffer> columns = new ArrayList<ByteBuffer>();
        for (int i = 0; i < size; ++i)
        {
            columns.add(ByteBufferUtil.readWithShortLength(dis));
        }
        SliceByNamesReadCommand command = new SliceByNamesReadCommand(table, key, columnParent, columns);
        command.setDigestQuery(isDigest);
        return command;
    }

+++++++++++++++++++++++
    public SliceByNamesReadCommand deserialize(DataInput dis, int version) throws IOException
    {
        boolean isDigest = dis.readBoolean();
        String table = dis.readUTF();
        ByteBuffer key = ByteBufferUtil.readWithShortLength(dis);
        QueryPath columnParent = QueryPath.deserialize(dis);

        int size = dis.readInt();
        List<ByteBuffer> columns = new ArrayList<ByteBuffer>(size);
        for (int i = 0; i < size; ++i)
        {
            columns.add(ByteBufferUtil.readWithShortLength(dis));
        }
        SliceByNamesReadCommand command = new SliceByNamesReadCommand(table, key, columnParent, columns);
        command.setDigestQuery(isDigest);
        return command;
    }

    public int serializedSize()
    {
    	/*
    	 * We need to keep the way we are calculating the column size in sync with the
    	 * way we are calculating the size for the column family serializer.
    	 */
      return DBConstants.shortSize + name.remaining() + DBConstants.intSize + DBConstants.longSize + DBConstants.intSize + size();
    }

+++++++++++++++++++++++
    public int serializedSize()
    {
    	/*
    	 * We need to keep the way we are calculating the column size in sync with the
    	 * way we are calculating the size for the column family serializer.
    	 */
        int size = DBConstants.shortSize + name.remaining() + DBConstants.intSize + DBConstants.longSize + DBConstants.intSize;
        for (IColumn subColumn : getSubColumns())
        {
            size += subColumn.serializedSize();
        }
        return size;
    }

add: 12, delete: 1, change: 167, unhandled: 1 size_exceptions: 2 size_serialize: 4
-----------cassandra-1.1.1 vs cassandra-1.1.2-----------
add: 4, delete: 0, change: 44, unhandled: 0 size_exceptions: 2 size_serialize: 0
-----------cassandra-1.1.2 vs cassandra-1.1.3-----------
add: 2, delete: 0, change: 72, unhandled: 0 size_exceptions: 0 size_serialize: 0
-----------cassandra-1.1.3 vs cassandra-1.1.4-----------
add: 0, delete: 0, change: 5, unhandled: 0 size_exceptions: 0 size_serialize: 0
-----------cassandra-1.1.4 vs cassandra-1.1.5-----------
    public Column deserialize(DataInput dis, IColumnSerializer.Flag flag, int expireBefore) throws IOException
    {
        ByteBuffer name = ByteBufferUtil.readWithShortLength(dis);
        if (name.remaining() <= 0)
        {
            String format = "invalid column name length %d%s";
            String details = "";
            if (dis instanceof FileDataInput)
            {
                FileDataInput fdis = (FileDataInput)dis;
                details = String.format(" (%s, %d bytes remaining)", fdis.getPath(), fdis.bytesRemaining());
            }
            throw new CorruptColumnException(String.format(format, name.remaining(), details));
        }

        int b = dis.readUnsignedByte();
        if ((b & COUNTER_MASK) != 0)
        {
            long timestampOfLastDelete = dis.readLong();
            long ts = dis.readLong();
            ByteBuffer value = ByteBufferUtil.readWithLength(dis);
            return CounterColumn.create(name, value, ts, timestampOfLastDelete, flag);
        }
        else if ((b & EXPIRATION_MASK) != 0)
        {
            int ttl = dis.readInt();
            int expiration = dis.readInt();
            long ts = dis.readLong();
            ByteBuffer value = ByteBufferUtil.readWithLength(dis);
            return ExpiringColumn.create(name, value, ts, ttl, expiration, expireBefore, flag);
        }
        else
        {
            long ts = dis.readLong();
            ByteBuffer value = ByteBufferUtil.readWithLength(dis);
            return (b & COUNTER_UPDATE_MASK) != 0
                   ? new CounterUpdateColumn(name, value, ts)
                   : ((b & DELETION_MASK) == 0
                      ? new Column(name, value, ts)
                      : new DeletedColumn(name, value, ts));
        }
    }

+++++++++++++++++++++++
    public Column deserialize(DataInput dis, IColumnSerializer.Flag flag, int expireBefore) throws IOException
    {
        ByteBuffer name = ByteBufferUtil.readWithShortLength(dis);
        if (name.remaining() <= 0)
        {
            String format = "invalid column name length %d%s";
            String details = "";
            if (dis instanceof FileDataInput)
            {
                FileDataInput fdis = (FileDataInput)dis;
                details = String.format(" (%s, %d bytes remaining)", fdis.getPath(), fdis.bytesRemaining());
            }
            throw new CorruptColumnException(String.format(format, name.remaining(), details));
        }

        int b = dis.readUnsignedByte();
        if ((b & COUNTER_MASK) != 0)
        {
            long timestampOfLastDelete = dis.readLong();
            long ts = dis.readLong();
            ByteBuffer value = ByteBufferUtil.readWithLength(dis);
            return CounterColumn.create(name, value, ts, timestampOfLastDelete, flag);
        }
        else if ((b & EXPIRATION_MASK) != 0)
        {
            int ttl = dis.readInt();
            int expiration = dis.readInt();
            long ts = dis.readLong();
            ByteBuffer value = ByteBufferUtil.readWithLength(dis);
            return ExpiringColumn.create(name, value, ts, ttl, expiration, expireBefore, flag);
        }
        else
        {
            long ts = dis.readLong();
            long now = FBUtilities.timestampMicros();

            if (ts > now) // fixing the timestamp from the future to be 'now' in micros
                ts = now; // helps with CASSANDRA-4561 as remote nodes can send schema with wrong nanoTime() timestamps

            ByteBuffer value = ByteBufferUtil.readWithLength(dis);
            return (b & COUNTER_UPDATE_MASK) != 0
                   ? new CounterUpdateColumn(name, value, ts)
                   : ((b & DELETION_MASK) == 0
                      ? new Column(name, value, ts)
                      : new DeletedColumn(name, value, ts));
        }
    }

add: 3, delete: 0, change: 45, unhandled: 0 size_exceptions: 1 size_serialize: 1
-----------cassandra-1.1.5 vs cassandra-1.1.6-----------
    public Column deserialize(DataInput dis, IColumnSerializer.Flag flag, int expireBefore) throws IOException
    {
        ByteBuffer name = ByteBufferUtil.readWithShortLength(dis);
        if (name.remaining() <= 0)
        {
            String format = "invalid column name length %d%s";
            String details = "";
            if (dis instanceof FileDataInput)
            {
                FileDataInput fdis = (FileDataInput)dis;
                details = String.format(" (%s, %d bytes remaining)", fdis.getPath(), fdis.bytesRemaining());
            }
            throw new CorruptColumnException(String.format(format, name.remaining(), details));
        }

        int b = dis.readUnsignedByte();
        if ((b & COUNTER_MASK) != 0)
        {
            long timestampOfLastDelete = dis.readLong();
            long ts = dis.readLong();
            ByteBuffer value = ByteBufferUtil.readWithLength(dis);
            return CounterColumn.create(name, value, ts, timestampOfLastDelete, flag);
        }
        else if ((b & EXPIRATION_MASK) != 0)
        {
            int ttl = dis.readInt();
            int expiration = dis.readInt();
            long ts = dis.readLong();
            ByteBuffer value = ByteBufferUtil.readWithLength(dis);
            return ExpiringColumn.create(name, value, ts, ttl, expiration, expireBefore, flag);
        }
        else
        {
            long ts = dis.readLong();
            long now = FBUtilities.timestampMicros();

            if (ts > now) // fixing the timestamp from the future to be 'now' in micros
                ts = now; // helps with CASSANDRA-4561 as remote nodes can send schema with wrong nanoTime() timestamps

            ByteBuffer value = ByteBufferUtil.readWithLength(dis);
            return (b & COUNTER_UPDATE_MASK) != 0
                   ? new CounterUpdateColumn(name, value, ts)
                   : ((b & DELETION_MASK) == 0
                      ? new Column(name, value, ts)
                      : new DeletedColumn(name, value, ts));
        }
    }

+++++++++++++++++++++++
    public Column deserialize(DataInput dis, IColumnSerializer.Flag flag, int expireBefore) throws IOException
    {
        ByteBuffer name = ByteBufferUtil.readWithShortLength(dis);
        if (name.remaining() <= 0)
        {
            String format = "invalid column name length %d%s";
            String details = "";
            if (dis instanceof FileDataInput)
            {
                FileDataInput fdis = (FileDataInput)dis;
                details = String.format(" (%s, %d bytes remaining)", fdis.getPath(), fdis.bytesRemaining());
            }
            throw new CorruptColumnException(String.format(format, name.remaining(), details));
        }

        int b = dis.readUnsignedByte();
        if ((b & COUNTER_MASK) != 0)
        {
            long timestampOfLastDelete = dis.readLong();
            long ts = dis.readLong();
            ByteBuffer value = ByteBufferUtil.readWithLength(dis);
            return CounterColumn.create(name, value, ts, timestampOfLastDelete, flag);
        }
        else if ((b & EXPIRATION_MASK) != 0)
        {
            int ttl = dis.readInt();
            int expiration = dis.readInt();
            long ts = dis.readLong();
            ByteBuffer value = ByteBufferUtil.readWithLength(dis);
            return ExpiringColumn.create(name, value, ts, ttl, expiration, expireBefore, flag);
        }
        else
        {
            long ts = dis.readLong();
            ByteBuffer value = ByteBufferUtil.readWithLength(dis);
            return (b & COUNTER_UPDATE_MASK) != 0
                   ? new CounterUpdateColumn(name, value, ts)
                   : ((b & DELETION_MASK) == 0
                      ? new Column(name, value, ts)
                      : new DeletedColumn(name, value, ts));
        }
    }

    public static Collection<RowMutation> deserializeMigrationMessage(byte[] data, int version) throws IOException
    {
        Collection<RowMutation> schema = new ArrayList<RowMutation>();
        DataInputStream in = new DataInputStream(new FastByteArrayInputStream(data));

        int count = in.readInt();

        for (int i = 0; i < count; i++)
            schema.add(RowMutation.serializer().deserialize(in, version));

        return schema;
    }

    /**
     * Clear all locally stored schema information and reset schema to initial state.
     * Called by user (via JMX) who wants to get rid of schema disagreement.
     *
     * @throws IOException if schema tables truncation fails
     */
+++++++++++++++++++++++
    public static Collection<RowMutation> deserializeMigrationMessage(byte[] data, int version) throws IOException
    {
        Collection<RowMutation> schema = new ArrayList<RowMutation>();
        DataInputStream in = new DataInputStream(new FastByteArrayInputStream(data));

        int count = in.readInt();

        for (int i = 0; i < count; i++)
            schema.add(RowMutation.serializer().deserializeFixingTimestamps(in, version));

        return schema;
    }

    /**
     * Clear all locally stored schema information and reset schema to initial state.
     * Called by user (via JMX) who wants to get rid of schema disagreement.
     *
     * @throws IOException if schema tables truncation fails
     */
add: 10, delete: 0, change: 51, unhandled: 0 size_exceptions: 2 size_serialize: 2
-----------cassandra-1.1.6 vs cassandra-1.1.7-----------
    public static Collection<RowMutation> deserializeMigrationMessage(byte[] data, int version) throws IOException
    {
        Collection<RowMutation> schema = new ArrayList<RowMutation>();
        DataInputStream in = new DataInputStream(new FastByteArrayInputStream(data));

        int count = in.readInt();

        for (int i = 0; i < count; i++)
            schema.add(RowMutation.serializer().deserializeFixingTimestamps(in, version));

        return schema;
    }

    /**
     * Clear all locally stored schema information and reset schema to initial state.
     * Called by user (via JMX) who wants to get rid of schema disagreement.
     *
     * @throws IOException if schema tables truncation fails
     */
+++++++++++++++++++++++
    public static Collection<RowMutation> deserializeMigrationMessage(byte[] data, int version) throws IOException
    {
        Collection<RowMutation> schema = new ArrayList<RowMutation>();
        DataInputStream in = new DataInputStream(new FastByteArrayInputStream(data));

        int count = in.readInt();

        for (int i = 0; i < count; i++)
            schema.add(RowMutation.serializer().deserialize(in, version));

        return schema;
    }

    /**
     * Clear all locally stored schema information and reset schema to initial state.
     * Called by user (via JMX) who wants to get rid of schema disagreement.
     *
     * @throws IOException if schema tables truncation fails
     */
add: 3, delete: 5, change: 55, unhandled: 0 size_exceptions: 0 size_serialize: 1
-----------cassandra-1.1.7 vs cassandra-1.1.8-----------
add: 0, delete: 0, change: 24, unhandled: 0 size_exceptions: 0 size_serialize: 0
-----------cassandra-1.1.8 vs cassandra-1.1.9-----------
add: 2, delete: 0, change: 26, unhandled: 0 size_exceptions: 0 size_serialize: 0
-----------cassandra-1.1.9 vs cassandra-1.1.10-----------
add: 1, delete: 1, change: 22, unhandled: 0 size_exceptions: 0 size_serialize: 0
-----------cassandra-1.1.10 vs cassandra-1.1.11-----------
add: 4, delete: 0, change: 44, unhandled: 0 size_exceptions: 0 size_serialize: 0
-----------cassandra-1.1.11 vs cassandra-1.1.12-----------
    public synchronized void serialize()
    {
        File manifestFile = cfs.directories.getOrCreateLeveledManifest();
        File oldFile = new File(manifestFile.getPath().replace(EXTENSION, "-old.json"));
        File tmpFile = new File(manifestFile.getPath().replace(EXTENSION, "-tmp.json"));

        JsonFactory f = new JsonFactory();
        try
        {
            JsonGenerator g = f.createJsonGenerator(tmpFile, JsonEncoding.UTF8);
            g.useDefaultPrettyPrinter();
            g.writeStartObject();
            g.writeArrayFieldStart("generations");
            for (int level = 0; level < generations.length; level++)
            {
                g.writeStartObject();
                g.writeNumberField("generation", level);
                g.writeArrayFieldStart("members");
                for (SSTableReader ssTableReader : generations[level])
                    g.writeNumber(ssTableReader.descriptor.generation);
                g.writeEndArray(); // members

                g.writeEndObject(); // generation
            }
            g.writeEndArray(); // for field generations
            g.writeEndObject(); // write global object
            g.close();

            if (oldFile.exists() && manifestFile.exists())
                FileUtils.deleteWithConfirm(oldFile);
            if (manifestFile.exists())
                FileUtils.renameWithConfirm(manifestFile, oldFile);
            assert tmpFile.exists();
            FileUtils.renameWithConfirm(tmpFile, manifestFile);
            logger.debug("Saved manifest {}", manifestFile);
        }
        catch (IOException e)
        {
            throw new IOError(e);
        }
    }

    @Override
+++++++++++++++++++++++
    public synchronized void serialize()
    {
        File manifestFile = cfs.directories.getOrCreateLeveledManifest();
        File oldFile = new File(manifestFile.getPath().replace(EXTENSION, "-old.json"));
        File tmpFile = new File(manifestFile.getPath().replace(EXTENSION, "-tmp.json"));

        JsonFactory f = new JsonFactory();
        try
        {
            FileOutputStream fos = new FileOutputStream(tmpFile);
            JsonGenerator g = f.createJsonGenerator(fos, JsonEncoding.UTF8);
            g.useDefaultPrettyPrinter();
            g.writeStartObject();
            g.writeArrayFieldStart("generations");
            for (int level = 0; level < generations.length; level++)
            {
                g.writeStartObject();
                g.writeNumberField("generation", level);
                g.writeArrayFieldStart("members");
                for (SSTableReader ssTableReader : generations[level])
                    g.writeNumber(ssTableReader.descriptor.generation);
                g.writeEndArray(); // members

                g.writeEndObject(); // generation
            }
            g.writeEndArray(); // for field generations
            g.writeEndObject(); // write global object
            g.flush();
            fos.getFD().sync();
            g.close();

            if (oldFile.exists() && manifestFile.exists())
                FileUtils.deleteWithConfirm(oldFile);
            if (manifestFile.exists())
                FileUtils.renameWithConfirm(manifestFile, oldFile);
            assert tmpFile.exists();
            FileUtils.renameWithConfirm(tmpFile, manifestFile);
            logger.debug("Saved manifest {}", manifestFile);
        }
        catch (IOException e)
        {
            throw new IOError(e);
        }
    }

    @Override
add: 0, delete: 1, change: 17, unhandled: 0 size_exceptions: 0 size_serialize: 1
-----------cassandra-1.1.12 vs cassandra-1.2.0-----------
    private FreeableMemory serialize(V value)
    {
        long serializedSize = serializer.serializedSize(value);
        if (serializedSize > Integer.MAX_VALUE)
            throw new IllegalArgumentException("Unable to allocate " + serializedSize + " bytes");

        FreeableMemory freeableMemory;
        try
        {
            freeableMemory = new FreeableMemory(serializedSize);
        }
        catch (OutOfMemoryError e)
        {
            return null;
        }

        try
        {
            serializer.serialize(value, new DataOutputStream(new MemoryOutputStream(freeableMemory)));
        }
        catch (IOException e)
        {
            throw new IOError(e);
        }
        return freeableMemory;
    }

+++++++++++++++++++++++
    private RefCountedMemory serialize(V value)
    {
        long serializedSize = serializer.serializedSize(value, ENCODED_TYPE_SIZES);
        if (serializedSize > Integer.MAX_VALUE)
            throw new IllegalArgumentException("Unable to allocate " + serializedSize + " bytes");

        RefCountedMemory freeableMemory;
        try
        {
            freeableMemory = new RefCountedMemory(serializedSize);
        }
        catch (OutOfMemoryError e)
        {
            return null;
        }

        try
        {
            serializer.serialize(value, new EncodedDataOutputStream(new MemoryOutputStream(freeableMemory)));
        }
        catch (IOException e)
        {
            throw new RuntimeException(e);
        }
        return freeableMemory;
    }

	private V deserialize(FreeableMemory mem)
    {
        try
        {
            return serializer.deserialize(new MemoryInputStream(mem));
        }
        catch (IOException e)
        {
            logger.debug("Cannot fetch in memory data, we will failback to read from disk ", e);
            return null;
        }
    }

+++++++++++++++++++++++
    private V deserialize(RefCountedMemory mem)
    {
        try
        {
            return serializer.deserialize(new EncodedDataInputStream(new MemoryInputStream(mem)));
        }
        catch (IOException e)
        {
            logger.debug("Cannot fetch in memory data, we will failback to read from disk ", e);
            return null;
        }
    }

    public int serializedSize()
    {
        return size();
    }

+++++++++++++++++++++++
    public int serializedSize(TypeSizes typeSizes)
    {
        /*
         * Size of a column is =
         *   size of a name (short + length of the string)
         * + 1 byte to indicate if the column has been deleted
         * + 8 bytes for timestamp
         * + 4 bytes which basically indicates the size of the byte array
         * + entire byte array.
        */
        int nameSize = name.remaining();
        int valueSize = value.remaining();
        return typeSizes.sizeof((short) nameSize) + nameSize + 1 + typeSizes.sizeof(timestamp) + typeSizes.sizeof(valueSize) + valueSize;
    }

    public int serializeForSSTable(ColumnFamily columnFamily, DataOutput dos)
    {
        try
        {
            serializeCFInfo(columnFamily, dos);

            Collection<IColumn> columns = columnFamily.getSortedColumns();
            int count = columns.size();
            dos.writeInt(count);
            int i = 0;
            for (IColumn column : columns)
            {
                columnFamily.getColumnSerializer().serialize(column, dos);
                i++;
            }
            assert count == i: "CF size changed during serialization: was " + count + " initially but " + i + " written";
            return count;
        }
        catch (IOException e)
        {
            throw new RuntimeException(e);
        }
    }

+++++++++++++++++++++++
    public void serializeForSSTable(ColumnFamily cf, DataOutput dos)
    {
        // Column families shouldn't be written directly to disk, use ColumnIndex.Builder instead
        throw new UnsupportedOperationException();
    }

    public void serialize(ColumnFamily columnFamily, DataOutput dos)
    {
        try
        {
            if (columnFamily == null)
            {
                dos.writeBoolean(false);
                return;
            }

            dos.writeBoolean(true);
            dos.writeInt(columnFamily.id());
        }
        catch (IOException e)
        {
            throw new RuntimeException(e);
        }
        serializeForSSTable(columnFamily, dos);
    }

+++++++++++++++++++++++
    public void serialize(ColumnFamily cf, DataOutput dos, int version)
    {
        try
        {
            if (cf == null)
            {
                dos.writeBoolean(false);
                return;
            }

            dos.writeBoolean(true);
            serializeCfId(cf.id(), dos, version);

            DeletionInfo.serializer().serialize(cf.deletionInfo(), dos, version);

            IColumnSerializer columnSerializer = cf.getColumnSerializer();
            int count = cf.getColumnCount();
            dos.writeInt(count);
            int written = 0;
            for (IColumn column : cf)
            {
                columnSerializer.serialize(column, dos);
                written++;
            }
            assert count == written: "Column family had " + count + " columns, but " + written + " written";
        }
        catch (IOException e)
        {
            throw new RuntimeException(e);
        }
    }

    public long serializedSize(ColumnFamily cf)
    {
        return cf == null ? DBConstants.boolSize : cf.serializedSize();
+++++++++++++++++++++++
    public long serializedSize(ColumnFamily cf, int version)
    {
        return serializedSize(cf, TypeSizes.NATIVE, version);
    }

    public ColumnFamily deserialize(DataInput dis, IColumnSerializer.Flag flag, ISortedColumns.Factory factory) throws IOException
    {
        if (!dis.readBoolean())
            return null;

        // create a ColumnFamily based on the cf id
        int cfId = dis.readInt();
        if (Schema.instance.getCF(cfId) == null)
            throw new UnknownColumnFamilyException("Couldn't find cfId=" + cfId, cfId);
        ColumnFamily cf = ColumnFamily.create(cfId, factory);
        deserializeFromSSTableNoColumns(cf, dis);
        deserializeColumns(dis, cf, flag);
        return cf;
    }

+++++++++++++++++++++++
    public ColumnFamily deserialize(DataInput dis, IColumnSerializer.Flag flag, ISortedColumns.Factory factory, int version) throws IOException
    {
        if (!dis.readBoolean())
            return null;

        ColumnFamily cf = ColumnFamily.create(deserializeCfId(dis, version), factory);
        IColumnSerializer columnSerializer = cf.getColumnSerializer();
        cf.delete(DeletionInfo.serializer().deserialize(dis, version, cf.getComparator()));
        int expireBefore = (int) (System.currentTimeMillis() / 1000);
        int size = dis.readInt();
        for (int i = 0; i < size; ++i)
        {
            cf.addColumn(columnSerializer.deserialize(dis, flag, expireBefore));
        }
        return cf;
    }

    public void serialize(IColumn column, DataOutput dos)
    {
        assert column.name().remaining() > 0;
        ByteBufferUtil.writeWithShortLength(column.name(), dos);
        try
        {
            dos.writeByte(column.serializationFlags());
            if (column instanceof CounterColumn)
            {
                dos.writeLong(((CounterColumn)column).timestampOfLastDelete());
            }
            else if (column instanceof ExpiringColumn)
            {
                dos.writeInt(((ExpiringColumn) column).getTimeToLive());
                dos.writeInt(column.getLocalDeletionTime());
            }
            dos.writeLong(column.timestamp());
            ByteBufferUtil.writeWithLength(column.value(), dos);
        }
        catch (IOException e)
        {
            throw new RuntimeException(e);
        }
    }

+++++++++++++++++++++++
    public void serialize(IColumn column, DataOutput dos) throws IOException
    {
        assert column.name().remaining() > 0;
        ByteBufferUtil.writeWithShortLength(column.name(), dos);
        try
        {
            dos.writeByte(column.serializationFlags());
            if (column instanceof CounterColumn)
            {
                dos.writeLong(((CounterColumn)column).timestampOfLastDelete());
            }
            else if (column instanceof ExpiringColumn)
            {
                dos.writeInt(((ExpiringColumn) column).getTimeToLive());
                dos.writeInt(column.getLocalDeletionTime());
            }
            dos.writeLong(column.timestamp());
            ByteBufferUtil.writeWithLength(column.value(), dos);
        }
        catch (IOException e)
        {
            throw new RuntimeException(e);
        }
    }

    public long serializedSize(IColumn object)
    {
        return object.serializedSize();
    }

+++++++++++++++++++++++
    public long serializedSize(IColumn column, TypeSizes type)
    {
        return column.serializedSize(type);
    }

    public Column deserialize(DataInput dis, IColumnSerializer.Flag flag, int expireBefore) throws IOException
    {
        ByteBuffer name = ByteBufferUtil.readWithShortLength(dis);
        if (name.remaining() <= 0)
        {
            String format = "invalid column name length %d%s";
            String details = "";
            if (dis instanceof FileDataInput)
            {
                FileDataInput fdis = (FileDataInput)dis;
                details = String.format(" (%s, %d bytes remaining)", fdis.getPath(), fdis.bytesRemaining());
            }
            throw new CorruptColumnException(String.format(format, name.remaining(), details));
        }

        int b = dis.readUnsignedByte();
        if ((b & COUNTER_MASK) != 0)
        {
            long timestampOfLastDelete = dis.readLong();
            long ts = dis.readLong();
            ByteBuffer value = ByteBufferUtil.readWithLength(dis);
            return CounterColumn.create(name, value, ts, timestampOfLastDelete, flag);
        }
        else if ((b & EXPIRATION_MASK) != 0)
        {
            int ttl = dis.readInt();
            int expiration = dis.readInt();
            long ts = dis.readLong();
            ByteBuffer value = ByteBufferUtil.readWithLength(dis);
            return ExpiringColumn.create(name, value, ts, ttl, expiration, expireBefore, flag);
        }
        else
        {
            long ts = dis.readLong();
            ByteBuffer value = ByteBufferUtil.readWithLength(dis);
            return (b & COUNTER_UPDATE_MASK) != 0
                   ? new CounterUpdateColumn(name, value, ts)
                   : ((b & DELETION_MASK) == 0
                      ? new Column(name, value, ts)
                      : new DeletedColumn(name, value, ts));
        }
    }

+++++++++++++++++++++++
    public Column deserialize(DataInput dis, IColumnSerializer.Flag flag, int expireBefore) throws IOException
    {
        ByteBuffer name = ByteBufferUtil.readWithShortLength(dis);
        if (name.remaining() <= 0)
            throw CorruptColumnException.create(dis, name);

        int b = dis.readUnsignedByte();
        return deserializeColumnBody(dis, name, b, flag, expireBefore);
    }

    public void serialize(CounterMutation cm, DataOutput dos, int version) throws IOException
    {
        RowMutation.serializer().serialize(cm.rowMutation(), dos, version);
        dos.writeUTF(cm.consistency().name());
    }

+++++++++++++++++++++++
    public void serialize(CounterMutation cm, DataOutput dos, int version) throws IOException
    {
        RowMutation.serializer.serialize(cm.rowMutation(), dos, version);
        dos.writeUTF(cm.consistency().name());
    }

    public long serializedSize(CounterMutation cm, int version)
    {
        return RowMutation.serializer().serializedSize(cm.rowMutation(), version)
               + DBConstants.shortSize + FBUtilities.encodedUTF8Length(cm.consistency().name());
+++++++++++++++++++++++
    public long serializedSize(CounterMutation cm, int version)
    {
        return RowMutation.serializer.serializedSize(cm.rowMutation(), version)
             + TypeSizes.NATIVE.sizeof(cm.consistency().name());
    public CounterMutation deserialize(DataInput dis, int version) throws IOException
    {
        RowMutation rm = RowMutation.serializer().deserialize(dis, version);
        ConsistencyLevel consistency = Enum.valueOf(ConsistencyLevel.class, dis.readUTF());
        return new CounterMutation(rm, consistency);
    }

+++++++++++++++++++++++
    public CounterMutation deserialize(DataInput dis, int version) throws IOException
    {
        RowMutation rm = RowMutation.serializer.deserialize(dis, version);
        ConsistencyLevel consistency = Enum.valueOf(ConsistencyLevel.class, dis.readUTF());
        return new CounterMutation(rm, consistency);
    }

    public void serialize(RangeSliceCommand sliceCommand, DataOutput dos, int version) throws IOException
    {
        dos.writeUTF(sliceCommand.keyspace);
        dos.writeUTF(sliceCommand.column_family);
        ByteBuffer sc = sliceCommand.super_column;
        dos.writeInt(sc == null ? 0 : sc.remaining());
        if (sc != null)
            ByteBufferUtil.write(sc, dos);

        TSerializer ser = new TSerializer(new TBinaryProtocol.Factory());
        FBUtilities.serialize(ser, sliceCommand.predicate, dos);

        if (version >= MessagingService.VERSION_11)
        {
            if (sliceCommand.row_filter == null)
            {
                dos.writeInt(0);
            }
            else
            {
                dos.writeInt(sliceCommand.row_filter.size());
                for (IndexExpression expr : sliceCommand.row_filter)
                    FBUtilities.serialize(ser, expr, dos);
            }
        }
        AbstractBounds.serializer().serialize(sliceCommand.range, dos, version);
        dos.writeInt(sliceCommand.maxResults);
        if (version >= MessagingService.VERSION_11)
        {
            dos.writeBoolean(sliceCommand.maxIsColumns);
            dos.writeBoolean(sliceCommand.isPaging);
        }
    }

+++++++++++++++++++++++
    public void serialize(RangeSliceCommand sliceCommand, DataOutput dos, int version) throws IOException
    {
        dos.writeUTF(sliceCommand.keyspace);
        dos.writeUTF(sliceCommand.column_family);
        ByteBuffer sc = sliceCommand.super_column;
        dos.writeInt(sc == null ? 0 : sc.remaining());
        if (sc != null)
            ByteBufferUtil.write(sc, dos);

        if (version < MessagingService.VERSION_12)
        {
            FBUtilities.serialize(new TSerializer(new TBinaryProtocol.Factory()), asSlicePredicate(sliceCommand.predicate), dos);
        }
        else
        {
            IDiskAtomFilter.Serializer.instance.serialize(sliceCommand.predicate, dos, version);
        }

        if (version >= MessagingService.VERSION_11)
        {
            if (sliceCommand.row_filter == null)
            {
                dos.writeInt(0);
            }
            else
            {
                dos.writeInt(sliceCommand.row_filter.size());
                for (IndexExpression expr : sliceCommand.row_filter)
                {
                    if (version < MessagingService.VERSION_12)
                    {
                        FBUtilities.serialize(new TSerializer(new TBinaryProtocol.Factory()), expr, dos);
                    }
                    else
                    {
                        ByteBufferUtil.writeWithShortLength(expr.column_name, dos);
                        dos.writeInt(expr.op.getValue());
                        ByteBufferUtil.writeWithShortLength(expr.value, dos);
                    }
                }
            }
        }
        AbstractBounds.serializer.serialize(sliceCommand.range, dos, version);
        dos.writeInt(sliceCommand.maxResults);
        if (version >= MessagingService.VERSION_11)
        {
            dos.writeBoolean(sliceCommand.countCQL3Rows);
            dos.writeBoolean(sliceCommand.isPaging);
        }
    }

    public long serializedSize(RangeSliceCommand rangeSliceCommand, int version)
    {
        throw new UnsupportedOperationException();
+++++++++++++++++++++++
    public long serializedSize(RangeSliceCommand rsc, int version)
    {
        long size = TypeSizes.NATIVE.sizeof(rsc.keyspace);
        size += TypeSizes.NATIVE.sizeof(rsc.column_family);

        ByteBuffer sc = rsc.super_column;
        if (sc != null)
        {
            size += TypeSizes.NATIVE.sizeof(sc.remaining());
            size += sc.remaining();
        }
        else
        {
            size += TypeSizes.NATIVE.sizeof(0);
        }

        if (version < MessagingService.VERSION_12)
        {
            TSerializer ser = new TSerializer(new TBinaryProtocol.Factory());
            try
            {
                int predicateLength = ser.serialize(asSlicePredicate(rsc.predicate)).length;
                if (version < MessagingService.VERSION_12)
                    size += TypeSizes.NATIVE.sizeof(predicateLength);
                size += predicateLength;
            }
            catch (TException e)
            {
                throw new RuntimeException(e);
            }
        }
        else
        {
            size += IDiskAtomFilter.Serializer.instance.serializedSize(rsc.predicate, version);
        }

        if (version >= MessagingService.VERSION_11)
        {
            if (rsc.row_filter == null)
            {
                size += TypeSizes.NATIVE.sizeof(0);
            }
            else
            {
                size += TypeSizes.NATIVE.sizeof(rsc.row_filter.size());
                for (IndexExpression expr : rsc.row_filter)
                {
                    if (version < MessagingService.VERSION_12)
                    {
                        try
                        {
                            int filterLength = new TSerializer(new TBinaryProtocol.Factory()).serialize(expr).length;
                            size += TypeSizes.NATIVE.sizeof(filterLength);
                            size += filterLength;
                        }
                        catch (TException e)
                        {
                            throw new RuntimeException(e);
                        }
                    }
                    else
                    {
                        size += TypeSizes.NATIVE.sizeofWithShortLength(expr.column_name);
                        size += TypeSizes.NATIVE.sizeof(expr.op.getValue());
                        size += TypeSizes.NATIVE.sizeofWithLength(expr.value);
                    }
                }
            }
        }
        size += AbstractBounds.serializer.serializedSize(rsc.range, version);
        size += TypeSizes.NATIVE.sizeof(rsc.maxResults);
        if (version >= MessagingService.VERSION_11)
        {
            size += TypeSizes.NATIVE.sizeof(rsc.countCQL3Rows);
            size += TypeSizes.NATIVE.sizeof(rsc.isPaging);
        }
        return size;
    public RangeSliceCommand deserialize(DataInput dis, int version) throws IOException
    {
        String keyspace = dis.readUTF();
        String columnFamily = dis.readUTF();

        int scLength = dis.readInt();
        ByteBuffer superColumn = null;
        if (scLength > 0)
        {
            byte[] buf = new byte[scLength];
            dis.readFully(buf);
            superColumn = ByteBuffer.wrap(buf);
        }

        TDeserializer dser = new TDeserializer(new TBinaryProtocol.Factory());
        SlicePredicate pred = new SlicePredicate();
        FBUtilities.deserialize(dser, pred, dis);

        List<IndexExpression> rowFilter = null;
        if (version >= MessagingService.VERSION_11)
        {
            int filterCount = dis.readInt();
            rowFilter = new ArrayList<IndexExpression>(filterCount);
            for (int i = 0; i < filterCount; i++)
            {
                IndexExpression expr = new IndexExpression();
                FBUtilities.deserialize(dser, expr, dis);
                rowFilter.add(expr);
            }
        }
        AbstractBounds<RowPosition> range = AbstractBounds.serializer().deserialize(dis, version).toRowBounds();

        int maxResults = dis.readInt();
        boolean maxIsColumns = false;
        boolean isPaging = false;
        if (version >= MessagingService.VERSION_11)
        {
            maxIsColumns = dis.readBoolean();
            isPaging = dis.readBoolean();
        }
        return new RangeSliceCommand(keyspace, columnFamily, superColumn, pred, range, rowFilter, maxResults, maxIsColumns, isPaging);
    }

+++++++++++++++++++++++
    public RangeSliceCommand deserialize(DataInput dis, int version) throws IOException
    {
        String keyspace = dis.readUTF();
        String columnFamily = dis.readUTF();

        int scLength = dis.readInt();
        ByteBuffer superColumn = null;
        if (scLength > 0)
        {
            byte[] buf = new byte[scLength];
            dis.readFully(buf);
            superColumn = ByteBuffer.wrap(buf);
        }

        IDiskAtomFilter predicate;
        AbstractType<?> comparator = ColumnFamily.getComparatorFor(keyspace, columnFamily, superColumn);
        if (version < MessagingService.VERSION_12)
        {
            SlicePredicate pred = new SlicePredicate();
            FBUtilities.deserialize(new TDeserializer(new TBinaryProtocol.Factory()), pred, dis);
            predicate = ThriftValidation.asIFilter(pred, comparator);
        }
        else
        {
            predicate = IDiskAtomFilter.Serializer.instance.deserialize(dis, version, comparator);
        }

        List<IndexExpression> rowFilter = null;
        if (version >= MessagingService.VERSION_11)
        {
            int filterCount = dis.readInt();
            rowFilter = new ArrayList<IndexExpression>(filterCount);
            for (int i = 0; i < filterCount; i++)
            {
                IndexExpression expr;
                if (version < MessagingService.VERSION_12)
                {
                    expr = new IndexExpression();
                    FBUtilities.deserialize(new TDeserializer(new TBinaryProtocol.Factory()), expr, dis);
                }
                else
                {
                    expr = new IndexExpression(ByteBufferUtil.readWithShortLength(dis),
                                               IndexOperator.findByValue(dis.readInt()),
                                               ByteBufferUtil.readWithShortLength(dis));
                }
                rowFilter.add(expr);
            }
        }
        AbstractBounds<RowPosition> range = AbstractBounds.serializer.deserialize(dis, version).toRowBounds();

        int maxResults = dis.readInt();
        boolean countCQL3Rows = false;
        boolean isPaging = false;
        if (version >= MessagingService.VERSION_11)
        {
            countCQL3Rows = dis.readBoolean();
            isPaging = dis.readBoolean();
        }
        return new RangeSliceCommand(keyspace, columnFamily, superColumn, predicate, range, rowFilter, maxResults, countCQL3Rows, isPaging);
    }

    public void serialize(ReadCommand cmd, DataOutput dos, int version) throws IOException
    {
        SliceByNamesReadCommand command = (SliceByNamesReadCommand) cmd;
        dos.writeBoolean(command.isDigestQuery());
        dos.writeUTF(command.table);
        ByteBufferUtil.writeWithShortLength(command.key, dos);
        command.queryPath.serialize(dos);
        dos.writeInt(command.columnNames.size());
        if (!command.columnNames.isEmpty())
        {
            for (ByteBuffer cName : command.columnNames)
            {
                ByteBufferUtil.writeWithShortLength(cName, dos);
            }
        }
    }

+++++++++++++++++++++++
    public void serialize(ReadCommand cmd, DataOutput dos, int version) throws IOException
    {
        SliceByNamesReadCommand command = (SliceByNamesReadCommand) cmd;
        dos.writeBoolean(command.isDigestQuery());
        dos.writeUTF(command.table);
        ByteBufferUtil.writeWithShortLength(command.key, dos);
        command.queryPath.serialize(dos);
        NamesQueryFilter.serializer.serialize(command.filter, dos, version);
    }

    public long serializedSize(ReadCommand cmd, int version)
    {
        SliceByNamesReadCommand command = (SliceByNamesReadCommand) cmd;
        int size = DBConstants.boolSize;
        size += DBConstants.shortSize + FBUtilities.encodedUTF8Length(command.table);
        size += DBConstants.shortSize + command.key.remaining();
        size += command.queryPath.serializedSize();
        size += DBConstants.intSize;
        if (!command.columnNames.isEmpty())
        {
            for (ByteBuffer cName : command.columnNames)
                size += DBConstants.shortSize + cName.remaining();
        }
        return size;
+++++++++++++++++++++++
    public long serializedSize(ReadCommand cmd, int version)
    {
        TypeSizes sizes = TypeSizes.NATIVE;
        SliceByNamesReadCommand command = (SliceByNamesReadCommand) cmd;
        int size = sizes.sizeof(command.isDigestQuery());
        int keySize = command.key.remaining();

        size += sizes.sizeof(command.table);
        size += sizes.sizeof((short)keySize) + keySize;
        size += command.queryPath.serializedSize(sizes);
        size += NamesQueryFilter.serializer.serializedSize(command.filter, version);
        return size;
    public SliceByNamesReadCommand deserialize(DataInput dis, int version) throws IOException
    {
        boolean isDigest = dis.readBoolean();
        String table = dis.readUTF();
        ByteBuffer key = ByteBufferUtil.readWithShortLength(dis);
        QueryPath columnParent = QueryPath.deserialize(dis);

        int size = dis.readInt();
        List<ByteBuffer> columns = new ArrayList<ByteBuffer>(size);
        for (int i = 0; i < size; ++i)
        {
            columns.add(ByteBufferUtil.readWithShortLength(dis));
        }
        SliceByNamesReadCommand command = new SliceByNamesReadCommand(table, key, columnParent, columns);
        command.setDigestQuery(isDigest);
        return command;
    }

+++++++++++++++++++++++
    public SliceByNamesReadCommand deserialize(DataInput dis, int version) throws IOException
    {
        boolean isDigest = dis.readBoolean();
        String table = dis.readUTF();
        ByteBuffer key = ByteBufferUtil.readWithShortLength(dis);
        QueryPath columnParent = QueryPath.deserialize(dis);

        AbstractType<?> comparator = ColumnFamily.getComparatorFor(table, columnParent.columnFamilyName, columnParent.superColumnName);
        NamesQueryFilter filter = NamesQueryFilter.serializer.deserialize(dis, version, comparator);
        SliceByNamesReadCommand command = new SliceByNamesReadCommand(table, key, columnParent, filter);
        command.setDigestQuery(isDigest);
        return command;
    }

    public void serialize(ReadCommand rm, DataOutput dos, int version) throws IOException
    {
        SliceFromReadCommand realRM = (SliceFromReadCommand)rm;
        dos.writeBoolean(realRM.isDigestQuery());
        dos.writeUTF(realRM.table);
        ByteBufferUtil.writeWithShortLength(realRM.key, dos);
        realRM.queryPath.serialize(dos);
        ByteBufferUtil.writeWithShortLength(realRM.start, dos);
        ByteBufferUtil.writeWithShortLength(realRM.finish, dos);
        dos.writeBoolean(realRM.reversed);
        dos.writeInt(realRM.count);
    }

+++++++++++++++++++++++
    public void serialize(ReadCommand rm, DataOutput dos, int version) throws IOException
    {
        SliceFromReadCommand realRM = (SliceFromReadCommand)rm;
        dos.writeBoolean(realRM.isDigestQuery());
        dos.writeUTF(realRM.table);
        ByteBufferUtil.writeWithShortLength(realRM.key, dos);
        realRM.queryPath.serialize(dos);
        SliceQueryFilter.serializer.serialize(realRM.filter, dos, version);
    }

    public long serializedSize(ReadCommand cmd, int version)
    {
        SliceFromReadCommand command = (SliceFromReadCommand) cmd;
        int size = DBConstants.boolSize;
        size += DBConstants.shortSize + FBUtilities.encodedUTF8Length(command.table);
        size += DBConstants.shortSize + command.key.remaining();
        size += command.queryPath.serializedSize();
        size += DBConstants.shortSize + command.start.remaining();
        size += DBConstants.shortSize + command.finish.remaining();
        size += DBConstants.boolSize;
        size += DBConstants.intSize;
        return size;
+++++++++++++++++++++++
    public long serializedSize(ReadCommand cmd, int version)
    {
        TypeSizes sizes = TypeSizes.NATIVE;
        SliceFromReadCommand command = (SliceFromReadCommand) cmd;
        int keySize = command.key.remaining();

        int size = sizes.sizeof(cmd.isDigestQuery()); // boolean
        size += sizes.sizeof(command.table);
        size += sizes.sizeof((short) keySize) + keySize;
        size += command.queryPath.serializedSize(sizes);
        size += SliceQueryFilter.serializer.serializedSize(command.filter, version);
        return size;
    public ReadCommand deserialize(DataInput dis, int version) throws IOException
    {
        boolean isDigest = dis.readBoolean();
        SliceFromReadCommand rm = new SliceFromReadCommand(dis.readUTF(),
                                                           ByteBufferUtil.readWithShortLength(dis),
                                                           QueryPath.deserialize(dis),
                                                           ByteBufferUtil.readWithShortLength(dis),
                                                           ByteBufferUtil.readWithShortLength(dis),
                                                           dis.readBoolean(),
                                                           dis.readInt());
        rm.setDigestQuery(isDigest);
        return rm;
    }

+++++++++++++++++++++++
    public ReadCommand deserialize(DataInput dis, int version) throws IOException
    {
        boolean isDigest = dis.readBoolean();
        String table = dis.readUTF();
        ByteBuffer key = ByteBufferUtil.readWithShortLength(dis);
        QueryPath path = QueryPath.deserialize(dis);
        SliceQueryFilter filter = SliceQueryFilter.serializer.deserialize(dis, version);
        SliceFromReadCommand rm = new SliceFromReadCommand(table, key, path, filter);
        rm.setDigestQuery(isDigest);
        return rm;
    }

    public long serializedSize(SnapshotCommand snapshot_command, int version)
    {
        throw new UnsupportedOperationException();
+++++++++++++++++++++++
    public long serializedSize(SnapshotCommand sc, int version)
    {
        return TypeSizes.NATIVE.sizeof(sc.keyspace)
             + TypeSizes.NATIVE.sizeof(sc.column_family)
             + TypeSizes.NATIVE.sizeof(sc.snapshot_name)
             + TypeSizes.NATIVE.sizeof(sc.clear_snapshot);
    public int serializedSize()
    {
    	/*
    	 * We need to keep the way we are calculating the column size in sync with the
    	 * way we are calculating the size for the column family serializer.
    	 */
        int size = DBConstants.shortSize + name.remaining() + DBConstants.intSize + DBConstants.longSize + DBConstants.intSize;
        for (IColumn subColumn : getSubColumns())
        {
            size += subColumn.serializedSize();
        }
        return size;
    }

+++++++++++++++++++++++
    public int serializedSize(TypeSizes typeSizes)
    {
        /*
         * We need to keep the way we are calculating the column size in sync with the
         * way we are calculating the size for the column family serializer.
         *
         * 2 bytes for name size
         * n bytes for the name
         * 4 bytes for getLocalDeletionTime
         * 8 bytes for getMarkedForDeleteAt
         * 4 bytes for the subcolumns size
         * size(constantSize) of subcolumns.
         */
        int nameSize = name.remaining();
        int subColumnsSize = 0;
        for (IColumn subColumn : getSubColumns())
            subColumnsSize += subColumn.serializedSize(typeSizes);
        int size = typeSizes.sizeof((short) nameSize) + nameSize
                 + typeSizes.sizeof(getLocalDeletionTime())
                 + typeSizes.sizeof(getMarkedForDeleteAt())
                 + typeSizes.sizeof(subColumnsSize) + subColumnsSize;
        return size;
    }

    public void serialize(IColumn column, DataOutput dos)
    {
        SuperColumn superColumn = (SuperColumn)column;
        ByteBufferUtil.writeWithShortLength(column.name(), dos);
        try
        {
            dos.writeInt(superColumn.getLocalDeletionTime());
            dos.writeLong(superColumn.getMarkedForDeleteAt());

            Collection<IColumn> columns = column.getSubColumns();
            dos.writeInt(columns.size());
            for (IColumn subColumn : columns)
            {
                Column.serializer().serialize(subColumn, dos);
            }
        }
        catch (IOException e)
        {
            throw new RuntimeException(e);
        }
    }

+++++++++++++++++++++++
    public void serialize(IColumn column, DataOutput dos) throws IOException
    {
        SuperColumn superColumn = (SuperColumn)column;
        ByteBufferUtil.writeWithShortLength(superColumn.name(), dos);
        DeletionInfo.serializer().serialize(superColumn.deletionInfo(), dos, MessagingService.VERSION_10);
        Collection<IColumn> columns = superColumn.getSubColumns();
        dos.writeInt(columns.size());
        for (IColumn subColumn : columns)
        {
            Column.serializer().serialize(subColumn, dos);
        }
    }

    public long serializedSize(IColumn object)
    {
        return object.serializedSize();
+++++++++++++++++++++++
    public long serializedSize(IColumn object, TypeSizes typeSizes)
    {
        return object.serializedSize(typeSizes);
    public IColumn deserialize(DataInput dis, IColumnSerializer.Flag flag, int expireBefore) throws IOException
    {
        ByteBuffer name = ByteBufferUtil.readWithShortLength(dis);
        int localDeleteTime = dis.readInt();
        if (localDeleteTime != Integer.MIN_VALUE && localDeleteTime <= 0)
        {
            throw new IOException("Invalid localDeleteTime read: " + localDeleteTime);
        }
        long markedForDeleteAt = dis.readLong();

        /* read the number of columns */
        int size = dis.readInt();
        ColumnSerializer serializer = Column.serializer();
        ColumnSortedMap preSortedMap = new ColumnSortedMap(comparator, serializer, dis, size, flag, expireBefore);
        SuperColumn superColumn = new SuperColumn(name, AtomicSortedColumns.factory().fromSorted(preSortedMap, false));
        superColumn.delete(localDeleteTime, markedForDeleteAt);
        return superColumn;
    }

+++++++++++++++++++++++
    public IColumn deserialize(DataInput dis, IColumnSerializer.Flag flag, int expireBefore) throws IOException
    {
        ByteBuffer name = ByteBufferUtil.readWithShortLength(dis);
        DeletionInfo delInfo = DeletionInfo.serializer().deserialize(dis, MessagingService.VERSION_10, comparator);

        /* read the number of columns */
        int size = dis.readInt();
        ColumnSerializer serializer = Column.serializer();
        ColumnSortedMap preSortedMap = new ColumnSortedMap(comparator, serializer, dis, size, flag, expireBefore);
        SuperColumn superColumn = new SuperColumn(name, AtomicSortedColumns.factory().fromSorted(preSortedMap, false));
        superColumn.delete(delInfo);
        return superColumn;
    }

    private static void serializeSchema(Map<DecoratedKey, RowMutation> mutationMap, String schemaCfName)
    {
        for (Row schemaRow : serializedSchema(schemaCfName))
        {
            RowMutation mutation = mutationMap.get(schemaRow.key);

            if (mutation == null)
            {
                mutationMap.put(schemaRow.key, new RowMutation(Table.SYSTEM_TABLE, schemaRow));
                continue;
            }

            mutation.add(schemaRow.cf);
        }
    }

+++++++++++++++++++++++
    private static void serializeSchema(Map<DecoratedKey, RowMutation> mutationMap, String schemaCfName)
    {
        for (Row schemaRow : serializedSchema(schemaCfName))
        {
            if (Schema.ignoredSchemaRow(schemaRow))
                continue;

            RowMutation mutation = mutationMap.get(schemaRow.key);

            if (mutation == null)
            {
                mutationMap.put(schemaRow.key, new RowMutation(Table.SYSTEM_KS, schemaRow));
                continue;
            }

            mutation.add(schemaRow.cf);
        }
    }

    public synchronized void serialize()
    {
        File manifestFile = cfs.directories.getOrCreateLeveledManifest();
        File oldFile = new File(manifestFile.getPath().replace(EXTENSION, "-old.json"));
        File tmpFile = new File(manifestFile.getPath().replace(EXTENSION, "-tmp.json"));

        JsonFactory f = new JsonFactory();
        try
        {
            FileOutputStream fos = new FileOutputStream(tmpFile);
            JsonGenerator g = f.createJsonGenerator(fos, JsonEncoding.UTF8);
            g.useDefaultPrettyPrinter();
            g.writeStartObject();
            g.writeArrayFieldStart("generations");
            for (int level = 0; level < generations.length; level++)
            {
                g.writeStartObject();
                g.writeNumberField("generation", level);
                g.writeArrayFieldStart("members");
                for (SSTableReader ssTableReader : generations[level])
                    g.writeNumber(ssTableReader.descriptor.generation);
                g.writeEndArray(); // members

                g.writeEndObject(); // generation
            }
            g.writeEndArray(); // for field generations
            g.writeEndObject(); // write global object
            g.flush();
            fos.getFD().sync();
            g.close();

            if (oldFile.exists() && manifestFile.exists())
                FileUtils.deleteWithConfirm(oldFile);
            if (manifestFile.exists())
                FileUtils.renameWithConfirm(manifestFile, oldFile);
            assert tmpFile.exists();
            FileUtils.renameWithConfirm(tmpFile, manifestFile);
            logger.debug("Saved manifest {}", manifestFile);
        }
        catch (IOException e)
        {
            throw new IOError(e);
        }
    }

    @Override
+++++++++++++++++++++++
    public synchronized void serialize()
    {
        File manifestFile = cfs.directories.getOrCreateLeveledManifest();
        File oldFile = new File(manifestFile.getPath().replace(EXTENSION, "-old.json"));
        File tmpFile = new File(manifestFile.getPath().replace(EXTENSION, "-tmp.json"));

        JsonFactory f = new JsonFactory();
        try
        {
            JsonGenerator g = f.createJsonGenerator(tmpFile, JsonEncoding.UTF8);
            g.useDefaultPrettyPrinter();
            g.writeStartObject();
            g.writeArrayFieldStart("generations");
            for (int level = 0; level < generations.length; level++)
            {
                g.writeStartObject();
                g.writeNumberField("generation", level);
                g.writeArrayFieldStart("members");
                for (SSTableReader ssTableReader : generations[level])
                    g.writeNumber(ssTableReader.descriptor.generation);
                g.writeEndArray(); // members

                g.writeEndObject(); // generation
            }
            g.writeEndArray(); // for field generations
            g.writeEndObject(); // write global object
            g.close();
        }
        catch (IOException e)
        {
            throw new FSWriteError(e, tmpFile);
        }

        if (oldFile.exists() && manifestFile.exists())
            FileUtils.deleteWithConfirm(oldFile);

        if (manifestFile.exists())
            FileUtils.renameWithConfirm(manifestFile, oldFile);

        assert tmpFile.exists();

        FileUtils.renameWithConfirm(tmpFile, manifestFile);

        logger.debug("Saved manifest {}", manifestFile);
    }

    @Override
    public int serializedSize()
    {
        int size = DBConstants.shortSize + (columnFamilyName == null ? 0 : FBUtilities.encodedUTF8Length(columnFamilyName));
        size += DBConstants.shortSize + (superColumnName == null ? 0 : superColumnName.remaining());
        size += DBConstants.shortSize + (columnName == null ? 0 : columnName.remaining());
        return size;
+++++++++++++++++++++++
    public int serializedSize(TypeSizes typeSizes)
    {
        int size = 0;

        if (columnFamilyName == null)
            size += typeSizes.sizeof((short) 0);
        else
            size += typeSizes.sizeof(columnFamilyName);

        if (superColumnName == null)
        {
            size += typeSizes.sizeof((short) 0);
        }
        else
        {
            int scNameSize = superColumnName.remaining();
            size += typeSizes.sizeof((short) scNameSize);
            size += scNameSize;
        }

        if (columnName == null)
        {
            size += typeSizes.sizeof((short) 0);
        }
        else
        {
            int cNameSize = columnName.remaining();
            size += typeSizes.sizeof((short) cNameSize);
            size += cNameSize;
        }

        return size;
    public void serialize(EndpointState epState, DataOutput dos, int version) throws IOException
    {
        /* serialize the HeartBeatState */
        HeartBeatState hbState = epState.getHeartBeatState();
        HeartBeatState.serializer().serialize(hbState, dos, version);

        /* serialize the map of ApplicationState objects */
        int size = epState.applicationState.size();
        dos.writeInt(size);
        for (Map.Entry<ApplicationState, VersionedValue> entry : epState.applicationState.entrySet())
        {
            VersionedValue value = entry.getValue();
            dos.writeInt(entry.getKey().ordinal());
            VersionedValue.serializer.serialize(value, dos, version);
        }
    }

+++++++++++++++++++++++
    public void serialize(EndpointState epState, DataOutput dos, int version) throws IOException
    {
        /* serialize the HeartBeatState */
        HeartBeatState hbState = epState.getHeartBeatState();
        HeartBeatState.serializer.serialize(hbState, dos, version);

        /* serialize the map of ApplicationState objects */
        int size = epState.applicationState.size();
        dos.writeInt(size);
        for (Map.Entry<ApplicationState, VersionedValue> entry : epState.applicationState.entrySet())
        {
            VersionedValue value = entry.getValue();
            dos.writeInt(entry.getKey().ordinal());
            VersionedValue.serializer.serialize(value, dos, version);
        }
    }

    public long serializedSize(EndpointState endpointState, int version)
    {
        throw new UnsupportedOperationException();
+++++++++++++++++++++++
    public long serializedSize(EndpointState epState, int version)
    {
        long size = HeartBeatState.serializer.serializedSize(epState.getHeartBeatState(), version);
        size += TypeSizes.NATIVE.sizeof(epState.applicationState.size());
        for (Map.Entry<ApplicationState, VersionedValue> entry : epState.applicationState.entrySet())
        {
            VersionedValue value = entry.getValue();
            size += TypeSizes.NATIVE.sizeof(entry.getKey().ordinal());
            size += VersionedValue.serializer.serializedSize(value, version);
        }
        return size;
    public EndpointState deserialize(DataInput dis, int version) throws IOException
    {
        HeartBeatState hbState = HeartBeatState.serializer().deserialize(dis, version);
        EndpointState epState = new EndpointState(hbState);

        int appStateSize = dis.readInt();
        for ( int i = 0; i < appStateSize; ++i )
        {
            int key = dis.readInt();
            VersionedValue value = VersionedValue.serializer.deserialize(dis, version);
            epState.addApplicationState(Gossiper.STATES[key], value);
        }
        return epState;
    }

+++++++++++++++++++++++
    public EndpointState deserialize(DataInput dis, int version) throws IOException
    {
        HeartBeatState hbState = HeartBeatState.serializer.deserialize(dis, version);
        EndpointState epState = new EndpointState(hbState);

        int appStateSize = dis.readInt();
        for ( int i = 0; i < appStateSize; ++i )
        {
            int key = dis.readInt();
            VersionedValue value = VersionedValue.serializer.deserialize(dis, version);
            epState.addApplicationState(Gossiper.STATES[key], value);
        }
        return epState;
    }

	public static ArrayList<IndexInfo> deserializeIndex(FileDataInput in) throws IOException
	{
		int columnIndexSize = in.readInt();
        if (columnIndexSize == 0)
            return null;
        ArrayList<IndexInfo> indexList = new ArrayList<IndexInfo>();
        FileMark mark = in.mark();
        while (in.bytesPastMark(mark) < columnIndexSize)
        {
            indexList.add(IndexInfo.deserialize(in));
        }
        assert in.bytesPastMark(mark) == columnIndexSize;

        return indexList;
	}

+++++++++++++++++++++++
    public static List<IndexInfo> deserializeIndex(FileDataInput in) throws IOException
    {
        int columnIndexSize = in.readInt();
        if (columnIndexSize == 0)
            return Collections.<IndexInfo>emptyList();
        ArrayList<IndexInfo> indexList = new ArrayList<IndexInfo>();
        FileMark mark = in.mark();
        while (in.bytesPastMark(mark) < columnIndexSize)
        {
            indexList.add(IndexInfo.deserialize(in));
        }
        assert in.bytesPastMark(mark) == columnIndexSize;

        return indexList;
    }

    public void serialize(BloomFilter bf, DataOutput dos) throws IOException
    {
        int bitLength = bf.bitset.getNumWords();
        int pageSize = bf.bitset.getPageSize();
        int pageCount = bf.bitset.getPageCount();

        dos.writeInt(bf.getHashCount());
        dos.writeInt(bitLength);

        for (int p = 0; p < pageCount; p++)
        {
            long[] bits = bf.bitset.getPage(p);
            for (int i = 0; i < pageSize && bitLength-- > 0; i++)
                dos.writeLong(bits[i]);
        }
    }

+++++++++++++++++++++++
    public void serialize(BloomFilter bf, DataOutput dos) throws IOException
    {
        dos.writeInt(bf.hashCount);
        bf.bitset.serialize(dos);
    }

    public long serializedSize(BloomFilter bf)
    {
        return DBConstants.intSize // hash count
               + DBConstants.intSize // length
               + bf.bitset.getNumWords() * DBConstants.longSize; // buckets
+++++++++++++++++++++++
    public long serializedSize(BloomFilter bf, TypeSizes typeSizes)
    {
        int size = typeSizes.sizeof(bf.hashCount); // hash count
        size += bf.bitset.serializedSize(typeSizes);
        return size;
    public BloomFilter deserialize(DataInput dis) throws IOException
    {
        int hashes = dis.readInt();
        long bitLength = dis.readInt();
        OpenBitSet bs = new OpenBitSet(bitLength << 6);
        int pageSize = bs.getPageSize();
        int pageCount = bs.getPageCount();

        for (int p = 0; p < pageCount; p++)
        {
            long[] bits = bs.getPage(p);
            for (int i = 0; i < pageSize && bitLength-- > 0; i++)
                bits[i] = dis.readLong();
        }

        return new BloomFilter(hashes, bs);
    }

    /**
     * Calculates a serialized size of the given Bloom Filter
     * @see BloomFilterSerializer#serialize(BloomFilter, DataOutput)
     *
     * @param bf Bloom filter to calculate serialized size
     *
     * @return serialized size of the given bloom filter
     */
+++++++++++++++++++++++
    public BloomFilter deserialize(DataInput dis, boolean offheap) throws IOException
    {
        int hashes = dis.readInt();
        IBitSet bs = offheap ? OffHeapBitSet.deserialize(dis) : OpenBitSet.deserialize(dis);
        return createFilter(hashes, bs);
    }

    public static <T> byte[] serialize(T object, IVersionedSerializer<T> serializer, int version) throws IOException
    {
        int size = (int) serializer.serializedSize(object, version);
        DataOutputBuffer buffer = new DataOutputBuffer(size);
        serializer.serialize(object, buffer, version);
        assert buffer.getLength() == size && buffer.getData().length == size
               : String.format("Final buffer length %s to accomodate data size of %s (predicted %s) for %s",
                               buffer.getData().length, buffer.getLength(), size, object);
        return buffer.getData();
    }

+++++++++++++++++++++++
    public static <T> byte[] serialize(T object, IVersionedSerializer<T> serializer, int version) throws IOException
    {
        int size = (int) serializer.serializedSize(object, version);
        DataOutputBuffer buffer = new DataOutputBuffer(size);
        serializer.serialize(object, buffer, version);
        assert buffer.getLength() == size && buffer.getData().length == size
               : String.format("Final buffer length %s to accommodate data size of %s (predicted %s) for %s",
                               buffer.getData().length, buffer.getLength(), size, object);
        return buffer.getData();
    public LegacyBloomFilter deserialize(DataInputStream dis) throws IOException
    {
        int hashes = dis.readInt();
        ObjectInputStream ois = new ObjectInputStream(dis);
        try
        {
          BitSet bs = (BitSet) ois.readObject();
          return new LegacyBloomFilter(hashes, bs);
        } catch (ClassNotFoundException e)
        {
          throw new RuntimeException(e);
        }
    }

+++++++++++++++++++++++
    public LegacyBloomFilter deserialize(final DataInput dis) throws IOException
    {
        int hashes = dis.readInt();
        ObjectInputStream ois = new ObjectInputStream(new InputStream()
        {
            @Override
            public int read() throws IOException
            {
                return dis.readByte() & 0xFF;
            }
        });
        try
        {
          BitSet bs = (BitSet) ois.readObject();
          return new LegacyBloomFilter(hashes, bs);
        } catch (ClassNotFoundException e)
        {
          throw new RuntimeException(e);
        }
    }

    private ReadCommand serializeAndDeserializeReadMessage(ReadCommand rm) throws IOException
    {
        ReadCommandSerializer rms = ReadCommand.serializer();
        DataOutputBuffer dos = new DataOutputBuffer();
        ByteArrayInputStream bis;

        rms.serialize(rm, dos, MessagingService.version_);
        bis = new ByteArrayInputStream(dos.getData(), 0, dos.getLength());
        return rms.deserialize(new DataInputStream(bis), MessagingService.version_);
    }

    @Test
+++++++++++++++++++++++
    private ReadCommand serializeAndDeserializeReadMessage(ReadCommand rm) throws IOException
    {
        ReadCommandSerializer rms = ReadCommand.serializer;
        DataOutputBuffer dos = new DataOutputBuffer();
        ByteArrayInputStream bis;

        rms.serialize(rm, dos, MessagingService.current_version);
        bis = new ByteArrayInputStream(dos.getData(), 0, dos.getLength());
        return rms.deserialize(new DataInputStream(bis), MessagingService.current_version);
    }

    @Test
add: 195, delete: 48, change: 628, unhandled: 0 size_exceptions: 229 size_serialize: 40
-----------cassandra-1.2.0 vs cassandra-1.2.0-beta1-----------
    private RefCountedMemory serialize(V value)
    {
        long serializedSize = serializer.serializedSize(value, ENCODED_TYPE_SIZES);
        if (serializedSize > Integer.MAX_VALUE)
            throw new IllegalArgumentException("Unable to allocate " + serializedSize + " bytes");

        RefCountedMemory freeableMemory;
        try
        {
            freeableMemory = new RefCountedMemory(serializedSize);
        }
        catch (OutOfMemoryError e)
        {
            return null;
        }

        try
        {
            serializer.serialize(value, new EncodedDataOutputStream(new MemoryOutputStream(freeableMemory)));
        }
        catch (IOException e)
        {
            throw new RuntimeException(e);
        }
        return freeableMemory;
    }

+++++++++++++++++++++++
    private FreeableMemory serialize(V value)
    {
        long serializedSize = serializer.serializedSize(value, ENCODED_TYPE_SIZES);
        if (serializedSize > Integer.MAX_VALUE)
            throw new IllegalArgumentException("Unable to allocate " + serializedSize + " bytes");

        FreeableMemory freeableMemory;
        try
        {
            freeableMemory = new FreeableMemory(serializedSize);
        }
        catch (OutOfMemoryError e)
        {
            return null;
        }

        try
        {
            serializer.serialize(value, new EncodedDataOutputStream(new MemoryOutputStream(freeableMemory)));
        }
        catch (IOException e)
        {
            throw new RuntimeException(e);
        }
        return freeableMemory;
    }

    private V deserialize(RefCountedMemory mem)
    {
        try
        {
            return serializer.deserialize(new EncodedDataInputStream(new MemoryInputStream(mem)));
        }
        catch (IOException e)
        {
            logger.debug("Cannot fetch in memory data, we will failback to read from disk ", e);
            return null;
        }
    }

+++++++++++++++++++++++
    private V deserialize(FreeableMemory mem)
    {
        try
        {
            return serializer.deserialize(new EncodedDataInputStream(new MemoryInputStream(mem)));
        }
        catch (IOException e)
        {
            logger.debug("Cannot fetch in memory data, we will failback to read from disk ", e);
            return null;
        }
    }

    Column deserializeColumnBody(DataInput dis, ByteBuffer name, int mask, IColumnSerializer.Flag flag, int expireBefore) throws IOException
    {
        if ((mask & COUNTER_MASK) != 0)
        {
            long timestampOfLastDelete = dis.readLong();
            long ts = dis.readLong();
            ByteBuffer value = ByteBufferUtil.readWithLength(dis);
            return CounterColumn.create(name, value, ts, timestampOfLastDelete, flag);
        }
        else if ((mask & EXPIRATION_MASK) != 0)
        {
            int ttl = dis.readInt();
            int expiration = dis.readInt();
            long ts = dis.readLong();
            ByteBuffer value = ByteBufferUtil.readWithLength(dis);
            return ExpiringColumn.create(name, value, ts, ttl, expiration, expireBefore, flag);
        }
        else
        {
            long ts = dis.readLong();
            ByteBuffer value = ByteBufferUtil.readWithLength(dis);
            return (mask & COUNTER_UPDATE_MASK) != 0
                   ? new CounterUpdateColumn(name, value, ts)
                   : ((mask & DELETION_MASK) == 0
                      ? new Column(name, value, ts)
                      : new DeletedColumn(name, value, ts));
        }
    }

+++++++++++++++++++++++
    Column deserializeColumnBody(DataInput dis, ByteBuffer name, int mask, IColumnSerializer.Flag flag, int expireBefore) throws IOException
    {
        if ((mask & COUNTER_MASK) != 0)
        {
            long timestampOfLastDelete = dis.readLong();
            long ts = dis.readLong();
            ByteBuffer value = ByteBufferUtil.readWithLength(dis);
            return CounterColumn.create(name, value, ts, timestampOfLastDelete, flag);
        }
        else if ((mask & EXPIRATION_MASK) != 0)
        {
            int ttl = dis.readInt();
            int expiration = dis.readInt();
            long ts = dis.readLong();
            ByteBuffer value = ByteBufferUtil.readWithLength(dis);
            return ExpiringColumn.create(name, value, ts, ttl, expiration, expireBefore, flag);
        }
        else
        {
            long ts = dis.readLong();
            long now = FBUtilities.timestampMicros();

            if (ts > now) // fixing the timestamp from the future to be 'now' in micros
                ts = now; // helps with CASSANDRA-4561 as remote nodes can send schema with wrong nanoTime() timestamps

            ByteBuffer value = ByteBufferUtil.readWithLength(dis);
            return (mask & COUNTER_UPDATE_MASK) != 0
                   ? new CounterUpdateColumn(name, value, ts)
                   : ((mask & DELETION_MASK) == 0
                      ? new Column(name, value, ts)
                      : new DeletedColumn(name, value, ts));
        }
    }

    public void serialize(RangeSliceCommand sliceCommand, DataOutput dos, int version) throws IOException
    {
        dos.writeUTF(sliceCommand.keyspace);
        dos.writeUTF(sliceCommand.column_family);
        ByteBuffer sc = sliceCommand.super_column;
        dos.writeInt(sc == null ? 0 : sc.remaining());
        if (sc != null)
            ByteBufferUtil.write(sc, dos);

        if (version < MessagingService.VERSION_12)
        {
            FBUtilities.serialize(new TSerializer(new TBinaryProtocol.Factory()), asSlicePredicate(sliceCommand.predicate), dos);
        }
        else
        {
            IDiskAtomFilter.Serializer.instance.serialize(sliceCommand.predicate, dos, version);
        }

        if (version >= MessagingService.VERSION_11)
        {
            if (sliceCommand.row_filter == null)
            {
                dos.writeInt(0);
            }
            else
            {
                dos.writeInt(sliceCommand.row_filter.size());
                for (IndexExpression expr : sliceCommand.row_filter)
                {
                    if (version < MessagingService.VERSION_12)
                    {
                        FBUtilities.serialize(new TSerializer(new TBinaryProtocol.Factory()), expr, dos);
                    }
                    else
                    {
                        ByteBufferUtil.writeWithShortLength(expr.column_name, dos);
                        dos.writeInt(expr.op.getValue());
                        ByteBufferUtil.writeWithShortLength(expr.value, dos);
                    }
                }
            }
        }
        AbstractBounds.serializer.serialize(sliceCommand.range, dos, version);
        dos.writeInt(sliceCommand.maxResults);
        if (version >= MessagingService.VERSION_11)
        {
            dos.writeBoolean(sliceCommand.countCQL3Rows);
            dos.writeBoolean(sliceCommand.isPaging);
        }
    }

+++++++++++++++++++++++
    public void serialize(RangeSliceCommand sliceCommand, DataOutput dos, int version) throws IOException
    {
        dos.writeUTF(sliceCommand.keyspace);
        dos.writeUTF(sliceCommand.column_family);
        ByteBuffer sc = sliceCommand.super_column;
        dos.writeInt(sc == null ? 0 : sc.remaining());
        if (sc != null)
            ByteBufferUtil.write(sc, dos);

        if (version < MessagingService.VERSION_12)
        {
            FBUtilities.serialize(new TSerializer(new TBinaryProtocol.Factory()), asSlicePredicate(sliceCommand.predicate), dos);
        }
        else
        {
            IFilter.Serializer.instance.serialize(sliceCommand.predicate, dos, version);
        }

        if (version >= MessagingService.VERSION_11)
        {
            if (sliceCommand.row_filter == null)
            {
                dos.writeInt(0);
            }
            else
            {
                dos.writeInt(sliceCommand.row_filter.size());
                for (IndexExpression expr : sliceCommand.row_filter)
                {
                    if (version < MessagingService.VERSION_12)
                    {
                        FBUtilities.serialize(new TSerializer(new TBinaryProtocol.Factory()), expr, dos);
                    }
                    else
                    {
                        ByteBufferUtil.writeWithShortLength(expr.column_name, dos);
                        dos.writeInt(expr.op.getValue());
                        ByteBufferUtil.writeWithShortLength(expr.value, dos);
                    }
                }
            }
        }
        AbstractBounds.serializer.serialize(sliceCommand.range, dos, version);
        dos.writeInt(sliceCommand.maxResults);
        if (version >= MessagingService.VERSION_11)
        {
            dos.writeBoolean(sliceCommand.maxIsColumns);
            dos.writeBoolean(sliceCommand.isPaging);
        }
    }

    public long serializedSize(RangeSliceCommand rsc, int version)
    {
        long size = TypeSizes.NATIVE.sizeof(rsc.keyspace);
        size += TypeSizes.NATIVE.sizeof(rsc.column_family);

        ByteBuffer sc = rsc.super_column;
        if (sc != null)
        {
            size += TypeSizes.NATIVE.sizeof(sc.remaining());
            size += sc.remaining();
        }
        else
        {
            size += TypeSizes.NATIVE.sizeof(0);
        }

        if (version < MessagingService.VERSION_12)
        {
            TSerializer ser = new TSerializer(new TBinaryProtocol.Factory());
            try
            {
                int predicateLength = ser.serialize(asSlicePredicate(rsc.predicate)).length;
                if (version < MessagingService.VERSION_12)
                    size += TypeSizes.NATIVE.sizeof(predicateLength);
                size += predicateLength;
            }
            catch (TException e)
            {
                throw new RuntimeException(e);
            }
        }
        else
        {
            size += IDiskAtomFilter.Serializer.instance.serializedSize(rsc.predicate, version);
        }

        if (version >= MessagingService.VERSION_11)
        {
            if (rsc.row_filter == null)
            {
                size += TypeSizes.NATIVE.sizeof(0);
            }
            else
            {
                size += TypeSizes.NATIVE.sizeof(rsc.row_filter.size());
                for (IndexExpression expr : rsc.row_filter)
                {
                    if (version < MessagingService.VERSION_12)
                    {
                        try
                        {
                            int filterLength = new TSerializer(new TBinaryProtocol.Factory()).serialize(expr).length;
                            size += TypeSizes.NATIVE.sizeof(filterLength);
                            size += filterLength;
                        }
                        catch (TException e)
                        {
                            throw new RuntimeException(e);
                        }
                    }
                    else
                    {
                        size += TypeSizes.NATIVE.sizeofWithShortLength(expr.column_name);
                        size += TypeSizes.NATIVE.sizeof(expr.op.getValue());
                        size += TypeSizes.NATIVE.sizeofWithLength(expr.value);
                    }
                }
            }
        }
        size += AbstractBounds.serializer.serializedSize(rsc.range, version);
        size += TypeSizes.NATIVE.sizeof(rsc.maxResults);
        if (version >= MessagingService.VERSION_11)
        {
            size += TypeSizes.NATIVE.sizeof(rsc.countCQL3Rows);
            size += TypeSizes.NATIVE.sizeof(rsc.isPaging);
        }
        return size;
+++++++++++++++++++++++
    public long serializedSize(RangeSliceCommand rsc, int version)
    {
        long size = TypeSizes.NATIVE.sizeof(rsc.keyspace);
        size += TypeSizes.NATIVE.sizeof(rsc.column_family);

        ByteBuffer sc = rsc.super_column;
        if (sc != null)
        {
            size += TypeSizes.NATIVE.sizeof(sc.remaining());
            size += sc.remaining();
        }
        else
        {
            size += TypeSizes.NATIVE.sizeof(0);
        }

        if (version < MessagingService.VERSION_12)
        {
            TSerializer ser = new TSerializer(new TBinaryProtocol.Factory());
            try
            {
                int predicateLength = ser.serialize(asSlicePredicate(rsc.predicate)).length;
                if (version < MessagingService.VERSION_12)
                    size += TypeSizes.NATIVE.sizeof(predicateLength);
                size += predicateLength;
            }
            catch (TException e)
            {
                throw new RuntimeException(e);
            }
        }
        else
        {
            size += IFilter.Serializer.instance.serializedSize(rsc.predicate, version);
        }

        if (version >= MessagingService.VERSION_11)
        {
            if (rsc.row_filter == null)
            {
                size += TypeSizes.NATIVE.sizeof(0);
            }
            else
            {
                size += TypeSizes.NATIVE.sizeof(rsc.row_filter.size());
                for (IndexExpression expr : rsc.row_filter)
                {
                    if (version < MessagingService.VERSION_12)
                    {
                        try
                        {
                            int filterLength = new TSerializer(new TBinaryProtocol.Factory()).serialize(expr).length;
                            size += TypeSizes.NATIVE.sizeof(filterLength);
                            size += filterLength;
                        }
                        catch (TException e)
                        {
                            throw new RuntimeException(e);
                        }
                    }
                    else
                    {
                        size += TypeSizes.NATIVE.sizeofWithShortLength(expr.column_name);
                        size += TypeSizes.NATIVE.sizeof(expr.op.getValue());
                        size += TypeSizes.NATIVE.sizeofWithLength(expr.value);
                    }
                }
            }
        }
        size += AbstractBounds.serializer.serializedSize(rsc.range, version);
        size += TypeSizes.NATIVE.sizeof(rsc.maxResults);
        if (version >= MessagingService.VERSION_11)
        {
            size += TypeSizes.NATIVE.sizeof(rsc.maxIsColumns);
            size += TypeSizes.NATIVE.sizeof(rsc.isPaging);
        }
        return size;
    public RangeSliceCommand deserialize(DataInput dis, int version) throws IOException
    {
        String keyspace = dis.readUTF();
        String columnFamily = dis.readUTF();

        int scLength = dis.readInt();
        ByteBuffer superColumn = null;
        if (scLength > 0)
        {
            byte[] buf = new byte[scLength];
            dis.readFully(buf);
            superColumn = ByteBuffer.wrap(buf);
        }

        IDiskAtomFilter predicate;
        AbstractType<?> comparator = ColumnFamily.getComparatorFor(keyspace, columnFamily, superColumn);
        if (version < MessagingService.VERSION_12)
        {
            SlicePredicate pred = new SlicePredicate();
            FBUtilities.deserialize(new TDeserializer(new TBinaryProtocol.Factory()), pred, dis);
            predicate = ThriftValidation.asIFilter(pred, comparator);
        }
        else
        {
            predicate = IDiskAtomFilter.Serializer.instance.deserialize(dis, version, comparator);
        }

        List<IndexExpression> rowFilter = null;
        if (version >= MessagingService.VERSION_11)
        {
            int filterCount = dis.readInt();
            rowFilter = new ArrayList<IndexExpression>(filterCount);
            for (int i = 0; i < filterCount; i++)
            {
                IndexExpression expr;
                if (version < MessagingService.VERSION_12)
                {
                    expr = new IndexExpression();
                    FBUtilities.deserialize(new TDeserializer(new TBinaryProtocol.Factory()), expr, dis);
                }
                else
                {
                    expr = new IndexExpression(ByteBufferUtil.readWithShortLength(dis),
                                               IndexOperator.findByValue(dis.readInt()),
                                               ByteBufferUtil.readWithShortLength(dis));
                }
                rowFilter.add(expr);
            }
        }
        AbstractBounds<RowPosition> range = AbstractBounds.serializer.deserialize(dis, version).toRowBounds();

        int maxResults = dis.readInt();
        boolean countCQL3Rows = false;
        boolean isPaging = false;
        if (version >= MessagingService.VERSION_11)
        {
            countCQL3Rows = dis.readBoolean();
            isPaging = dis.readBoolean();
        }
        return new RangeSliceCommand(keyspace, columnFamily, superColumn, predicate, range, rowFilter, maxResults, countCQL3Rows, isPaging);
    }

+++++++++++++++++++++++
    public RangeSliceCommand deserialize(DataInput dis, int version) throws IOException
    {
        String keyspace = dis.readUTF();
        String columnFamily = dis.readUTF();

        int scLength = dis.readInt();
        ByteBuffer superColumn = null;
        if (scLength > 0)
        {
            byte[] buf = new byte[scLength];
            dis.readFully(buf);
            superColumn = ByteBuffer.wrap(buf);
        }

        IFilter predicate;
        AbstractType<?> comparator = ColumnFamily.getComparatorFor(keyspace, columnFamily, superColumn);
        if (version < MessagingService.VERSION_12)
        {
            SlicePredicate pred = new SlicePredicate();
            FBUtilities.deserialize(new TDeserializer(new TBinaryProtocol.Factory()), pred, dis);
            predicate = ThriftValidation.asIFilter(pred, comparator);
        }
        else
        {
            predicate = IFilter.Serializer.instance.deserialize(dis, version, comparator);
        }

        List<IndexExpression> rowFilter = null;
        if (version >= MessagingService.VERSION_11)
        {
            int filterCount = dis.readInt();
            rowFilter = new ArrayList<IndexExpression>(filterCount);
            for (int i = 0; i < filterCount; i++)
            {
                IndexExpression expr;
                if (version < MessagingService.VERSION_12)
                {
                    expr = new IndexExpression();
                    FBUtilities.deserialize(new TDeserializer(new TBinaryProtocol.Factory()), expr, dis);
                }
                else
                {
                    expr = new IndexExpression(ByteBufferUtil.readWithShortLength(dis),
                                               IndexOperator.findByValue(dis.readInt()),
                                               ByteBufferUtil.readWithShortLength(dis));
                }
                rowFilter.add(expr);
            }
        }
        AbstractBounds<RowPosition> range = AbstractBounds.serializer.deserialize(dis, version).toRowBounds();

        int maxResults = dis.readInt();
        boolean maxIsColumns = false;
        boolean isPaging = false;
        if (version >= MessagingService.VERSION_11)
        {
            maxIsColumns = dis.readBoolean();
            isPaging = dis.readBoolean();
        }
        return new RangeSliceCommand(keyspace, columnFamily, superColumn, predicate, range, rowFilter, maxResults, maxIsColumns, isPaging);
    }

    private static void serializeSchema(Map<DecoratedKey, RowMutation> mutationMap, String schemaCfName)
    {
        for (Row schemaRow : serializedSchema(schemaCfName))
        {
            if (Schema.ignoredSchemaRow(schemaRow))
                continue;

            RowMutation mutation = mutationMap.get(schemaRow.key);

            if (mutation == null)
            {
                mutationMap.put(schemaRow.key, new RowMutation(Table.SYSTEM_KS, schemaRow));
                continue;
            }

            mutation.add(schemaRow.cf);
        }
    }

+++++++++++++++++++++++
    private static void serializeSchema(Map<DecoratedKey, RowMutation> mutationMap, String schemaCfName)
    {
        for (Row schemaRow : serializedSchema(schemaCfName))
        {
            RowMutation mutation = mutationMap.get(schemaRow.key);

            if (mutation == null)
            {
                mutationMap.put(schemaRow.key, new RowMutation(Table.SYSTEM_KS, schemaRow));
                continue;
            }

            mutation.add(schemaRow.cf);
        }
    }

    private static Collection<Token> deserializeTokens(Collection<String> tokensStrings)
    {
        Token.TokenFactory factory = StorageService.getPartitioner().getTokenFactory();
        List<Token> tokens = new ArrayList<Token>(tokensStrings.size());
        for (String tk : tokensStrings)
            tokens.add(factory.fromString(tk));
        return tokens;
    }

    /**
     * Remove stored tokens being used by another node
     */
+++++++++++++++++++++++
    private static Collection<Token> deserializeTokens(ByteBuffer tokenBytes)
    {
        List<Token> tokens = new ArrayList<Token>();
        IPartitioner p = StorageService.getPartitioner();

        while(tokenBytes.hasRemaining())
        {
            short len = tokenBytes.getShort();
            ByteBuffer dup = tokenBytes.slice();
            dup.limit(len);
            tokenBytes.position(tokenBytes.position() + len);
            tokens.add(p.getTokenFactory().fromByteArray(dup));
        }

        return tokens;
    }

    public void serialize(GossipDigestAck gDigestAckMessage, DataOutput dos, int version) throws IOException
    {
        GossipDigestSerializationHelper.serialize(gDigestAckMessage.gDigestList, dos, version);
        if (version <= MessagingService.VERSION_11)
            dos.writeBoolean(true); // 0.6 compatibility
        dos.writeInt(gDigestAckMessage.epStateMap.size());
        for (Map.Entry<InetAddress, EndpointState> entry : gDigestAckMessage.epStateMap.entrySet())
        {
            InetAddress ep = entry.getKey();
            CompactEndpointSerializationHelper.serialize(ep, dos);
            EndpointState.serializer.serialize(entry.getValue(), dos, version);
        }
    }

+++++++++++++++++++++++
    public void serialize(GossipDigestAck gDigestAckMessage, DataOutput dos, int version) throws IOException
    {
        GossipDigestSerializationHelper.serialize(gDigestAckMessage.gDigestList, dos, version);
        if (version <= MessagingService.VERSION_10)
            dos.writeBoolean(true); // 0.6 compatibility
        dos.writeInt(gDigestAckMessage.epStateMap.size());
        for (Map.Entry<InetAddress, EndpointState> entry : gDigestAckMessage.epStateMap.entrySet())
        {
            InetAddress ep = entry.getKey();
            CompactEndpointSerializationHelper.serialize(ep, dos);
            EndpointState.serializer.serialize(entry.getValue(), dos, version);
        }
    }

    public GossipDigestAck deserialize(DataInput dis, int version) throws IOException
    {
        List<GossipDigest> gDigestList = GossipDigestSerializationHelper.deserialize(dis, version);
        if (version <= MessagingService.VERSION_11)
            dis.readBoolean(); // 0.6 compatibility
        int size = dis.readInt();
        Map<InetAddress, EndpointState> epStateMap = new HashMap<InetAddress, EndpointState>(size);

        for (int i = 0; i < size; ++i)
        {
            InetAddress ep = CompactEndpointSerializationHelper.deserialize(dis);
            EndpointState epState = EndpointState.serializer.deserialize(dis, version);
            epStateMap.put(ep, epState);
        }
        return new GossipDigestAck(gDigestList, epStateMap);
    }

+++++++++++++++++++++++
    public GossipDigestAck deserialize(DataInput dis, int version) throws IOException
    {
        List<GossipDigest> gDigestList = GossipDigestSerializationHelper.deserialize(dis, version);
        if (version <= MessagingService.VERSION_10)
            dis.readBoolean(); // 0.6 compatibility
        int size = dis.readInt();
        Map<InetAddress, EndpointState> epStateMap = new HashMap<InetAddress, EndpointState>(size);

        for (int i = 0; i < size; ++i)
        {
            InetAddress ep = CompactEndpointSerializationHelper.deserialize(dis);
            EndpointState epState = EndpointState.serializer.deserialize(dis, version);
            epStateMap.put(ep, epState);
        }
        return new GossipDigestAck(gDigestList, epStateMap);
    }

    public ChannelBuffer serialize()
    {
        return ChannelBuffers.wrappedBuffer(CBUtil.enumValueToCB(type), serializeEvent());
    }

+++++++++++++++++++++++
    public ChannelBuffer serialize()
    {
        return ChannelBuffers.wrappedBuffer(CBUtil.stringToCB(type.toString()),
                                            serializeEvent());
    }

    public static Event deserialize(ChannelBuffer cb)
    {
        switch (CBUtil.readEnumValue(Type.class, cb))
        {
            case TOPOLOGY_CHANGE:
                return TopologyChange.deserializeEvent(cb);
            case STATUS_CHANGE:
                return StatusChange.deserializeEvent(cb);
            case SCHEMA_CHANGE:
                return SchemaChange.deserializeEvent(cb);
        }
        throw new AssertionError();
    }

+++++++++++++++++++++++
    public static Event deserialize(ChannelBuffer cb)
    {
        switch (Enum.valueOf(Type.class, CBUtil.readString(cb).toUpperCase()))
        {
            case TOPOLOGY_CHANGE:
                return TopologyChange.deserializeEvent(cb);
            case STATUS_CHANGE:
                return StatusChange.deserializeEvent(cb);
        }
        throw new AssertionError();
    }

    public void serialize(BloomFilter bf, DataOutput dos) throws IOException
    {
        dos.writeInt(bf.hashCount);
        bf.bitset.serialize(dos);
    }

+++++++++++++++++++++++
    public void serialize(BloomFilter bf, DataOutput dos) throws IOException
    {
        int bitLength = bf.bitset.getNumWords();
        int pageSize = bf.bitset.getPageSize();
        int pageCount = bf.bitset.getPageCount();

        dos.writeInt(bf.getHashCount());
        dos.writeInt(bitLength);

        for (int p = 0; p < pageCount; p++)
        {
            long[] bits = bf.bitset.getPage(p);
            for (int i = 0; i < pageSize && bitLength-- > 0; i++)
                dos.writeLong(bits[i]);
        }
    }

    public long serializedSize(BloomFilter bf, TypeSizes typeSizes)
    {
        int size = typeSizes.sizeof(bf.hashCount); // hash count
        size += bf.bitset.serializedSize(typeSizes);
        return size;
+++++++++++++++++++++++
    public long serializedSize(BloomFilter bf, TypeSizes typeSizes)
    {
        int bitLength = bf.bitset.getNumWords();
        int pageSize = bf.bitset.getPageSize();
        int pageCount = bf.bitset.getPageCount();

        int size = 0;
        size += typeSizes.sizeof(bf.getHashCount()); // hash count
        size += typeSizes.sizeof(bitLength); // length

        for (int p = 0; p < pageCount; p++)
        {
            long[] bits = bf.bitset.getPage(p);
            for (int i = 0; i < pageSize && bitLength-- > 0; i++)
                size += typeSizes.sizeof(bits[i]); // bucket
        }
        return size;
    public BloomFilter deserialize(DataInput dis, boolean offheap) throws IOException
    {
        int hashes = dis.readInt();
        IBitSet bs = offheap ? OffHeapBitSet.deserialize(dis) : OpenBitSet.deserialize(dis);
        return createFilter(hashes, bs);
    }

+++++++++++++++++++++++
    public BloomFilter deserialize(DataInput dis) throws IOException
    {
        int hashes = dis.readInt();
        long bitLength = dis.readInt();
        OpenBitSet bs = new OpenBitSet(bitLength << 6);
        int pageSize = bs.getPageSize();
        int pageCount = bs.getPageCount();

        for (int p = 0; p < pageCount; p++)
        {
            long[] bits = bs.getPage(p);
            for (int i = 0; i < pageSize && bitLength-- > 0; i++)
                bits[i] = dis.readLong();
        }

        return createFilter(hashes, bs);
    }

    public static void serialize(IFilter bf, DataOutput output, Type type) throws IOException
    {
        switch (type)
        {
            case SHA:
                LegacyBloomFilter.serializer.serialize((LegacyBloomFilter) bf, output);
                break;
            case MURMUR2:
                Murmur2BloomFilter.serializer.serialize((Murmur2BloomFilter) bf, output);
                break;
            default:
                Murmur3BloomFilter.serializer.serialize((Murmur3BloomFilter) bf, output);
                break;
        }
    }

+++++++++++++++++++++++
    public static void serialize(Filter bf, DataOutput output, Type type) throws IOException
    {
        switch (type)
        {
            case SHA:
                LegacyBloomFilter.serializer.serialize((LegacyBloomFilter) bf, output);
                break;
            case MURMUR2:
                Murmur2BloomFilter.serializer.serialize((Murmur2BloomFilter) bf, output);
                break;
            default:
                Murmur3BloomFilter.serializer.serialize((Murmur3BloomFilter) bf, output);
                break;
        }
    }

    public static long serializedSize(IFilter bf, Type type)
    {
        switch (type)
        {
            case SHA:
                return LegacyBloomFilter.serializer.serializedSize((LegacyBloomFilter) bf);
            case MURMUR2:
                return Murmur2BloomFilter.serializer.serializedSize((Murmur2BloomFilter) bf, TYPE_SIZES);
            default:
                return Murmur3BloomFilter.serializer.serializedSize((Murmur3BloomFilter) bf, TYPE_SIZES);
        }
    }

    /**
     * @return A BloomFilter with the lowest practical false positive
     *         probability for the given number of elements.
     */
+++++++++++++++++++++++
    public static long serializedSize(Filter bf, Type type)
    {
        switch (type)
        {
            case SHA:
                return LegacyBloomFilter.serializer.serializedSize((LegacyBloomFilter) bf);
            case MURMUR2:
                return Murmur2BloomFilter.serializer.serializedSize((Murmur2BloomFilter) bf, TYPE_SIZES);
            default:
                return Murmur3BloomFilter.serializer.serializedSize((Murmur3BloomFilter) bf, TYPE_SIZES);
        }
    }

    /**
     * @return A BloomFilter with the lowest practical false positive
     *         probability for the given number of elements.
     */
    public static IFilter deserialize(DataInput input, Type type, boolean offheap) throws IOException
    {
        switch (type)
        {
            case SHA:
                return LegacyBloomFilter.serializer.deserialize(input);
            case MURMUR2:
                return Murmur2BloomFilter.serializer.deserialize(input, offheap);
            default:
                return Murmur3BloomFilter.serializer.deserialize(input, offheap);
        }
    }

+++++++++++++++++++++++
    public static Filter deserialize(DataInput input, Type type) throws IOException
    {
        switch (type)
        {
            case SHA:
                return LegacyBloomFilter.serializer.deserialize(input);
            case MURMUR2:
                return Murmur2BloomFilter.serializer.deserialize(input);
            default:
                return Murmur3BloomFilter.serializer.deserialize(input);
        }
    }

add: 15, delete: 56, change: 352, unhandled: 0 size_exceptions: 19 size_serialize: 18
-----------cassandra-1.2.0-beta1 vs cassandra-1.2.0-beta2-----------
    private FreeableMemory serialize(V value)
    {
        long serializedSize = serializer.serializedSize(value, ENCODED_TYPE_SIZES);
        if (serializedSize > Integer.MAX_VALUE)
            throw new IllegalArgumentException("Unable to allocate " + serializedSize + " bytes");

        FreeableMemory freeableMemory;
        try
        {
            freeableMemory = new FreeableMemory(serializedSize);
        }
        catch (OutOfMemoryError e)
        {
            return null;
        }

        try
        {
            serializer.serialize(value, new EncodedDataOutputStream(new MemoryOutputStream(freeableMemory)));
        }
        catch (IOException e)
        {
            throw new RuntimeException(e);
        }
        return freeableMemory;
    }

+++++++++++++++++++++++
    private RefCountedMemory serialize(V value)
    {
        long serializedSize = serializer.serializedSize(value, ENCODED_TYPE_SIZES);
        if (serializedSize > Integer.MAX_VALUE)
            throw new IllegalArgumentException("Unable to allocate " + serializedSize + " bytes");

        RefCountedMemory freeableMemory;
        try
        {
            freeableMemory = new RefCountedMemory(serializedSize);
        }
        catch (OutOfMemoryError e)
        {
            return null;
        }

        try
        {
            serializer.serialize(value, new EncodedDataOutputStream(new MemoryOutputStream(freeableMemory)));
        }
        catch (IOException e)
        {
            throw new RuntimeException(e);
        }
        return freeableMemory;
    }

    private V deserialize(FreeableMemory mem)
    {
        try
        {
            return serializer.deserialize(new EncodedDataInputStream(new MemoryInputStream(mem)));
        }
        catch (IOException e)
        {
            logger.debug("Cannot fetch in memory data, we will failback to read from disk ", e);
            return null;
        }
    }

+++++++++++++++++++++++
    private V deserialize(RefCountedMemory mem)
    {
        try
        {
            return serializer.deserialize(new EncodedDataInputStream(new MemoryInputStream(mem)));
        }
        catch (IOException e)
        {
            logger.debug("Cannot fetch in memory data, we will failback to read from disk ", e);
            return null;
        }
    }

    Column deserializeColumnBody(DataInput dis, ByteBuffer name, int mask, IColumnSerializer.Flag flag, int expireBefore) throws IOException
    {
        if ((mask & COUNTER_MASK) != 0)
        {
            long timestampOfLastDelete = dis.readLong();
            long ts = dis.readLong();
            ByteBuffer value = ByteBufferUtil.readWithLength(dis);
            return CounterColumn.create(name, value, ts, timestampOfLastDelete, flag);
        }
        else if ((mask & EXPIRATION_MASK) != 0)
        {
            int ttl = dis.readInt();
            int expiration = dis.readInt();
            long ts = dis.readLong();
            ByteBuffer value = ByteBufferUtil.readWithLength(dis);
            return ExpiringColumn.create(name, value, ts, ttl, expiration, expireBefore, flag);
        }
        else
        {
            long ts = dis.readLong();
            long now = FBUtilities.timestampMicros();

            if (ts > now) // fixing the timestamp from the future to be 'now' in micros
                ts = now; // helps with CASSANDRA-4561 as remote nodes can send schema with wrong nanoTime() timestamps

            ByteBuffer value = ByteBufferUtil.readWithLength(dis);
            return (mask & COUNTER_UPDATE_MASK) != 0
                   ? new CounterUpdateColumn(name, value, ts)
                   : ((mask & DELETION_MASK) == 0
                      ? new Column(name, value, ts)
                      : new DeletedColumn(name, value, ts));
        }
    }

+++++++++++++++++++++++
    Column deserializeColumnBody(DataInput dis, ByteBuffer name, int mask, IColumnSerializer.Flag flag, int expireBefore) throws IOException
    {
        if ((mask & COUNTER_MASK) != 0)
        {
            long timestampOfLastDelete = dis.readLong();
            long ts = dis.readLong();
            ByteBuffer value = ByteBufferUtil.readWithLength(dis);
            return CounterColumn.create(name, value, ts, timestampOfLastDelete, flag);
        }
        else if ((mask & EXPIRATION_MASK) != 0)
        {
            int ttl = dis.readInt();
            int expiration = dis.readInt();
            long ts = dis.readLong();
            ByteBuffer value = ByteBufferUtil.readWithLength(dis);
            return ExpiringColumn.create(name, value, ts, ttl, expiration, expireBefore, flag);
        }
        else
        {
            long ts = dis.readLong();
            ByteBuffer value = ByteBufferUtil.readWithLength(dis);
            return (mask & COUNTER_UPDATE_MASK) != 0
                   ? new CounterUpdateColumn(name, value, ts)
                   : ((mask & DELETION_MASK) == 0
                      ? new Column(name, value, ts)
                      : new DeletedColumn(name, value, ts));
        }
    }

    public void serialize(RangeSliceCommand sliceCommand, DataOutput dos, int version) throws IOException
    {
        dos.writeUTF(sliceCommand.keyspace);
        dos.writeUTF(sliceCommand.column_family);
        ByteBuffer sc = sliceCommand.super_column;
        dos.writeInt(sc == null ? 0 : sc.remaining());
        if (sc != null)
            ByteBufferUtil.write(sc, dos);

        if (version < MessagingService.VERSION_12)
        {
            FBUtilities.serialize(new TSerializer(new TBinaryProtocol.Factory()), asSlicePredicate(sliceCommand.predicate), dos);
        }
        else
        {
            IFilter.Serializer.instance.serialize(sliceCommand.predicate, dos, version);
        }

        if (version >= MessagingService.VERSION_11)
        {
            if (sliceCommand.row_filter == null)
            {
                dos.writeInt(0);
            }
            else
            {
                dos.writeInt(sliceCommand.row_filter.size());
                for (IndexExpression expr : sliceCommand.row_filter)
                {
                    if (version < MessagingService.VERSION_12)
                    {
                        FBUtilities.serialize(new TSerializer(new TBinaryProtocol.Factory()), expr, dos);
                    }
                    else
                    {
                        ByteBufferUtil.writeWithShortLength(expr.column_name, dos);
                        dos.writeInt(expr.op.getValue());
                        ByteBufferUtil.writeWithShortLength(expr.value, dos);
                    }
                }
            }
        }
        AbstractBounds.serializer.serialize(sliceCommand.range, dos, version);
        dos.writeInt(sliceCommand.maxResults);
        if (version >= MessagingService.VERSION_11)
        {
            dos.writeBoolean(sliceCommand.maxIsColumns);
            dos.writeBoolean(sliceCommand.isPaging);
        }
    }

+++++++++++++++++++++++
    public void serialize(RangeSliceCommand sliceCommand, DataOutput dos, int version) throws IOException
    {
        dos.writeUTF(sliceCommand.keyspace);
        dos.writeUTF(sliceCommand.column_family);
        ByteBuffer sc = sliceCommand.super_column;
        dos.writeInt(sc == null ? 0 : sc.remaining());
        if (sc != null)
            ByteBufferUtil.write(sc, dos);

        if (version < MessagingService.VERSION_12)
        {
            FBUtilities.serialize(new TSerializer(new TBinaryProtocol.Factory()), asSlicePredicate(sliceCommand.predicate), dos);
        }
        else
        {
            IDiskAtomFilter.Serializer.instance.serialize(sliceCommand.predicate, dos, version);
        }

        if (version >= MessagingService.VERSION_11)
        {
            if (sliceCommand.row_filter == null)
            {
                dos.writeInt(0);
            }
            else
            {
                dos.writeInt(sliceCommand.row_filter.size());
                for (IndexExpression expr : sliceCommand.row_filter)
                {
                    if (version < MessagingService.VERSION_12)
                    {
                        FBUtilities.serialize(new TSerializer(new TBinaryProtocol.Factory()), expr, dos);
                    }
                    else
                    {
                        ByteBufferUtil.writeWithShortLength(expr.column_name, dos);
                        dos.writeInt(expr.op.getValue());
                        ByteBufferUtil.writeWithShortLength(expr.value, dos);
                    }
                }
            }
        }
        AbstractBounds.serializer.serialize(sliceCommand.range, dos, version);
        dos.writeInt(sliceCommand.maxResults);
        if (version >= MessagingService.VERSION_11)
        {
            dos.writeBoolean(sliceCommand.maxIsColumns);
            dos.writeBoolean(sliceCommand.isPaging);
        }
    }

    public long serializedSize(RangeSliceCommand rsc, int version)
    {
        long size = TypeSizes.NATIVE.sizeof(rsc.keyspace);
        size += TypeSizes.NATIVE.sizeof(rsc.column_family);

        ByteBuffer sc = rsc.super_column;
        if (sc != null)
        {
            size += TypeSizes.NATIVE.sizeof(sc.remaining());
            size += sc.remaining();
        }
        else
        {
            size += TypeSizes.NATIVE.sizeof(0);
        }

        if (version < MessagingService.VERSION_12)
        {
            TSerializer ser = new TSerializer(new TBinaryProtocol.Factory());
            try
            {
                int predicateLength = ser.serialize(asSlicePredicate(rsc.predicate)).length;
                if (version < MessagingService.VERSION_12)
                    size += TypeSizes.NATIVE.sizeof(predicateLength);
                size += predicateLength;
            }
            catch (TException e)
            {
                throw new RuntimeException(e);
            }
        }
        else
        {
            size += IFilter.Serializer.instance.serializedSize(rsc.predicate, version);
        }

        if (version >= MessagingService.VERSION_11)
        {
            if (rsc.row_filter == null)
            {
                size += TypeSizes.NATIVE.sizeof(0);
            }
            else
            {
                size += TypeSizes.NATIVE.sizeof(rsc.row_filter.size());
                for (IndexExpression expr : rsc.row_filter)
                {
                    if (version < MessagingService.VERSION_12)
                    {
                        try
                        {
                            int filterLength = new TSerializer(new TBinaryProtocol.Factory()).serialize(expr).length;
                            size += TypeSizes.NATIVE.sizeof(filterLength);
                            size += filterLength;
                        }
                        catch (TException e)
                        {
                            throw new RuntimeException(e);
                        }
                    }
                    else
                    {
                        size += TypeSizes.NATIVE.sizeofWithShortLength(expr.column_name);
                        size += TypeSizes.NATIVE.sizeof(expr.op.getValue());
                        size += TypeSizes.NATIVE.sizeofWithLength(expr.value);
                    }
                }
            }
        }
        size += AbstractBounds.serializer.serializedSize(rsc.range, version);
        size += TypeSizes.NATIVE.sizeof(rsc.maxResults);
        if (version >= MessagingService.VERSION_11)
        {
            size += TypeSizes.NATIVE.sizeof(rsc.maxIsColumns);
            size += TypeSizes.NATIVE.sizeof(rsc.isPaging);
        }
        return size;
+++++++++++++++++++++++
    public long serializedSize(RangeSliceCommand rsc, int version)
    {
        long size = TypeSizes.NATIVE.sizeof(rsc.keyspace);
        size += TypeSizes.NATIVE.sizeof(rsc.column_family);

        ByteBuffer sc = rsc.super_column;
        if (sc != null)
        {
            size += TypeSizes.NATIVE.sizeof(sc.remaining());
            size += sc.remaining();
        }
        else
        {
            size += TypeSizes.NATIVE.sizeof(0);
        }

        if (version < MessagingService.VERSION_12)
        {
            TSerializer ser = new TSerializer(new TBinaryProtocol.Factory());
            try
            {
                int predicateLength = ser.serialize(asSlicePredicate(rsc.predicate)).length;
                if (version < MessagingService.VERSION_12)
                    size += TypeSizes.NATIVE.sizeof(predicateLength);
                size += predicateLength;
            }
            catch (TException e)
            {
                throw new RuntimeException(e);
            }
        }
        else
        {
            size += IDiskAtomFilter.Serializer.instance.serializedSize(rsc.predicate, version);
        }

        if (version >= MessagingService.VERSION_11)
        {
            if (rsc.row_filter == null)
            {
                size += TypeSizes.NATIVE.sizeof(0);
            }
            else
            {
                size += TypeSizes.NATIVE.sizeof(rsc.row_filter.size());
                for (IndexExpression expr : rsc.row_filter)
                {
                    if (version < MessagingService.VERSION_12)
                    {
                        try
                        {
                            int filterLength = new TSerializer(new TBinaryProtocol.Factory()).serialize(expr).length;
                            size += TypeSizes.NATIVE.sizeof(filterLength);
                            size += filterLength;
                        }
                        catch (TException e)
                        {
                            throw new RuntimeException(e);
                        }
                    }
                    else
                    {
                        size += TypeSizes.NATIVE.sizeofWithShortLength(expr.column_name);
                        size += TypeSizes.NATIVE.sizeof(expr.op.getValue());
                        size += TypeSizes.NATIVE.sizeofWithLength(expr.value);
                    }
                }
            }
        }
        size += AbstractBounds.serializer.serializedSize(rsc.range, version);
        size += TypeSizes.NATIVE.sizeof(rsc.maxResults);
        if (version >= MessagingService.VERSION_11)
        {
            size += TypeSizes.NATIVE.sizeof(rsc.maxIsColumns);
            size += TypeSizes.NATIVE.sizeof(rsc.isPaging);
        }
        return size;
    public RangeSliceCommand deserialize(DataInput dis, int version) throws IOException
    {
        String keyspace = dis.readUTF();
        String columnFamily = dis.readUTF();

        int scLength = dis.readInt();
        ByteBuffer superColumn = null;
        if (scLength > 0)
        {
            byte[] buf = new byte[scLength];
            dis.readFully(buf);
            superColumn = ByteBuffer.wrap(buf);
        }

        IFilter predicate;
        AbstractType<?> comparator = ColumnFamily.getComparatorFor(keyspace, columnFamily, superColumn);
        if (version < MessagingService.VERSION_12)
        {
            SlicePredicate pred = new SlicePredicate();
            FBUtilities.deserialize(new TDeserializer(new TBinaryProtocol.Factory()), pred, dis);
            predicate = ThriftValidation.asIFilter(pred, comparator);
        }
        else
        {
            predicate = IFilter.Serializer.instance.deserialize(dis, version, comparator);
        }

        List<IndexExpression> rowFilter = null;
        if (version >= MessagingService.VERSION_11)
        {
            int filterCount = dis.readInt();
            rowFilter = new ArrayList<IndexExpression>(filterCount);
            for (int i = 0; i < filterCount; i++)
            {
                IndexExpression expr;
                if (version < MessagingService.VERSION_12)
                {
                    expr = new IndexExpression();
                    FBUtilities.deserialize(new TDeserializer(new TBinaryProtocol.Factory()), expr, dis);
                }
                else
                {
                    expr = new IndexExpression(ByteBufferUtil.readWithShortLength(dis),
                                               IndexOperator.findByValue(dis.readInt()),
                                               ByteBufferUtil.readWithShortLength(dis));
                }
                rowFilter.add(expr);
            }
        }
        AbstractBounds<RowPosition> range = AbstractBounds.serializer.deserialize(dis, version).toRowBounds();

        int maxResults = dis.readInt();
        boolean maxIsColumns = false;
        boolean isPaging = false;
        if (version >= MessagingService.VERSION_11)
        {
            maxIsColumns = dis.readBoolean();
            isPaging = dis.readBoolean();
        }
        return new RangeSliceCommand(keyspace, columnFamily, superColumn, predicate, range, rowFilter, maxResults, maxIsColumns, isPaging);
    }

+++++++++++++++++++++++
    public RangeSliceCommand deserialize(DataInput dis, int version) throws IOException
    {
        String keyspace = dis.readUTF();
        String columnFamily = dis.readUTF();

        int scLength = dis.readInt();
        ByteBuffer superColumn = null;
        if (scLength > 0)
        {
            byte[] buf = new byte[scLength];
            dis.readFully(buf);
            superColumn = ByteBuffer.wrap(buf);
        }

        IDiskAtomFilter predicate;
        AbstractType<?> comparator = ColumnFamily.getComparatorFor(keyspace, columnFamily, superColumn);
        if (version < MessagingService.VERSION_12)
        {
            SlicePredicate pred = new SlicePredicate();
            FBUtilities.deserialize(new TDeserializer(new TBinaryProtocol.Factory()), pred, dis);
            predicate = ThriftValidation.asIFilter(pred, comparator);
        }
        else
        {
            predicate = IDiskAtomFilter.Serializer.instance.deserialize(dis, version, comparator);
        }

        List<IndexExpression> rowFilter = null;
        if (version >= MessagingService.VERSION_11)
        {
            int filterCount = dis.readInt();
            rowFilter = new ArrayList<IndexExpression>(filterCount);
            for (int i = 0; i < filterCount; i++)
            {
                IndexExpression expr;
                if (version < MessagingService.VERSION_12)
                {
                    expr = new IndexExpression();
                    FBUtilities.deserialize(new TDeserializer(new TBinaryProtocol.Factory()), expr, dis);
                }
                else
                {
                    expr = new IndexExpression(ByteBufferUtil.readWithShortLength(dis),
                                               IndexOperator.findByValue(dis.readInt()),
                                               ByteBufferUtil.readWithShortLength(dis));
                }
                rowFilter.add(expr);
            }
        }
        AbstractBounds<RowPosition> range = AbstractBounds.serializer.deserialize(dis, version).toRowBounds();

        int maxResults = dis.readInt();
        boolean maxIsColumns = false;
        boolean isPaging = false;
        if (version >= MessagingService.VERSION_11)
        {
            maxIsColumns = dis.readBoolean();
            isPaging = dis.readBoolean();
        }
        return new RangeSliceCommand(keyspace, columnFamily, superColumn, predicate, range, rowFilter, maxResults, maxIsColumns, isPaging);
    }

    private static void serializeSchema(Map<DecoratedKey, RowMutation> mutationMap, String schemaCfName)
    {
        for (Row schemaRow : serializedSchema(schemaCfName))
        {
            RowMutation mutation = mutationMap.get(schemaRow.key);

            if (mutation == null)
            {
                mutationMap.put(schemaRow.key, new RowMutation(Table.SYSTEM_KS, schemaRow));
                continue;
            }

            mutation.add(schemaRow.cf);
        }
    }

+++++++++++++++++++++++
    private static void serializeSchema(Map<DecoratedKey, RowMutation> mutationMap, String schemaCfName)
    {
        for (Row schemaRow : serializedSchema(schemaCfName))
        {
            if (Schema.ignoredSchemaRow(schemaRow))
                continue;

            RowMutation mutation = mutationMap.get(schemaRow.key);

            if (mutation == null)
            {
                mutationMap.put(schemaRow.key, new RowMutation(Table.SYSTEM_KS, schemaRow));
                continue;
            }

            mutation.add(schemaRow.cf);
        }
    }

    private static Collection<Token> deserializeTokens(ByteBuffer tokenBytes)
    {
        List<Token> tokens = new ArrayList<Token>();
        IPartitioner p = StorageService.getPartitioner();

        while(tokenBytes.hasRemaining())
        {
            short len = tokenBytes.getShort();
            ByteBuffer dup = tokenBytes.slice();
            dup.limit(len);
            tokenBytes.position(tokenBytes.position() + len);
            tokens.add(p.getTokenFactory().fromByteArray(dup));
        }

        return tokens;
    }

+++++++++++++++++++++++
    private static Collection<Token> deserializeTokens(Collection<String> tokensStrings)
    {
        Token.TokenFactory factory = StorageService.getPartitioner().getTokenFactory();
        List<Token> tokens = new ArrayList<Token>(tokensStrings.size());
        for (String tk : tokensStrings)
            tokens.add(factory.fromString(tk));
        return tokens;
    }

    /**
     * Remove stored tokens being used by another node
     */
    public void serialize(GossipDigestAck gDigestAckMessage, DataOutput dos, int version) throws IOException
    {
        GossipDigestSerializationHelper.serialize(gDigestAckMessage.gDigestList, dos, version);
        if (version <= MessagingService.VERSION_10)
            dos.writeBoolean(true); // 0.6 compatibility
        dos.writeInt(gDigestAckMessage.epStateMap.size());
        for (Map.Entry<InetAddress, EndpointState> entry : gDigestAckMessage.epStateMap.entrySet())
        {
            InetAddress ep = entry.getKey();
            CompactEndpointSerializationHelper.serialize(ep, dos);
            EndpointState.serializer.serialize(entry.getValue(), dos, version);
        }
    }

+++++++++++++++++++++++
    public void serialize(GossipDigestAck gDigestAckMessage, DataOutput dos, int version) throws IOException
    {
        GossipDigestSerializationHelper.serialize(gDigestAckMessage.gDigestList, dos, version);
        if (version <= MessagingService.VERSION_11)
            dos.writeBoolean(true); // 0.6 compatibility
        dos.writeInt(gDigestAckMessage.epStateMap.size());
        for (Map.Entry<InetAddress, EndpointState> entry : gDigestAckMessage.epStateMap.entrySet())
        {
            InetAddress ep = entry.getKey();
            CompactEndpointSerializationHelper.serialize(ep, dos);
            EndpointState.serializer.serialize(entry.getValue(), dos, version);
        }
    }

    public GossipDigestAck deserialize(DataInput dis, int version) throws IOException
    {
        List<GossipDigest> gDigestList = GossipDigestSerializationHelper.deserialize(dis, version);
        if (version <= MessagingService.VERSION_10)
            dis.readBoolean(); // 0.6 compatibility
        int size = dis.readInt();
        Map<InetAddress, EndpointState> epStateMap = new HashMap<InetAddress, EndpointState>(size);

        for (int i = 0; i < size; ++i)
        {
            InetAddress ep = CompactEndpointSerializationHelper.deserialize(dis);
            EndpointState epState = EndpointState.serializer.deserialize(dis, version);
            epStateMap.put(ep, epState);
        }
        return new GossipDigestAck(gDigestList, epStateMap);
    }

+++++++++++++++++++++++
    public GossipDigestAck deserialize(DataInput dis, int version) throws IOException
    {
        List<GossipDigest> gDigestList = GossipDigestSerializationHelper.deserialize(dis, version);
        if (version <= MessagingService.VERSION_11)
            dis.readBoolean(); // 0.6 compatibility
        int size = dis.readInt();
        Map<InetAddress, EndpointState> epStateMap = new HashMap<InetAddress, EndpointState>(size);

        for (int i = 0; i < size; ++i)
        {
            InetAddress ep = CompactEndpointSerializationHelper.deserialize(dis);
            EndpointState epState = EndpointState.serializer.deserialize(dis, version);
            epStateMap.put(ep, epState);
        }
        return new GossipDigestAck(gDigestList, epStateMap);
    }

    public static Event deserialize(ChannelBuffer cb)
    {
        switch (Enum.valueOf(Type.class, CBUtil.readString(cb).toUpperCase()))
        {
            case TOPOLOGY_CHANGE:
                return TopologyChange.deserializeEvent(cb);
            case STATUS_CHANGE:
                return StatusChange.deserializeEvent(cb);
        }
        throw new AssertionError();
    }

+++++++++++++++++++++++
    public static Event deserialize(ChannelBuffer cb)
    {
        switch (Enum.valueOf(Type.class, CBUtil.readString(cb).toUpperCase()))
        {
            case TOPOLOGY_CHANGE:
                return TopologyChange.deserializeEvent(cb);
            case STATUS_CHANGE:
                return StatusChange.deserializeEvent(cb);
            case SCHEMA_CHANGE:
                return SchemaChange.deserializeEvent(cb);
        }
        throw new AssertionError();
    }

    public void serialize(BloomFilter bf, DataOutput dos) throws IOException
    {
        int bitLength = bf.bitset.getNumWords();
        int pageSize = bf.bitset.getPageSize();
        int pageCount = bf.bitset.getPageCount();

        dos.writeInt(bf.getHashCount());
        dos.writeInt(bitLength);

        for (int p = 0; p < pageCount; p++)
        {
            long[] bits = bf.bitset.getPage(p);
            for (int i = 0; i < pageSize && bitLength-- > 0; i++)
                dos.writeLong(bits[i]);
        }
    }

+++++++++++++++++++++++
    public void serialize(BloomFilter bf, DataOutput dos) throws IOException
    {
        dos.writeInt(bf.hashCount);
        bf.bitset.serialize(dos);
    }

    public long serializedSize(BloomFilter bf, TypeSizes typeSizes)
    {
        int bitLength = bf.bitset.getNumWords();
        int pageSize = bf.bitset.getPageSize();
        int pageCount = bf.bitset.getPageCount();

        int size = 0;
        size += typeSizes.sizeof(bf.getHashCount()); // hash count
        size += typeSizes.sizeof(bitLength); // length

        for (int p = 0; p < pageCount; p++)
        {
            long[] bits = bf.bitset.getPage(p);
            for (int i = 0; i < pageSize && bitLength-- > 0; i++)
                size += typeSizes.sizeof(bits[i]); // bucket
        }
        return size;
+++++++++++++++++++++++
    public long serializedSize(BloomFilter bf, TypeSizes typeSizes)
    {
        int size = typeSizes.sizeof(bf.hashCount); // hash count
        size += bf.bitset.serializedSize(typeSizes);
        return size;
    public BloomFilter deserialize(DataInput dis) throws IOException
    {
        int hashes = dis.readInt();
        long bitLength = dis.readInt();
        OpenBitSet bs = new OpenBitSet(bitLength << 6);
        int pageSize = bs.getPageSize();
        int pageCount = bs.getPageCount();

        for (int p = 0; p < pageCount; p++)
        {
            long[] bits = bs.getPage(p);
            for (int i = 0; i < pageSize && bitLength-- > 0; i++)
                bits[i] = dis.readLong();
        }

        return createFilter(hashes, bs);
    }

+++++++++++++++++++++++
    public BloomFilter deserialize(DataInput dis, boolean offheap) throws IOException
    {
        int hashes = dis.readInt();
        IBitSet bs = offheap ? OffHeapBitSet.deserialize(dis) : OpenBitSet.deserialize(dis);
        return createFilter(hashes, bs);
    }

    public static void serialize(Filter bf, DataOutput output, Type type) throws IOException
    {
        switch (type)
        {
            case SHA:
                LegacyBloomFilter.serializer.serialize((LegacyBloomFilter) bf, output);
                break;
            case MURMUR2:
                Murmur2BloomFilter.serializer.serialize((Murmur2BloomFilter) bf, output);
                break;
            default:
                Murmur3BloomFilter.serializer.serialize((Murmur3BloomFilter) bf, output);
                break;
        }
    }

+++++++++++++++++++++++
    public static void serialize(IFilter bf, DataOutput output, Type type) throws IOException
    {
        switch (type)
        {
            case SHA:
                LegacyBloomFilter.serializer.serialize((LegacyBloomFilter) bf, output);
                break;
            case MURMUR2:
                Murmur2BloomFilter.serializer.serialize((Murmur2BloomFilter) bf, output);
                break;
            default:
                Murmur3BloomFilter.serializer.serialize((Murmur3BloomFilter) bf, output);
                break;
        }
    }

    public static long serializedSize(Filter bf, Type type)
    {
        switch (type)
        {
            case SHA:
                return LegacyBloomFilter.serializer.serializedSize((LegacyBloomFilter) bf);
            case MURMUR2:
                return Murmur2BloomFilter.serializer.serializedSize((Murmur2BloomFilter) bf, TYPE_SIZES);
            default:
                return Murmur3BloomFilter.serializer.serializedSize((Murmur3BloomFilter) bf, TYPE_SIZES);
        }
    }

    /**
     * @return A BloomFilter with the lowest practical false positive
     *         probability for the given number of elements.
     */
+++++++++++++++++++++++
    public static long serializedSize(IFilter bf, Type type)
    {
        switch (type)
        {
            case SHA:
                return LegacyBloomFilter.serializer.serializedSize((LegacyBloomFilter) bf);
            case MURMUR2:
                return Murmur2BloomFilter.serializer.serializedSize((Murmur2BloomFilter) bf, TYPE_SIZES);
            default:
                return Murmur3BloomFilter.serializer.serializedSize((Murmur3BloomFilter) bf, TYPE_SIZES);
        }
    }

    /**
     * @return A BloomFilter with the lowest practical false positive
     *         probability for the given number of elements.
     */
    public static Filter deserialize(DataInput input, Type type) throws IOException
    {
        switch (type)
        {
            case SHA:
                return LegacyBloomFilter.serializer.deserialize(input);
            case MURMUR2:
                return Murmur2BloomFilter.serializer.deserialize(input);
            default:
                return Murmur3BloomFilter.serializer.deserialize(input);
        }
    }

+++++++++++++++++++++++
    public static IFilter deserialize(DataInput input, Type type, boolean offheap) throws IOException
    {
        switch (type)
        {
            case SHA:
                return LegacyBloomFilter.serializer.deserialize(input);
            case MURMUR2:
                return Murmur2BloomFilter.serializer.deserialize(input, offheap);
            default:
                return Murmur3BloomFilter.serializer.deserialize(input, offheap);
        }
    }

add: 36, delete: 6, change: 281, unhandled: 0 size_exceptions: 12 size_serialize: 17
-----------cassandra-1.2.0-beta2 vs cassandra-1.2.0-beta3-----------
    public void serialize(RangeSliceCommand sliceCommand, DataOutput dos, int version) throws IOException
    {
        dos.writeUTF(sliceCommand.keyspace);
        dos.writeUTF(sliceCommand.column_family);
        ByteBuffer sc = sliceCommand.super_column;
        dos.writeInt(sc == null ? 0 : sc.remaining());
        if (sc != null)
            ByteBufferUtil.write(sc, dos);

        if (version < MessagingService.VERSION_12)
        {
            FBUtilities.serialize(new TSerializer(new TBinaryProtocol.Factory()), asSlicePredicate(sliceCommand.predicate), dos);
        }
        else
        {
            IDiskAtomFilter.Serializer.instance.serialize(sliceCommand.predicate, dos, version);
        }

        if (version >= MessagingService.VERSION_11)
        {
            if (sliceCommand.row_filter == null)
            {
                dos.writeInt(0);
            }
            else
            {
                dos.writeInt(sliceCommand.row_filter.size());
                for (IndexExpression expr : sliceCommand.row_filter)
                {
                    if (version < MessagingService.VERSION_12)
                    {
                        FBUtilities.serialize(new TSerializer(new TBinaryProtocol.Factory()), expr, dos);
                    }
                    else
                    {
                        ByteBufferUtil.writeWithShortLength(expr.column_name, dos);
                        dos.writeInt(expr.op.getValue());
                        ByteBufferUtil.writeWithShortLength(expr.value, dos);
                    }
                }
            }
        }
        AbstractBounds.serializer.serialize(sliceCommand.range, dos, version);
        dos.writeInt(sliceCommand.maxResults);
        if (version >= MessagingService.VERSION_11)
        {
            dos.writeBoolean(sliceCommand.maxIsColumns);
            dos.writeBoolean(sliceCommand.isPaging);
        }
    }

+++++++++++++++++++++++
    public void serialize(RangeSliceCommand sliceCommand, DataOutput dos, int version) throws IOException
    {
        dos.writeUTF(sliceCommand.keyspace);
        dos.writeUTF(sliceCommand.column_family);
        ByteBuffer sc = sliceCommand.super_column;
        dos.writeInt(sc == null ? 0 : sc.remaining());
        if (sc != null)
            ByteBufferUtil.write(sc, dos);

        if (version < MessagingService.VERSION_12)
        {
            FBUtilities.serialize(new TSerializer(new TBinaryProtocol.Factory()), asSlicePredicate(sliceCommand.predicate), dos);
        }
        else
        {
            IDiskAtomFilter.Serializer.instance.serialize(sliceCommand.predicate, dos, version);
        }

        if (version >= MessagingService.VERSION_11)
        {
            if (sliceCommand.row_filter == null)
            {
                dos.writeInt(0);
            }
            else
            {
                dos.writeInt(sliceCommand.row_filter.size());
                for (IndexExpression expr : sliceCommand.row_filter)
                {
                    if (version < MessagingService.VERSION_12)
                    {
                        FBUtilities.serialize(new TSerializer(new TBinaryProtocol.Factory()), expr, dos);
                    }
                    else
                    {
                        ByteBufferUtil.writeWithShortLength(expr.column_name, dos);
                        dos.writeInt(expr.op.getValue());
                        ByteBufferUtil.writeWithShortLength(expr.value, dos);
                    }
                }
            }
        }
        AbstractBounds.serializer.serialize(sliceCommand.range, dos, version);
        dos.writeInt(sliceCommand.maxResults);
        if (version >= MessagingService.VERSION_11)
        {
            dos.writeBoolean(sliceCommand.countCQL3Rows);
            dos.writeBoolean(sliceCommand.isPaging);
        }
    }

    public long serializedSize(RangeSliceCommand rsc, int version)
    {
        long size = TypeSizes.NATIVE.sizeof(rsc.keyspace);
        size += TypeSizes.NATIVE.sizeof(rsc.column_family);

        ByteBuffer sc = rsc.super_column;
        if (sc != null)
        {
            size += TypeSizes.NATIVE.sizeof(sc.remaining());
            size += sc.remaining();
        }
        else
        {
            size += TypeSizes.NATIVE.sizeof(0);
        }

        if (version < MessagingService.VERSION_12)
        {
            TSerializer ser = new TSerializer(new TBinaryProtocol.Factory());
            try
            {
                int predicateLength = ser.serialize(asSlicePredicate(rsc.predicate)).length;
                if (version < MessagingService.VERSION_12)
                    size += TypeSizes.NATIVE.sizeof(predicateLength);
                size += predicateLength;
            }
            catch (TException e)
            {
                throw new RuntimeException(e);
            }
        }
        else
        {
            size += IDiskAtomFilter.Serializer.instance.serializedSize(rsc.predicate, version);
        }

        if (version >= MessagingService.VERSION_11)
        {
            if (rsc.row_filter == null)
            {
                size += TypeSizes.NATIVE.sizeof(0);
            }
            else
            {
                size += TypeSizes.NATIVE.sizeof(rsc.row_filter.size());
                for (IndexExpression expr : rsc.row_filter)
                {
                    if (version < MessagingService.VERSION_12)
                    {
                        try
                        {
                            int filterLength = new TSerializer(new TBinaryProtocol.Factory()).serialize(expr).length;
                            size += TypeSizes.NATIVE.sizeof(filterLength);
                            size += filterLength;
                        }
                        catch (TException e)
                        {
                            throw new RuntimeException(e);
                        }
                    }
                    else
                    {
                        size += TypeSizes.NATIVE.sizeofWithShortLength(expr.column_name);
                        size += TypeSizes.NATIVE.sizeof(expr.op.getValue());
                        size += TypeSizes.NATIVE.sizeofWithLength(expr.value);
                    }
                }
            }
        }
        size += AbstractBounds.serializer.serializedSize(rsc.range, version);
        size += TypeSizes.NATIVE.sizeof(rsc.maxResults);
        if (version >= MessagingService.VERSION_11)
        {
            size += TypeSizes.NATIVE.sizeof(rsc.maxIsColumns);
            size += TypeSizes.NATIVE.sizeof(rsc.isPaging);
        }
        return size;
+++++++++++++++++++++++
    public long serializedSize(RangeSliceCommand rsc, int version)
    {
        long size = TypeSizes.NATIVE.sizeof(rsc.keyspace);
        size += TypeSizes.NATIVE.sizeof(rsc.column_family);

        ByteBuffer sc = rsc.super_column;
        if (sc != null)
        {
            size += TypeSizes.NATIVE.sizeof(sc.remaining());
            size += sc.remaining();
        }
        else
        {
            size += TypeSizes.NATIVE.sizeof(0);
        }

        if (version < MessagingService.VERSION_12)
        {
            TSerializer ser = new TSerializer(new TBinaryProtocol.Factory());
            try
            {
                int predicateLength = ser.serialize(asSlicePredicate(rsc.predicate)).length;
                if (version < MessagingService.VERSION_12)
                    size += TypeSizes.NATIVE.sizeof(predicateLength);
                size += predicateLength;
            }
            catch (TException e)
            {
                throw new RuntimeException(e);
            }
        }
        else
        {
            size += IDiskAtomFilter.Serializer.instance.serializedSize(rsc.predicate, version);
        }

        if (version >= MessagingService.VERSION_11)
        {
            if (rsc.row_filter == null)
            {
                size += TypeSizes.NATIVE.sizeof(0);
            }
            else
            {
                size += TypeSizes.NATIVE.sizeof(rsc.row_filter.size());
                for (IndexExpression expr : rsc.row_filter)
                {
                    if (version < MessagingService.VERSION_12)
                    {
                        try
                        {
                            int filterLength = new TSerializer(new TBinaryProtocol.Factory()).serialize(expr).length;
                            size += TypeSizes.NATIVE.sizeof(filterLength);
                            size += filterLength;
                        }
                        catch (TException e)
                        {
                            throw new RuntimeException(e);
                        }
                    }
                    else
                    {
                        size += TypeSizes.NATIVE.sizeofWithShortLength(expr.column_name);
                        size += TypeSizes.NATIVE.sizeof(expr.op.getValue());
                        size += TypeSizes.NATIVE.sizeofWithLength(expr.value);
                    }
                }
            }
        }
        size += AbstractBounds.serializer.serializedSize(rsc.range, version);
        size += TypeSizes.NATIVE.sizeof(rsc.maxResults);
        if (version >= MessagingService.VERSION_11)
        {
            size += TypeSizes.NATIVE.sizeof(rsc.countCQL3Rows);
            size += TypeSizes.NATIVE.sizeof(rsc.isPaging);
        }
        return size;
    public RangeSliceCommand deserialize(DataInput dis, int version) throws IOException
    {
        String keyspace = dis.readUTF();
        String columnFamily = dis.readUTF();

        int scLength = dis.readInt();
        ByteBuffer superColumn = null;
        if (scLength > 0)
        {
            byte[] buf = new byte[scLength];
            dis.readFully(buf);
            superColumn = ByteBuffer.wrap(buf);
        }

        IDiskAtomFilter predicate;
        AbstractType<?> comparator = ColumnFamily.getComparatorFor(keyspace, columnFamily, superColumn);
        if (version < MessagingService.VERSION_12)
        {
            SlicePredicate pred = new SlicePredicate();
            FBUtilities.deserialize(new TDeserializer(new TBinaryProtocol.Factory()), pred, dis);
            predicate = ThriftValidation.asIFilter(pred, comparator);
        }
        else
        {
            predicate = IDiskAtomFilter.Serializer.instance.deserialize(dis, version, comparator);
        }

        List<IndexExpression> rowFilter = null;
        if (version >= MessagingService.VERSION_11)
        {
            int filterCount = dis.readInt();
            rowFilter = new ArrayList<IndexExpression>(filterCount);
            for (int i = 0; i < filterCount; i++)
            {
                IndexExpression expr;
                if (version < MessagingService.VERSION_12)
                {
                    expr = new IndexExpression();
                    FBUtilities.deserialize(new TDeserializer(new TBinaryProtocol.Factory()), expr, dis);
                }
                else
                {
                    expr = new IndexExpression(ByteBufferUtil.readWithShortLength(dis),
                                               IndexOperator.findByValue(dis.readInt()),
                                               ByteBufferUtil.readWithShortLength(dis));
                }
                rowFilter.add(expr);
            }
        }
        AbstractBounds<RowPosition> range = AbstractBounds.serializer.deserialize(dis, version).toRowBounds();

        int maxResults = dis.readInt();
        boolean maxIsColumns = false;
        boolean isPaging = false;
        if (version >= MessagingService.VERSION_11)
        {
            maxIsColumns = dis.readBoolean();
            isPaging = dis.readBoolean();
        }
        return new RangeSliceCommand(keyspace, columnFamily, superColumn, predicate, range, rowFilter, maxResults, maxIsColumns, isPaging);
    }

+++++++++++++++++++++++
    public RangeSliceCommand deserialize(DataInput dis, int version) throws IOException
    {
        String keyspace = dis.readUTF();
        String columnFamily = dis.readUTF();

        int scLength = dis.readInt();
        ByteBuffer superColumn = null;
        if (scLength > 0)
        {
            byte[] buf = new byte[scLength];
            dis.readFully(buf);
            superColumn = ByteBuffer.wrap(buf);
        }

        IDiskAtomFilter predicate;
        AbstractType<?> comparator = ColumnFamily.getComparatorFor(keyspace, columnFamily, superColumn);
        if (version < MessagingService.VERSION_12)
        {
            SlicePredicate pred = new SlicePredicate();
            FBUtilities.deserialize(new TDeserializer(new TBinaryProtocol.Factory()), pred, dis);
            predicate = ThriftValidation.asIFilter(pred, comparator);
        }
        else
        {
            predicate = IDiskAtomFilter.Serializer.instance.deserialize(dis, version, comparator);
        }

        List<IndexExpression> rowFilter = null;
        if (version >= MessagingService.VERSION_11)
        {
            int filterCount = dis.readInt();
            rowFilter = new ArrayList<IndexExpression>(filterCount);
            for (int i = 0; i < filterCount; i++)
            {
                IndexExpression expr;
                if (version < MessagingService.VERSION_12)
                {
                    expr = new IndexExpression();
                    FBUtilities.deserialize(new TDeserializer(new TBinaryProtocol.Factory()), expr, dis);
                }
                else
                {
                    expr = new IndexExpression(ByteBufferUtil.readWithShortLength(dis),
                                               IndexOperator.findByValue(dis.readInt()),
                                               ByteBufferUtil.readWithShortLength(dis));
                }
                rowFilter.add(expr);
            }
        }
        AbstractBounds<RowPosition> range = AbstractBounds.serializer.deserialize(dis, version).toRowBounds();

        int maxResults = dis.readInt();
        boolean countCQL3Rows = false;
        boolean isPaging = false;
        if (version >= MessagingService.VERSION_11)
        {
            countCQL3Rows = dis.readBoolean();
            isPaging = dis.readBoolean();
        }
        return new RangeSliceCommand(keyspace, columnFamily, superColumn, predicate, range, rowFilter, maxResults, countCQL3Rows, isPaging);
    }

    public ChannelBuffer serialize()
    {
        return ChannelBuffers.wrappedBuffer(CBUtil.stringToCB(type.toString()),
                                            serializeEvent());
    }

+++++++++++++++++++++++
    public ChannelBuffer serialize()
    {
        return ChannelBuffers.wrappedBuffer(CBUtil.enumValueToCB(type), serializeEvent());
    }

    public static Event deserialize(ChannelBuffer cb)
    {
        switch (Enum.valueOf(Type.class, CBUtil.readString(cb).toUpperCase()))
        {
            case TOPOLOGY_CHANGE:
                return TopologyChange.deserializeEvent(cb);
            case STATUS_CHANGE:
                return StatusChange.deserializeEvent(cb);
            case SCHEMA_CHANGE:
                return SchemaChange.deserializeEvent(cb);
        }
        throw new AssertionError();
    }

+++++++++++++++++++++++
    public static Event deserialize(ChannelBuffer cb)
    {
        switch (CBUtil.readEnumValue(Type.class, cb))
        {
            case TOPOLOGY_CHANGE:
                return TopologyChange.deserializeEvent(cb);
            case STATUS_CHANGE:
                return StatusChange.deserializeEvent(cb);
            case SCHEMA_CHANGE:
                return SchemaChange.deserializeEvent(cb);
        }
        throw new AssertionError();
    }

add: 11, delete: 9, change: 158, unhandled: 0 size_exceptions: 9 size_serialize: 5
-----------cassandra-1.2.0-beta3 vs cassandra-1.2.0-rc1-----------
add: 9, delete: 0, change: 83, unhandled: 0 size_exceptions: 3 size_serialize: 0
-----------cassandra-1.2.0-rc1 vs cassandra-1.2.0-rc2-----------
add: 0, delete: 0, change: 25, unhandled: 0 size_exceptions: 0 size_serialize: 0
-----------cassandra-1.2.0-rc2 vs cassandra-1.2.1-----------
    public void serialize(GossipDigestAck gDigestAckMessage, DataOutput dos, int version) throws IOException
    {
        GossipDigestSerializationHelper.serialize(gDigestAckMessage.gDigestList, dos, version);
        if (version <= MessagingService.VERSION_11)
            dos.writeBoolean(true); // 0.6 compatibility
        dos.writeInt(gDigestAckMessage.epStateMap.size());
        for (Map.Entry<InetAddress, EndpointState> entry : gDigestAckMessage.epStateMap.entrySet())
        {
            InetAddress ep = entry.getKey();
            CompactEndpointSerializationHelper.serialize(ep, dos);
            EndpointState.serializer.serialize(entry.getValue(), dos, version);
        }
    }

+++++++++++++++++++++++
    public void serialize(GossipDigestAck gDigestAckMessage, DataOutput dos, int version) throws IOException
    {
        GossipDigestSerializationHelper.serialize(gDigestAckMessage.gDigestList, dos, version);
        if (version < MessagingService.VERSION_12)
            dos.writeBoolean(true); // 0.6 compatibility
        dos.writeInt(gDigestAckMessage.epStateMap.size());
        for (Map.Entry<InetAddress, EndpointState> entry : gDigestAckMessage.epStateMap.entrySet())
        {
            InetAddress ep = entry.getKey();
            CompactEndpointSerializationHelper.serialize(ep, dos);
            EndpointState.serializer.serialize(entry.getValue(), dos, version);
        }
    }

    public long serializedSize(GossipDigestAck ack, int version)
    {
        int size = GossipDigestSerializationHelper.serializedSize(ack.gDigestList, version);
        if (version <= MessagingService.VERSION_11)
            size += TypeSizes.NATIVE.sizeof(true);
        size += TypeSizes.NATIVE.sizeof(ack.epStateMap.size());
        for (Map.Entry<InetAddress, EndpointState> entry : ack.epStateMap.entrySet())
            size += CompactEndpointSerializationHelper.serializedSize(entry.getKey())
                  + EndpointState.serializer.serializedSize(entry.getValue(), version);
        return size;
+++++++++++++++++++++++
    public long serializedSize(GossipDigestAck ack, int version)
    {
        int size = GossipDigestSerializationHelper.serializedSize(ack.gDigestList, version);
        if (version < MessagingService.VERSION_12)
            size += TypeSizes.NATIVE.sizeof(true);
        size += TypeSizes.NATIVE.sizeof(ack.epStateMap.size());
        for (Map.Entry<InetAddress, EndpointState> entry : ack.epStateMap.entrySet())
            size += CompactEndpointSerializationHelper.serializedSize(entry.getKey())
                  + EndpointState.serializer.serializedSize(entry.getValue(), version);
        return size;
    public GossipDigestAck deserialize(DataInput dis, int version) throws IOException
    {
        List<GossipDigest> gDigestList = GossipDigestSerializationHelper.deserialize(dis, version);
        if (version <= MessagingService.VERSION_11)
            dis.readBoolean(); // 0.6 compatibility
        int size = dis.readInt();
        Map<InetAddress, EndpointState> epStateMap = new HashMap<InetAddress, EndpointState>(size);

        for (int i = 0; i < size; ++i)
        {
            InetAddress ep = CompactEndpointSerializationHelper.deserialize(dis);
            EndpointState epState = EndpointState.serializer.deserialize(dis, version);
            epStateMap.put(ep, epState);
        }
        return new GossipDigestAck(gDigestList, epStateMap);
    }

+++++++++++++++++++++++
    public GossipDigestAck deserialize(DataInput dis, int version) throws IOException
    {
        List<GossipDigest> gDigestList = GossipDigestSerializationHelper.deserialize(dis, version);
        if (version < MessagingService.VERSION_12)
            dis.readBoolean(); // 0.6 compatibility
        int size = dis.readInt();
        Map<InetAddress, EndpointState> epStateMap = new HashMap<InetAddress, EndpointState>(size);

        for (int i = 0; i < size; ++i)
        {
            InetAddress ep = CompactEndpointSerializationHelper.deserialize(dis);
            EndpointState epState = EndpointState.serializer.deserialize(dis, version);
            epStateMap.put(ep, epState);
        }
        return new GossipDigestAck(gDigestList, epStateMap);
    }

add: 5, delete: 5, change: 158, unhandled: 0 size_exceptions: 0 size_serialize: 3
-----------cassandra-1.2.1 vs cassandra-1.2.2-----------
add: 25, delete: 10, change: 133, unhandled: 0 size_exceptions: 16 size_serialize: 0
-----------cassandra-1.2.2 vs cassandra-1.2.3-----------
add: 3, delete: 0, change: 80, unhandled: 0 size_exceptions: 1 size_serialize: 0
-----------cassandra-1.2.3 vs cassandra-1.2.4-----------
add: 3, delete: 1, change: 139, unhandled: 0 size_exceptions: 14 size_serialize: 0
-----------cassandra-1.2.4 vs cassandra-1.2.5-----------
    public int serializedSize()
    {
        return TypeSizes.NATIVE.sizeof(position);
    }

+++++++++++++++++++++++
    public int serializedSize()
    {
        return TypeSizes.NATIVE.sizeof(position) + promotedSize();
    }

    public synchronized void serialize()
    {
        File manifestFile = cfs.directories.getOrCreateLeveledManifest();
        File oldFile = new File(manifestFile.getPath().replace(EXTENSION, "-old.json"));
        File tmpFile = new File(manifestFile.getPath().replace(EXTENSION, "-tmp.json"));

        JsonFactory f = new JsonFactory();
        try
        {
            JsonGenerator g = f.createJsonGenerator(tmpFile, JsonEncoding.UTF8);
            g.useDefaultPrettyPrinter();
            g.writeStartObject();
            g.writeArrayFieldStart("generations");
            for (int level = 0; level < generations.length; level++)
            {
                g.writeStartObject();
                g.writeNumberField("generation", level);
                g.writeArrayFieldStart("members");
                for (SSTableReader ssTableReader : generations[level])
                    g.writeNumber(ssTableReader.descriptor.generation);
                g.writeEndArray(); // members

                g.writeEndObject(); // generation
            }
            g.writeEndArray(); // for field generations
            g.writeEndObject(); // write global object
            g.close();
        }
        catch (IOException e)
        {
            throw new FSWriteError(e, tmpFile);
        }

        if (oldFile.exists() && manifestFile.exists())
            FileUtils.deleteWithConfirm(oldFile);

        if (manifestFile.exists())
            FileUtils.renameWithConfirm(manifestFile, oldFile);

        assert tmpFile.exists();

        FileUtils.renameWithConfirm(tmpFile, manifestFile);

        logger.debug("Saved manifest {}", manifestFile);
    }

    @Override
+++++++++++++++++++++++
    public synchronized void serialize()
    {
        File manifestFile = cfs.directories.getOrCreateLeveledManifest();
        File oldFile = new File(manifestFile.getPath().replace(EXTENSION, "-old.json"));
        File tmpFile = new File(manifestFile.getPath().replace(EXTENSION, "-tmp.json"));

        JsonFactory f = new JsonFactory();
        try
        {
            FileOutputStream fos = new FileOutputStream(tmpFile);
            JsonGenerator g = f.createJsonGenerator(fos, JsonEncoding.UTF8);
            g.useDefaultPrettyPrinter();
            g.writeStartObject();
            g.writeArrayFieldStart("generations");
            for (int level = 0; level < generations.length; level++)
            {
                g.writeStartObject();
                g.writeNumberField("generation", level);
                g.writeArrayFieldStart("members");
                for (SSTableReader ssTableReader : generations[level])
                    g.writeNumber(ssTableReader.descriptor.generation);
                g.writeEndArray(); // members

                g.writeEndObject(); // generation
            }
            g.writeEndArray(); // for field generations
            g.writeEndObject(); // write global object
            g.flush();
            fos.getFD().sync();
            g.close();
        }
        catch (IOException e)
        {
            throw new FSWriteError(e, tmpFile);
        }

        if (oldFile.exists() && manifestFile.exists())
            FileUtils.deleteWithConfirm(oldFile);

        if (manifestFile.exists())
            FileUtils.renameWithConfirm(manifestFile, oldFile);

        assert tmpFile.exists();

        FileUtils.renameWithConfirm(tmpFile, manifestFile);

        logger.debug("Saved manifest {}", manifestFile);
    }

    @Override
    public static ByteBuffer serializeForSSTable(ColumnFamily cf)
    {
        try
        {
            ByteArrayOutputStream baos = new ByteArrayOutputStream();
            DataOutputStream dos = new DataOutputStream(baos);
            DeletionInfo.serializer().serializeForSSTable(cf.deletionInfo(), dos);
            dos.writeInt(cf.getColumnCount());
            new ColumnIndex.Builder(cf, ByteBufferUtil.EMPTY_BYTE_BUFFER, cf.getColumnCount(), dos).build(cf);
            return ByteBuffer.wrap(baos.toByteArray());
        }
        catch (IOException e)
        {
            throw new RuntimeException(e);
        }
+++++++++++++++++++++++
    public static ByteBuffer serializeForSSTable(ColumnFamily cf)
    {
        try
        {
            ByteArrayOutputStream baos = new ByteArrayOutputStream();
            DataOutputStream dos = new DataOutputStream(baos);
            DeletionInfo.serializer().serializeForSSTable(cf.deletionInfo(), dos);
            dos.writeInt(cf.getColumnCount());
            new ColumnIndex.Builder(cf, ByteBufferUtil.EMPTY_BYTE_BUFFER, dos).build(cf);
            return ByteBuffer.wrap(baos.toByteArray());
        }
        catch (IOException e)
        {
            throw new RuntimeException(e);
        }
add: 9, delete: 4, change: 91, unhandled: 0 size_exceptions: 0 size_serialize: 3
-----------cassandra-1.2.5 vs cassandra-1.2.6-----------
    public long serializedSize(ReadResponse response, int version)
    {
        TypeSizes typeSizes = TypeSizes.NATIVE;
        ByteBuffer buffer = response.isDigestQuery() ? response.digest() : ByteBufferUtil.EMPTY_BYTE_BUFFER;
        int size = typeSizes.sizeof(buffer.remaining());
        size += typeSizes.sizeof(response.isDigestQuery());
        if (!response.isDigestQuery())
            size += Row.serializer.serializedSize(response.row(), version);
        return size;
+++++++++++++++++++++++
    public long serializedSize(ReadResponse response, int version)
    {
        TypeSizes typeSizes = TypeSizes.NATIVE;
        ByteBuffer buffer = response.isDigestQuery() ? response.digest() : ByteBufferUtil.EMPTY_BYTE_BUFFER;
        int size = typeSizes.sizeof(buffer.remaining());
        size += buffer.remaining();
        size += typeSizes.sizeof(response.isDigestQuery());
        if (!response.isDigestQuery())
            size += Row.serializer.serializedSize(response.row(), version);
        return size;
add: 18, delete: 1, change: 93, unhandled: 0 size_exceptions: 1 size_serialize: 1
-----------cassandra-1.2.6 vs cassandra-1.2.7-----------
    public void deserializeFromSSTable(DataInput dis, ColumnFamily cf, IColumnSerializer.Flag flag, Descriptor.Version version) throws IOException
    {
        cf.delete(DeletionInfo.serializer().deserializeFromSSTable(dis, version));
        int size = dis.readInt();
        int expireBefore = (int) (System.currentTimeMillis() / 1000);
        deserializeColumnsFromSSTable(dis, cf, size, flag, expireBefore, version);
    }

+++++++++++++++++++++++
    public void deserializeFromSSTable(DataInput dis, ColumnFamily cf, IColumnSerializer.Flag flag, Descriptor.Version version) throws IOException
    {
        cf.delete(DeletionTime.serializer.deserialize(dis));
        int size = dis.readInt();
        int expireBefore = (int) (System.currentTimeMillis() / 1000);
        deserializeColumnsFromSSTable(dis, cf, size, flag, expireBefore, version);
    }

    public static ByteBuffer serializeForSSTable(ColumnFamily cf)
    {
        try
        {
            ByteArrayOutputStream baos = new ByteArrayOutputStream();
            DataOutputStream dos = new DataOutputStream(baos);
            DeletionInfo.serializer().serializeForSSTable(cf.deletionInfo(), dos);
            dos.writeInt(cf.getColumnCount());
            new ColumnIndex.Builder(cf, ByteBufferUtil.EMPTY_BYTE_BUFFER, dos).build(cf);
            return ByteBuffer.wrap(baos.toByteArray());
        }
        catch (IOException e)
        {
            throw new RuntimeException(e);
        }
+++++++++++++++++++++++
    public static ByteBuffer serializeForSSTable(ColumnFamily cf)
    {
        try
        {
            ByteArrayOutputStream baos = new ByteArrayOutputStream();
            DataOutputStream dos = new DataOutputStream(baos);
            DeletionTime.serializer.serialize(cf.deletionInfo().getTopLevelDeletion(), dos);
            dos.writeInt(cf.getColumnCount());
            new ColumnIndex.Builder(cf, ByteBufferUtil.EMPTY_BYTE_BUFFER, dos).build(cf);
            return ByteBuffer.wrap(baos.toByteArray());
        }
        catch (IOException e)
        {
            throw new RuntimeException(e);
        }
add: 6, delete: 0, change: 73, unhandled: 0 size_exceptions: 0 size_serialize: 2
-----------cassandra-1.2.7 vs cassandra-1.2.8-----------
add: 0, delete: 0, change: 10, unhandled: 0 size_exceptions: 1 size_serialize: 0
-----------cassandra-1.2.8 vs cassandra-1.2.9-----------
    public static OffHeapBitSet deserialize(DataInput dis) throws IOException
    {
        int byteCount = dis.readInt() * 8;
        Memory memory = RefCountedMemory.allocate(byteCount);
        for (int i = 0; i < byteCount;)
        {
            long v = dis.readLong();
            memory.setByte(i++, (byte) (v >>> 0));
            memory.setByte(i++, (byte) (v >>> 8));
            memory.setByte(i++, (byte) (v >>> 16));
            memory.setByte(i++, (byte) (v >>> 24));
            memory.setByte(i++, (byte) (v >>> 32));
            memory.setByte(i++, (byte) (v >>> 40));
            memory.setByte(i++, (byte) (v >>> 48));
            memory.setByte(i++, (byte) (v >>> 56));
        }
        return new OffHeapBitSet(memory);
    }

+++++++++++++++++++++++
    public static OffHeapBitSet deserialize(DataInput dis) throws IOException
    {
        long byteCount = dis.readInt() * 8L;
        Memory memory = RefCountedMemory.allocate(byteCount);
        for (long i = 0; i < byteCount;)
        {
            long v = dis.readLong();
            memory.setByte(i++, (byte) (v >>> 0));
            memory.setByte(i++, (byte) (v >>> 8));
            memory.setByte(i++, (byte) (v >>> 16));
            memory.setByte(i++, (byte) (v >>> 24));
            memory.setByte(i++, (byte) (v >>> 32));
            memory.setByte(i++, (byte) (v >>> 40));
            memory.setByte(i++, (byte) (v >>> 48));
            memory.setByte(i++, (byte) (v >>> 56));
        }
        return new OffHeapBitSet(memory);
    }

add: 4, delete: 1, change: 43, unhandled: 0 size_exceptions: 0 size_serialize: 1
-----------cassandra-1.2.9 vs cassandra-1.2.10-----------
add: 3, delete: 0, change: 71, unhandled: 0 size_exceptions: 6 size_serialize: 0
-----------cassandra-1.2.10 vs cassandra-1.2.11-----------
add: 8, delete: 0, change: 76, unhandled: 0 size_exceptions: 21 size_serialize: 0
-----------cassandra-1.2.11 vs cassandra-1.2.12-----------
add: 3, delete: 0, change: 50, unhandled: 0 size_exceptions: 0 size_serialize: 0
-----------cassandra-1.2.12 vs cassandra-1.2.13-----------
    public ByteBuffer serialize(List<Pair<ByteBuffer, IColumn>> columns)
    {
        List<ByteBuffer> bbs = new ArrayList<ByteBuffer>(columns.size());
        int size = 0;
        for (Pair<ByteBuffer, IColumn> p : columns)
        {
            bbs.add(p.right.value());
            size += 2 + p.right.value().remaining();
        }
        return pack(bbs, columns.size(), size);
+++++++++++++++++++++++
    public ByteBuffer serialize(List<Pair<ByteBuffer, IColumn>> columns)
    {
        columns = enforceLimit(columns);

        List<ByteBuffer> bbs = new ArrayList<ByteBuffer>(columns.size());
        int size = 0;
        for (Pair<ByteBuffer, IColumn> p : columns)
        {
            bbs.add(p.right.value());
            size += 2 + p.right.value().remaining();
        }
        return pack(bbs, columns.size(), size);
    public ByteBuffer serialize(List<Pair<ByteBuffer, IColumn>> columns)
    {
        List<ByteBuffer> bbs = new ArrayList<ByteBuffer>(2 * columns.size());
        int size = 0;
        for (Pair<ByteBuffer, IColumn> p : columns)
        {
            bbs.add(p.left);
            bbs.add(p.right.value());
            size += 4 + p.left.remaining() + p.right.value().remaining();
        }
        return pack(bbs, columns.size(), size);
+++++++++++++++++++++++
    public ByteBuffer serialize(List<Pair<ByteBuffer, IColumn>> columns)
    {
        columns = enforceLimit(columns);

        List<ByteBuffer> bbs = new ArrayList<ByteBuffer>(2 * columns.size());
        int size = 0;
        for (Pair<ByteBuffer, IColumn> p : columns)
        {
            bbs.add(p.left);
            bbs.add(p.right.value());
            size += 4 + p.left.remaining() + p.right.value().remaining();
        }
        return pack(bbs, columns.size(), size);
    public ByteBuffer serialize(List<Pair<ByteBuffer, IColumn>> columns)
    {
        List<ByteBuffer> bbs = new ArrayList<ByteBuffer>(columns.size());
        int size = 0;
        for (Pair<ByteBuffer, IColumn> p : columns)
        {
            bbs.add(p.left);
            size += 2 + p.left.remaining();
        }
        return pack(bbs, columns.size(), size);
+++++++++++++++++++++++
    public ByteBuffer serialize(List<Pair<ByteBuffer, IColumn>> columns)
    {
        columns = enforceLimit(columns);

        List<ByteBuffer> bbs = new ArrayList<ByteBuffer>(columns.size());
        int size = 0;
        for (Pair<ByteBuffer, IColumn> p : columns)
        {
            bbs.add(p.left);
            size += 2 + p.left.remaining();
        }
        return pack(bbs, columns.size(), size);
add: 0, delete: 0, change: 33, unhandled: 0 size_exceptions: 0 size_serialize: 3
-----------cassandra-1.2.13 vs cassandra-1.2.14-----------
add: 1, delete: 0, change: 40, unhandled: 0 size_exceptions: 0 size_serialize: 0
-----------cassandra-1.2.14 vs cassandra-1.2.15-----------
add: 0, delete: 0, change: 6, unhandled: 0 size_exceptions: 0 size_serialize: 0
-----------cassandra-1.2.15 vs cassandra-1.2.16-----------
add: 0, delete: 0, change: 33, unhandled: 0 size_exceptions: 0 size_serialize: 0
-----------cassandra-1.2.16 vs cassandra-1.2.17-----------
    private RefCountedMemory serialize(V value)
    {
        long serializedSize = serializer.serializedSize(value, ENCODED_TYPE_SIZES);
        if (serializedSize > Integer.MAX_VALUE)
            throw new IllegalArgumentException("Unable to allocate " + serializedSize + " bytes");

        RefCountedMemory freeableMemory;
        try
        {
            freeableMemory = new RefCountedMemory(serializedSize);
        }
        catch (OutOfMemoryError e)
        {
            return null;
        }

        try
        {
            serializer.serialize(value, new EncodedDataOutputStream(new MemoryOutputStream(freeableMemory)));
        }
        catch (IOException e)
        {
            throw new RuntimeException(e);
        }
        return freeableMemory;
    }

+++++++++++++++++++++++
    private RefCountedMemory serialize(V value)
    {
        long serializedSize = serializer.serializedSize(value, ENCODED_TYPE_SIZES);
        if (serializedSize > Integer.MAX_VALUE)
            throw new IllegalArgumentException("Unable to allocate " + serializedSize + " bytes");

        RefCountedMemory freeableMemory;
        try
        {
            freeableMemory = new RefCountedMemory(serializedSize);
        }
        catch (OutOfMemoryError e)
        {
            return null;
        }

        try
        {
            serializer.serialize(value, new EncodedDataOutputStream(new MemoryOutputStream(freeableMemory)));
        }
        catch (IOException e)
        {
            freeableMemory.unreference();
            throw new RuntimeException(e);
        }
        return freeableMemory;
    }

    private V deserialize(RefCountedMemory mem)
    {
        try
        {
            return serializer.deserialize(new EncodedDataInputStream(new MemoryInputStream(mem)));
        }
        catch (IOException e)
        {
            logger.debug("Cannot fetch in memory data, we will failback to read from disk ", e);
            return null;
        }
    }

+++++++++++++++++++++++
    private V deserialize(RefCountedMemory mem)
    {
        try
        {
            return serializer.deserialize(new EncodedDataInputStream(new MemoryInputStream(mem)));
        }
        catch (IOException e)
        {
            logger.debug("Cannot fetch in memory data, we will fallback to read from disk ", e);
            return null;
        }
    }

add: 6, delete: 0, change: 45, unhandled: 0 size_exceptions: 0 size_serialize: 2
-----------cassandra-1.2.17 vs cassandra-1.2.18-----------
add: 0, delete: 0, change: 5, unhandled: 0 size_exceptions: 0 size_serialize: 0
-----------cassandra-1.2.18 vs cassandra-1.2.19-----------
add: 0, delete: 3, change: 26, unhandled: 0 size_exceptions: 0 size_serialize: 0
-----------cassandra-1.2.19 vs cassandra-2.0.0-----------
    private RefCountedMemory serialize(V value)
    {
        long serializedSize = serializer.serializedSize(value, ENCODED_TYPE_SIZES);
        if (serializedSize > Integer.MAX_VALUE)
            throw new IllegalArgumentException("Unable to allocate " + serializedSize + " bytes");

        RefCountedMemory freeableMemory;
        try
        {
            freeableMemory = new RefCountedMemory(serializedSize);
        }
        catch (OutOfMemoryError e)
        {
            return null;
        }

        try
        {
            serializer.serialize(value, new EncodedDataOutputStream(new MemoryOutputStream(freeableMemory)));
        }
        catch (IOException e)
        {
            freeableMemory.unreference();
            throw new RuntimeException(e);
        }
        return freeableMemory;
    }

+++++++++++++++++++++++
    private RefCountedMemory serialize(V value)
    {
        long serializedSize = serializer.serializedSize(value, ENCODED_TYPE_SIZES);
        if (serializedSize > Integer.MAX_VALUE)
            throw new IllegalArgumentException("Unable to allocate " + serializedSize + " bytes");

        RefCountedMemory freeableMemory;
        try
        {
            freeableMemory = new RefCountedMemory(serializedSize);
        }
        catch (OutOfMemoryError e)
        {
            return null;
        }

        try
        {
            serializer.serialize(value, new EncodedDataOutputStream(new MemoryOutputStream(freeableMemory)));
        }
        catch (IOException e)
        {
            throw new RuntimeException(e);
        }
        return freeableMemory;
    }

    private V deserialize(RefCountedMemory mem)
    {
        try
        {
            return serializer.deserialize(new EncodedDataInputStream(new MemoryInputStream(mem)));
        }
        catch (IOException e)
        {
            logger.debug("Cannot fetch in memory data, we will fallback to read from disk ", e);
            return null;
        }
    }

+++++++++++++++++++++++
    private V deserialize(RefCountedMemory mem)
    {
        try
        {
            return serializer.deserialize(new EncodedDataInputStream(new MemoryInputStream(mem)));
        }
        catch (IOException e)
        {
            logger.debug("Cannot fetch in memory data, we will failback to read from disk ", e);
            return null;
        }
    }

    public static Map<String, CFMetaData> deserializeColumnFamilies(Row row)
    {
        if (row.cf == null)
            return Collections.emptyMap();

        Map<String, CFMetaData> cfms = new HashMap<String, CFMetaData>();
        UntypedResultSet results = QueryProcessor.resultify("SELECT * FROM system.schema_columnfamilies", row);
        for (UntypedResultSet.Row result : results)
        {
            CFMetaData cfm = CFMetaData.fromSchema(result);
            cfms.put(cfm.cfName, cfm);
        }

        for (CFMetaData cfm : cfms.values())
        {
            Row columnRow = ColumnDefinition.readSchema(cfm.ksName, cfm.cfName);
            for (ColumnDefinition cd : ColumnDefinition.fromSchema(columnRow, cfm))
                cfm.column_metadata.put(cd.name, cd);
        }

        return cfms;
+++++++++++++++++++++++
    public static Map<String, CFMetaData> deserializeColumnFamilies(Row row)
    {
        if (row.cf == null)
            return Collections.emptyMap();

        Map<String, CFMetaData> cfms = new HashMap<String, CFMetaData>();
        UntypedResultSet results = QueryProcessor.resultify("SELECT * FROM system.schema_columnfamilies", row);
        for (UntypedResultSet.Row result : results)
        {
            CFMetaData cfm = CFMetaData.fromSchema(result);
            cfms.put(cfm.cfName, cfm);
        }

        for (CFMetaData cfm : cfms.values())
        {
            Row columnRow = SystemKeyspace.readSchemaRow(SystemKeyspace.SCHEMA_COLUMNS_CF, cfm.ksName, cfm.cfName);
            // This may replace some existing definition coming from the old key, column and
            // value aliases. But that's what we want (see CFMetaData.fromSchemaNoColumnsNoTriggers).
            for (ColumnDefinition cd : ColumnDefinition.fromSchema(columnRow, cfm))
                cfm.addOrReplaceColumnDefinition(cd);
            cfm.rebuild();
        }

        return cfms;
    private static ByteBuffer serializeRowMutations(Collection<RowMutation> mutations)
    {
        FastByteArrayOutputStream bos = new FastByteArrayOutputStream();
        DataOutputStream dos = new DataOutputStream(bos);

        try
        {
            dos.writeInt(mutations.size());
            for (RowMutation rm : mutations)
                RowMutation.serializer.serialize(rm, dos, VERSION);
        }
        catch (IOException e)
        {
            throw new AssertionError(); // cannot happen.
        }

        return ByteBuffer.wrap(bos.toByteArray());
    }

    @VisibleForTesting
+++++++++++++++++++++++
    private static ByteBuffer serializeRowMutations(Collection<RowMutation> mutations)
    {
        FastByteArrayOutputStream bos = new FastByteArrayOutputStream();
        DataOutputStream out = new DataOutputStream(bos);

        try
        {
            out.writeInt(mutations.size());
            for (RowMutation rm : mutations)
                RowMutation.serializer.serialize(rm, out, VERSION);
        }
        catch (IOException e)
        {
            throw new AssertionError(); // cannot happen.
        }

        return ByteBuffer.wrap(bos.toByteArray());
    }

    public void deserializeFromSSTable(DataInput dis, ColumnFamily cf, IColumnSerializer.Flag flag, Descriptor.Version version) throws IOException
    {
        cf.delete(DeletionTime.serializer.deserialize(dis));
        int size = dis.readInt();
        int expireBefore = (int) (System.currentTimeMillis() / 1000);
        deserializeColumnsFromSSTable(dis, cf, size, flag, expireBefore, version);
    }

+++++++++++++++++++++++
    public ColumnFamily deserializeFromSSTable(DataInput in, Descriptor.Version version)
    {
        throw new UnsupportedOperationException();
    }

    public void serializeCfId(UUID cfId, DataOutput dos, int version) throws IOException
    {
        if (version < MessagingService.VERSION_12) // try to use CF's old id where possible (CASSANDRA-3794)
        {
            Integer oldId = Schema.instance.convertNewCfId(cfId);

            if (oldId == null)
                throw new IOException("Can't serialize ColumnFamily ID " + cfId + " to be used by version " + version +
                                      ", because int <-> uuid mapping could not be established (CF was created in mixed version cluster).");

            dos.writeInt(oldId);
        }
        else
            UUIDSerializer.serializer.serialize(cfId, dos, version);
    }

+++++++++++++++++++++++
    public void serializeCfId(UUID cfId, DataOutput out, int version) throws IOException
    {
        UUIDSerializer.serializer.serialize(cfId, out, version);
    }

    public void serializeForSSTable(ColumnFamily cf, DataOutput dos)
    {
        // Column families shouldn't be written directly to disk, use ColumnIndex.Builder instead
        throw new UnsupportedOperationException();
    }

+++++++++++++++++++++++
    public void serializeForSSTable(ColumnFamily cf, DataOutput out)
    {
        // Column families shouldn't be written directly to disk, use ColumnIndex.Builder instead
        throw new UnsupportedOperationException();
    }

    public UUID deserializeCfId(DataInput dis, int version) throws IOException
    {
        // create a ColumnFamily based on the cf id
        UUID cfId = (version < MessagingService.VERSION_12)
                     ? Schema.instance.convertOldCfId(dis.readInt())
                     : UUIDSerializer.serializer.deserialize(dis, version);

        if (Schema.instance.getCF(cfId) == null)
            throw new UnknownColumnFamilyException("Couldn't find cfId=" + cfId, cfId);

        return cfId;
    }

+++++++++++++++++++++++
    public UUID deserializeCfId(DataInput in, int version) throws IOException
    {
        UUID cfId = UUIDSerializer.serializer.deserialize(in, version);
        if (Schema.instance.getCF(cfId) == null)
            throw new UnknownColumnFamilyException("Couldn't find cfId=" + cfId, cfId);

        return cfId;
    }

    public void serialize(ColumnFamily cf, DataOutput dos, int version)
    {
        try
        {
            if (cf == null)
            {
                dos.writeBoolean(false);
                return;
            }

            dos.writeBoolean(true);
            serializeCfId(cf.id(), dos, version);

            DeletionInfo.serializer().serialize(cf.deletionInfo(), dos, version);

            IColumnSerializer columnSerializer = cf.getColumnSerializer();
            int count = cf.getColumnCount();
            dos.writeInt(count);
            int written = 0;
            for (IColumn column : cf)
            {
                columnSerializer.serialize(column, dos);
                written++;
            }
            assert count == written: "Column family had " + count + " columns, but " + written + " written";
        }
        catch (IOException e)
        {
            throw new RuntimeException(e);
        }
    }

+++++++++++++++++++++++
    public void serialize(ColumnFamily cf, DataOutput out, int version)
    {
        try
        {
            if (cf == null)
            {
                out.writeBoolean(false);
                return;
            }

            out.writeBoolean(true);
            serializeCfId(cf.id(), out, version);

            if (cf.metadata().isSuper() && version < MessagingService.VERSION_20)
            {
                SuperColumns.serializeSuperColumnFamily(cf, out, version);
                return;
            }

            DeletionInfo.serializer().serialize(cf.deletionInfo(), out, version);
            ColumnSerializer columnSerializer = Column.serializer;
            int count = cf.getColumnCount();
            out.writeInt(count);
            int written = 0;
            for (Column column : cf)
            {
                columnSerializer.serialize(column, out);
                written++;
            }
            assert count == written: "Column family had " + count + " columns, but " + written + " written";
        }
        catch (IOException e)
        {
            throw new RuntimeException(e);
        }
    }

    public ColumnFamily deserialize(DataInput dis, IColumnSerializer.Flag flag, ISortedColumns.Factory factory, int version) throws IOException
    {
        if (!dis.readBoolean())
            return null;

        ColumnFamily cf = ColumnFamily.create(deserializeCfId(dis, version), factory);
        IColumnSerializer columnSerializer = cf.getColumnSerializer();
        cf.delete(DeletionInfo.serializer().deserialize(dis, version, cf.getComparator()));
        int expireBefore = (int) (System.currentTimeMillis() / 1000);
        int size = dis.readInt();
        for (int i = 0; i < size; ++i)
        {
            cf.addColumn(columnSerializer.deserialize(dis, flag, expireBefore));
        }
        return cf;
    }

+++++++++++++++++++++++
    public ColumnFamily deserialize(DataInput in, ColumnFamily.Factory factory, ColumnSerializer.Flag flag, int version) throws IOException
    {
        if (!in.readBoolean())
            return null;

        ColumnFamily cf = factory.create(Schema.instance.getCFMetaData(deserializeCfId(in, version)));

        if (cf.metadata().isSuper() && version < MessagingService.VERSION_20)
        {
            SuperColumns.deserializerSuperColumnFamily(in, cf, flag, version);
        }
        else
        {
            cf.delete(DeletionInfo.serializer().deserialize(in, version, cf.getComparator()));

            ColumnSerializer columnSerializer = Column.serializer;
            int size = in.readInt();
            for (int i = 0; i < size; ++i)
                cf.addColumn(columnSerializer.deserialize(in, flag));
        }
        return cf;
    }

    public void serialize(IColumn column, DataOutput dos) throws IOException
    {
        assert column.name().remaining() > 0;
        ByteBufferUtil.writeWithShortLength(column.name(), dos);
        try
        {
            dos.writeByte(column.serializationFlags());
            if (column instanceof CounterColumn)
            {
                dos.writeLong(((CounterColumn)column).timestampOfLastDelete());
            }
            else if (column instanceof ExpiringColumn)
            {
                dos.writeInt(((ExpiringColumn) column).getTimeToLive());
                dos.writeInt(column.getLocalDeletionTime());
            }
            dos.writeLong(column.timestamp());
            ByteBufferUtil.writeWithLength(column.value(), dos);
        }
        catch (IOException e)
        {
            throw new RuntimeException(e);
        }
    }

+++++++++++++++++++++++
    public void serialize(Column column, DataOutput out) throws IOException
    {
        assert column.name().remaining() > 0;
        ByteBufferUtil.writeWithShortLength(column.name(), out);
        try
        {
            out.writeByte(column.serializationFlags());
            if (column instanceof CounterColumn)
            {
                out.writeLong(((CounterColumn)column).timestampOfLastDelete());
            }
            else if (column instanceof ExpiringColumn)
            {
                out.writeInt(((ExpiringColumn) column).getTimeToLive());
                out.writeInt(column.getLocalDeletionTime());
            }
            out.writeLong(column.timestamp());
            ByteBufferUtil.writeWithLength(column.value(), out);
        }
        catch (IOException e)
        {
            throw new RuntimeException(e);
        }
    }

    Column deserializeColumnBody(DataInput dis, ByteBuffer name, int mask, IColumnSerializer.Flag flag, int expireBefore) throws IOException
    {
        if ((mask & COUNTER_MASK) != 0)
        {
            long timestampOfLastDelete = dis.readLong();
            long ts = dis.readLong();
            ByteBuffer value = ByteBufferUtil.readWithLength(dis);
            return CounterColumn.create(name, value, ts, timestampOfLastDelete, flag);
        }
        else if ((mask & EXPIRATION_MASK) != 0)
        {
            int ttl = dis.readInt();
            int expiration = dis.readInt();
            long ts = dis.readLong();
            ByteBuffer value = ByteBufferUtil.readWithLength(dis);
            return ExpiringColumn.create(name, value, ts, ttl, expiration, expireBefore, flag);
        }
        else
        {
            long ts = dis.readLong();
            ByteBuffer value = ByteBufferUtil.readWithLength(dis);
            return (mask & COUNTER_UPDATE_MASK) != 0
                   ? new CounterUpdateColumn(name, value, ts)
                   : ((mask & DELETION_MASK) == 0
                      ? new Column(name, value, ts)
                      : new DeletedColumn(name, value, ts));
        }
    }

+++++++++++++++++++++++
    Column deserializeColumnBody(DataInput in, ByteBuffer name, int mask, ColumnSerializer.Flag flag, int expireBefore) throws IOException
    {
        if ((mask & COUNTER_MASK) != 0)
        {
            long timestampOfLastDelete = in.readLong();
            long ts = in.readLong();
            ByteBuffer value = ByteBufferUtil.readWithLength(in);
            return CounterColumn.create(name, value, ts, timestampOfLastDelete, flag);
        }
        else if ((mask & EXPIRATION_MASK) != 0)
        {
            int ttl = in.readInt();
            int expiration = in.readInt();
            long ts = in.readLong();
            ByteBuffer value = ByteBufferUtil.readWithLength(in);
            return ExpiringColumn.create(name, value, ts, ttl, expiration, expireBefore, flag);
        }
        else
        {
            long ts = in.readLong();
            ByteBuffer value = ByteBufferUtil.readWithLength(in);
            return (mask & COUNTER_UPDATE_MASK) != 0
                   ? new CounterUpdateColumn(name, value, ts)
                   : ((mask & DELETION_MASK) == 0
                      ? new Column(name, value, ts)
                      : new DeletedColumn(name, value, ts));
        }
    }

    public long serializedSize(IColumn column, TypeSizes type)
    {
        return column.serializedSize(type);
    }

+++++++++++++++++++++++
    public long serializedSize(Column column, TypeSizes type)
    {
        return column.serializedSize(type);
    }

    public Column deserialize(DataInput dis, IColumnSerializer.Flag flag, int expireBefore) throws IOException
    {
        ByteBuffer name = ByteBufferUtil.readWithShortLength(dis);
        if (name.remaining() <= 0)
            throw CorruptColumnException.create(dis, name);

        int b = dis.readUnsignedByte();
        return deserializeColumnBody(dis, name, b, flag, expireBefore);
    }

+++++++++++++++++++++++
    public Column deserialize(DataInput in, ColumnSerializer.Flag flag, int expireBefore) throws IOException
    {
        ByteBuffer name = ByteBufferUtil.readWithShortLength(in);
        if (name.remaining() <= 0)
            throw CorruptColumnException.create(in, name);

        int b = in.readUnsignedByte();
        return deserializeColumnBody(in, name, b, flag, expireBefore);
    }

    public void serialize(CounterMutation cm, DataOutput dos, int version) throws IOException
    {
        RowMutation.serializer.serialize(cm.rowMutation(), dos, version);
        dos.writeUTF(cm.consistency().name());
    }

+++++++++++++++++++++++
    public void serialize(CounterMutation cm, DataOutput out, int version) throws IOException
    {
        RowMutation.serializer.serialize(cm.rowMutation(), out, version);
        out.writeUTF(cm.consistency().name());
    }

    public CounterMutation deserialize(DataInput dis, int version) throws IOException
    {
        RowMutation rm = RowMutation.serializer.deserialize(dis, version);
        ConsistencyLevel consistency = Enum.valueOf(ConsistencyLevel.class, dis.readUTF());
        return new CounterMutation(rm, consistency);
    }

+++++++++++++++++++++++
    public CounterMutation deserialize(DataInput in, int version) throws IOException
    {
        RowMutation rm = RowMutation.serializer.deserialize(in, version);
        ConsistencyLevel consistency = Enum.valueOf(ConsistencyLevel.class, in.readUTF());
        return new CounterMutation(rm, consistency);
    }

    public void serialize(RangeSliceCommand sliceCommand, DataOutput dos, int version) throws IOException
    {
        dos.writeUTF(sliceCommand.keyspace);
        dos.writeUTF(sliceCommand.column_family);
        ByteBuffer sc = sliceCommand.super_column;
        dos.writeInt(sc == null ? 0 : sc.remaining());
        if (sc != null)
            ByteBufferUtil.write(sc, dos);

        if (version < MessagingService.VERSION_12)
        {
            FBUtilities.serialize(new TSerializer(new TBinaryProtocol.Factory()), asSlicePredicate(sliceCommand.predicate), dos);
        }
        else
        {
            IDiskAtomFilter.Serializer.instance.serialize(sliceCommand.predicate, dos, version);
        }

        if (version >= MessagingService.VERSION_11)
        {
            if (sliceCommand.row_filter == null)
            {
                dos.writeInt(0);
            }
            else
            {
                dos.writeInt(sliceCommand.row_filter.size());
                for (IndexExpression expr : sliceCommand.row_filter)
                {
                    if (version < MessagingService.VERSION_12)
                    {
                        FBUtilities.serialize(new TSerializer(new TBinaryProtocol.Factory()), expr, dos);
                    }
                    else
                    {
                        ByteBufferUtil.writeWithShortLength(expr.column_name, dos);
                        dos.writeInt(expr.op.getValue());
                        ByteBufferUtil.writeWithShortLength(expr.value, dos);
                    }
                }
            }
        }
        AbstractBounds.serializer.serialize(sliceCommand.range, dos, version);
        dos.writeInt(sliceCommand.maxResults);
        if (version >= MessagingService.VERSION_11)
        {
            dos.writeBoolean(sliceCommand.countCQL3Rows);
            dos.writeBoolean(sliceCommand.isPaging);
        }
    }

+++++++++++++++++++++++
    public void serialize(RangeSliceCommand sliceCommand, DataOutput out, int version) throws IOException
    {
        out.writeUTF(sliceCommand.keyspace);
        out.writeUTF(sliceCommand.columnFamily);

        if (version >= MessagingService.VERSION_20)
            out.writeLong(sliceCommand.timestamp);

        IDiskAtomFilter filter = sliceCommand.predicate;
        if (version < MessagingService.VERSION_20)
        {
            // Pre-2.0, we need to know if it's a super column. If it is, we
            // must extract the super column name from the predicate (and
            // modify the predicate accordingly)
            ByteBuffer sc = null;
            CFMetaData metadata = Schema.instance.getCFMetaData(sliceCommand.getKeyspace(), sliceCommand.columnFamily);
            if (metadata.cfType == ColumnFamilyType.Super)
            {
                SuperColumns.SCFilter scFilter = SuperColumns.filterToSC((CompositeType)metadata.comparator, filter);
                sc = scFilter.scName;
                filter = scFilter.updatedFilter;
            }

            out.writeInt(sc == null ? 0 : sc.remaining());
            if (sc != null)
                ByteBufferUtil.write(sc, out);
        }

        IDiskAtomFilter.Serializer.instance.serialize(filter, out, version);

        if (sliceCommand.rowFilter == null)
        {
            out.writeInt(0);
        }
        else
        {
            out.writeInt(sliceCommand.rowFilter.size());
            for (IndexExpression expr : sliceCommand.rowFilter)
            {
                ByteBufferUtil.writeWithShortLength(expr.column_name, out);
                out.writeInt(expr.op.getValue());
                ByteBufferUtil.writeWithShortLength(expr.value, out);
            }
        }
        AbstractBounds.serializer.serialize(sliceCommand.keyRange, out, version);
        out.writeInt(sliceCommand.maxResults);
        out.writeBoolean(sliceCommand.countCQL3Rows);
        out.writeBoolean(sliceCommand.isPaging);
    }

    public long serializedSize(RangeSliceCommand rsc, int version)
    {
        long size = TypeSizes.NATIVE.sizeof(rsc.keyspace);
        size += TypeSizes.NATIVE.sizeof(rsc.column_family);

        ByteBuffer sc = rsc.super_column;
        if (sc != null)
        {
            size += TypeSizes.NATIVE.sizeof(sc.remaining());
            size += sc.remaining();
        }
        else
        {
            size += TypeSizes.NATIVE.sizeof(0);
        }

        if (version < MessagingService.VERSION_12)
        {
            TSerializer ser = new TSerializer(new TBinaryProtocol.Factory());
            try
            {
                int predicateLength = ser.serialize(asSlicePredicate(rsc.predicate)).length;
                if (version < MessagingService.VERSION_12)
                    size += TypeSizes.NATIVE.sizeof(predicateLength);
                size += predicateLength;
            }
            catch (TException e)
            {
                throw new RuntimeException(e);
            }
        }
        else
        {
            size += IDiskAtomFilter.Serializer.instance.serializedSize(rsc.predicate, version);
        }

        if (version >= MessagingService.VERSION_11)
        {
            if (rsc.row_filter == null)
            {
                size += TypeSizes.NATIVE.sizeof(0);
            }
            else
            {
                size += TypeSizes.NATIVE.sizeof(rsc.row_filter.size());
                for (IndexExpression expr : rsc.row_filter)
                {
                    if (version < MessagingService.VERSION_12)
                    {
                        try
                        {
                            int filterLength = new TSerializer(new TBinaryProtocol.Factory()).serialize(expr).length;
                            size += TypeSizes.NATIVE.sizeof(filterLength);
                            size += filterLength;
                        }
                        catch (TException e)
                        {
                            throw new RuntimeException(e);
                        }
                    }
                    else
                    {
                        size += TypeSizes.NATIVE.sizeofWithShortLength(expr.column_name);
                        size += TypeSizes.NATIVE.sizeof(expr.op.getValue());
                        size += TypeSizes.NATIVE.sizeofWithLength(expr.value);
                    }
                }
            }
        }
        size += AbstractBounds.serializer.serializedSize(rsc.range, version);
        size += TypeSizes.NATIVE.sizeof(rsc.maxResults);
        if (version >= MessagingService.VERSION_11)
        {
            size += TypeSizes.NATIVE.sizeof(rsc.countCQL3Rows);
            size += TypeSizes.NATIVE.sizeof(rsc.isPaging);
        }
        return size;
+++++++++++++++++++++++
    public long serializedSize(RangeSliceCommand rsc, int version)
    {
        long size = TypeSizes.NATIVE.sizeof(rsc.keyspace);
        size += TypeSizes.NATIVE.sizeof(rsc.columnFamily);

        if (version >= MessagingService.VERSION_20)
            size += TypeSizes.NATIVE.sizeof(rsc.timestamp);

        IDiskAtomFilter filter = rsc.predicate;
        if (version < MessagingService.VERSION_20)
        {
            ByteBuffer sc = null;
            CFMetaData metadata = Schema.instance.getCFMetaData(rsc.keyspace, rsc.columnFamily);
            if (metadata.cfType == ColumnFamilyType.Super)
            {
                SuperColumns.SCFilter scFilter = SuperColumns.filterToSC((CompositeType)metadata.comparator, filter);
                sc = scFilter.scName;
                filter = scFilter.updatedFilter;
            }

            if (sc != null)
            {
                size += TypeSizes.NATIVE.sizeof(sc.remaining());
                size += sc.remaining();
            }
            else
            {
                size += TypeSizes.NATIVE.sizeof(0);
            }
        }

        size += IDiskAtomFilter.Serializer.instance.serializedSize(filter, version);

        if (rsc.rowFilter == null)
        {
            size += TypeSizes.NATIVE.sizeof(0);
        }
        else
        {
            size += TypeSizes.NATIVE.sizeof(rsc.rowFilter.size());
            for (IndexExpression expr : rsc.rowFilter)
            {
                size += TypeSizes.NATIVE.sizeofWithShortLength(expr.column_name);
                size += TypeSizes.NATIVE.sizeof(expr.op.getValue());
                size += TypeSizes.NATIVE.sizeofWithLength(expr.value);
            }
        }
        size += AbstractBounds.serializer.serializedSize(rsc.keyRange, version);
        size += TypeSizes.NATIVE.sizeof(rsc.maxResults);
        size += TypeSizes.NATIVE.sizeof(rsc.countCQL3Rows);
        size += TypeSizes.NATIVE.sizeof(rsc.isPaging);
        return size;
    public RangeSliceCommand deserialize(DataInput dis, int version) throws IOException
    {
        String keyspace = dis.readUTF();
        String columnFamily = dis.readUTF();

        int scLength = dis.readInt();
        ByteBuffer superColumn = null;
        if (scLength > 0)
        {
            byte[] buf = new byte[scLength];
            dis.readFully(buf);
            superColumn = ByteBuffer.wrap(buf);
        }

        IDiskAtomFilter predicate;
        AbstractType<?> comparator = ColumnFamily.getComparatorFor(keyspace, columnFamily, superColumn);
        if (version < MessagingService.VERSION_12)
        {
            SlicePredicate pred = new SlicePredicate();
            FBUtilities.deserialize(new TDeserializer(new TBinaryProtocol.Factory()), pred, dis);
            predicate = ThriftValidation.asIFilter(pred, comparator);
        }
        else
        {
            predicate = IDiskAtomFilter.Serializer.instance.deserialize(dis, version, comparator);
        }

        List<IndexExpression> rowFilter = null;
        if (version >= MessagingService.VERSION_11)
        {
            int filterCount = dis.readInt();
            rowFilter = new ArrayList<IndexExpression>(filterCount);
            for (int i = 0; i < filterCount; i++)
            {
                IndexExpression expr;
                if (version < MessagingService.VERSION_12)
                {
                    expr = new IndexExpression();
                    FBUtilities.deserialize(new TDeserializer(new TBinaryProtocol.Factory()), expr, dis);
                }
                else
                {
                    expr = new IndexExpression(ByteBufferUtil.readWithShortLength(dis),
                                               IndexOperator.findByValue(dis.readInt()),
                                               ByteBufferUtil.readWithShortLength(dis));
                }
                rowFilter.add(expr);
            }
        }
        AbstractBounds<RowPosition> range = AbstractBounds.serializer.deserialize(dis, version).toRowBounds();

        int maxResults = dis.readInt();
        boolean countCQL3Rows = false;
        boolean isPaging = false;
        if (version >= MessagingService.VERSION_11)
        {
            countCQL3Rows = dis.readBoolean();
            isPaging = dis.readBoolean();
        }
        return new RangeSliceCommand(keyspace, columnFamily, superColumn, predicate, range, rowFilter, maxResults, countCQL3Rows, isPaging);
    }

+++++++++++++++++++++++
    public RangeSliceCommand deserialize(DataInput in, int version) throws IOException
    {
        String keyspace = in.readUTF();
        String columnFamily = in.readUTF();

        long timestamp = version < MessagingService.VERSION_20 ? System.currentTimeMillis() : in.readLong();

        CFMetaData metadata = Schema.instance.getCFMetaData(keyspace, columnFamily);

        IDiskAtomFilter predicate;
        if (version < MessagingService.VERSION_20)
        {
            int scLength = in.readInt();
            ByteBuffer superColumn = null;
            if (scLength > 0)
            {
                byte[] buf = new byte[scLength];
                in.readFully(buf);
                superColumn = ByteBuffer.wrap(buf);
            }

            AbstractType<?> comparator;
            if (metadata.cfType == ColumnFamilyType.Super)
            {
                CompositeType type = (CompositeType)metadata.comparator;
                comparator = superColumn == null ? type.types.get(0) : type.types.get(1);
            }
            else
            {
                comparator = metadata.comparator;
            }

            predicate = IDiskAtomFilter.Serializer.instance.deserialize(in, version, comparator);

            if (metadata.cfType == ColumnFamilyType.Super)
                predicate = SuperColumns.fromSCFilter((CompositeType)metadata.comparator, superColumn, predicate);
        }
        else
        {
            predicate = IDiskAtomFilter.Serializer.instance.deserialize(in, version, metadata.comparator);
        }

        List<IndexExpression> rowFilter;
        int filterCount = in.readInt();
        rowFilter = new ArrayList<IndexExpression>(filterCount);
        for (int i = 0; i < filterCount; i++)
        {
            IndexExpression expr;
            expr = new IndexExpression(ByteBufferUtil.readWithShortLength(in),
                                       IndexOperator.findByValue(in.readInt()),
                                       ByteBufferUtil.readWithShortLength(in));
            rowFilter.add(expr);
        }
        AbstractBounds<RowPosition> range = AbstractBounds.serializer.deserialize(in, version).toRowBounds();

        int maxResults = in.readInt();
        boolean countCQL3Rows = in.readBoolean();
        boolean isPaging = in.readBoolean();
        return new RangeSliceCommand(keyspace, columnFamily, timestamp, predicate, range, rowFilter, maxResults, countCQL3Rows, isPaging);
    }

    public void serialize(ReadResponse response, DataOutput dos, int version) throws IOException
    {
        dos.writeInt(response.isDigestQuery() ? response.digest().remaining() : 0);
        ByteBuffer buffer = response.isDigestQuery() ? response.digest() : ByteBufferUtil.EMPTY_BYTE_BUFFER;
        ByteBufferUtil.write(buffer, dos);
        dos.writeBoolean(response.isDigestQuery());
        if (!response.isDigestQuery())
            Row.serializer.serialize(response.row(), dos, version);
    }

+++++++++++++++++++++++
    public void serialize(ReadResponse response, DataOutput out, int version) throws IOException
    {
        out.writeInt(response.isDigestQuery() ? response.digest().remaining() : 0);
        ByteBuffer buffer = response.isDigestQuery() ? response.digest() : ByteBufferUtil.EMPTY_BYTE_BUFFER;
        ByteBufferUtil.write(buffer, out);
        out.writeBoolean(response.isDigestQuery());
        if (!response.isDigestQuery())
            Row.serializer.serialize(response.row(), out, version);
    }

    public ReadResponse deserialize(DataInput dis, int version) throws IOException
    {
        byte[] digest = null;
        int digestSize = dis.readInt();
        if (digestSize > 0)
        {
            digest = new byte[digestSize];
            dis.readFully(digest, 0, digestSize);
        }
        boolean isDigest = dis.readBoolean();
        assert isDigest == digestSize > 0;

        Row row = null;
        if (!isDigest)
        {
            // This is coming from a remote host
            row = Row.serializer.deserialize(dis, version, IColumnSerializer.Flag.FROM_REMOTE, ArrayBackedSortedColumns.factory());
        }

        return isDigest ? new ReadResponse(ByteBuffer.wrap(digest)) : new ReadResponse(row);
    }

+++++++++++++++++++++++
    public ReadResponse deserialize(DataInput in, int version) throws IOException
    {
        byte[] digest = null;
        int digestSize = in.readInt();
        if (digestSize > 0)
        {
            digest = new byte[digestSize];
            in.readFully(digest, 0, digestSize);
        }
        boolean isDigest = in.readBoolean();
        assert isDigest == digestSize > 0;

        Row row = null;
        if (!isDigest)
        {
            // This is coming from a remote host
            row = Row.serializer.deserialize(in, version, ColumnSerializer.Flag.FROM_REMOTE);
        }

        return isDigest ? new ReadResponse(ByteBuffer.wrap(digest)) : new ReadResponse(row);
    }

    public void serialize(ReadCommand cmd, DataOutput dos, int version) throws IOException
    {
        SliceByNamesReadCommand command = (SliceByNamesReadCommand) cmd;
        dos.writeBoolean(command.isDigestQuery());
        dos.writeUTF(command.table);
        ByteBufferUtil.writeWithShortLength(command.key, dos);
        command.queryPath.serialize(dos);
        NamesQueryFilter.serializer.serialize(command.filter, dos, version);
    }

+++++++++++++++++++++++
    public void serialize(ReadCommand cmd, ByteBuffer superColumn, DataOutput out, int version) throws IOException
    {
        SliceByNamesReadCommand command = (SliceByNamesReadCommand) cmd;
        out.writeBoolean(command.isDigestQuery());
        out.writeUTF(command.ksName);
        ByteBufferUtil.writeWithShortLength(command.key, out);

        if (version < MessagingService.VERSION_20)
            new QueryPath(command.cfName, superColumn).serialize(out);
        else
            out.writeUTF(command.cfName);

        if (version >= MessagingService.VERSION_20)
            out.writeLong(cmd.timestamp);

        NamesQueryFilter.serializer.serialize(command.filter, out, version);
    }

    public long serializedSize(ReadCommand cmd, int version)
    {
        TypeSizes sizes = TypeSizes.NATIVE;
        SliceByNamesReadCommand command = (SliceByNamesReadCommand) cmd;
        int size = sizes.sizeof(command.isDigestQuery());
        int keySize = command.key.remaining();

        size += sizes.sizeof(command.table);
        size += sizes.sizeof((short)keySize) + keySize;
        size += command.queryPath.serializedSize(sizes);
        size += NamesQueryFilter.serializer.serializedSize(command.filter, version);
        return size;
+++++++++++++++++++++++
    public long serializedSize(ReadCommand cmd, ByteBuffer superColumn, int version)
    {
        TypeSizes sizes = TypeSizes.NATIVE;
        SliceByNamesReadCommand command = (SliceByNamesReadCommand) cmd;
        int size = sizes.sizeof(command.isDigestQuery());
        int keySize = command.key.remaining();

        size += sizes.sizeof(command.ksName);
        size += sizes.sizeof((short)keySize) + keySize;

        if (version < MessagingService.VERSION_20)
            size += new QueryPath(command.cfName, superColumn).serializedSize(sizes);
        else
            size += sizes.sizeof(command.cfName);

        if (version >= MessagingService.VERSION_20)
            size += sizes.sizeof(cmd.timestamp);

        size += NamesQueryFilter.serializer.serializedSize(command.filter, version);

        return size;
    public SliceByNamesReadCommand deserialize(DataInput dis, int version) throws IOException
    {
        boolean isDigest = dis.readBoolean();
        String table = dis.readUTF();
        ByteBuffer key = ByteBufferUtil.readWithShortLength(dis);
        QueryPath columnParent = QueryPath.deserialize(dis);

        AbstractType<?> comparator = ColumnFamily.getComparatorFor(table, columnParent.columnFamilyName, columnParent.superColumnName);
        NamesQueryFilter filter = NamesQueryFilter.serializer.deserialize(dis, version, comparator);
        SliceByNamesReadCommand command = new SliceByNamesReadCommand(table, key, columnParent, filter);
        command.setDigestQuery(isDigest);
        return command;
    }

+++++++++++++++++++++++
    public ReadCommand deserialize(DataInput in, int version) throws IOException
    {
        boolean isDigest = in.readBoolean();
        String keyspaceName = in.readUTF();
        ByteBuffer key = ByteBufferUtil.readWithShortLength(in);

        String cfName;
        ByteBuffer sc = null;
        if (version < MessagingService.VERSION_20)
        {
            QueryPath path = QueryPath.deserialize(in);
            cfName = path.columnFamilyName;
            sc = path.superColumnName;
        }
        else
        {
            cfName = in.readUTF();
        }

        long timestamp = version < MessagingService.VERSION_20 ? System.currentTimeMillis() : in.readLong();

        CFMetaData metadata = Schema.instance.getCFMetaData(keyspaceName, cfName);
        ReadCommand command;
        if (version < MessagingService.VERSION_20)
        {
            AbstractType<?> comparator;
            if (metadata.cfType == ColumnFamilyType.Super)
            {
                CompositeType type = (CompositeType)metadata.comparator;
                comparator = sc == null ? type.types.get(0) : type.types.get(1);
            }
            else
            {
                comparator = metadata.comparator;
            }

            IDiskAtomFilter filter = NamesQueryFilter.serializer.deserialize(in, version, comparator);

            if (metadata.cfType == ColumnFamilyType.Super)
                filter = SuperColumns.fromSCFilter((CompositeType)metadata.comparator, sc, filter);

            // Due to SC compat, it's possible we get back a slice filter at this point
            if (filter instanceof NamesQueryFilter)
                command = new SliceByNamesReadCommand(keyspaceName, key, cfName, timestamp, (NamesQueryFilter)filter);
            else
                command = new SliceFromReadCommand(keyspaceName, key, cfName, timestamp, (SliceQueryFilter)filter);
        }
        else
        {
            NamesQueryFilter filter = NamesQueryFilter.serializer.deserialize(in, version, metadata.comparator);
            command = new SliceByNamesReadCommand(keyspaceName, key, cfName, timestamp, filter);
        }

        command.setDigestQuery(isDigest);
        return command;
    }

    public void serialize(ReadCommand rm, DataOutput dos, int version) throws IOException
    {
        SliceFromReadCommand realRM = (SliceFromReadCommand)rm;
        dos.writeBoolean(realRM.isDigestQuery());
        dos.writeUTF(realRM.table);
        ByteBufferUtil.writeWithShortLength(realRM.key, dos);
        realRM.queryPath.serialize(dos);
        SliceQueryFilter.serializer.serialize(realRM.filter, dos, version);
    }

+++++++++++++++++++++++
    public void serialize(ReadCommand rm, ByteBuffer superColumn, DataOutput out, int version) throws IOException
    {
        SliceFromReadCommand realRM = (SliceFromReadCommand)rm;
        out.writeBoolean(realRM.isDigestQuery());
        out.writeUTF(realRM.ksName);
        ByteBufferUtil.writeWithShortLength(realRM.key, out);

        if (version < MessagingService.VERSION_20)
            new QueryPath(realRM.cfName, superColumn).serialize(out);
        else
            out.writeUTF(realRM.cfName);

        if (version >= MessagingService.VERSION_20)
            out.writeLong(realRM.timestamp);

        SliceQueryFilter.serializer.serialize(realRM.filter, out, version);
    }

    public long serializedSize(ReadCommand cmd, int version)
    {
        TypeSizes sizes = TypeSizes.NATIVE;
        SliceFromReadCommand command = (SliceFromReadCommand) cmd;
        int keySize = command.key.remaining();

        int size = sizes.sizeof(cmd.isDigestQuery()); // boolean
        size += sizes.sizeof(command.table);
        size += sizes.sizeof((short) keySize) + keySize;
        size += command.queryPath.serializedSize(sizes);
        size += SliceQueryFilter.serializer.serializedSize(command.filter, version);
        return size;
+++++++++++++++++++++++
    public long serializedSize(ReadCommand cmd, ByteBuffer superColumn, int version)
    {
        TypeSizes sizes = TypeSizes.NATIVE;
        SliceFromReadCommand command = (SliceFromReadCommand) cmd;
        int keySize = command.key.remaining();

        int size = sizes.sizeof(cmd.isDigestQuery()); // boolean
        size += sizes.sizeof(command.ksName);
        size += sizes.sizeof((short) keySize) + keySize;

        if (version < MessagingService.VERSION_20)
            size += new QueryPath(command.cfName, superColumn).serializedSize(sizes);
        else
            size += sizes.sizeof(command.cfName);

        if (version >= MessagingService.VERSION_20)
            size += sizes.sizeof(cmd.timestamp);

        size += SliceQueryFilter.serializer.serializedSize(command.filter, version);

        return size;
    public ReadCommand deserialize(DataInput dis, int version) throws IOException
    {
        boolean isDigest = dis.readBoolean();
        String table = dis.readUTF();
        ByteBuffer key = ByteBufferUtil.readWithShortLength(dis);
        QueryPath path = QueryPath.deserialize(dis);
        SliceQueryFilter filter = SliceQueryFilter.serializer.deserialize(dis, version);
        SliceFromReadCommand rm = new SliceFromReadCommand(table, key, path, filter);
        rm.setDigestQuery(isDigest);
        return rm;
    }

+++++++++++++++++++++++
    public ReadCommand deserialize(DataInput in, int version) throws IOException
    {
        boolean isDigest = in.readBoolean();
        String keyspaceName = in.readUTF();
        ByteBuffer key = ByteBufferUtil.readWithShortLength(in);

        String cfName;
        ByteBuffer sc = null;
        if (version < MessagingService.VERSION_20)
        {
            QueryPath path = QueryPath.deserialize(in);
            cfName = path.columnFamilyName;
            sc = path.superColumnName;
        }
        else
        {
            cfName = in.readUTF();
        }

        long timestamp = version < MessagingService.VERSION_20 ? System.currentTimeMillis() : in.readLong();

        CFMetaData metadata = Schema.instance.getCFMetaData(keyspaceName, cfName);
        SliceQueryFilter filter;
        if (version < MessagingService.VERSION_20)
        {
            filter = SliceQueryFilter.serializer.deserialize(in, version);

            if (metadata.cfType == ColumnFamilyType.Super)
                filter = SuperColumns.fromSCSliceFilter((CompositeType)metadata.comparator, sc, filter);
        }
        else
        {
            filter = SliceQueryFilter.serializer.deserialize(in, version);
        }

        ReadCommand command = new SliceFromReadCommand(keyspaceName, key, cfName, timestamp, filter);
        command.setDigestQuery(isDigest);
        return command;
    }

    public void serialize(SnapshotCommand snapshot_command, DataOutput dos, int version) throws IOException
    {
        dos.writeUTF(snapshot_command.keyspace);
        dos.writeUTF(snapshot_command.column_family);
        dos.writeUTF(snapshot_command.snapshot_name);
        dos.writeBoolean(snapshot_command.clear_snapshot);
    }

+++++++++++++++++++++++
    public void serialize(SnapshotCommand snapshot_command, DataOutput out, int version) throws IOException
    {
        out.writeUTF(snapshot_command.keyspace);
        out.writeUTF(snapshot_command.column_family);
        out.writeUTF(snapshot_command.snapshot_name);
        out.writeBoolean(snapshot_command.clear_snapshot);
    }

    public SnapshotCommand deserialize(DataInput dis, int version) throws IOException
    {
        String keyspace = dis.readUTF();
        String column_family = dis.readUTF();
        String snapshot_name = dis.readUTF();
        boolean clear_snapshot = dis.readBoolean();
        return new SnapshotCommand(keyspace, column_family, snapshot_name, clear_snapshot);
    }

+++++++++++++++++++++++
    public SnapshotCommand deserialize(DataInput in, int version) throws IOException
    {
        String keyspace = in.readUTF();
        String column_family = in.readUTF();
        String snapshot_name = in.readUTF();
        boolean clear_snapshot = in.readBoolean();
        return new SnapshotCommand(keyspace, column_family, snapshot_name, clear_snapshot);
    }

    public void serialize(Truncation t, DataOutput dos, int version) throws IOException
    {
        dos.writeUTF(t.keyspace);
        dos.writeUTF(t.columnFamily);
    }

+++++++++++++++++++++++
    public void serialize(Truncation t, DataOutput out, int version) throws IOException
    {
        out.writeUTF(t.keyspace);
        out.writeUTF(t.columnFamily);
    }

    public Truncation deserialize(DataInput dis, int version) throws IOException
    {
        String keyspace = dis.readUTF();
        String columnFamily = dis.readUTF();
        return new Truncation(keyspace, columnFamily);
    }

+++++++++++++++++++++++
    public Truncation deserialize(DataInput in, int version) throws IOException
    {
        String keyspace = in.readUTF();
        String columnFamily = in.readUTF();
        return new Truncation(keyspace, columnFamily);
    }

    public void serialize(DataOutput dos) throws IOException
    {
        assert !"".equals(columnFamilyName);
        assert superColumnName == null || superColumnName.remaining() > 0;
        assert columnName == null || columnName.remaining() > 0;
        dos.writeUTF(columnFamilyName == null ? "" : columnFamilyName);
        ByteBufferUtil.writeWithShortLength(superColumnName == null ? ByteBufferUtil.EMPTY_BYTE_BUFFER : superColumnName, dos);
        ByteBufferUtil.writeWithShortLength(columnName == null ? ByteBufferUtil.EMPTY_BYTE_BUFFER : columnName, dos);
    }

+++++++++++++++++++++++
    public void serialize(DataOutput out) throws IOException
    {
        assert !"".equals(columnFamilyName);
        assert superColumnName == null || superColumnName.remaining() > 0;
        assert columnName == null || columnName.remaining() > 0;
        out.writeUTF(columnFamilyName == null ? "" : columnFamilyName);
        ByteBufferUtil.writeWithShortLength(superColumnName == null ? ByteBufferUtil.EMPTY_BYTE_BUFFER : superColumnName, out);
        ByteBufferUtil.writeWithShortLength(columnName == null ? ByteBufferUtil.EMPTY_BYTE_BUFFER : columnName, out);
    }

    public abstract ByteBuffer serialize(List<Pair<ByteBuffer, IColumn>> columns);

    @Override
+++++++++++++++++++++++
    public abstract ByteBuffer serialize(List<Pair<ByteBuffer, Column>> columns);

    @Override
    public ByteBuffer serialize(List<Pair<ByteBuffer, IColumn>> columns)
    {
        columns = enforceLimit(columns);

        List<ByteBuffer> bbs = new ArrayList<ByteBuffer>(columns.size());
        int size = 0;
        for (Pair<ByteBuffer, IColumn> p : columns)
        {
            bbs.add(p.right.value());
            size += 2 + p.right.value().remaining();
        }
        return pack(bbs, columns.size(), size);
+++++++++++++++++++++++
    public ByteBuffer serialize(List<Pair<ByteBuffer, Column>> columns)
    {
        List<ByteBuffer> bbs = new ArrayList<ByteBuffer>(columns.size());
        int size = 0;
        for (Pair<ByteBuffer, Column> p : columns)
        {
            bbs.add(p.right.value());
            size += 2 + p.right.value().remaining();
        }
        return pack(bbs, columns.size(), size);
    public ByteBuffer serialize(List<Pair<ByteBuffer, IColumn>> columns)
    {
        columns = enforceLimit(columns);

        List<ByteBuffer> bbs = new ArrayList<ByteBuffer>(2 * columns.size());
        int size = 0;
        for (Pair<ByteBuffer, IColumn> p : columns)
        {
            bbs.add(p.left);
            bbs.add(p.right.value());
            size += 4 + p.left.remaining() + p.right.value().remaining();
        }
        return pack(bbs, columns.size(), size);
+++++++++++++++++++++++
    public ByteBuffer serialize(List<Pair<ByteBuffer, Column>> columns)
    {
        List<ByteBuffer> bbs = new ArrayList<ByteBuffer>(2 * columns.size());
        int size = 0;
        for (Pair<ByteBuffer, Column> p : columns)
        {
            bbs.add(p.left);
            bbs.add(p.right.value());
            size += 4 + p.left.remaining() + p.right.value().remaining();
        }
        return pack(bbs, columns.size(), size);
    public ByteBuffer serialize(List<Pair<ByteBuffer, IColumn>> columns)
    {
        columns = enforceLimit(columns);

        List<ByteBuffer> bbs = new ArrayList<ByteBuffer>(columns.size());
        int size = 0;
        for (Pair<ByteBuffer, IColumn> p : columns)
        {
            bbs.add(p.left);
            size += 2 + p.left.remaining();
        }
        return pack(bbs, columns.size(), size);
+++++++++++++++++++++++
    public ByteBuffer serialize(List<Pair<ByteBuffer, Column>> columns)
    {
        List<ByteBuffer> bbs = new ArrayList<ByteBuffer>(columns.size());
        int size = 0;
        for (Pair<ByteBuffer, Column> p : columns)
        {
            bbs.add(p.left);
            size += 2 + p.left.remaining();
        }
        return pack(bbs, columns.size(), size);
    public void serialize(EndpointState epState, DataOutput dos, int version) throws IOException
    {
        /* serialize the HeartBeatState */
        HeartBeatState hbState = epState.getHeartBeatState();
        HeartBeatState.serializer.serialize(hbState, dos, version);

        /* serialize the map of ApplicationState objects */
        int size = epState.applicationState.size();
        dos.writeInt(size);
        for (Map.Entry<ApplicationState, VersionedValue> entry : epState.applicationState.entrySet())
        {
            VersionedValue value = entry.getValue();
            dos.writeInt(entry.getKey().ordinal());
            VersionedValue.serializer.serialize(value, dos, version);
        }
    }

+++++++++++++++++++++++
    public void serialize(EndpointState epState, DataOutput out, int version) throws IOException
    {
        /* serialize the HeartBeatState */
        HeartBeatState hbState = epState.getHeartBeatState();
        HeartBeatState.serializer.serialize(hbState, out, version);

        /* serialize the map of ApplicationState objects */
        int size = epState.applicationState.size();
        out.writeInt(size);
        for (Map.Entry<ApplicationState, VersionedValue> entry : epState.applicationState.entrySet())
        {
            VersionedValue value = entry.getValue();
            out.writeInt(entry.getKey().ordinal());
            VersionedValue.serializer.serialize(value, out, version);
        }
    }

    public EndpointState deserialize(DataInput dis, int version) throws IOException
    {
        HeartBeatState hbState = HeartBeatState.serializer.deserialize(dis, version);
        EndpointState epState = new EndpointState(hbState);

        int appStateSize = dis.readInt();
        for ( int i = 0; i < appStateSize; ++i )
        {
            int key = dis.readInt();
            VersionedValue value = VersionedValue.serializer.deserialize(dis, version);
            epState.addApplicationState(Gossiper.STATES[key], value);
        }
        return epState;
    }

+++++++++++++++++++++++
    public EndpointState deserialize(DataInput in, int version) throws IOException
    {
        HeartBeatState hbState = HeartBeatState.serializer.deserialize(in, version);
        EndpointState epState = new EndpointState(hbState);

        int appStateSize = in.readInt();
        for (int i = 0; i < appStateSize; ++i)
        {
            int key = in.readInt();
            VersionedValue value = VersionedValue.serializer.deserialize(in, version);
            epState.addApplicationState(Gossiper.STATES[key], value);
        }
        return epState;
    }

    public void serialize(GossipDigest gDigest, DataOutput dos, int version) throws IOException
    {
        CompactEndpointSerializationHelper.serialize(gDigest.endpoint, dos);
        dos.writeInt(gDigest.generation);
        dos.writeInt(gDigest.maxVersion);
    }

+++++++++++++++++++++++
    public void serialize(GossipDigest gDigest, DataOutput out, int version) throws IOException
    {
        CompactEndpointSerializationHelper.serialize(gDigest.endpoint, out);
        out.writeInt(gDigest.generation);
        out.writeInt(gDigest.maxVersion);
    }

    public GossipDigest deserialize(DataInput dis, int version) throws IOException
    {
        InetAddress endpoint = CompactEndpointSerializationHelper.deserialize(dis);
        int generation = dis.readInt();
        int maxVersion = dis.readInt();
        return new GossipDigest(endpoint, generation, maxVersion);
    }

+++++++++++++++++++++++
    public GossipDigest deserialize(DataInput in, int version) throws IOException
    {
        InetAddress endpoint = CompactEndpointSerializationHelper.deserialize(in);
        int generation = in.readInt();
        int maxVersion = in.readInt();
        return new GossipDigest(endpoint, generation, maxVersion);
    }

    public void serialize(GossipDigestAck gDigestAckMessage, DataOutput dos, int version) throws IOException
    {
        GossipDigestSerializationHelper.serialize(gDigestAckMessage.gDigestList, dos, version);
        if (version < MessagingService.VERSION_12)
            dos.writeBoolean(true); // 0.6 compatibility
        dos.writeInt(gDigestAckMessage.epStateMap.size());
        for (Map.Entry<InetAddress, EndpointState> entry : gDigestAckMessage.epStateMap.entrySet())
        {
            InetAddress ep = entry.getKey();
            CompactEndpointSerializationHelper.serialize(ep, dos);
            EndpointState.serializer.serialize(entry.getValue(), dos, version);
        }
    }

+++++++++++++++++++++++
    public void serialize(GossipDigestAck gDigestAckMessage, DataOutput out, int version) throws IOException
    {
        GossipDigestSerializationHelper.serialize(gDigestAckMessage.gDigestList, out, version);
        out.writeInt(gDigestAckMessage.epStateMap.size());
        for (Map.Entry<InetAddress, EndpointState> entry : gDigestAckMessage.epStateMap.entrySet())
        {
            InetAddress ep = entry.getKey();
            CompactEndpointSerializationHelper.serialize(ep, out);
            EndpointState.serializer.serialize(entry.getValue(), out, version);
        }
    }

    public long serializedSize(GossipDigestAck ack, int version)
    {
        int size = GossipDigestSerializationHelper.serializedSize(ack.gDigestList, version);
        if (version < MessagingService.VERSION_12)
            size += TypeSizes.NATIVE.sizeof(true);
        size += TypeSizes.NATIVE.sizeof(ack.epStateMap.size());
        for (Map.Entry<InetAddress, EndpointState> entry : ack.epStateMap.entrySet())
            size += CompactEndpointSerializationHelper.serializedSize(entry.getKey())
                  + EndpointState.serializer.serializedSize(entry.getValue(), version);
        return size;
+++++++++++++++++++++++
    public long serializedSize(GossipDigestAck ack, int version)
    {
        int size = GossipDigestSerializationHelper.serializedSize(ack.gDigestList, version);
        size += TypeSizes.NATIVE.sizeof(ack.epStateMap.size());
        for (Map.Entry<InetAddress, EndpointState> entry : ack.epStateMap.entrySet())
            size += CompactEndpointSerializationHelper.serializedSize(entry.getKey())
                    + EndpointState.serializer.serializedSize(entry.getValue(), version);
        return size;
    public GossipDigestAck deserialize(DataInput dis, int version) throws IOException
    {
        List<GossipDigest> gDigestList = GossipDigestSerializationHelper.deserialize(dis, version);
        if (version < MessagingService.VERSION_12)
            dis.readBoolean(); // 0.6 compatibility
        int size = dis.readInt();
        Map<InetAddress, EndpointState> epStateMap = new HashMap<InetAddress, EndpointState>(size);

        for (int i = 0; i < size; ++i)
        {
            InetAddress ep = CompactEndpointSerializationHelper.deserialize(dis);
            EndpointState epState = EndpointState.serializer.deserialize(dis, version);
            epStateMap.put(ep, epState);
        }
        return new GossipDigestAck(gDigestList, epStateMap);
    }

+++++++++++++++++++++++
    public GossipDigestAck deserialize(DataInput in, int version) throws IOException
    {
        List<GossipDigest> gDigestList = GossipDigestSerializationHelper.deserialize(in, version);
        int size = in.readInt();
        Map<InetAddress, EndpointState> epStateMap = new HashMap<InetAddress, EndpointState>(size);

        for (int i = 0; i < size; ++i)
        {
            InetAddress ep = CompactEndpointSerializationHelper.deserialize(in);
            EndpointState epState = EndpointState.serializer.deserialize(in, version);
            epStateMap.put(ep, epState);
        }
        return new GossipDigestAck(gDigestList, epStateMap);
    }

    public void serialize(GossipDigestAck2 ack2, DataOutput dos, int version) throws IOException
    {
        dos.writeInt(ack2.epStateMap.size());
        for (Map.Entry<InetAddress, EndpointState> entry : ack2.epStateMap.entrySet())
        {
            InetAddress ep = entry.getKey();
            CompactEndpointSerializationHelper.serialize(ep, dos);
            EndpointState.serializer.serialize(entry.getValue(), dos, version);
        }
    }

+++++++++++++++++++++++
    public void serialize(GossipDigestAck2 ack2, DataOutput out, int version) throws IOException
    {
        out.writeInt(ack2.epStateMap.size());
        for (Map.Entry<InetAddress, EndpointState> entry : ack2.epStateMap.entrySet())
        {
            InetAddress ep = entry.getKey();
            CompactEndpointSerializationHelper.serialize(ep, out);
            EndpointState.serializer.serialize(entry.getValue(), out, version);
        }
    }

    public GossipDigestAck2 deserialize(DataInput dis, int version) throws IOException
    {
        int size = dis.readInt();
        Map<InetAddress, EndpointState> epStateMap = new HashMap<InetAddress, EndpointState>(size);

        for (int i = 0; i < size; ++i)
        {
            InetAddress ep = CompactEndpointSerializationHelper.deserialize(dis);
            EndpointState epState = EndpointState.serializer.deserialize(dis, version);
            epStateMap.put(ep, epState);
        }
        return new GossipDigestAck2(epStateMap);
    }

+++++++++++++++++++++++
    public GossipDigestAck2 deserialize(DataInput in, int version) throws IOException
    {
        int size = in.readInt();
        Map<InetAddress, EndpointState> epStateMap = new HashMap<InetAddress, EndpointState>(size);

        for (int i = 0; i < size; ++i)
        {
            InetAddress ep = CompactEndpointSerializationHelper.deserialize(in);
            EndpointState epState = EndpointState.serializer.deserialize(in, version);
            epStateMap.put(ep, epState);
        }
        return new GossipDigestAck2(epStateMap);
    }

    public void serialize(GossipDigestSyn gDigestSynMessage, DataOutput dos, int version) throws IOException
    {
        dos.writeUTF(gDigestSynMessage.clusterId);
        if (version >= MessagingService.VERSION_12)
            dos.writeUTF(gDigestSynMessage.partioner);
        GossipDigestSerializationHelper.serialize(gDigestSynMessage.gDigests, dos, version);
    }

+++++++++++++++++++++++
    public void serialize(GossipDigestSyn gDigestSynMessage, DataOutput out, int version) throws IOException
    {
        out.writeUTF(gDigestSynMessage.clusterId);
        out.writeUTF(gDigestSynMessage.partioner);
        GossipDigestSerializationHelper.serialize(gDigestSynMessage.gDigests, out, version);
    }

    public long serializedSize(GossipDigestSyn syn, int version)
    {
        long size = TypeSizes.NATIVE.sizeof(syn.clusterId);
        if (version >= MessagingService.VERSION_12)
            size += TypeSizes.NATIVE.sizeof(syn.partioner);
        size += GossipDigestSerializationHelper.serializedSize(syn.gDigests, version);
        return size;
+++++++++++++++++++++++
    public long serializedSize(GossipDigestSyn syn, int version)
    {
        long size = TypeSizes.NATIVE.sizeof(syn.clusterId);
        size += TypeSizes.NATIVE.sizeof(syn.partioner);
        size += GossipDigestSerializationHelper.serializedSize(syn.gDigests, version);
        return size;
    public GossipDigestSyn deserialize(DataInput dis, int version) throws IOException
    {
        String clusterId = dis.readUTF();
        String partioner = null;
        if (version >= MessagingService.VERSION_12)
            partioner = dis.readUTF();
        List<GossipDigest> gDigests = GossipDigestSerializationHelper.deserialize(dis, version);
        return new GossipDigestSyn(clusterId, partioner, gDigests);
    }

+++++++++++++++++++++++
    public GossipDigestSyn deserialize(DataInput in, int version) throws IOException
    {
        String clusterId = in.readUTF();
        String partioner = null;
        partioner = in.readUTF();
        List<GossipDigest> gDigests = GossipDigestSerializationHelper.deserialize(in, version);
        return new GossipDigestSyn(clusterId, partioner, gDigests);
    }

    static void serialize(List<GossipDigest> gDigestList, DataOutput dos, int version) throws IOException
    {
        dos.writeInt(gDigestList.size());
        for (GossipDigest gDigest : gDigestList)
            GossipDigest.serializer.serialize(gDigest, dos, version);
    }

+++++++++++++++++++++++
    static void serialize(List<GossipDigest> gDigestList, DataOutput out, int version) throws IOException
    {
        out.writeInt(gDigestList.size());
        for (GossipDigest gDigest : gDigestList)
            GossipDigest.serializer.serialize(gDigest, out, version);
    }

    static List<GossipDigest> deserialize(DataInput dis, int version) throws IOException
    {
        int size = dis.readInt();
        List<GossipDigest> gDigests = new ArrayList<GossipDigest>(size);
        for (int i = 0; i < size; ++i)
            gDigests.add(GossipDigest.serializer.deserialize(dis, version));
        return gDigests;
    }
    
+++++++++++++++++++++++
    static List<GossipDigest> deserialize(DataInput in, int version) throws IOException
    {
        int size = in.readInt();
        List<GossipDigest> gDigests = new ArrayList<GossipDigest>(size);
        for (int i = 0; i < size; ++i)
            gDigests.add(GossipDigest.serializer.deserialize(in, version));
        return gDigests;
    }

    public void serialize(HeartBeatState hbState, DataOutput dos, int version) throws IOException
    {
        dos.writeInt(hbState.getGeneration());
        dos.writeInt(hbState.getHeartBeatVersion());
    }

+++++++++++++++++++++++
    public void serialize(HeartBeatState hbState, DataOutput out, int version) throws IOException
    {
        out.writeInt(hbState.getGeneration());
        out.writeInt(hbState.getHeartBeatVersion());
    }

    public HeartBeatState deserialize(DataInput dis, int version) throws IOException
    {
        return new HeartBeatState(dis.readInt(), dis.readInt());
    }

+++++++++++++++++++++++
    public HeartBeatState deserialize(DataInput in, int version) throws IOException
    {
        return new HeartBeatState(in.readInt(), in.readInt());
    }

    public static void serialize(IPartitioner partitioner, Collection<Token> tokens, DataOutput dos) throws IOException
    {
        for (Token<?> token : tokens)
        {
            byte[] bintoken = partitioner.getTokenFactory().toByteArray(token).array();
            dos.writeInt(bintoken.length);
            dos.write(bintoken);
        }
        dos.writeInt(0);
    }

+++++++++++++++++++++++
    public static void serialize(IPartitioner partitioner, Collection<Token> tokens, DataOutput out) throws IOException
    {
        for (Token<?> token : tokens)
        {
            byte[] bintoken = partitioner.getTokenFactory().toByteArray(token).array();
            out.writeInt(bintoken.length);
            out.write(bintoken);
        }
        out.writeInt(0);
    }

    public static Collection<Token> deserialize(IPartitioner partitioner, DataInput dis) throws IOException
    {
        Collection<Token> tokens = new ArrayList<Token>();
        while (true)
        {
            int size = dis.readInt();
            if (size < 1)
                break;
            logger.trace("Reading token of {} bytes", size);
            byte[] bintoken = new byte[size];
            dis.readFully(bintoken);
            tokens.add(partitioner.getTokenFactory().fromByteArray(ByteBuffer.wrap(bintoken)));
        }
        return tokens;
+++++++++++++++++++++++
    public static Collection<Token> deserialize(IPartitioner partitioner, DataInput in) throws IOException
    {
        Collection<Token> tokens = new ArrayList<Token>();
        while (true)
        {
            int size = in.readInt();
            if (size < 1)
                break;
            logger.trace("Reading token of {} bytes", size);
            byte[] bintoken = new byte[size];
            in.readFully(bintoken);
            tokens.add(partitioner.getTokenFactory().fromByteArray(ByteBuffer.wrap(bintoken)));
        }
        return tokens;
    }

    public static void serialize(InetAddress endpoint, DataOutput dos) throws IOException
    {
        byte[] buf = endpoint.getAddress();
        dos.writeByte(buf.length);
        dos.write(buf);
    }

+++++++++++++++++++++++
    public static void serialize(InetAddress endpoint, DataOutput out) throws IOException
    {
        byte[] buf = endpoint.getAddress();
        out.writeByte(buf.length);
        out.write(buf);
    }

    public static InetAddress deserialize(DataInput dis) throws IOException
    {
        byte[] bytes = new byte[dis.readByte()];
        dis.readFully(bytes, 0, bytes.length);
        return InetAddress.getByAddress(bytes);
    }

+++++++++++++++++++++++
    public static InetAddress deserialize(DataInput in) throws IOException
    {
        byte[] bytes = new byte[in.readByte()];
        in.readFully(bytes, 0, bytes.length);
        return InetAddress.getByAddress(bytes);
    }

    public void serialize(BloomFilter bf, DataOutput dos) throws IOException
    {
        dos.writeInt(bf.hashCount);
        bf.bitset.serialize(dos);
    }

+++++++++++++++++++++++
    public void serialize(BloomFilter bf, DataOutput out) throws IOException
    {
        out.writeInt(bf.hashCount);
        bf.bitset.serialize(out);
    }

    public BloomFilter deserialize(DataInput dis, boolean offheap) throws IOException
    {
        int hashes = dis.readInt();
        IBitSet bs = offheap ? OffHeapBitSet.deserialize(dis) : OpenBitSet.deserialize(dis);
        return createFilter(hashes, bs);
    }

+++++++++++++++++++++++
    public BloomFilter deserialize(DataInput in, boolean offheap) throws IOException
    {
        int hashes = in.readInt();
        IBitSet bs = offheap ? OffHeapBitSet.deserialize(in) : OpenBitSet.deserialize(in);
        return createFilter(hashes, bs);
    }

    public static <T> byte[] serialize(T object, IVersionedSerializer<T> serializer, int version) throws IOException
    {
        int size = (int) serializer.serializedSize(object, version);
        DataOutputBuffer buffer = new DataOutputBuffer(size);
        serializer.serialize(object, buffer, version);
        assert buffer.getLength() == size && buffer.getData().length == size
               : String.format("Final buffer length %s to accommodate data size of %s (predicted %s) for %s",
                               buffer.getData().length, buffer.getLength(), size, object);
        return buffer.getData();
    }

+++++++++++++++++++++++
    public static <T> byte[] serialize(T object, IVersionedSerializer<T> serializer, int version)
    {
        try
        {
            int size = (int) serializer.serializedSize(object, version);
            DataOutputBuffer buffer = new DataOutputBuffer(size);
            serializer.serialize(object, buffer, version);
            assert buffer.getLength() == size && buffer.getData().length == size
                : String.format("Final buffer length %s to accommodate data size of %s (predicted %s) for %s",
                        buffer.getData().length, buffer.getLength(), size, object);
            return buffer.getData();
        }
        catch (IOException e)
        {
            // We're doing in-memory serialization...
            throw new AssertionError(e);
        }
    }

    public static void serialize(IFilter bf, DataOutput output, Type type) throws IOException
    {
        switch (type)
        {
            case SHA:
                LegacyBloomFilter.serializer.serialize((LegacyBloomFilter) bf, output);
                break;
            case MURMUR2:
                Murmur2BloomFilter.serializer.serialize((Murmur2BloomFilter) bf, output);
                break;
            default:
                Murmur3BloomFilter.serializer.serialize((Murmur3BloomFilter) bf, output);
                break;
        }
    }

+++++++++++++++++++++++
    public static void serialize(IFilter bf, DataOutput output) throws IOException
    {
        Murmur3BloomFilter.serializer.serialize((Murmur3BloomFilter) bf, output);
    }

    public static IFilter deserialize(DataInput input, Type type, boolean offheap) throws IOException
    {
        switch (type)
        {
            case SHA:
                return LegacyBloomFilter.serializer.deserialize(input);
            case MURMUR2:
                return Murmur2BloomFilter.serializer.deserialize(input, offheap);
            default:
                return Murmur3BloomFilter.serializer.deserialize(input, offheap);
        }
    }

+++++++++++++++++++++++
    public static IFilter deserialize(DataInput input, boolean offheap) throws IOException
    {
        return Murmur3BloomFilter.serializer.deserialize(input, offheap);
    }

    /**
     * @return A BloomFilter with the lowest practical false positive
     *         probability for the given number of elements.
     */
    public static OffHeapBitSet deserialize(DataInput dis) throws IOException
    {
        long byteCount = dis.readInt() * 8L;
        Memory memory = RefCountedMemory.allocate(byteCount);
        for (long i = 0; i < byteCount;)
        {
            long v = dis.readLong();
            memory.setByte(i++, (byte) (v >>> 0));
            memory.setByte(i++, (byte) (v >>> 8));
            memory.setByte(i++, (byte) (v >>> 16));
            memory.setByte(i++, (byte) (v >>> 24));
            memory.setByte(i++, (byte) (v >>> 32));
            memory.setByte(i++, (byte) (v >>> 40));
            memory.setByte(i++, (byte) (v >>> 48));
            memory.setByte(i++, (byte) (v >>> 56));
        }
        return new OffHeapBitSet(memory);
    }

+++++++++++++++++++++++
    public static OffHeapBitSet deserialize(DataInput in) throws IOException
    {
        long byteCount = in.readInt() * 8L;
        Memory memory = RefCountedMemory.allocate(byteCount);
        for (long i = 0; i < byteCount;)
        {
            long v = in.readLong();
            memory.setByte(i++, (byte) (v >>> 0));
            memory.setByte(i++, (byte) (v >>> 8));
            memory.setByte(i++, (byte) (v >>> 16));
            memory.setByte(i++, (byte) (v >>> 24));
            memory.setByte(i++, (byte) (v >>> 32));
            memory.setByte(i++, (byte) (v >>> 40));
            memory.setByte(i++, (byte) (v >>> 48));
            memory.setByte(i++, (byte) (v >>> 56));
        }
        return new OffHeapBitSet(memory);
    }

    public void serialize(DataOutput dos) throws IOException
    {
        dos.writeInt((int) (bytes.size() / 8));
        for (long i = 0; i < bytes.size();)
        {
            long value = ((bytes.getByte(i++) & 0xff) << 0) 
                       + ((bytes.getByte(i++) & 0xff) << 8)
                       + ((bytes.getByte(i++) & 0xff) << 16)
                       + ((long) (bytes.getByte(i++) & 0xff) << 24)
                       + ((long) (bytes.getByte(i++) & 0xff) << 32)
                       + ((long) (bytes.getByte(i++) & 0xff) << 40)
                       + ((long) (bytes.getByte(i++) & 0xff) << 48)
                       + ((long) bytes.getByte(i++) << 56);
            dos.writeLong(value);
        }
    }

+++++++++++++++++++++++
    public void serialize(DataOutput out) throws IOException
    {
        out.writeInt((int) (bytes.size() / 8));
        for (long i = 0; i < bytes.size();)
        {
            long value = ((bytes.getByte(i++) & 0xff) << 0) 
                       + ((bytes.getByte(i++) & 0xff) << 8)
                       + ((bytes.getByte(i++) & 0xff) << 16)
                       + ((long) (bytes.getByte(i++) & 0xff) << 24)
                       + ((long) (bytes.getByte(i++) & 0xff) << 32)
                       + ((long) (bytes.getByte(i++) & 0xff) << 40)
                       + ((long) (bytes.getByte(i++) & 0xff) << 48)
                       + ((long) bytes.getByte(i++) << 56);
            out.writeLong(value);
        }
    }

  public void serialize(DataOutput dos) throws IOException {
    int bitLength = getNumWords();
    int pageSize = getPageSize();
    int pageCount = getPageCount();

    dos.writeInt(bitLength);
    for (int p = 0; p < pageCount; p++) {
      long[] bits = getPage(p);
      for (int i = 0; i < pageSize && bitLength-- > 0; i++) {
        dos.writeLong(bits[i]);
      }
    }
}

+++++++++++++++++++++++
  public void serialize(DataOutput out) throws IOException {
    int bitLength = getNumWords();
    int pageSize = getPageSize();
    int pageCount = getPageCount();

    out.writeInt(bitLength);
    for (int p = 0; p < pageCount; p++) {
      long[] bits = getPage(p);
      for (int i = 0; i < pageSize && bitLength-- > 0; i++) {
        out.writeLong(bits[i]);
      }
    }
}

  public static OpenBitSet deserialize(DataInput dis) throws IOException {
    long bitLength = dis.readInt();

    OpenBitSet bs = new OpenBitSet(bitLength << 6);
    int pageSize = bs.getPageSize();
    int pageCount = bs.getPageCount();

    for (int p = 0; p < pageCount; p++) {
      long[] bits = bs.getPage(p);
      for (int i = 0; i < pageSize && bitLength-- > 0; i++)
        bits[i] = dis.readLong();
    }
    return bs;
+++++++++++++++++++++++
  public static OpenBitSet deserialize(DataInput in) throws IOException {
    long bitLength = in.readInt();

    OpenBitSet bs = new OpenBitSet(bitLength << 6);
    int pageSize = bs.getPageSize();
    int pageCount = bs.getPageCount();

    for (int p = 0; p < pageCount; p++) {
      long[] bits = bs.getPage(p);
      for (int i = 0; i < pageSize && bitLength-- > 0; i++)
        bits[i] = in.readLong();
    }
    return bs;
    protected void testSerializeDeserialize(ColumnDefinition cd) throws Exception
    {
        ColumnDefinition newCd = ColumnDefinition.fromThrift(cd.toThrift());
        assert cd != newCd;
        assert cd.hashCode() == newCd.hashCode();
        assert cd.equals(newCd);
+++++++++++++++++++++++
    protected void testSerializeDeserialize(ColumnDefinition cd) throws Exception
    {
        ColumnDefinition newCd = ColumnDefinition.fromThrift(cd.toThrift(), false);
        assert cd != newCd;
        assert cd.hashCode() == newCd.hashCode();
        assert cd.equals(newCd);
    private ReadCommand serializeAndDeserializeReadMessage(ReadCommand rm) throws IOException
    {
        ReadCommandSerializer rms = ReadCommand.serializer;
        DataOutputBuffer dos = new DataOutputBuffer();
        ByteArrayInputStream bis;

        rms.serialize(rm, dos, MessagingService.current_version);
        bis = new ByteArrayInputStream(dos.getData(), 0, dos.getLength());
        return rms.deserialize(new DataInputStream(bis), MessagingService.current_version);
    }

    @Test
+++++++++++++++++++++++
    private ReadCommand serializeAndDeserializeReadMessage(ReadCommand rm) throws IOException
    {
        ReadCommandSerializer rms = ReadCommand.serializer;
        DataOutputBuffer out = new DataOutputBuffer();
        ByteArrayInputStream bis;

        rms.serialize(rm, out, MessagingService.current_version);
        bis = new ByteArrayInputStream(out.getData(), 0, out.getLength());
        return rms.deserialize(new DataInputStream(bis), MessagingService.current_version);
    }

    @Test
    private void populateAndReserialize(IBitSet bs) throws IOException
    {
        for (long i = 0; i < bs.capacity(); i++)
            if (random.nextBoolean())
                bs.set(i);

        DataOutputBuffer dos = new DataOutputBuffer();
        bs.serialize(dos);
        DataInputStream dis = new DataInputStream(new ByteArrayInputStream(dos.getData()));
        OffHeapBitSet newbs = OffHeapBitSet.deserialize(dis);
        compare(bs, newbs);
    }

+++++++++++++++++++++++
    private void populateAndReserialize(IBitSet bs) throws IOException
    {
        for (long i = 0; i < bs.capacity(); i++)
            if (random.nextBoolean())
                bs.set(i);

        DataOutputBuffer out = new DataOutputBuffer();
        bs.serialize(out);
        DataInputStream in = new DataInputStream(new ByteArrayInputStream(out.getData()));
        OffHeapBitSet newbs = OffHeapBitSet.deserialize(in);
        compare(bs, newbs);
    }

add: 166, delete: 113, change: 594, unhandled: 0 size_exceptions: 71 size_serialize: 68
-----------cassandra-2.0.0 vs cassandra-2.0.0-beta1-----------
    public static Map<String, CFMetaData> deserializeColumnFamilies(Row row)
    {
        if (row.cf == null)
            return Collections.emptyMap();

        Map<String, CFMetaData> cfms = new HashMap<String, CFMetaData>();
        UntypedResultSet results = QueryProcessor.resultify("SELECT * FROM system.schema_columnfamilies", row);
        for (UntypedResultSet.Row result : results)
        {
            CFMetaData cfm = CFMetaData.fromSchema(result);
            cfms.put(cfm.cfName, cfm);
        }

        for (CFMetaData cfm : cfms.values())
        {
            Row columnRow = SystemKeyspace.readSchemaRow(SystemKeyspace.SCHEMA_COLUMNS_CF, cfm.ksName, cfm.cfName);
            // This may replace some existing definition coming from the old key, column and
            // value aliases. But that's what we want (see CFMetaData.fromSchemaNoColumnsNoTriggers).
            for (ColumnDefinition cd : ColumnDefinition.fromSchema(columnRow, cfm))
                cfm.addOrReplaceColumnDefinition(cd);
            cfm.rebuild();
        }

        return cfms;
+++++++++++++++++++++++
    public static Map<String, CFMetaData> deserializeColumnFamilies(Row row)
    {
        if (row.cf == null)
            return Collections.emptyMap();

        Map<String, CFMetaData> cfms = new HashMap<String, CFMetaData>();
        UntypedResultSet results = QueryProcessor.resultify("SELECT * FROM system.schema_columnfamilies", row);
        for (UntypedResultSet.Row result : results)
        {
            CFMetaData cfm = CFMetaData.fromSchema(result);
            cfms.put(cfm.cfName, cfm);
        }

        for (CFMetaData cfm : cfms.values())
        {
            Row columnRow = ColumnDefinition.readSchema(cfm.ksName, cfm.cfName);
            for (ColumnDefinition cd : ColumnDefinition.fromSchema(columnRow, cfm))
            {
                // This may replace some existing definition coming from the old key, column and
                // value aliases. But that's what we want (see CFMetaData.fromSchemaNoColumns).
                cfm.addOrReplaceColumnDefinition(cd);
            }
            cfm.rebuild();
        }

        return cfms;
    public static void serializeSuperColumnFamily(ColumnFamily scf, DataOutput out, int version) throws IOException
    {
        /*
         * There is 2 complications:
         *   1) We need to know the number of super columns in the column
         *   family to write in the header (so we do a first pass to group
         *   columns before serializing).
         *   2) For deletion infos, we need to figure out which are top-level
         *   deletions and which are super columns deletions (i.e. the
         *   subcolumns range deletions).
         */
        DeletionInfo delInfo = scf.deletionInfo();
        Map<ByteBuffer, List<Column>> scMap = groupSuperColumns(scf);

        // Actually Serialize
        DeletionInfo.serializer().serialize(new DeletionInfo(delInfo.getTopLevelDeletion()), out, version);
        out.writeInt(scMap.size());

        for (Map.Entry<ByteBuffer, List<Column>> entry : scMap.entrySet())
        {
            ByteBufferUtil.writeWithShortLength(entry.getKey(), out);

            DeletionTime delTime = delInfo.rangeCovering(entry.getKey());
            DeletionInfo scDelInfo = delTime == null ? DeletionInfo.live() : new DeletionInfo(delTime);
            DeletionTime.serializer.serialize(scDelInfo.getTopLevelDeletion(), out);

            out.writeInt(entry.getValue().size());
            for (Column subColumn : entry.getValue())
                Column.serializer.serialize(subColumn, out);
        }
    }

+++++++++++++++++++++++
    public static void serializeSuperColumnFamily(ColumnFamily scf, DataOutput out, int version) throws IOException
    {
        /*
         * There is 2 complications:
         *   1) We need to know the number of super columns in the column
         *   family to write in the header (so we do a first pass to group
         *   columns before serializing).
         *   2) For deletion infos, we need to figure out which are top-level
         *   deletions and which are super columns deletions (i.e. the
         *   subcolumns range deletions).
         */
        DeletionInfo delInfo = scf.deletionInfo();
        Map<ByteBuffer, List<Column>> scMap = groupSuperColumns(scf);

        // Actually Serialize
        DeletionInfo.serializer().serialize(new DeletionInfo(delInfo.getTopLevelDeletion()), out, version);
        out.writeInt(scMap.size());

        for (Map.Entry<ByteBuffer, List<Column>> entry : scMap.entrySet())
        {
            ByteBufferUtil.writeWithShortLength(entry.getKey(), out);

            List<DeletionTime> delTimes = delInfo.rangeCovering(entry.getKey());
            assert delTimes.size() <= 1; // We're supposed to have either no deletion, or a full SC deletion.
            DeletionInfo scDelInfo = delTimes.isEmpty() ? DeletionInfo.LIVE : new DeletionInfo(delTimes.get(0));
            DeletionTime.serializer.serialize(scDelInfo.getTopLevelDeletion(), out);

            out.writeInt(entry.getValue().size());
            for (Column subColumn : entry.getValue())
                Column.serializer.serialize(subColumn, out);
        }
    }

    public static long serializedSize(ColumnFamily scf, TypeSizes typeSizes, int version)
    {
        Map<ByteBuffer, List<Column>> scMap = groupSuperColumns(scf);
        DeletionInfo delInfo = scf.deletionInfo();

        // Actually Serialize
        long size = DeletionInfo.serializer().serializedSize(new DeletionInfo(delInfo.getTopLevelDeletion()), version);
        for (Map.Entry<ByteBuffer, List<Column>> entry : scMap.entrySet())
        {
            int nameSize = entry.getKey().remaining();
            size += typeSizes.sizeof((short) nameSize) + nameSize;

            DeletionTime delTime = delInfo.rangeCovering(entry.getKey());
            DeletionInfo scDelInfo = delTime == null ? DeletionInfo.live() : new DeletionInfo(delTime);
            size += DeletionTime.serializer.serializedSize(scDelInfo.getTopLevelDeletion(), TypeSizes.NATIVE);

            size += typeSizes.sizeof(entry.getValue().size());
            for (Column subColumn : entry.getValue())
                size += Column.serializer.serializedSize(subColumn, typeSizes);
        }
        return size;
    }

+++++++++++++++++++++++
    public static long serializedSize(ColumnFamily scf, TypeSizes typeSizes, int version)
    {
        Map<ByteBuffer, List<Column>> scMap = groupSuperColumns(scf);
        DeletionInfo delInfo = scf.deletionInfo();

        // Actually Serialize
        long size = DeletionInfo.serializer().serializedSize(new DeletionInfo(delInfo.getTopLevelDeletion()), version);
        for (Map.Entry<ByteBuffer, List<Column>> entry : scMap.entrySet())
        {
            int nameSize = entry.getKey().remaining();
            size += typeSizes.sizeof((short) nameSize) + nameSize;

            List<DeletionTime> delTimes = delInfo.rangeCovering(entry.getKey());
            assert delTimes.size() <= 1; // We're supposed to have either no deletion, or a full SC deletion.
            DeletionInfo scDelInfo = delTimes.isEmpty() ? DeletionInfo.LIVE : new DeletionInfo(delTimes.get(0));
            size += DeletionTime.serializer.serializedSize(scDelInfo.getTopLevelDeletion(), TypeSizes.NATIVE);

            size += typeSizes.sizeof(entry.getValue().size());
            for (Column subColumn : entry.getValue())
                size += Column.serializer.serializedSize(subColumn, typeSizes);
        }
        return size;
    }

    public ByteBuffer serialize(String value)
    {
        return ByteBufferUtil.bytes(value, charset);
    }

+++++++++++++++++++++++
    public String serialize(ByteBuffer bytes)
    {
        return getString(bytes);
    }

    public String deserialize(ByteBuffer bytes)
    {
        try
        {
            return ByteBufferUtil.string(bytes, charset);
        }
        catch (CharacterCodingException e)
        {
            throw new MarshalException("Invalid " + charset + " bytes " + ByteBufferUtil.bytesToHex(bytes));
        }
    }

+++++++++++++++++++++++
    public ByteBuffer deserialize(String value)
    {
        return ByteBufferUtil.bytes(value, charset);
    }

    public ByteBuffer serialize(Boolean value)
    {
        return (value == null) ? ByteBufferUtil.EMPTY_BYTE_BUFFER
                : value ? TRUE : FALSE; // false
    }

+++++++++++++++++++++++
    public Boolean serialize(ByteBuffer bytes)
    {
        byte value = bytes.get(bytes.position());
        return value != 0;
    }

    public Boolean deserialize(ByteBuffer bytes)
    {
        if (bytes.remaining() == 0)
            return null;

        byte value = bytes.get(bytes.position());
        return value != 0;
    }

+++++++++++++++++++++++
    public ByteBuffer deserialize(Boolean value)
    {
        return (value == null) ? ByteBufferUtil.EMPTY_BYTE_BUFFER
                : value ? TRUE : FALSE; // false
    }

    public ByteBuffer serialize(BigDecimal value)
    {
        if (value == null)
            return ByteBufferUtil.EMPTY_BYTE_BUFFER;

        BigInteger bi = value.unscaledValue();
        int scale = value.scale();
        byte[] bibytes = bi.toByteArray();

        ByteBuffer bytes = ByteBuffer.allocate(4 + bibytes.length);
        bytes.putInt(scale);
        bytes.put(bibytes);
        bytes.rewind();
        return bytes;
    }

+++++++++++++++++++++++
    public BigDecimal serialize(ByteBuffer bytes)
    {
        if (bytes == null)
            return null;

        // do not consume the contents of the ByteBuffer
        bytes = bytes.duplicate();
        int scale = bytes.getInt();
        byte[] bibytes = new byte[bytes.remaining()];
        bytes.get(bibytes);

        BigInteger bi = new BigInteger(bibytes);
        return new BigDecimal(bi, scale);
    }

    public BigDecimal deserialize(ByteBuffer bytes)
    {
        if (bytes == null || bytes.remaining() == 0)
            return null;

        // do not consume the contents of the ByteBuffer
        bytes = bytes.duplicate();
        int scale = bytes.getInt();
        byte[] bibytes = new byte[bytes.remaining()];
        bytes.get(bibytes);

        BigInteger bi = new BigInteger(bibytes);
        return new BigDecimal(bi, scale);
    }

+++++++++++++++++++++++
    public ByteBuffer deserialize(BigDecimal value)
    {
        if (value == null)
            return ByteBufferUtil.EMPTY_BYTE_BUFFER;

        BigInteger bi = value.unscaledValue();
        Integer scale = value.scale();
        byte[] bibytes = bi.toByteArray();
        byte[] sbytes = ByteBufferUtil.bytes(scale).array();
        byte[] bytes = new byte[bi.toByteArray().length + 4];

        for (int i = 0; i < 4; i++)
            bytes[i] = sbytes[i];
        for (int i = 4; i < bibytes.length + 4; i++)
            bytes[i] = bibytes[i - 4];

        return ByteBuffer.wrap(bytes);
    }

    public ByteBuffer serialize(Double value)
    {
        return (value == null) ? ByteBufferUtil.EMPTY_BYTE_BUFFER : ByteBufferUtil.bytes(value);
    }

+++++++++++++++++++++++
    public Double serialize(ByteBuffer bytes)
    {
        return ByteBufferUtil.toDouble(bytes);
    }

    public Double deserialize(ByteBuffer bytes)
    {
        if (bytes.remaining() == 0)
            return null;
        return ByteBufferUtil.toDouble(bytes);
    }

+++++++++++++++++++++++
    public ByteBuffer deserialize(Double value)
    {
        return (value == null) ? ByteBufferUtil.EMPTY_BYTE_BUFFER : ByteBufferUtil.bytes(value);
    }

    public ByteBuffer serialize(Void value)
    {
        return ByteBufferUtil.EMPTY_BYTE_BUFFER;
    }

+++++++++++++++++++++++
    public Void serialize(ByteBuffer bytes)
    {
        return null;
    }

    public Void deserialize(ByteBuffer bytes)
    {
        return null;
    }

+++++++++++++++++++++++
    public ByteBuffer deserialize(Void value)
    {
        return ByteBufferUtil.EMPTY_BYTE_BUFFER;
    }

    public ByteBuffer serialize(Float value)
    {
        return (value == null) ? ByteBufferUtil.EMPTY_BYTE_BUFFER : ByteBufferUtil.bytes(value);
    }

+++++++++++++++++++++++
    public Float serialize(ByteBuffer bytes)
    {
        return ByteBufferUtil.toFloat(bytes);
    }

    public Float deserialize(ByteBuffer bytes)
    {
        if (bytes.remaining() == 0)
            return null;

        return ByteBufferUtil.toFloat(bytes);
    }

+++++++++++++++++++++++
    public ByteBuffer deserialize(Float value)
    {
        return (value == null) ? ByteBufferUtil.EMPTY_BYTE_BUFFER : ByteBufferUtil.bytes(value);
    }

    public ByteBuffer serialize(InetAddress value)
    {
        return value == null ? ByteBufferUtil.EMPTY_BYTE_BUFFER : ByteBuffer.wrap(value.getAddress());
    }

+++++++++++++++++++++++
    public InetAddress serialize(ByteBuffer bytes)
    {
        try
        {
            return InetAddress.getByAddress(ByteBufferUtil.getArray(bytes));
        }
        catch (UnknownHostException e)
        {
            throw new AssertionError(e);
        }
    }

    public InetAddress deserialize(ByteBuffer bytes)
    {
        if (bytes.remaining() == 0)
            return null;

        try
        {
            return InetAddress.getByAddress(ByteBufferUtil.getArray(bytes));
        }
        catch (UnknownHostException e)
        {
            throw new AssertionError(e);
        }
    }

+++++++++++++++++++++++
    public ByteBuffer deserialize(InetAddress value)
    {
        return ByteBuffer.wrap(value.getAddress());
    }

    public ByteBuffer serialize(Integer value)
    {
        return value == null ? ByteBufferUtil.EMPTY_BYTE_BUFFER : ByteBufferUtil.bytes(value);
    }

+++++++++++++++++++++++
    public Integer serialize(ByteBuffer bytes)
    {
        return ByteBufferUtil.toInt(bytes);
    }

    public Integer deserialize(ByteBuffer bytes)
    {
        return bytes.remaining() == 0 ? null : ByteBufferUtil.toInt(bytes);
    }

+++++++++++++++++++++++
    public ByteBuffer deserialize(Integer value)
    {
        return value == null ? ByteBufferUtil.EMPTY_BYTE_BUFFER : ByteBufferUtil.bytes(value);
    }

    public ByteBuffer serialize(BigInteger value)
    {
        return ByteBuffer.wrap(value.toByteArray());
    }

+++++++++++++++++++++++
    public BigInteger serialize(ByteBuffer bytes)
    {
        return new BigInteger(ByteBufferUtil.getArray(bytes));
    }

    public BigInteger deserialize(ByteBuffer bytes)
    {
        return new BigInteger(ByteBufferUtil.getArray(bytes));
    }

+++++++++++++++++++++++
    public ByteBuffer deserialize(BigInteger value)
    {
        return ByteBuffer.wrap(value.toByteArray());
    }

    public ByteBuffer serialize(List<T> value)
    {
        List<ByteBuffer> bbs = new ArrayList<ByteBuffer>(value.size());
        int size = 0;
        for (T elt : value)
        {
            ByteBuffer bb = elements.serialize(elt);
            bbs.add(bb);
            size += 2 + bb.remaining();
        }
        return pack(bbs, value.size(), size);
    }

+++++++++++++++++++++++
    public List<T> serialize(ByteBuffer bytes)
    {
        try
        {
            ByteBuffer input = bytes.duplicate();
            int n = getUnsignedShort(input);
            List<T> l = new ArrayList<T>(n);
            for (int i = 0; i < n; i++)
            {
                int s = getUnsignedShort(input);
                byte[] data = new byte[s];
                input.get(data);
                ByteBuffer databb = ByteBuffer.wrap(data);
                elements.validate(databb);
                l.add(elements.serialize(databb));
            }
            return l;
        }
        catch (BufferUnderflowException e)
        {
            throw new MarshalException("Not enough bytes to read a list");
        }
    }

    /**
     * Layout is: {@code <n><s_1><b_1>...<s_n><b_n> }
     * where:
     *   n is the number of elements
     *   s_i is the number of bytes composing the ith element
     *   b_i is the s_i bytes composing the ith element
     */
    public List<T> deserialize(ByteBuffer bytes)
    {
        try
        {
            ByteBuffer input = bytes.duplicate();
            int n = getUnsignedShort(input);
            List<T> l = new ArrayList<T>(n);
            for (int i = 0; i < n; i++)
            {
                int s = getUnsignedShort(input);
                byte[] data = new byte[s];
                input.get(data);
                ByteBuffer databb = ByteBuffer.wrap(data);
                elements.validate(databb);
                l.add(elements.deserialize(databb));
            }
            return l;
        }
        catch (BufferUnderflowException e)
        {
            throw new MarshalException("Not enough bytes to read a list");
        }
    }

    /**
     * Layout is: {@code <n><s_1><b_1>...<s_n><b_n> }
     * where:
     *   n is the number of elements
     *   s_i is the number of bytes composing the ith element
     *   b_i is the s_i bytes composing the ith element
     */
+++++++++++++++++++++++
    public ByteBuffer deserialize(List<T> value)
    {
        List<ByteBuffer> bbs = new ArrayList<ByteBuffer>(value.size());
        int size = 0;
        for (T elt : value)
        {
            ByteBuffer bb = elements.deserialize(elt);
            bbs.add(bb);
            size += 2 + bb.remaining();
        }
        return pack(bbs, value.size(), size);
    }

    public ByteBuffer serialize(Long value)
    {
        return value == null ? ByteBufferUtil.EMPTY_BYTE_BUFFER : ByteBufferUtil.bytes(value);
    }

+++++++++++++++++++++++
    public Long serialize(ByteBuffer bytes)
    {
        return ByteBufferUtil.toLong(bytes);
    }

    public Long deserialize(ByteBuffer bytes)
    {
        return bytes.remaining() == 0 ? null : ByteBufferUtil.toLong(bytes);
    }

+++++++++++++++++++++++
    public ByteBuffer deserialize(Long value)
    {
        return ByteBufferUtil.bytes(value);
    }

    public ByteBuffer serialize(Map<K, V> value)
    {
        List<ByteBuffer> bbs = new ArrayList<ByteBuffer>(2 * value.size());
        int size = 0;
        for (Map.Entry<K, V> entry : value.entrySet())
        {
            ByteBuffer bbk = keys.serialize(entry.getKey());
            ByteBuffer bbv = values.serialize(entry.getValue());
            bbs.add(bbk);
            bbs.add(bbv);
            size += 4 + bbk.remaining() + bbv.remaining();
        }
        return pack(bbs, value.size(), size);
    }

+++++++++++++++++++++++
    public Map<K, V> serialize(ByteBuffer bytes)
    {
        try
        {
            ByteBuffer input = bytes.duplicate();
            int n = getUnsignedShort(input);
            Map<K, V> m = new LinkedHashMap<K, V>(n);
            for (int i = 0; i < n; i++)
            {
                int sk = getUnsignedShort(input);
                byte[] datak = new byte[sk];
                input.get(datak);
                ByteBuffer kbb = ByteBuffer.wrap(datak);
                keys.validate(kbb);

                int sv = getUnsignedShort(input);
                byte[] datav = new byte[sv];
                input.get(datav);
                ByteBuffer vbb = ByteBuffer.wrap(datav);
                values.validate(vbb);

                m.put(keys.serialize(kbb), values.serialize(vbb));
            }
            return m;
        }
        catch (BufferUnderflowException e)
        {
            throw new MarshalException("Not enough bytes to read a map");
        }
    }

    public Map<K, V> deserialize(ByteBuffer bytes)
    {
        try
        {
            ByteBuffer input = bytes.duplicate();
            int n = getUnsignedShort(input);
            Map<K, V> m = new LinkedHashMap<K, V>(n);
            for (int i = 0; i < n; i++)
            {
                int sk = getUnsignedShort(input);
                byte[] datak = new byte[sk];
                input.get(datak);
                ByteBuffer kbb = ByteBuffer.wrap(datak);
                keys.validate(kbb);

                int sv = getUnsignedShort(input);
                byte[] datav = new byte[sv];
                input.get(datav);
                ByteBuffer vbb = ByteBuffer.wrap(datav);
                values.validate(vbb);

                m.put(keys.deserialize(kbb), values.deserialize(vbb));
            }
            return m;
        }
        catch (BufferUnderflowException e)
        {
            throw new MarshalException("Not enough bytes to read a map");
        }
    }

    /**
     * Layout is: {@code <n><sk_1><k_1><sv_1><v_1>...<sk_n><k_n><sv_n><v_n> }
     * where:
     *   n is the number of elements
     *   sk_i is the number of bytes composing the ith key k_i
     *   k_i is the sk_i bytes composing the ith key
     *   sv_i is the number of bytes composing the ith value v_i
     *   v_i is the sv_i bytes composing the ith value
     */
+++++++++++++++++++++++
    public ByteBuffer deserialize(Map<K, V> value)
    {
        List<ByteBuffer> bbs = new ArrayList<ByteBuffer>(2 * value.size());
        int size = 0;
        for (Map.Entry<K, V> entry : value.entrySet())
        {
            ByteBuffer bbk = keys.deserialize(entry.getKey());
            ByteBuffer bbv = values.deserialize(entry.getValue());
            bbs.add(bbk);
            bbs.add(bbv);
            size += 4 + bbk.remaining() + bbv.remaining();
        }
        return pack(bbs, value.size(), size);
    }

    public ByteBuffer serialize(Set<T> value)
    {
        List<ByteBuffer> bbs = new ArrayList<ByteBuffer>(value.size());
        int size = 0;
        for (T elt : value)
        {
            ByteBuffer bb = elements.serialize(elt);
            bbs.add(bb);
            size += 2 + bb.remaining();
        }
        return pack(bbs, value.size(), size);
    }

+++++++++++++++++++++++
    public Set<T> serialize(ByteBuffer bytes)
    {
        try
        {
            ByteBuffer input = bytes.duplicate();
            int n = getUnsignedShort(input);
            Set<T> l = new LinkedHashSet<T>(n);
            for (int i = 0; i < n; i++)
            {
                int s = getUnsignedShort(input);
                byte[] data = new byte[s];
                input.get(data);
                ByteBuffer databb = ByteBuffer.wrap(data);
                elements.validate(databb);
                l.add(elements.serialize(databb));
            }
            return l;
        }
        catch (BufferUnderflowException e)
        {
            throw new MarshalException("Not enough bytes to read a list");
        }
    }

    /**
     * Layout is: {@code <n><s_1><b_1>...<s_n><b_n> }
     * where:
     *   n is the number of elements
     *   s_i is the number of bytes composing the ith element
     *   b_i is the s_i bytes composing the ith element
     */
    public Set<T> deserialize(ByteBuffer bytes)
    {
        try
        {
            ByteBuffer input = bytes.duplicate();
            int n = getUnsignedShort(input);
            Set<T> l = new LinkedHashSet<T>(n);
            for (int i = 0; i < n; i++)
            {
                int s = getUnsignedShort(input);
                byte[] data = new byte[s];
                input.get(data);
                ByteBuffer databb = ByteBuffer.wrap(data);
                elements.validate(databb);
                l.add(elements.deserialize(databb));
            }
            return l;
        }
        catch (BufferUnderflowException e)
        {
            throw new MarshalException("Not enough bytes to read a list");
        }
    }

    /**
     * Layout is: {@code <n><s_1><b_1>...<s_n><b_n> }
     * where:
     *   n is the number of elements
     *   s_i is the number of bytes composing the ith element
     *   b_i is the s_i bytes composing the ith element
     */
+++++++++++++++++++++++
    public ByteBuffer deserialize(Set<T> value)
    {
        List<ByteBuffer> bbs = new ArrayList<ByteBuffer>(value.size());
        int size = 0;
        for (T elt : value)
        {
            ByteBuffer bb = elements.deserialize(elt);
            bbs.add(bb);
            size += 2 + bb.remaining();
        }
        return pack(bbs, value.size(), size);
    }

    public ByteBuffer serialize(Date value)
    {
        return value == null ? ByteBufferUtil.EMPTY_BYTE_BUFFER : ByteBufferUtil.bytes(value.getTime());
    }

+++++++++++++++++++++++
    public Date serialize(ByteBuffer bytes)
    {
        return bytes.remaining() > 0
                ? new Date(ByteBufferUtil.toLong(bytes))
                : null;
    }

    public Date deserialize(ByteBuffer bytes)
    {
        return bytes.remaining() == 0 ? null : new Date(ByteBufferUtil.toLong(bytes));
    }

+++++++++++++++++++++++
    public ByteBuffer deserialize(Date value)
    {
        return (value == null)
                ? ByteBufferUtil.EMPTY_BYTE_BUFFER
                : ByteBufferUtil.bytes(value.getTime());
    }

    public ByteBuffer serialize(UUID value)
    {
        return value == null ? ByteBufferUtil.EMPTY_BYTE_BUFFER : ByteBuffer.wrap(UUIDGen.decompose(value));
    }

+++++++++++++++++++++++
    public UUID serialize(ByteBuffer bytes)
    {
        return UUIDGen.getUUID(bytes);
    }

    public UUID deserialize(ByteBuffer bytes)
    {
        return bytes.remaining() == 0 ? null : UUIDGen.getUUID(bytes);
    }

+++++++++++++++++++++++
    public ByteBuffer deserialize(UUID value)
    {
        return ByteBuffer.wrap(UUIDGen.decompose(value));
    }

    public ByteBuffer serialize()
    {
        try
        {
            ByteBuffer result = ByteBuffer.allocate(serializedSize());
            DataOutput out = new DataOutputStream(new ByteBufferOutputStream(result));
            ByteBufferUtil.writeWithShortLength(partitionKey, out);
            ByteBufferUtil.writeWithShortLength(cellName, out);
            out.writeInt(remaining);
            result.flip();
            return result;
        }
        catch (IOException e)
        {
            throw new RuntimeException(e);
        }
    }

+++++++++++++++++++++++
    public ByteBuffer serialize()
    {
        try
        {
            ByteBuffer result = ByteBuffer.allocate(serializedSize());
            DataOutput out = new DataOutputStream(new ByteBufferOutputStream(result));
            ByteBufferUtil.writeWithShortLength(partitionKey, out);
            ByteBufferUtil.writeWithShortLength(cellName, out);
            out.writeInt(remaining);
            return result;
        }
        catch (IOException e)
        {
            throw new RuntimeException(e);
        }
    }

    public static StreamMessage deserialize(ReadableByteChannel in, int version, StreamSession session) throws IOException
    {
        ByteBuffer buff = ByteBuffer.allocate(1);
        if (in.read(buff) > 0)
        {
            buff.flip();
            Type type = Type.get(buff.get());
            return type.serializer.deserialize(in, version, session);
        }
        else
        {
            // when socket gets closed, there is a chance that buff is empty
            // in that case, just return null
            return null;
        }
    }

    /** StreamMessage serializer */
+++++++++++++++++++++++
    public static StreamMessage deserialize(ReadableByteChannel in, int version, StreamSession session) throws IOException
    {
        ByteBuffer buff = ByteBuffer.allocate(1);
        in.read(buff);
        buff.flip();
        Type type = Type.get(buff.get());
        return type.serializer.deserialize(in, version, session);
    }

    /** StreamMessage serializer */
    public static OffHeapBitSet deserialize(DataInput in) throws IOException
    {
        long byteCount = in.readInt() * 8L;
        Memory memory = RefCountedMemory.allocate(byteCount);
        for (long i = 0; i < byteCount;)
        {
            long v = in.readLong();
            memory.setByte(i++, (byte) (v >>> 0));
            memory.setByte(i++, (byte) (v >>> 8));
            memory.setByte(i++, (byte) (v >>> 16));
            memory.setByte(i++, (byte) (v >>> 24));
            memory.setByte(i++, (byte) (v >>> 32));
            memory.setByte(i++, (byte) (v >>> 40));
            memory.setByte(i++, (byte) (v >>> 48));
            memory.setByte(i++, (byte) (v >>> 56));
        }
        return new OffHeapBitSet(memory);
    }

+++++++++++++++++++++++
    public static OffHeapBitSet deserialize(DataInput in) throws IOException
    {
        int byteCount = in.readInt() * 8;
        Memory memory = RefCountedMemory.allocate(byteCount);
        for (int i = 0; i < byteCount;)
        {
            long v = in.readLong();
            memory.setByte(i++, (byte) (v >>> 0));
            memory.setByte(i++, (byte) (v >>> 8));
            memory.setByte(i++, (byte) (v >>> 16));
            memory.setByte(i++, (byte) (v >>> 24));
            memory.setByte(i++, (byte) (v >>> 32));
            memory.setByte(i++, (byte) (v >>> 40));
            memory.setByte(i++, (byte) (v >>> 48));
            memory.setByte(i++, (byte) (v >>> 56));
        }
        return new OffHeapBitSet(memory);
    }

add: 4, delete: 22, change: 287, unhandled: 0 size_exceptions: 2 size_serialize: 36
-----------cassandra-2.0.0-beta1 vs cassandra-2.0.0-beta2-----------
    public static void serializeSuperColumnFamily(ColumnFamily scf, DataOutput out, int version) throws IOException
    {
        /*
         * There is 2 complications:
         *   1) We need to know the number of super columns in the column
         *   family to write in the header (so we do a first pass to group
         *   columns before serializing).
         *   2) For deletion infos, we need to figure out which are top-level
         *   deletions and which are super columns deletions (i.e. the
         *   subcolumns range deletions).
         */
        DeletionInfo delInfo = scf.deletionInfo();
        Map<ByteBuffer, List<Column>> scMap = groupSuperColumns(scf);

        // Actually Serialize
        DeletionInfo.serializer().serialize(new DeletionInfo(delInfo.getTopLevelDeletion()), out, version);
        out.writeInt(scMap.size());

        for (Map.Entry<ByteBuffer, List<Column>> entry : scMap.entrySet())
        {
            ByteBufferUtil.writeWithShortLength(entry.getKey(), out);

            List<DeletionTime> delTimes = delInfo.rangeCovering(entry.getKey());
            assert delTimes.size() <= 1; // We're supposed to have either no deletion, or a full SC deletion.
            DeletionInfo scDelInfo = delTimes.isEmpty() ? DeletionInfo.LIVE : new DeletionInfo(delTimes.get(0));
            DeletionTime.serializer.serialize(scDelInfo.getTopLevelDeletion(), out);

            out.writeInt(entry.getValue().size());
            for (Column subColumn : entry.getValue())
                Column.serializer.serialize(subColumn, out);
        }
    }

+++++++++++++++++++++++
    public static void serializeSuperColumnFamily(ColumnFamily scf, DataOutput out, int version) throws IOException
    {
        /*
         * There is 2 complications:
         *   1) We need to know the number of super columns in the column
         *   family to write in the header (so we do a first pass to group
         *   columns before serializing).
         *   2) For deletion infos, we need to figure out which are top-level
         *   deletions and which are super columns deletions (i.e. the
         *   subcolumns range deletions).
         */
        DeletionInfo delInfo = scf.deletionInfo();
        Map<ByteBuffer, List<Column>> scMap = groupSuperColumns(scf);

        // Actually Serialize
        DeletionInfo.serializer().serialize(new DeletionInfo(delInfo.getTopLevelDeletion()), out, version);
        out.writeInt(scMap.size());

        for (Map.Entry<ByteBuffer, List<Column>> entry : scMap.entrySet())
        {
            ByteBufferUtil.writeWithShortLength(entry.getKey(), out);

            DeletionTime delTime = delInfo.rangeCovering(entry.getKey());
            DeletionInfo scDelInfo = delTime == null ? DeletionInfo.live() : new DeletionInfo(delTime);
            DeletionTime.serializer.serialize(scDelInfo.getTopLevelDeletion(), out);

            out.writeInt(entry.getValue().size());
            for (Column subColumn : entry.getValue())
                Column.serializer.serialize(subColumn, out);
        }
    }

    public static long serializedSize(ColumnFamily scf, TypeSizes typeSizes, int version)
    {
        Map<ByteBuffer, List<Column>> scMap = groupSuperColumns(scf);
        DeletionInfo delInfo = scf.deletionInfo();

        // Actually Serialize
        long size = DeletionInfo.serializer().serializedSize(new DeletionInfo(delInfo.getTopLevelDeletion()), version);
        for (Map.Entry<ByteBuffer, List<Column>> entry : scMap.entrySet())
        {
            int nameSize = entry.getKey().remaining();
            size += typeSizes.sizeof((short) nameSize) + nameSize;

            List<DeletionTime> delTimes = delInfo.rangeCovering(entry.getKey());
            assert delTimes.size() <= 1; // We're supposed to have either no deletion, or a full SC deletion.
            DeletionInfo scDelInfo = delTimes.isEmpty() ? DeletionInfo.LIVE : new DeletionInfo(delTimes.get(0));
            size += DeletionTime.serializer.serializedSize(scDelInfo.getTopLevelDeletion(), TypeSizes.NATIVE);

            size += typeSizes.sizeof(entry.getValue().size());
            for (Column subColumn : entry.getValue())
                size += Column.serializer.serializedSize(subColumn, typeSizes);
        }
        return size;
    }

+++++++++++++++++++++++
    public static long serializedSize(ColumnFamily scf, TypeSizes typeSizes, int version)
    {
        Map<ByteBuffer, List<Column>> scMap = groupSuperColumns(scf);
        DeletionInfo delInfo = scf.deletionInfo();

        // Actually Serialize
        long size = DeletionInfo.serializer().serializedSize(new DeletionInfo(delInfo.getTopLevelDeletion()), version);
        for (Map.Entry<ByteBuffer, List<Column>> entry : scMap.entrySet())
        {
            int nameSize = entry.getKey().remaining();
            size += typeSizes.sizeof((short) nameSize) + nameSize;

            DeletionTime delTime = delInfo.rangeCovering(entry.getKey());
            DeletionInfo scDelInfo = delTime == null ? DeletionInfo.live() : new DeletionInfo(delTime);
            size += DeletionTime.serializer.serializedSize(scDelInfo.getTopLevelDeletion(), TypeSizes.NATIVE);

            size += typeSizes.sizeof(entry.getValue().size());
            for (Column subColumn : entry.getValue())
                size += Column.serializer.serializedSize(subColumn, typeSizes);
        }
        return size;
    }

    public String serialize(ByteBuffer bytes)
    {
        return getString(bytes);
    }

+++++++++++++++++++++++
    public ByteBuffer serialize(String value)
    {
        return ByteBufferUtil.bytes(value, charset);
    }

    public ByteBuffer deserialize(String value)
    {
        return ByteBufferUtil.bytes(value, charset);
    }

+++++++++++++++++++++++
    public String deserialize(ByteBuffer bytes)
    {
        try
        {
            return ByteBufferUtil.string(bytes, charset);
        }
        catch (CharacterCodingException e)
        {
            throw new MarshalException("Invalid " + charset + " bytes " + ByteBufferUtil.bytesToHex(bytes));
        }
    }

    public Boolean serialize(ByteBuffer bytes)
    {
        byte value = bytes.get(bytes.position());
        return value != 0;
    }

+++++++++++++++++++++++
    public ByteBuffer serialize(Boolean value)
    {
        return (value == null) ? ByteBufferUtil.EMPTY_BYTE_BUFFER
                : value ? TRUE : FALSE; // false
    }

    public ByteBuffer deserialize(Boolean value)
    {
        return (value == null) ? ByteBufferUtil.EMPTY_BYTE_BUFFER
                : value ? TRUE : FALSE; // false
    }

+++++++++++++++++++++++
    public Boolean deserialize(ByteBuffer bytes)
    {
        if (bytes.remaining() == 0)
            return null;

        byte value = bytes.get(bytes.position());
        return value != 0;
    }

    public BigDecimal serialize(ByteBuffer bytes)
    {
        if (bytes == null)
            return null;

        // do not consume the contents of the ByteBuffer
        bytes = bytes.duplicate();
        int scale = bytes.getInt();
        byte[] bibytes = new byte[bytes.remaining()];
        bytes.get(bibytes);

        BigInteger bi = new BigInteger(bibytes);
        return new BigDecimal(bi, scale);
    }

+++++++++++++++++++++++
    public ByteBuffer serialize(BigDecimal value)
    {
        if (value == null)
            return ByteBufferUtil.EMPTY_BYTE_BUFFER;

        BigInteger bi = value.unscaledValue();
        Integer scale = value.scale();
        byte[] bibytes = bi.toByteArray();
        byte[] sbytes = ByteBufferUtil.bytes(scale).array();
        byte[] bytes = new byte[bi.toByteArray().length + 4];

        for (int i = 0; i < 4; i++)
            bytes[i] = sbytes[i];
        for (int i = 4; i < bibytes.length + 4; i++)
            bytes[i] = bibytes[i - 4];

        return ByteBuffer.wrap(bytes);
    }

    public ByteBuffer deserialize(BigDecimal value)
    {
        if (value == null)
            return ByteBufferUtil.EMPTY_BYTE_BUFFER;

        BigInteger bi = value.unscaledValue();
        Integer scale = value.scale();
        byte[] bibytes = bi.toByteArray();
        byte[] sbytes = ByteBufferUtil.bytes(scale).array();
        byte[] bytes = new byte[bi.toByteArray().length + 4];

        for (int i = 0; i < 4; i++)
            bytes[i] = sbytes[i];
        for (int i = 4; i < bibytes.length + 4; i++)
            bytes[i] = bibytes[i - 4];

        return ByteBuffer.wrap(bytes);
    }

+++++++++++++++++++++++
    public BigDecimal deserialize(ByteBuffer bytes)
    {
        if (bytes == null || bytes.remaining() == 0)
            return null;

        // do not consume the contents of the ByteBuffer
        bytes = bytes.duplicate();
        int scale = bytes.getInt();
        byte[] bibytes = new byte[bytes.remaining()];
        bytes.get(bibytes);

        BigInteger bi = new BigInteger(bibytes);
        return new BigDecimal(bi, scale);
    }

    public Double serialize(ByteBuffer bytes)
    {
        return ByteBufferUtil.toDouble(bytes);
    }

+++++++++++++++++++++++
    public ByteBuffer serialize(Double value)
    {
        return (value == null) ? ByteBufferUtil.EMPTY_BYTE_BUFFER : ByteBufferUtil.bytes(value);
    }

    public ByteBuffer deserialize(Double value)
    {
        return (value == null) ? ByteBufferUtil.EMPTY_BYTE_BUFFER : ByteBufferUtil.bytes(value);
    }

+++++++++++++++++++++++
    public Double deserialize(ByteBuffer bytes)
    {
        if (bytes.remaining() == 0)
            return null;
        return ByteBufferUtil.toDouble(bytes);
    }

    public Void serialize(ByteBuffer bytes)
    {
        return null;
    }

+++++++++++++++++++++++
    public ByteBuffer serialize(Void value)
    {
        return ByteBufferUtil.EMPTY_BYTE_BUFFER;
    }

    public ByteBuffer deserialize(Void value)
    {
        return ByteBufferUtil.EMPTY_BYTE_BUFFER;
    }

+++++++++++++++++++++++
    public Void deserialize(ByteBuffer bytes)
    {
        return null;
    }

    public Float serialize(ByteBuffer bytes)
    {
        return ByteBufferUtil.toFloat(bytes);
    }

+++++++++++++++++++++++
    public ByteBuffer serialize(Float value)
    {
        return (value == null) ? ByteBufferUtil.EMPTY_BYTE_BUFFER : ByteBufferUtil.bytes(value);
    }

    public ByteBuffer deserialize(Float value)
    {
        return (value == null) ? ByteBufferUtil.EMPTY_BYTE_BUFFER : ByteBufferUtil.bytes(value);
    }

+++++++++++++++++++++++
    public Float deserialize(ByteBuffer bytes)
    {
        if (bytes.remaining() == 0)
            return null;

        return ByteBufferUtil.toFloat(bytes);
    }

    public InetAddress serialize(ByteBuffer bytes)
    {
        try
        {
            return InetAddress.getByAddress(ByteBufferUtil.getArray(bytes));
        }
        catch (UnknownHostException e)
        {
            throw new AssertionError(e);
        }
    }

+++++++++++++++++++++++
    public ByteBuffer serialize(InetAddress value)
    {
        return value == null ? ByteBufferUtil.EMPTY_BYTE_BUFFER : ByteBuffer.wrap(value.getAddress());
    }

    public ByteBuffer deserialize(InetAddress value)
    {
        return ByteBuffer.wrap(value.getAddress());
    }

+++++++++++++++++++++++
    public InetAddress deserialize(ByteBuffer bytes)
    {
        if (bytes.remaining() == 0)
            return null;

        try
        {
            return InetAddress.getByAddress(ByteBufferUtil.getArray(bytes));
        }
        catch (UnknownHostException e)
        {
            throw new AssertionError(e);
        }
    }

    public Integer serialize(ByteBuffer bytes)
    {
        return ByteBufferUtil.toInt(bytes);
    }

+++++++++++++++++++++++
    public ByteBuffer serialize(Integer value)
    {
        return value == null ? ByteBufferUtil.EMPTY_BYTE_BUFFER : ByteBufferUtil.bytes(value);
    }

    public ByteBuffer deserialize(Integer value)
    {
        return value == null ? ByteBufferUtil.EMPTY_BYTE_BUFFER : ByteBufferUtil.bytes(value);
    }

+++++++++++++++++++++++
    public Integer deserialize(ByteBuffer bytes)
    {
        return bytes.remaining() == 0 ? null : ByteBufferUtil.toInt(bytes);
    }

    public BigInteger serialize(ByteBuffer bytes)
    {
        return new BigInteger(ByteBufferUtil.getArray(bytes));
    }

+++++++++++++++++++++++
    public ByteBuffer serialize(BigInteger value)
    {
        return ByteBuffer.wrap(value.toByteArray());
    }

    public ByteBuffer deserialize(BigInteger value)
    {
        return ByteBuffer.wrap(value.toByteArray());
    }

+++++++++++++++++++++++
    public BigInteger deserialize(ByteBuffer bytes)
    {
        return new BigInteger(ByteBufferUtil.getArray(bytes));
    }

    public List<T> serialize(ByteBuffer bytes)
    {
        try
        {
            ByteBuffer input = bytes.duplicate();
            int n = getUnsignedShort(input);
            List<T> l = new ArrayList<T>(n);
            for (int i = 0; i < n; i++)
            {
                int s = getUnsignedShort(input);
                byte[] data = new byte[s];
                input.get(data);
                ByteBuffer databb = ByteBuffer.wrap(data);
                elements.validate(databb);
                l.add(elements.serialize(databb));
            }
            return l;
        }
        catch (BufferUnderflowException e)
        {
            throw new MarshalException("Not enough bytes to read a list");
        }
    }

    /**
     * Layout is: {@code <n><s_1><b_1>...<s_n><b_n> }
     * where:
     *   n is the number of elements
     *   s_i is the number of bytes composing the ith element
     *   b_i is the s_i bytes composing the ith element
     */
+++++++++++++++++++++++
    public ByteBuffer serialize(List<T> value)
    {
        List<ByteBuffer> bbs = new ArrayList<ByteBuffer>(value.size());
        int size = 0;
        for (T elt : value)
        {
            ByteBuffer bb = elements.serialize(elt);
            bbs.add(bb);
            size += 2 + bb.remaining();
        }
        return pack(bbs, value.size(), size);
    }

    public ByteBuffer deserialize(List<T> value)
    {
        List<ByteBuffer> bbs = new ArrayList<ByteBuffer>(value.size());
        int size = 0;
        for (T elt : value)
        {
            ByteBuffer bb = elements.deserialize(elt);
            bbs.add(bb);
            size += 2 + bb.remaining();
        }
        return pack(bbs, value.size(), size);
    }

+++++++++++++++++++++++
    public List<T> deserialize(ByteBuffer bytes)
    {
        try
        {
            ByteBuffer input = bytes.duplicate();
            int n = getUnsignedShort(input);
            List<T> l = new ArrayList<T>(n);
            for (int i = 0; i < n; i++)
            {
                int s = getUnsignedShort(input);
                byte[] data = new byte[s];
                input.get(data);
                ByteBuffer databb = ByteBuffer.wrap(data);
                elements.validate(databb);
                l.add(elements.deserialize(databb));
            }
            return l;
        }
        catch (BufferUnderflowException e)
        {
            throw new MarshalException("Not enough bytes to read a list");
        }
    }

    /**
     * Layout is: {@code <n><s_1><b_1>...<s_n><b_n> }
     * where:
     *   n is the number of elements
     *   s_i is the number of bytes composing the ith element
     *   b_i is the s_i bytes composing the ith element
     */
    public Long serialize(ByteBuffer bytes)
    {
        return ByteBufferUtil.toLong(bytes);
    }

+++++++++++++++++++++++
    public ByteBuffer serialize(Long value)
    {
        return value == null ? ByteBufferUtil.EMPTY_BYTE_BUFFER : ByteBufferUtil.bytes(value);
    }

    public ByteBuffer deserialize(Long value)
    {
        return ByteBufferUtil.bytes(value);
    }

+++++++++++++++++++++++
    public Long deserialize(ByteBuffer bytes)
    {
        return bytes.remaining() == 0 ? null : ByteBufferUtil.toLong(bytes);
    }

    public Map<K, V> serialize(ByteBuffer bytes)
    {
        try
        {
            ByteBuffer input = bytes.duplicate();
            int n = getUnsignedShort(input);
            Map<K, V> m = new LinkedHashMap<K, V>(n);
            for (int i = 0; i < n; i++)
            {
                int sk = getUnsignedShort(input);
                byte[] datak = new byte[sk];
                input.get(datak);
                ByteBuffer kbb = ByteBuffer.wrap(datak);
                keys.validate(kbb);

                int sv = getUnsignedShort(input);
                byte[] datav = new byte[sv];
                input.get(datav);
                ByteBuffer vbb = ByteBuffer.wrap(datav);
                values.validate(vbb);

                m.put(keys.serialize(kbb), values.serialize(vbb));
            }
            return m;
        }
        catch (BufferUnderflowException e)
        {
            throw new MarshalException("Not enough bytes to read a map");
        }
    }

+++++++++++++++++++++++
    public ByteBuffer serialize(Map<K, V> value)
    {
        List<ByteBuffer> bbs = new ArrayList<ByteBuffer>(2 * value.size());
        int size = 0;
        for (Map.Entry<K, V> entry : value.entrySet())
        {
            ByteBuffer bbk = keys.serialize(entry.getKey());
            ByteBuffer bbv = values.serialize(entry.getValue());
            bbs.add(bbk);
            bbs.add(bbv);
            size += 4 + bbk.remaining() + bbv.remaining();
        }
        return pack(bbs, value.size(), size);
    }

    public ByteBuffer deserialize(Map<K, V> value)
    {
        List<ByteBuffer> bbs = new ArrayList<ByteBuffer>(2 * value.size());
        int size = 0;
        for (Map.Entry<K, V> entry : value.entrySet())
        {
            ByteBuffer bbk = keys.deserialize(entry.getKey());
            ByteBuffer bbv = values.deserialize(entry.getValue());
            bbs.add(bbk);
            bbs.add(bbv);
            size += 4 + bbk.remaining() + bbv.remaining();
        }
        return pack(bbs, value.size(), size);
    }

+++++++++++++++++++++++
    public Map<K, V> deserialize(ByteBuffer bytes)
    {
        try
        {
            ByteBuffer input = bytes.duplicate();
            int n = getUnsignedShort(input);
            Map<K, V> m = new LinkedHashMap<K, V>(n);
            for (int i = 0; i < n; i++)
            {
                int sk = getUnsignedShort(input);
                byte[] datak = new byte[sk];
                input.get(datak);
                ByteBuffer kbb = ByteBuffer.wrap(datak);
                keys.validate(kbb);

                int sv = getUnsignedShort(input);
                byte[] datav = new byte[sv];
                input.get(datav);
                ByteBuffer vbb = ByteBuffer.wrap(datav);
                values.validate(vbb);

                m.put(keys.deserialize(kbb), values.deserialize(vbb));
            }
            return m;
        }
        catch (BufferUnderflowException e)
        {
            throw new MarshalException("Not enough bytes to read a map");
        }
    }

    /**
     * Layout is: {@code <n><sk_1><k_1><sv_1><v_1>...<sk_n><k_n><sv_n><v_n> }
     * where:
     *   n is the number of elements
     *   sk_i is the number of bytes composing the ith key k_i
     *   k_i is the sk_i bytes composing the ith key
     *   sv_i is the number of bytes composing the ith value v_i
     *   v_i is the sv_i bytes composing the ith value
     */
    public Set<T> serialize(ByteBuffer bytes)
    {
        try
        {
            ByteBuffer input = bytes.duplicate();
            int n = getUnsignedShort(input);
            Set<T> l = new LinkedHashSet<T>(n);
            for (int i = 0; i < n; i++)
            {
                int s = getUnsignedShort(input);
                byte[] data = new byte[s];
                input.get(data);
                ByteBuffer databb = ByteBuffer.wrap(data);
                elements.validate(databb);
                l.add(elements.serialize(databb));
            }
            return l;
        }
        catch (BufferUnderflowException e)
        {
            throw new MarshalException("Not enough bytes to read a list");
        }
    }

    /**
     * Layout is: {@code <n><s_1><b_1>...<s_n><b_n> }
     * where:
     *   n is the number of elements
     *   s_i is the number of bytes composing the ith element
     *   b_i is the s_i bytes composing the ith element
     */
+++++++++++++++++++++++
    public ByteBuffer serialize(Set<T> value)
    {
        List<ByteBuffer> bbs = new ArrayList<ByteBuffer>(value.size());
        int size = 0;
        for (T elt : value)
        {
            ByteBuffer bb = elements.serialize(elt);
            bbs.add(bb);
            size += 2 + bb.remaining();
        }
        return pack(bbs, value.size(), size);
    }

    public ByteBuffer deserialize(Set<T> value)
    {
        List<ByteBuffer> bbs = new ArrayList<ByteBuffer>(value.size());
        int size = 0;
        for (T elt : value)
        {
            ByteBuffer bb = elements.deserialize(elt);
            bbs.add(bb);
            size += 2 + bb.remaining();
        }
        return pack(bbs, value.size(), size);
    }

+++++++++++++++++++++++
    public Set<T> deserialize(ByteBuffer bytes)
    {
        try
        {
            ByteBuffer input = bytes.duplicate();
            int n = getUnsignedShort(input);
            Set<T> l = new LinkedHashSet<T>(n);
            for (int i = 0; i < n; i++)
            {
                int s = getUnsignedShort(input);
                byte[] data = new byte[s];
                input.get(data);
                ByteBuffer databb = ByteBuffer.wrap(data);
                elements.validate(databb);
                l.add(elements.deserialize(databb));
            }
            return l;
        }
        catch (BufferUnderflowException e)
        {
            throw new MarshalException("Not enough bytes to read a list");
        }
    }

    /**
     * Layout is: {@code <n><s_1><b_1>...<s_n><b_n> }
     * where:
     *   n is the number of elements
     *   s_i is the number of bytes composing the ith element
     *   b_i is the s_i bytes composing the ith element
     */
    public Date serialize(ByteBuffer bytes)
    {
        return bytes.remaining() > 0
                ? new Date(ByteBufferUtil.toLong(bytes))
                : null;
    }

+++++++++++++++++++++++
    public ByteBuffer serialize(Date value)
    {
        return value == null ? ByteBufferUtil.EMPTY_BYTE_BUFFER : ByteBufferUtil.bytes(value.getTime());
    }

    public ByteBuffer deserialize(Date value)
    {
        return (value == null)
                ? ByteBufferUtil.EMPTY_BYTE_BUFFER
                : ByteBufferUtil.bytes(value.getTime());
    }

+++++++++++++++++++++++
    public Date deserialize(ByteBuffer bytes)
    {
        return bytes.remaining() == 0 ? null : new Date(ByteBufferUtil.toLong(bytes));
    }

    public UUID serialize(ByteBuffer bytes)
    {
        return UUIDGen.getUUID(bytes);
    }

+++++++++++++++++++++++
    public ByteBuffer serialize(UUID value)
    {
        return value == null ? ByteBufferUtil.EMPTY_BYTE_BUFFER : ByteBuffer.wrap(UUIDGen.decompose(value));
    }

    public ByteBuffer deserialize(UUID value)
    {
        return ByteBuffer.wrap(UUIDGen.decompose(value));
    }

+++++++++++++++++++++++
    public UUID deserialize(ByteBuffer bytes)
    {
        return bytes.remaining() == 0 ? null : UUIDGen.getUUID(bytes);
    }

    public ByteBuffer serialize()
    {
        try
        {
            ByteBuffer result = ByteBuffer.allocate(serializedSize());
            DataOutput out = new DataOutputStream(new ByteBufferOutputStream(result));
            ByteBufferUtil.writeWithShortLength(partitionKey, out);
            ByteBufferUtil.writeWithShortLength(cellName, out);
            out.writeInt(remaining);
            return result;
        }
        catch (IOException e)
        {
            throw new RuntimeException(e);
        }
    }

+++++++++++++++++++++++
    public ByteBuffer serialize()
    {
        try
        {
            ByteBuffer result = ByteBuffer.allocate(serializedSize());
            DataOutput out = new DataOutputStream(new ByteBufferOutputStream(result));
            ByteBufferUtil.writeWithShortLength(partitionKey, out);
            ByteBufferUtil.writeWithShortLength(cellName, out);
            out.writeInt(remaining);
            result.flip();
            return result;
        }
        catch (IOException e)
        {
            throw new RuntimeException(e);
        }
    }

add: 9, delete: 3, change: 182, unhandled: 0 size_exceptions: 1 size_serialize: 33
-----------cassandra-2.0.0-beta2 vs cassandra-2.0.0-rc1-----------
    public static Map<String, CFMetaData> deserializeColumnFamilies(Row row)
    {
        if (row.cf == null)
            return Collections.emptyMap();

        Map<String, CFMetaData> cfms = new HashMap<String, CFMetaData>();
        UntypedResultSet results = QueryProcessor.resultify("SELECT * FROM system.schema_columnfamilies", row);
        for (UntypedResultSet.Row result : results)
        {
            CFMetaData cfm = CFMetaData.fromSchema(result);
            cfms.put(cfm.cfName, cfm);
        }

        for (CFMetaData cfm : cfms.values())
        {
            Row columnRow = ColumnDefinition.readSchema(cfm.ksName, cfm.cfName);
            for (ColumnDefinition cd : ColumnDefinition.fromSchema(columnRow, cfm))
            {
                // This may replace some existing definition coming from the old key, column and
                // value aliases. But that's what we want (see CFMetaData.fromSchemaNoColumns).
                cfm.addOrReplaceColumnDefinition(cd);
            }
            cfm.rebuild();
        }

        return cfms;
+++++++++++++++++++++++
    public static Map<String, CFMetaData> deserializeColumnFamilies(Row row)
    {
        if (row.cf == null)
            return Collections.emptyMap();

        Map<String, CFMetaData> cfms = new HashMap<String, CFMetaData>();
        UntypedResultSet results = QueryProcessor.resultify("SELECT * FROM system.schema_columnfamilies", row);
        for (UntypedResultSet.Row result : results)
        {
            CFMetaData cfm = CFMetaData.fromSchema(result);
            cfms.put(cfm.cfName, cfm);
        }

        for (CFMetaData cfm : cfms.values())
        {
            Row columnRow = SystemKeyspace.readSchemaRow(SystemKeyspace.SCHEMA_COLUMNS_CF, cfm.ksName, cfm.cfName);
            // This may replace some existing definition coming from the old key, column and
            // value aliases. But that's what we want (see CFMetaData.fromSchemaNoColumnsNoTriggers).
            for (ColumnDefinition cd : ColumnDefinition.fromSchema(columnRow, cfm))
                cfm.addOrReplaceColumnDefinition(cd);
            cfm.rebuild();
        }

        return cfms;
    public ByteBuffer serialize(BigDecimal value)
    {
        if (value == null)
            return ByteBufferUtil.EMPTY_BYTE_BUFFER;

        BigInteger bi = value.unscaledValue();
        Integer scale = value.scale();
        byte[] bibytes = bi.toByteArray();
        byte[] sbytes = ByteBufferUtil.bytes(scale).array();
        byte[] bytes = new byte[bi.toByteArray().length + 4];

        for (int i = 0; i < 4; i++)
            bytes[i] = sbytes[i];
        for (int i = 4; i < bibytes.length + 4; i++)
            bytes[i] = bibytes[i - 4];

        return ByteBuffer.wrap(bytes);
    }

+++++++++++++++++++++++
    public ByteBuffer serialize(BigDecimal value)
    {
        if (value == null)
            return ByteBufferUtil.EMPTY_BYTE_BUFFER;

        BigInteger bi = value.unscaledValue();
        int scale = value.scale();
        byte[] bibytes = bi.toByteArray();

        ByteBuffer bytes = ByteBuffer.allocate(4 + bibytes.length);
        bytes.putInt(scale);
        bytes.put(bibytes);
        bytes.rewind();
        return bytes;
    }

    public static StreamMessage deserialize(ReadableByteChannel in, int version, StreamSession session) throws IOException
    {
        ByteBuffer buff = ByteBuffer.allocate(1);
        in.read(buff);
        buff.flip();
        Type type = Type.get(buff.get());
        return type.serializer.deserialize(in, version, session);
    }

    /** StreamMessage serializer */
+++++++++++++++++++++++
    public static StreamMessage deserialize(ReadableByteChannel in, int version, StreamSession session) throws IOException
    {
        ByteBuffer buff = ByteBuffer.allocate(1);
        if (in.read(buff) > 0)
        {
            buff.flip();
            Type type = Type.get(buff.get());
            return type.serializer.deserialize(in, version, session);
        }
        else
        {
            // when socket gets closed, there is a chance that buff is empty
            // in that case, just return null
            return null;
        }
    }

    /** StreamMessage serializer */
add: 5, delete: 1, change: 146, unhandled: 0 size_exceptions: 1 size_serialize: 3
-----------cassandra-2.0.0-rc1 vs cassandra-2.0.0-rc2-----------
add: 6, delete: 0, change: 25, unhandled: 0 size_exceptions: 0 size_serialize: 0
-----------cassandra-2.0.0-rc2 vs cassandra-2.0.1-----------
    public ChannelBuffer serialize()
    {
        return ChannelBuffers.wrappedBuffer(CBUtil.enumValueToCB(type), serializeEvent());
    }

+++++++++++++++++++++++
    public void serialize(ChannelBuffer dest)
    {
        CBUtil.writeEnumValue(type, dest);
        serializeEvent(dest);
    }

    protected abstract ChannelBuffer serializeEvent();

+++++++++++++++++++++++
    protected abstract void serializeEvent(ChannelBuffer dest);
    public static OffHeapBitSet deserialize(DataInput in) throws IOException
    {
        int byteCount = in.readInt() * 8;
        Memory memory = RefCountedMemory.allocate(byteCount);
        for (int i = 0; i < byteCount;)
        {
            long v = in.readLong();
            memory.setByte(i++, (byte) (v >>> 0));
            memory.setByte(i++, (byte) (v >>> 8));
            memory.setByte(i++, (byte) (v >>> 16));
            memory.setByte(i++, (byte) (v >>> 24));
            memory.setByte(i++, (byte) (v >>> 32));
            memory.setByte(i++, (byte) (v >>> 40));
            memory.setByte(i++, (byte) (v >>> 48));
            memory.setByte(i++, (byte) (v >>> 56));
        }
        return new OffHeapBitSet(memory);
    }

+++++++++++++++++++++++
    public static OffHeapBitSet deserialize(DataInput in) throws IOException
    {
        long byteCount = in.readInt() * 8L;
        Memory memory = RefCountedMemory.allocate(byteCount);
        for (long i = 0; i < byteCount;)
        {
            long v = in.readLong();
            memory.setByte(i++, (byte) (v >>> 0));
            memory.setByte(i++, (byte) (v >>> 8));
            memory.setByte(i++, (byte) (v >>> 16));
            memory.setByte(i++, (byte) (v >>> 24));
            memory.setByte(i++, (byte) (v >>> 32));
            memory.setByte(i++, (byte) (v >>> 40));
            memory.setByte(i++, (byte) (v >>> 48));
            memory.setByte(i++, (byte) (v >>> 56));
        }
        return new OffHeapBitSet(memory);
    }

add: 12, delete: 0, change: 305, unhandled: 0 size_exceptions: 34 size_serialize: 3
-----------cassandra-2.0.1 vs cassandra-2.0.2-----------
add: 14, delete: 0, change: 152, unhandled: 0 size_exceptions: 21 size_serialize: 0
-----------cassandra-2.0.2 vs cassandra-2.0.3-----------
    public long serializedSize(RangeSliceCommand rsc, int version)
    {
        long size = TypeSizes.NATIVE.sizeof(rsc.keyspace);
        size += TypeSizes.NATIVE.sizeof(rsc.columnFamily);

        if (version >= MessagingService.VERSION_20)
            size += TypeSizes.NATIVE.sizeof(rsc.timestamp);

        IDiskAtomFilter filter = rsc.predicate;
        if (version < MessagingService.VERSION_20)
        {
            ByteBuffer sc = null;
            CFMetaData metadata = Schema.instance.getCFMetaData(rsc.keyspace, rsc.columnFamily);
            if (metadata.cfType == ColumnFamilyType.Super)
            {
                SuperColumns.SCFilter scFilter = SuperColumns.filterToSC((CompositeType)metadata.comparator, filter);
                sc = scFilter.scName;
                filter = scFilter.updatedFilter;
            }

            if (sc != null)
            {
                size += TypeSizes.NATIVE.sizeof(sc.remaining());
                size += sc.remaining();
            }
            else
            {
                size += TypeSizes.NATIVE.sizeof(0);
            }
        }

        size += IDiskAtomFilter.Serializer.instance.serializedSize(filter, version);

        if (rsc.rowFilter == null)
        {
            size += TypeSizes.NATIVE.sizeof(0);
        }
        else
        {
            size += TypeSizes.NATIVE.sizeof(rsc.rowFilter.size());
            for (IndexExpression expr : rsc.rowFilter)
            {
                size += TypeSizes.NATIVE.sizeofWithShortLength(expr.column_name);
                size += TypeSizes.NATIVE.sizeof(expr.op.getValue());
                size += TypeSizes.NATIVE.sizeofWithLength(expr.value);
            }
        }
        size += AbstractBounds.serializer.serializedSize(rsc.keyRange, version);
        size += TypeSizes.NATIVE.sizeof(rsc.maxResults);
        size += TypeSizes.NATIVE.sizeof(rsc.countCQL3Rows);
        size += TypeSizes.NATIVE.sizeof(rsc.isPaging);
        return size;
+++++++++++++++++++++++
    public long serializedSize(RangeSliceCommand rsc, int version)
    {
        long size = TypeSizes.NATIVE.sizeof(rsc.keyspace);
        size += TypeSizes.NATIVE.sizeof(rsc.columnFamily);

        if (version >= MessagingService.VERSION_20)
            size += TypeSizes.NATIVE.sizeof(rsc.timestamp);

        IDiskAtomFilter filter = rsc.predicate;
        if (version < MessagingService.VERSION_20)
        {
            ByteBuffer sc = null;
            CFMetaData metadata = Schema.instance.getCFMetaData(rsc.keyspace, rsc.columnFamily);
            if (metadata.cfType == ColumnFamilyType.Super)
            {
                SuperColumns.SCFilter scFilter = SuperColumns.filterToSC((CompositeType)metadata.comparator, filter);
                sc = scFilter.scName;
                filter = scFilter.updatedFilter;
            }

            if (sc != null)
            {
                size += TypeSizes.NATIVE.sizeof(sc.remaining());
                size += sc.remaining();
            }
            else
            {
                size += TypeSizes.NATIVE.sizeof(0);
            }
        }

        size += IDiskAtomFilter.Serializer.instance.serializedSize(filter, version);

        if (rsc.rowFilter == null)
        {
            size += TypeSizes.NATIVE.sizeof(0);
        }
        else
        {
            size += TypeSizes.NATIVE.sizeof(rsc.rowFilter.size());
            for (IndexExpression expr : rsc.rowFilter)
            {
                size += TypeSizes.NATIVE.sizeofWithShortLength(expr.column_name);
                size += TypeSizes.NATIVE.sizeof(expr.op.getValue());
                size += TypeSizes.NATIVE.sizeofWithShortLength(expr.value);
            }
        }
        size += AbstractBounds.serializer.serializedSize(rsc.keyRange, version);
        size += TypeSizes.NATIVE.sizeof(rsc.maxResults);
        size += TypeSizes.NATIVE.sizeof(rsc.countCQL3Rows);
        size += TypeSizes.NATIVE.sizeof(rsc.isPaging);
        return size;
add: 18, delete: 2, change: 114, unhandled: 0 size_exceptions: 2 size_serialize: 1
-----------cassandra-2.0.3 vs cassandra-2.0.4-----------
    public ByteBuffer serialize(List<Pair<ByteBuffer, Column>> columns)
    {
        List<ByteBuffer> bbs = new ArrayList<ByteBuffer>(columns.size());
        int size = 0;
        for (Pair<ByteBuffer, Column> p : columns)
        {
            bbs.add(p.right.value());
            size += 2 + p.right.value().remaining();
        }
        return pack(bbs, columns.size(), size);
+++++++++++++++++++++++
    public ByteBuffer serialize(List<Pair<ByteBuffer, Column>> columns)
    {
        columns = enforceLimit(columns);

        List<ByteBuffer> bbs = new ArrayList<ByteBuffer>(columns.size());
        int size = 0;
        for (Pair<ByteBuffer, Column> p : columns)
        {
            bbs.add(p.right.value());
            size += 2 + p.right.value().remaining();
        }
        return pack(bbs, columns.size(), size);
    public ByteBuffer serialize(List<Pair<ByteBuffer, Column>> columns)
    {
        List<ByteBuffer> bbs = new ArrayList<ByteBuffer>(2 * columns.size());
        int size = 0;
        for (Pair<ByteBuffer, Column> p : columns)
        {
            bbs.add(p.left);
            bbs.add(p.right.value());
            size += 4 + p.left.remaining() + p.right.value().remaining();
        }
        return pack(bbs, columns.size(), size);
+++++++++++++++++++++++
    public ByteBuffer serialize(List<Pair<ByteBuffer, Column>> columns)
    {
        columns = enforceLimit(columns);

        List<ByteBuffer> bbs = new ArrayList<ByteBuffer>(2 * columns.size());
        int size = 0;
        for (Pair<ByteBuffer, Column> p : columns)
        {
            bbs.add(p.left);
            bbs.add(p.right.value());
            size += 4 + p.left.remaining() + p.right.value().remaining();
        }
        return pack(bbs, columns.size(), size);
    public ByteBuffer serialize(List<Pair<ByteBuffer, Column>> columns)
    {
        List<ByteBuffer> bbs = new ArrayList<ByteBuffer>(columns.size());
        int size = 0;
        for (Pair<ByteBuffer, Column> p : columns)
        {
            bbs.add(p.left);
            size += 2 + p.left.remaining();
        }
        return pack(bbs, columns.size(), size);
+++++++++++++++++++++++
    public ByteBuffer serialize(List<Pair<ByteBuffer, Column>> columns)
    {
        columns = enforceLimit(columns);

        List<ByteBuffer> bbs = new ArrayList<ByteBuffer>(columns.size());
        int size = 0;
        for (Pair<ByteBuffer, Column> p : columns)
        {
            bbs.add(p.left);
            size += 2 + p.left.remaining();
        }
        return pack(bbs, columns.size(), size);
add: 8, delete: 7, change: 90, unhandled: 0 size_exceptions: 0 size_serialize: 3
-----------cassandra-2.0.4 vs cassandra-2.0.5-----------
    public static void serialize(StreamMessage message, WritableByteChannel out, int version, StreamSession session) throws IOException
    {
        ByteBuffer buff = ByteBuffer.allocate(1);
        // message type
        buff.put(message.type.type);
        buff.flip();
        out.write(buff);
        message.type.serializer.serialize(message, out, version, session);
    }

+++++++++++++++++++++++
    public static void serialize(StreamMessage message, WritableByteChannel out, int version, StreamSession session) throws IOException
    {
        ByteBuffer buff = ByteBuffer.allocate(1);
        // message type
        buff.put(message.type.type);
        buff.flip();
        while (buff.hasRemaining())
            out.write(buff);
        message.type.outSerializer.serialize(message, out, version, session);
    }

    public static StreamMessage deserialize(ReadableByteChannel in, int version, StreamSession session) throws IOException
    {
        ByteBuffer buff = ByteBuffer.allocate(1);
        if (in.read(buff) > 0)
        {
            buff.flip();
            Type type = Type.get(buff.get());
            return type.serializer.deserialize(in, version, session);
        }
        else
        {
            // when socket gets closed, there is a chance that buff is empty
            // in that case, just return null
            return null;
        }
    }

    /** StreamMessage serializer */
+++++++++++++++++++++++
    public static StreamMessage deserialize(ReadableByteChannel in, int version, StreamSession session) throws IOException
    {
        ByteBuffer buff = ByteBuffer.allocate(1);
        if (in.read(buff) > 0)
        {
            buff.flip();
            Type type = Type.get(buff.get());
            return type.inSerializer.deserialize(in, version, session);
        }
        else
        {
            // when socket gets closed, there is a chance that buff is empty
            // in that case, just return null
            return null;
        }
    }

    /** StreamMessage serializer */
add: 9, delete: 4, change: 97, unhandled: 0 size_exceptions: 1 size_serialize: 2
-----------cassandra-2.0.5 vs cassandra-2.0.6-----------
add: 6, delete: 2, change: 134, unhandled: 0 size_exceptions: 1 size_serialize: 0
-----------cassandra-2.0.6 vs cassandra-2.0.7-----------
    private static ByteBuffer serializeRowMutations(Collection<RowMutation> mutations)
    {
        FastByteArrayOutputStream bos = new FastByteArrayOutputStream();
        DataOutputStream out = new DataOutputStream(bos);

        try
        {
            out.writeInt(mutations.size());
            for (RowMutation rm : mutations)
                RowMutation.serializer.serialize(rm, out, VERSION);
        }
        catch (IOException e)
        {
            throw new AssertionError(); // cannot happen.
        }

        return ByteBuffer.wrap(bos.toByteArray());
    }

    @VisibleForTesting
+++++++++++++++++++++++
    private static ByteBuffer serializeRowMutations(Collection<RowMutation> mutations)
    {
        FastByteArrayOutputStream bos = new FastByteArrayOutputStream();
        DataOutputStream out = new DataOutputStream(bos);

        try
        {
            out.writeInt(mutations.size());
            for (RowMutation rm : mutations)
                RowMutation.serializer.serialize(rm, out, MessagingService.VERSION_12);
        }
        catch (IOException e)
        {
            throw new AssertionError(); // cannot happen.
        }

        return ByteBuffer.wrap(bos.toByteArray());
    }

    @VisibleForTesting
add: 10, delete: 6, change: 94, unhandled: 0 size_exceptions: 2 size_serialize: 1
-----------cassandra-2.0.7 vs cassandra-2.0.8-----------
    private RefCountedMemory serialize(V value)
    {
        long serializedSize = serializer.serializedSize(value, ENCODED_TYPE_SIZES);
        if (serializedSize > Integer.MAX_VALUE)
            throw new IllegalArgumentException("Unable to allocate " + serializedSize + " bytes");

        RefCountedMemory freeableMemory;
        try
        {
            freeableMemory = new RefCountedMemory(serializedSize);
        }
        catch (OutOfMemoryError e)
        {
            return null;
        }

        try
        {
            serializer.serialize(value, new EncodedDataOutputStream(new MemoryOutputStream(freeableMemory)));
        }
        catch (IOException e)
        {
            throw new RuntimeException(e);
        }
        return freeableMemory;
    }

+++++++++++++++++++++++
    private RefCountedMemory serialize(V value)
    {
        long serializedSize = serializer.serializedSize(value, ENCODED_TYPE_SIZES);
        if (serializedSize > Integer.MAX_VALUE)
            throw new IllegalArgumentException("Unable to allocate " + serializedSize + " bytes");

        RefCountedMemory freeableMemory;
        try
        {
            freeableMemory = new RefCountedMemory(serializedSize);
        }
        catch (OutOfMemoryError e)
        {
            return null;
        }

        try
        {
            serializer.serialize(value, new EncodedDataOutputStream(new MemoryOutputStream(freeableMemory)));
        }
        catch (IOException e)
        {
            freeableMemory.unreference();
            throw new RuntimeException(e);
        }
        return freeableMemory;
    }

    private V deserialize(RefCountedMemory mem)
    {
        try
        {
            return serializer.deserialize(new EncodedDataInputStream(new MemoryInputStream(mem)));
        }
        catch (IOException e)
        {
            logger.debug("Cannot fetch in memory data, we will failback to read from disk ", e);
            return null;
        }
    }

+++++++++++++++++++++++
    private V deserialize(RefCountedMemory mem)
    {
        try
        {
            return serializer.deserialize(new EncodedDataInputStream(new MemoryInputStream(mem)));
        }
        catch (IOException e)
        {
            logger.debug("Cannot fetch in memory data, we will fallback to read from disk ", e);
            return null;
        }
    }

    public ByteBuffer serialize(BigInteger value)
    {
        return ByteBuffer.wrap(value.toByteArray());
    }

+++++++++++++++++++++++
    public ByteBuffer serialize(BigInteger value)
    {
        return value == null ? ByteBufferUtil.EMPTY_BYTE_BUFFER : ByteBuffer.wrap(value.toByteArray());
    }

    public BigInteger deserialize(ByteBuffer bytes)
    {
        return new BigInteger(ByteBufferUtil.getArray(bytes));
    }

+++++++++++++++++++++++
    public BigInteger deserialize(ByteBuffer bytes)
    {
        return bytes.hasRemaining() ? new BigInteger(ByteBufferUtil.getArray(bytes)) : null;
    }

    public List<T> deserialize(ByteBuffer bytes)
    {
        try
        {
            ByteBuffer input = bytes.duplicate();
            int n = getUnsignedShort(input);
            List<T> l = new ArrayList<T>(n);
            for (int i = 0; i < n; i++)
            {
                int s = getUnsignedShort(input);
                byte[] data = new byte[s];
                input.get(data);
                ByteBuffer databb = ByteBuffer.wrap(data);
                elements.validate(databb);
                l.add(elements.deserialize(databb));
            }
            return l;
        }
        catch (BufferUnderflowException e)
        {
            throw new MarshalException("Not enough bytes to read a list");
        }
    }

    /**
     * Layout is: {@code <n><s_1><b_1>...<s_n><b_n> }
     * where:
     *   n is the number of elements
     *   s_i is the number of bytes composing the ith element
     *   b_i is the s_i bytes composing the ith element
     */
+++++++++++++++++++++++
    public List<T> deserialize(ByteBuffer bytes)
    {
        try
        {
            ByteBuffer input = bytes.duplicate();
            int n = ByteBufferUtil.readShortLength(input);
            List<T> l = new ArrayList<T>(n);
            for (int i = 0; i < n; i++)
            {
                ByteBuffer databb = ByteBufferUtil.readBytesWithShortLength(input);
                elements.validate(databb);
                l.add(elements.deserialize(databb));
            }
            return l;
        }
        catch (BufferUnderflowException e)
        {
            throw new MarshalException("Not enough bytes to read a list");
        }
    }

    /**
     * Layout is: {@code <n><s_1><b_1>...<s_n><b_n> }
     * where:
     *   n is the number of elements
     *   s_i is the number of bytes composing the ith element
     *   b_i is the s_i bytes composing the ith element
     */
    public Map<K, V> deserialize(ByteBuffer bytes)
    {
        try
        {
            ByteBuffer input = bytes.duplicate();
            int n = getUnsignedShort(input);
            Map<K, V> m = new LinkedHashMap<K, V>(n);
            for (int i = 0; i < n; i++)
            {
                int sk = getUnsignedShort(input);
                byte[] datak = new byte[sk];
                input.get(datak);
                ByteBuffer kbb = ByteBuffer.wrap(datak);
                keys.validate(kbb);

                int sv = getUnsignedShort(input);
                byte[] datav = new byte[sv];
                input.get(datav);
                ByteBuffer vbb = ByteBuffer.wrap(datav);
                values.validate(vbb);

                m.put(keys.deserialize(kbb), values.deserialize(vbb));
            }
            return m;
        }
        catch (BufferUnderflowException e)
        {
            throw new MarshalException("Not enough bytes to read a map");
        }
    }

    /**
     * Layout is: {@code <n><sk_1><k_1><sv_1><v_1>...<sk_n><k_n><sv_n><v_n> }
     * where:
     *   n is the number of elements
     *   sk_i is the number of bytes composing the ith key k_i
     *   k_i is the sk_i bytes composing the ith key
     *   sv_i is the number of bytes composing the ith value v_i
     *   v_i is the sv_i bytes composing the ith value
     */
+++++++++++++++++++++++
    public Map<K, V> deserialize(ByteBuffer bytes)
    {
        try
        {
            ByteBuffer input = bytes.duplicate();
            int n = ByteBufferUtil.readShortLength(input);
            Map<K, V> m = new LinkedHashMap<K, V>(n);
            for (int i = 0; i < n; i++)
            {
                ByteBuffer kbb = ByteBufferUtil.readBytesWithShortLength(input);
                keys.validate(kbb);

                ByteBuffer vbb = ByteBufferUtil.readBytesWithShortLength(input);
                values.validate(vbb);

                m.put(keys.deserialize(kbb), values.deserialize(vbb));
            }
            return m;
        }
        catch (BufferUnderflowException e)
        {
            throw new MarshalException("Not enough bytes to read a map");
        }
    }

    /**
     * Layout is: {@code <n><sk_1><k_1><sv_1><v_1>...<sk_n><k_n><sv_n><v_n> }
     * where:
     *   n is the number of elements
     *   sk_i is the number of bytes composing the ith key k_i
     *   k_i is the sk_i bytes composing the ith key
     *   sv_i is the number of bytes composing the ith value v_i
     *   v_i is the sv_i bytes composing the ith value
     */
    public Set<T> deserialize(ByteBuffer bytes)
    {
        try
        {
            ByteBuffer input = bytes.duplicate();
            int n = getUnsignedShort(input);
            Set<T> l = new LinkedHashSet<T>(n);
            for (int i = 0; i < n; i++)
            {
                int s = getUnsignedShort(input);
                byte[] data = new byte[s];
                input.get(data);
                ByteBuffer databb = ByteBuffer.wrap(data);
                elements.validate(databb);
                l.add(elements.deserialize(databb));
            }
            return l;
        }
        catch (BufferUnderflowException e)
        {
            throw new MarshalException("Not enough bytes to read a list");
        }
    }

    /**
     * Layout is: {@code <n><s_1><b_1>...<s_n><b_n> }
     * where:
     *   n is the number of elements
     *   s_i is the number of bytes composing the ith element
     *   b_i is the s_i bytes composing the ith element
     */
+++++++++++++++++++++++
    public Set<T> deserialize(ByteBuffer bytes)
    {
        try
        {
            ByteBuffer input = bytes.duplicate();
            int n = ByteBufferUtil.readShortLength(input);
            Set<T> l = new LinkedHashSet<T>(n);
            for (int i = 0; i < n; i++)
            {
                ByteBuffer databb = ByteBufferUtil.readBytesWithShortLength(input);
                elements.validate(databb);
                l.add(elements.deserialize(databb));
            }
            return l;
        }
        catch (BufferUnderflowException e)
        {
            throw new MarshalException("Not enough bytes to read a list");
        }
    }

    /**
     * Layout is: {@code <n><s_1><b_1>...<s_n><b_n> }
     * where:
     *   n is the number of elements
     *   s_i is the number of bytes composing the ith element
     *   b_i is the s_i bytes composing the ith element
     */
add: 8, delete: 0, change: 89, unhandled: 0 size_exceptions: 3 size_serialize: 7
-----------cassandra-2.0.8 vs cassandra-2.0.9-----------
add: 10, delete: 0, change: 86, unhandled: 0 size_exceptions: 0 size_serialize: 0
-----------cassandra-2.0.9 vs cassandra-2.0.10-----------
add: 13, delete: 5, change: 106, unhandled: 0 size_exceptions: 0 size_serialize: 0
-----------cassandra-2.0.10 vs cassandra-2.0.11-----------
    public int serializedSize()
    {
        return TypeSizes.NATIVE.sizeof(position) + promotedSize();
    }

+++++++++++++++++++++++
    public int serializedSize()
    {
        int size = TypeSizes.NATIVE.sizeof(position) + TypeSizes.NATIVE.sizeof(promotedSize());

        if (isIndexed())
        {
            List<IndexHelper.IndexInfo> index = columnsIndex();

            size += DeletionTime.serializer.serializedSize(deletionTime(), TypeSizes.NATIVE);
            size += TypeSizes.NATIVE.sizeof(index.size());
            for (IndexHelper.IndexInfo info : index)
                size += info.serializedSize(TypeSizes.NATIVE);
        }

        return size;
    }

    public List<T> deserialize(ByteBuffer bytes)
    {
        try
        {
            ByteBuffer input = bytes.duplicate();
            int n = ByteBufferUtil.readShortLength(input);
            List<T> l = new ArrayList<T>(n);
            for (int i = 0; i < n; i++)
            {
                ByteBuffer databb = ByteBufferUtil.readBytesWithShortLength(input);
                elements.validate(databb);
                l.add(elements.deserialize(databb));
            }
            return l;
        }
        catch (BufferUnderflowException e)
        {
            throw new MarshalException("Not enough bytes to read a list");
        }
    }

    /**
     * Layout is: {@code <n><s_1><b_1>...<s_n><b_n> }
     * where:
     *   n is the number of elements
     *   s_i is the number of bytes composing the ith element
     *   b_i is the s_i bytes composing the ith element
     */
+++++++++++++++++++++++
    public List<T> deserialize(ByteBuffer bytes)
    {
        try
        {
            ByteBuffer input = bytes.duplicate();
            int n = ByteBufferUtil.readShortLength(input);
            List<T> l = new ArrayList<T>(n);
            for (int i = 0; i < n; i++)
            {
                ByteBuffer databb = ByteBufferUtil.readBytesWithShortLength(input);
                elements.validate(databb);
                l.add(elements.deserialize(databb));
            }
            if (input.hasRemaining())
                throw new MarshalException("Unexpected extraneous bytes after list value");
            return l;
        }
        catch (BufferUnderflowException e)
        {
            throw new MarshalException("Not enough bytes to read a list");
        }
    }

    /**
     * Layout is: {@code <n><s_1><b_1>...<s_n><b_n> }
     * where:
     *   n is the number of elements
     *   s_i is the number of bytes composing the ith element
     *   b_i is the s_i bytes composing the ith element
     */
    public Map<K, V> deserialize(ByteBuffer bytes)
    {
        try
        {
            ByteBuffer input = bytes.duplicate();
            int n = ByteBufferUtil.readShortLength(input);
            Map<K, V> m = new LinkedHashMap<K, V>(n);
            for (int i = 0; i < n; i++)
            {
                ByteBuffer kbb = ByteBufferUtil.readBytesWithShortLength(input);
                keys.validate(kbb);

                ByteBuffer vbb = ByteBufferUtil.readBytesWithShortLength(input);
                values.validate(vbb);

                m.put(keys.deserialize(kbb), values.deserialize(vbb));
            }
            return m;
        }
        catch (BufferUnderflowException e)
        {
            throw new MarshalException("Not enough bytes to read a map");
        }
    }

    /**
     * Layout is: {@code <n><sk_1><k_1><sv_1><v_1>...<sk_n><k_n><sv_n><v_n> }
     * where:
     *   n is the number of elements
     *   sk_i is the number of bytes composing the ith key k_i
     *   k_i is the sk_i bytes composing the ith key
     *   sv_i is the number of bytes composing the ith value v_i
     *   v_i is the sv_i bytes composing the ith value
     */
+++++++++++++++++++++++
    public Map<K, V> deserialize(ByteBuffer bytes)
    {
        try
        {
            ByteBuffer input = bytes.duplicate();
            int n = ByteBufferUtil.readShortLength(input);
            Map<K, V> m = new LinkedHashMap<K, V>(n);
            for (int i = 0; i < n; i++)
            {
                ByteBuffer kbb = ByteBufferUtil.readBytesWithShortLength(input);
                keys.validate(kbb);

                ByteBuffer vbb = ByteBufferUtil.readBytesWithShortLength(input);
                values.validate(vbb);

                m.put(keys.deserialize(kbb), values.deserialize(vbb));
            }
            if (input.hasRemaining())
                throw new MarshalException("Unexpected extraneous bytes after map value");
            return m;
        }
        catch (BufferUnderflowException e)
        {
            throw new MarshalException("Not enough bytes to read a map");
        }
    }

    /**
     * Layout is: {@code <n><sk_1><k_1><sv_1><v_1>...<sk_n><k_n><sv_n><v_n> }
     * where:
     *   n is the number of elements
     *   sk_i is the number of bytes composing the ith key k_i
     *   k_i is the sk_i bytes composing the ith key
     *   sv_i is the number of bytes composing the ith value v_i
     *   v_i is the sv_i bytes composing the ith value
     */
    public Set<T> deserialize(ByteBuffer bytes)
    {
        try
        {
            ByteBuffer input = bytes.duplicate();
            int n = ByteBufferUtil.readShortLength(input);
            Set<T> l = new LinkedHashSet<T>(n);
            for (int i = 0; i < n; i++)
            {
                ByteBuffer databb = ByteBufferUtil.readBytesWithShortLength(input);
                elements.validate(databb);
                l.add(elements.deserialize(databb));
            }
            return l;
        }
        catch (BufferUnderflowException e)
        {
            throw new MarshalException("Not enough bytes to read a list");
        }
    }

    /**
     * Layout is: {@code <n><s_1><b_1>...<s_n><b_n> }
     * where:
     *   n is the number of elements
     *   s_i is the number of bytes composing the ith element
     *   b_i is the s_i bytes composing the ith element
     */
+++++++++++++++++++++++
    public Set<T> deserialize(ByteBuffer bytes)
    {
        try
        {
            ByteBuffer input = bytes.duplicate();
            int n = ByteBufferUtil.readShortLength(input);
            Set<T> l = new LinkedHashSet<T>(n);
            for (int i = 0; i < n; i++)
            {
                ByteBuffer databb = ByteBufferUtil.readBytesWithShortLength(input);
                elements.validate(databb);
                l.add(elements.deserialize(databb));
            }
            if (input.hasRemaining())
                throw new MarshalException("Unexpected extraneous bytes after set value");
            return l;
        }
        catch (BufferUnderflowException e)
        {
            throw new MarshalException("Not enough bytes to read a list");
        }
    }

    /**
     * Layout is: {@code <n><s_1><b_1>...<s_n><b_n> }
     * where:
     *   n is the number of elements
     *   s_i is the number of bytes composing the ith element
     *   b_i is the s_i bytes composing the ith element
     */
add: 9, delete: 2, change: 89, unhandled: 0 size_exceptions: 5 size_serialize: 4
-----------cassandra-2.0.11 vs cassandra-2.0.12-----------
    public int serializedSize(int version)
    {
        int size = CompactEndpointSerializationHelper.serializedSize(from);

        size += TypeSizes.NATIVE.sizeof(verb.ordinal());
        size += TypeSizes.NATIVE.sizeof(parameters.size());
        for (Map.Entry<String, byte[]> entry : parameters.entrySet())
        {
            TypeSizes.NATIVE.sizeof(entry.getKey());
            TypeSizes.NATIVE.sizeof(entry.getValue().length);
            size += entry.getValue().length;
        }

        long longSize = payload == null ? 0 : serializer.serializedSize(payload, version);
        assert longSize <= Integer.MAX_VALUE; // larger values are supported in sstables but not messages
        size += TypeSizes.NATIVE.sizeof((int) longSize);
        size += longSize;
        return size;
+++++++++++++++++++++++
    public int serializedSize(int version)
    {
        int size = CompactEndpointSerializationHelper.serializedSize(from);

        size += TypeSizes.NATIVE.sizeof(verb.ordinal());
        size += TypeSizes.NATIVE.sizeof(parameters.size());
        for (Map.Entry<String, byte[]> entry : parameters.entrySet())
        {
            size += TypeSizes.NATIVE.sizeof(entry.getKey());
            size += TypeSizes.NATIVE.sizeof(entry.getValue().length);
            size += entry.getValue().length;
        }

        long longSize = payload == null ? 0 : serializer.serializedSize(payload, version);
        assert longSize <= Integer.MAX_VALUE; // larger values are supported in sstables but not messages
        size += TypeSizes.NATIVE.sizeof((int) longSize);
        size += longSize;
        return size;
add: 15, delete: 3, change: 119, unhandled: 0 size_exceptions: 6 size_serialize: 1
-----------cassandra-2.0.12 vs cassandra-2.0.13-----------
add: 4, delete: 1, change: 41, unhandled: 0 size_exceptions: 0 size_serialize: 0
-----------cassandra-2.0.13 vs cassandra-2.0.14-----------
add: 1, delete: 0, change: 1, unhandled: 0 size_exceptions: 0 size_serialize: 0
-----------cassandra-2.0.14 vs cassandra-2.0.15-----------
    public RangeSliceCommand deserialize(DataInput in, int version) throws IOException
    {
        String keyspace = in.readUTF();
        String columnFamily = in.readUTF();

        long timestamp = version < MessagingService.VERSION_20 ? System.currentTimeMillis() : in.readLong();

        CFMetaData metadata = Schema.instance.getCFMetaData(keyspace, columnFamily);

        IDiskAtomFilter predicate;
        if (version < MessagingService.VERSION_20)
        {
            int scLength = in.readInt();
            ByteBuffer superColumn = null;
            if (scLength > 0)
            {
                byte[] buf = new byte[scLength];
                in.readFully(buf);
                superColumn = ByteBuffer.wrap(buf);
            }

            AbstractType<?> comparator;
            if (metadata.cfType == ColumnFamilyType.Super)
            {
                CompositeType type = (CompositeType)metadata.comparator;
                comparator = superColumn == null ? type.types.get(0) : type.types.get(1);
            }
            else
            {
                comparator = metadata.comparator;
            }

            predicate = IDiskAtomFilter.Serializer.instance.deserialize(in, version, comparator);

            if (metadata.cfType == ColumnFamilyType.Super)
                predicate = SuperColumns.fromSCFilter((CompositeType)metadata.comparator, superColumn, predicate);
        }
        else
        {
            predicate = IDiskAtomFilter.Serializer.instance.deserialize(in, version, metadata.comparator);
        }

        List<IndexExpression> rowFilter;
        int filterCount = in.readInt();
        rowFilter = new ArrayList<IndexExpression>(filterCount);
        for (int i = 0; i < filterCount; i++)
        {
            IndexExpression expr;
            expr = new IndexExpression(ByteBufferUtil.readWithShortLength(in),
                                       IndexOperator.findByValue(in.readInt()),
                                       ByteBufferUtil.readWithShortLength(in));
            rowFilter.add(expr);
        }
        AbstractBounds<RowPosition> range = AbstractBounds.serializer.deserialize(in, version).toRowBounds();

        int maxResults = in.readInt();
        boolean countCQL3Rows = in.readBoolean();
        boolean isPaging = in.readBoolean();
        return new RangeSliceCommand(keyspace, columnFamily, timestamp, predicate, range, rowFilter, maxResults, countCQL3Rows, isPaging);
    }

+++++++++++++++++++++++
    public RangeSliceCommand deserialize(DataInput in, int version) throws IOException
    {
        String keyspace = in.readUTF();
        String columnFamily = in.readUTF();

        long timestamp = version < MessagingService.VERSION_20 ? System.currentTimeMillis() : in.readLong();

        CFMetaData metadata = Schema.instance.getCFMetaData(keyspace, columnFamily);
        if (metadata == null)
        {
            String message = String.format("Got range slice command for nonexistent table %s.%s.  If the table was just " +
                    "created, this is likely due to the schema not being fully propagated.  Please wait for schema " +
                    "agreement on table creation." , keyspace, columnFamily);
            throw new UnknownColumnFamilyException(message, null);
        }

        IDiskAtomFilter predicate;
        if (version < MessagingService.VERSION_20)
        {
            int scLength = in.readInt();
            ByteBuffer superColumn = null;
            if (scLength > 0)
            {
                byte[] buf = new byte[scLength];
                in.readFully(buf);
                superColumn = ByteBuffer.wrap(buf);
            }

            AbstractType<?> comparator;
            if (metadata.cfType == ColumnFamilyType.Super)
            {
                CompositeType type = (CompositeType)metadata.comparator;
                comparator = superColumn == null ? type.types.get(0) : type.types.get(1);
            }
            else
            {
                comparator = metadata.comparator;
            }

            predicate = IDiskAtomFilter.Serializer.instance.deserialize(in, version, comparator);

            if (metadata.cfType == ColumnFamilyType.Super)
                predicate = SuperColumns.fromSCFilter((CompositeType)metadata.comparator, superColumn, predicate);
        }
        else
        {
            predicate = IDiskAtomFilter.Serializer.instance.deserialize(in, version, metadata.comparator);
        }

        List<IndexExpression> rowFilter;
        int filterCount = in.readInt();
        rowFilter = new ArrayList<IndexExpression>(filterCount);
        for (int i = 0; i < filterCount; i++)
        {
            IndexExpression expr;
            expr = new IndexExpression(ByteBufferUtil.readWithShortLength(in),
                                       IndexOperator.findByValue(in.readInt()),
                                       ByteBufferUtil.readWithShortLength(in));
            rowFilter.add(expr);
        }
        AbstractBounds<RowPosition> range = AbstractBounds.serializer.deserialize(in, version).toRowBounds();

        int maxResults = in.readInt();
        boolean countCQL3Rows = in.readBoolean();
        boolean isPaging = in.readBoolean();
        return new RangeSliceCommand(keyspace, columnFamily, timestamp, predicate, range, rowFilter, maxResults, countCQL3Rows, isPaging);
    }

    public ReadCommand deserialize(DataInput in, int version) throws IOException
    {
        boolean isDigest = in.readBoolean();
        String keyspaceName = in.readUTF();
        ByteBuffer key = ByteBufferUtil.readWithShortLength(in);

        String cfName;
        ByteBuffer sc = null;
        if (version < MessagingService.VERSION_20)
        {
            QueryPath path = QueryPath.deserialize(in);
            cfName = path.columnFamilyName;
            sc = path.superColumnName;
        }
        else
        {
            cfName = in.readUTF();
        }

        long timestamp = version < MessagingService.VERSION_20 ? System.currentTimeMillis() : in.readLong();

        CFMetaData metadata = Schema.instance.getCFMetaData(keyspaceName, cfName);
        ReadCommand command;
        if (version < MessagingService.VERSION_20)
        {
            AbstractType<?> comparator;
            if (metadata.cfType == ColumnFamilyType.Super)
            {
                CompositeType type = (CompositeType)metadata.comparator;
                comparator = sc == null ? type.types.get(0) : type.types.get(1);
            }
            else
            {
                comparator = metadata.comparator;
            }

            IDiskAtomFilter filter = NamesQueryFilter.serializer.deserialize(in, version, comparator);

            if (metadata.cfType == ColumnFamilyType.Super)
                filter = SuperColumns.fromSCFilter((CompositeType)metadata.comparator, sc, filter);

            // Due to SC compat, it's possible we get back a slice filter at this point
            if (filter instanceof NamesQueryFilter)
                command = new SliceByNamesReadCommand(keyspaceName, key, cfName, timestamp, (NamesQueryFilter)filter);
            else
                command = new SliceFromReadCommand(keyspaceName, key, cfName, timestamp, (SliceQueryFilter)filter);
        }
        else
        {
            NamesQueryFilter filter = NamesQueryFilter.serializer.deserialize(in, version, metadata.comparator);
            command = new SliceByNamesReadCommand(keyspaceName, key, cfName, timestamp, filter);
        }

        command.setDigestQuery(isDigest);
        return command;
    }

+++++++++++++++++++++++
    public ReadCommand deserialize(DataInput in, int version) throws IOException
    {
        boolean isDigest = in.readBoolean();
        String keyspaceName = in.readUTF();
        ByteBuffer key = ByteBufferUtil.readWithShortLength(in);

        String cfName;
        ByteBuffer sc = null;
        if (version < MessagingService.VERSION_20)
        {
            QueryPath path = QueryPath.deserialize(in);
            cfName = path.columnFamilyName;
            sc = path.superColumnName;
        }
        else
        {
            cfName = in.readUTF();
        }

        long timestamp = version < MessagingService.VERSION_20 ? System.currentTimeMillis() : in.readLong();

        CFMetaData metadata = Schema.instance.getCFMetaData(keyspaceName, cfName);
        if (metadata == null)
        {
            String message = String.format("Got slice command for nonexistent table %s.%s.  If the table was just " +
                    "created, this is likely due to the schema not being fully propagated.  Please wait for schema " +
                    "agreement on table creation." , keyspaceName, cfName);
            throw new UnknownColumnFamilyException(message, null);
        }

        ReadCommand command;
        if (version < MessagingService.VERSION_20)
        {
            AbstractType<?> comparator;
            if (metadata.cfType == ColumnFamilyType.Super)
            {
                CompositeType type = (CompositeType)metadata.comparator;
                comparator = sc == null ? type.types.get(0) : type.types.get(1);
            }
            else
            {
                comparator = metadata.comparator;
            }

            IDiskAtomFilter filter = NamesQueryFilter.serializer.deserialize(in, version, comparator);

            if (metadata.cfType == ColumnFamilyType.Super)
                filter = SuperColumns.fromSCFilter((CompositeType)metadata.comparator, sc, filter);

            // Due to SC compat, it's possible we get back a slice filter at this point
            if (filter instanceof NamesQueryFilter)
                command = new SliceByNamesReadCommand(keyspaceName, key, cfName, timestamp, (NamesQueryFilter)filter);
            else
                command = new SliceFromReadCommand(keyspaceName, key, cfName, timestamp, (SliceQueryFilter)filter);
        }
        else
        {
            NamesQueryFilter filter = NamesQueryFilter.serializer.deserialize(in, version, metadata.comparator);
            command = new SliceByNamesReadCommand(keyspaceName, key, cfName, timestamp, filter);
        }

        command.setDigestQuery(isDigest);
        return command;
    }

    public ReadCommand deserialize(DataInput in, int version) throws IOException
    {
        boolean isDigest = in.readBoolean();
        String keyspaceName = in.readUTF();
        ByteBuffer key = ByteBufferUtil.readWithShortLength(in);

        String cfName;
        ByteBuffer sc = null;
        if (version < MessagingService.VERSION_20)
        {
            QueryPath path = QueryPath.deserialize(in);
            cfName = path.columnFamilyName;
            sc = path.superColumnName;
        }
        else
        {
            cfName = in.readUTF();
        }

        long timestamp = version < MessagingService.VERSION_20 ? System.currentTimeMillis() : in.readLong();

        CFMetaData metadata = Schema.instance.getCFMetaData(keyspaceName, cfName);
        SliceQueryFilter filter;
        if (version < MessagingService.VERSION_20)
        {
            filter = SliceQueryFilter.serializer.deserialize(in, version);

            if (metadata.cfType == ColumnFamilyType.Super)
                filter = SuperColumns.fromSCSliceFilter((CompositeType)metadata.comparator, sc, filter);
        }
        else
        {
            filter = SliceQueryFilter.serializer.deserialize(in, version);
        }

        ReadCommand command = new SliceFromReadCommand(keyspaceName, key, cfName, timestamp, filter);
        command.setDigestQuery(isDigest);
        return command;
    }

+++++++++++++++++++++++
    public ReadCommand deserialize(DataInput in, int version) throws IOException
    {
        boolean isDigest = in.readBoolean();
        String keyspaceName = in.readUTF();
        ByteBuffer key = ByteBufferUtil.readWithShortLength(in);

        String cfName;
        ByteBuffer sc = null;
        if (version < MessagingService.VERSION_20)
        {
            QueryPath path = QueryPath.deserialize(in);
            cfName = path.columnFamilyName;
            sc = path.superColumnName;
        }
        else
        {
            cfName = in.readUTF();
        }

        long timestamp = version < MessagingService.VERSION_20 ? System.currentTimeMillis() : in.readLong();

        CFMetaData metadata = Schema.instance.getCFMetaData(keyspaceName, cfName);
        if (metadata == null)
        {
            String message = String.format("Got slice command for nonexistent table %s.%s.  If the table was just " +
                    "created, this is likely due to the schema not being fully propagated.  Please wait for schema " +
                    "agreement on table creation." , keyspaceName, cfName);
            throw new UnknownColumnFamilyException(message, null);
        }

        SliceQueryFilter filter;
        if (version < MessagingService.VERSION_20)
        {
            filter = SliceQueryFilter.serializer.deserialize(in, version);

            if (metadata.cfType == ColumnFamilyType.Super)
                filter = SuperColumns.fromSCSliceFilter((CompositeType)metadata.comparator, sc, filter);
        }
        else
        {
            filter = SliceQueryFilter.serializer.deserialize(in, version);
        }

        ReadCommand command = new SliceFromReadCommand(keyspaceName, key, cfName, timestamp, filter);
        command.setDigestQuery(isDigest);
        return command;
    }

    public abstract ByteBuffer serialize(List<Pair<ByteBuffer, Column>> columns);

    @Override
+++++++++++++++++++++++
    public abstract ByteBuffer serialize(CFDefinition.Name name, List<Pair<ByteBuffer, Column>> columns);

    @Override
    public ByteBuffer serialize(List<Pair<ByteBuffer, Column>> columns)
    {
        columns = enforceLimit(columns);

        List<ByteBuffer> bbs = new ArrayList<ByteBuffer>(columns.size());
        int size = 0;
        for (Pair<ByteBuffer, Column> p : columns)
        {
            bbs.add(p.right.value());
            size += 2 + p.right.value().remaining();
        }
        return pack(bbs, columns.size(), size);
+++++++++++++++++++++++
    public ByteBuffer serialize(CFDefinition.Name name, List<Pair<ByteBuffer, Column>> columns)
    {
        columns = enforceLimit(name, columns);

        List<ByteBuffer> bbs = new ArrayList<ByteBuffer>(columns.size());
        int size = 0;
        for (Pair<ByteBuffer, Column> p : columns)
        {
            bbs.add(p.right.value());
            size += 2 + p.right.value().remaining();
        }
        return pack(bbs, columns.size(), size);
    public ByteBuffer serialize(List<Pair<ByteBuffer, Column>> columns)
    {
        columns = enforceLimit(columns);

        List<ByteBuffer> bbs = new ArrayList<ByteBuffer>(2 * columns.size());
        int size = 0;
        for (Pair<ByteBuffer, Column> p : columns)
        {
            bbs.add(p.left);
            bbs.add(p.right.value());
            size += 4 + p.left.remaining() + p.right.value().remaining();
        }
        return pack(bbs, columns.size(), size);
+++++++++++++++++++++++
    public ByteBuffer serialize(CFDefinition.Name name, List<Pair<ByteBuffer, Column>> columns)
    {
        columns = enforceLimit(name, columns);

        List<ByteBuffer> bbs = new ArrayList<ByteBuffer>(2 * columns.size());
        int size = 0;
        for (Pair<ByteBuffer, Column> p : columns)
        {
            bbs.add(p.left);
            bbs.add(p.right.value());
            size += 4 + p.left.remaining() + p.right.value().remaining();
        }
        return pack(bbs, columns.size(), size);
    public ByteBuffer serialize(List<Pair<ByteBuffer, Column>> columns)
    {
        columns = enforceLimit(columns);

        List<ByteBuffer> bbs = new ArrayList<ByteBuffer>(columns.size());
        int size = 0;
        for (Pair<ByteBuffer, Column> p : columns)
        {
            bbs.add(p.left);
            size += 2 + p.left.remaining();
        }
        return pack(bbs, columns.size(), size);
+++++++++++++++++++++++
    public ByteBuffer serialize(CFDefinition.Name name, List<Pair<ByteBuffer, Column>> columns)
    {
        columns = enforceLimit(name, columns);

        List<ByteBuffer> bbs = new ArrayList<ByteBuffer>(columns.size());
        int size = 0;
        for (Pair<ByteBuffer, Column> p : columns)
        {
            bbs.add(p.left);
            size += 2 + p.left.remaining();
        }
        return pack(bbs, columns.size(), size);
add: 7, delete: 0, change: 87, unhandled: 0 size_exceptions: 1 size_serialize: 7
-----------cassandra-2.0.15 vs cassandra-2.0.16-----------
add: 4, delete: 2, change: 59, unhandled: 0 size_exceptions: 0 size_serialize: 0
-----------cassandra-2.0.16 vs cassandra-2.0.17-----------
add: 6, delete: 2, change: 55, unhandled: 0 size_exceptions: 0 size_serialize: 0
-----------cassandra-2.0.17 vs cassandra-2.1.0-----------
    public static Map<String, CFMetaData> deserializeColumnFamilies(Row row)
    {
        if (row.cf == null)
            return Collections.emptyMap();

        Map<String, CFMetaData> cfms = new HashMap<String, CFMetaData>();
        UntypedResultSet results = QueryProcessor.resultify("SELECT * FROM system.schema_columnfamilies", row);
        for (UntypedResultSet.Row result : results)
        {
            CFMetaData cfm = CFMetaData.fromSchema(result);
            cfms.put(cfm.cfName, cfm);
        }

        for (CFMetaData cfm : cfms.values())
        {
            Row columnRow = SystemKeyspace.readSchemaRow(SystemKeyspace.SCHEMA_COLUMNS_CF, cfm.ksName, cfm.cfName);
            // This may replace some existing definition coming from the old key, column and
            // value aliases. But that's what we want (see CFMetaData.fromSchemaNoColumnsNoTriggers).
            for (ColumnDefinition cd : ColumnDefinition.fromSchema(columnRow, cfm))
                cfm.addOrReplaceColumnDefinition(cd);
            cfm.rebuild();
        }

        return cfms;
+++++++++++++++++++++++
    public static Map<String, CFMetaData> deserializeColumnFamilies(Row row)
    {
        if (row.cf == null)
            return Collections.emptyMap();

        Map<String, CFMetaData> cfms = new HashMap<>();
        UntypedResultSet results = QueryProcessor.resultify("SELECT * FROM system.schema_columnfamilies", row);
        for (UntypedResultSet.Row result : results)
        {
            CFMetaData cfm = CFMetaData.fromSchema(result);
            cfms.put(cfm.cfName, cfm);
        }
        return cfms;
    public void serializeCfId(UUID cfId, DataOutput out, int version) throws IOException
    {
        UUIDSerializer.serializer.serialize(cfId, out, version);
    }

+++++++++++++++++++++++
    public void serializeCfId(UUID cfId, DataOutputPlus out, int version) throws IOException
    {
        UUIDSerializer.serializer.serialize(cfId, out, version);
    }

    public void serializeForSSTable(ColumnFamily cf, DataOutput out)
    {
        // Column families shouldn't be written directly to disk, use ColumnIndex.Builder instead
        throw new UnsupportedOperationException();
    }

+++++++++++++++++++++++
    public void serializeForSSTable(ColumnFamily cf, DataOutputPlus out)
    {
        // Column families shouldn't be written directly to disk, use ColumnIndex.Builder instead
        throw new UnsupportedOperationException();
    }

    public void serialize(ColumnFamily cf, DataOutput out, int version)
    {
        try
        {
            if (cf == null)
            {
                out.writeBoolean(false);
                return;
            }

            out.writeBoolean(true);
            serializeCfId(cf.id(), out, version);

            if (cf.metadata().isSuper() && version < MessagingService.VERSION_20)
            {
                SuperColumns.serializeSuperColumnFamily(cf, out, version);
                return;
            }

            DeletionInfo.serializer().serialize(cf.deletionInfo(), out, version);
            ColumnSerializer columnSerializer = Column.serializer;
            int count = cf.getColumnCount();
            out.writeInt(count);
            int written = 0;
            for (Column column : cf)
            {
                columnSerializer.serialize(column, out);
                written++;
            }
            assert count == written: "Column family had " + count + " columns, but " + written + " written";
        }
        catch (IOException e)
        {
            throw new RuntimeException(e);
        }
    }

+++++++++++++++++++++++
    public void serialize(ColumnFamily cf, DataOutputPlus out, int version)
    {
        try
        {
            if (cf == null)
            {
                out.writeBoolean(false);
                return;
            }

            out.writeBoolean(true);
            serializeCfId(cf.id(), out, version);
            cf.getComparator().deletionInfoSerializer().serialize(cf.deletionInfo(), out, version);
            ColumnSerializer columnSerializer = cf.getComparator().columnSerializer();
            int count = cf.getColumnCount();
            out.writeInt(count);
            int written = 0;
            for (Cell cell : cf)
            {
                columnSerializer.serialize(cell, out);
                written++;
            }
            assert count == written: "Column family had " + count + " columns, but " + written + " written";
        }
        catch (IOException e)
        {
            throw new RuntimeException(e);
        }
    }

    public ColumnFamily deserialize(DataInput in, ColumnFamily.Factory factory, ColumnSerializer.Flag flag, int version) throws IOException
    {
        if (!in.readBoolean())
            return null;

        ColumnFamily cf = factory.create(Schema.instance.getCFMetaData(deserializeCfId(in, version)));

        if (cf.metadata().isSuper() && version < MessagingService.VERSION_20)
        {
            SuperColumns.deserializerSuperColumnFamily(in, cf, flag, version);
        }
        else
        {
            cf.delete(DeletionInfo.serializer().deserialize(in, version, cf.getComparator()));

            ColumnSerializer columnSerializer = Column.serializer;
            int size = in.readInt();
            for (int i = 0; i < size; ++i)
                cf.addColumn(columnSerializer.deserialize(in, flag));
        }
        return cf;
    }

+++++++++++++++++++++++
    public ColumnFamily deserialize(DataInput in, ColumnFamily.Factory factory, ColumnSerializer.Flag flag, int version) throws IOException
    {
        if (!in.readBoolean())
            return null;

        ColumnFamily cf = factory.create(Schema.instance.getCFMetaData(deserializeCfId(in, version)));

        if (cf.metadata().isSuper() && version < MessagingService.VERSION_20)
        {
            SuperColumns.deserializerSuperColumnFamily(in, cf, flag, version);
        }
        else
        {
            cf.delete(cf.getComparator().deletionInfoSerializer().deserialize(in, version));

            ColumnSerializer columnSerializer = cf.getComparator().columnSerializer();
            int size = in.readInt();
            for (int i = 0; i < size; ++i)
                cf.addColumn(columnSerializer.deserialize(in, flag));
        }
        return cf;
    }

    public void serialize(Column column, DataOutput out) throws IOException
    {
        assert column.name().remaining() > 0;
        ByteBufferUtil.writeWithShortLength(column.name(), out);
        try
        {
            out.writeByte(column.serializationFlags());
            if (column instanceof CounterColumn)
            {
                out.writeLong(((CounterColumn)column).timestampOfLastDelete());
            }
            else if (column instanceof ExpiringColumn)
            {
                out.writeInt(((ExpiringColumn) column).getTimeToLive());
                out.writeInt(column.getLocalDeletionTime());
            }
            out.writeLong(column.timestamp());
            ByteBufferUtil.writeWithLength(column.value(), out);
        }
        catch (IOException e)
        {
            throw new RuntimeException(e);
        }
    }

+++++++++++++++++++++++
    public void serialize(Cell cell, DataOutputPlus out) throws IOException
    {
        assert !cell.name().isEmpty();
        type.cellSerializer().serialize(cell.name(), out);
        try
        {
            out.writeByte(cell.serializationFlags());
            if (cell instanceof CounterCell)
            {
                out.writeLong(((CounterCell) cell).timestampOfLastDelete());
            }
            else if (cell instanceof ExpiringCell)
            {
                out.writeInt(((ExpiringCell) cell).getTimeToLive());
                out.writeInt(cell.getLocalDeletionTime());
            }
            out.writeLong(cell.timestamp());
            ByteBufferUtil.writeWithLength(cell.value(), out);
        }
        catch (IOException e)
        {
            throw new RuntimeException(e);
        }
    }

    Column deserializeColumnBody(DataInput in, ByteBuffer name, int mask, ColumnSerializer.Flag flag, int expireBefore) throws IOException
    {
        if ((mask & COUNTER_MASK) != 0)
        {
            long timestampOfLastDelete = in.readLong();
            long ts = in.readLong();
            ByteBuffer value = ByteBufferUtil.readWithLength(in);
            return CounterColumn.create(name, value, ts, timestampOfLastDelete, flag);
        }
        else if ((mask & EXPIRATION_MASK) != 0)
        {
            int ttl = in.readInt();
            int expiration = in.readInt();
            long ts = in.readLong();
            ByteBuffer value = ByteBufferUtil.readWithLength(in);
            return ExpiringColumn.create(name, value, ts, ttl, expiration, expireBefore, flag);
        }
        else
        {
            long ts = in.readLong();
            ByteBuffer value = ByteBufferUtil.readWithLength(in);
            return (mask & COUNTER_UPDATE_MASK) != 0
                   ? new CounterUpdateColumn(name, value, ts)
                   : ((mask & DELETION_MASK) == 0
                      ? new Column(name, value, ts)
                      : new DeletedColumn(name, value, ts));
        }
    }

+++++++++++++++++++++++
    Cell deserializeColumnBody(DataInput in, CellName name, int mask, ColumnSerializer.Flag flag, int expireBefore) throws IOException
    {
        if ((mask & COUNTER_MASK) != 0)
        {
            long timestampOfLastDelete = in.readLong();
            long ts = in.readLong();
            ByteBuffer value = ByteBufferUtil.readWithLength(in);
            return BufferCounterCell.create(name, value, ts, timestampOfLastDelete, flag);
        }
        else if ((mask & EXPIRATION_MASK) != 0)
        {
            int ttl = in.readInt();
            int expiration = in.readInt();
            long ts = in.readLong();
            ByteBuffer value = ByteBufferUtil.readWithLength(in);
            return BufferExpiringCell.create(name, value, ts, ttl, expiration, expireBefore, flag);
        }
        else
        {
            long ts = in.readLong();
            ByteBuffer value = ByteBufferUtil.readWithLength(in);
            return (mask & COUNTER_UPDATE_MASK) != 0
                   ? new BufferCounterUpdateCell(name, value, ts)
                   : ((mask & DELETION_MASK) == 0
                      ? new BufferCell(name, value, ts)
                      : new BufferDeletedCell(name, value, ts));
        }
    }

    public long serializedSize(Column column, TypeSizes type)
    {
        return column.serializedSize(type);
    }

+++++++++++++++++++++++
    public long serializedSize(Cell cell, TypeSizes typeSizes)
    {
        return cell.serializedSize(type, typeSizes);
    }

    public Column deserialize(DataInput in, ColumnSerializer.Flag flag, int expireBefore) throws IOException
    {
        ByteBuffer name = ByteBufferUtil.readWithShortLength(in);
        if (name.remaining() <= 0)
            throw CorruptColumnException.create(in, name);

        int b = in.readUnsignedByte();
        return deserializeColumnBody(in, name, b, flag, expireBefore);
    }

+++++++++++++++++++++++
    public Cell deserialize(DataInput in, ColumnSerializer.Flag flag, int expireBefore) throws IOException
    {
        CellName name = type.cellSerializer().deserialize(in);

        int b = in.readUnsignedByte();
        return deserializeColumnBody(in, name, b, flag, expireBefore);
    }

    public void serialize(RangeSliceCommand sliceCommand, DataOutput out, int version) throws IOException
    {
        out.writeUTF(sliceCommand.keyspace);
        out.writeUTF(sliceCommand.columnFamily);

        if (version >= MessagingService.VERSION_20)
            out.writeLong(sliceCommand.timestamp);

        IDiskAtomFilter filter = sliceCommand.predicate;
        if (version < MessagingService.VERSION_20)
        {
            // Pre-2.0, we need to know if it's a super column. If it is, we
            // must extract the super column name from the predicate (and
            // modify the predicate accordingly)
            ByteBuffer sc = null;
            CFMetaData metadata = Schema.instance.getCFMetaData(sliceCommand.getKeyspace(), sliceCommand.columnFamily);
            if (metadata.cfType == ColumnFamilyType.Super)
            {
                SuperColumns.SCFilter scFilter = SuperColumns.filterToSC((CompositeType)metadata.comparator, filter);
                sc = scFilter.scName;
                filter = scFilter.updatedFilter;
            }

            out.writeInt(sc == null ? 0 : sc.remaining());
            if (sc != null)
                ByteBufferUtil.write(sc, out);
        }

        IDiskAtomFilter.Serializer.instance.serialize(filter, out, version);

        if (sliceCommand.rowFilter == null)
        {
            out.writeInt(0);
        }
        else
        {
            out.writeInt(sliceCommand.rowFilter.size());
            for (IndexExpression expr : sliceCommand.rowFilter)
            {
                ByteBufferUtil.writeWithShortLength(expr.column_name, out);
                out.writeInt(expr.op.getValue());
                ByteBufferUtil.writeWithShortLength(expr.value, out);
            }
        }
        AbstractBounds.serializer.serialize(sliceCommand.keyRange, out, version);
        out.writeInt(sliceCommand.maxResults);
        out.writeBoolean(sliceCommand.countCQL3Rows);
        out.writeBoolean(sliceCommand.isPaging);
    }

+++++++++++++++++++++++
    public void serialize(RangeSliceCommand sliceCommand, DataOutputPlus out, int version) throws IOException
    {
        out.writeUTF(sliceCommand.keyspace);
        out.writeUTF(sliceCommand.columnFamily);
        out.writeLong(sliceCommand.timestamp);

        CFMetaData metadata = Schema.instance.getCFMetaData(sliceCommand.keyspace, sliceCommand.columnFamily);

        metadata.comparator.diskAtomFilterSerializer().serialize(sliceCommand.predicate, out, version);

        if (sliceCommand.rowFilter == null)
        {
            out.writeInt(0);
        }
        else
        {
            out.writeInt(sliceCommand.rowFilter.size());
            for (IndexExpression expr : sliceCommand.rowFilter)
            {
                ByteBufferUtil.writeWithShortLength(expr.column, out);
                out.writeInt(expr.operator.ordinal());
                ByteBufferUtil.writeWithShortLength(expr.value, out);
            }
        }
        AbstractBounds.serializer.serialize(sliceCommand.keyRange, out, version);
        out.writeInt(sliceCommand.maxResults);
        out.writeBoolean(sliceCommand.countCQL3Rows);
        out.writeBoolean(sliceCommand.isPaging);
    }

    public long serializedSize(RangeSliceCommand rsc, int version)
    {
        long size = TypeSizes.NATIVE.sizeof(rsc.keyspace);
        size += TypeSizes.NATIVE.sizeof(rsc.columnFamily);

        if (version >= MessagingService.VERSION_20)
            size += TypeSizes.NATIVE.sizeof(rsc.timestamp);

        IDiskAtomFilter filter = rsc.predicate;
        if (version < MessagingService.VERSION_20)
        {
            ByteBuffer sc = null;
            CFMetaData metadata = Schema.instance.getCFMetaData(rsc.keyspace, rsc.columnFamily);
            if (metadata.cfType == ColumnFamilyType.Super)
            {
                SuperColumns.SCFilter scFilter = SuperColumns.filterToSC((CompositeType)metadata.comparator, filter);
                sc = scFilter.scName;
                filter = scFilter.updatedFilter;
            }

            if (sc != null)
            {
                size += TypeSizes.NATIVE.sizeof(sc.remaining());
                size += sc.remaining();
            }
            else
            {
                size += TypeSizes.NATIVE.sizeof(0);
            }
        }

        size += IDiskAtomFilter.Serializer.instance.serializedSize(filter, version);

        if (rsc.rowFilter == null)
        {
            size += TypeSizes.NATIVE.sizeof(0);
        }
        else
        {
            size += TypeSizes.NATIVE.sizeof(rsc.rowFilter.size());
            for (IndexExpression expr : rsc.rowFilter)
            {
                size += TypeSizes.NATIVE.sizeofWithShortLength(expr.column_name);
                size += TypeSizes.NATIVE.sizeof(expr.op.getValue());
                size += TypeSizes.NATIVE.sizeofWithShortLength(expr.value);
            }
        }
        size += AbstractBounds.serializer.serializedSize(rsc.keyRange, version);
        size += TypeSizes.NATIVE.sizeof(rsc.maxResults);
        size += TypeSizes.NATIVE.sizeof(rsc.countCQL3Rows);
        size += TypeSizes.NATIVE.sizeof(rsc.isPaging);
        return size;
+++++++++++++++++++++++
    public long serializedSize(RangeSliceCommand rsc, int version)
    {
        long size = TypeSizes.NATIVE.sizeof(rsc.keyspace);
        size += TypeSizes.NATIVE.sizeof(rsc.columnFamily);
        size += TypeSizes.NATIVE.sizeof(rsc.timestamp);

        CFMetaData metadata = Schema.instance.getCFMetaData(rsc.keyspace, rsc.columnFamily);

        IDiskAtomFilter filter = rsc.predicate;

        size += metadata.comparator.diskAtomFilterSerializer().serializedSize(filter, version);

        if (rsc.rowFilter == null)
        {
            size += TypeSizes.NATIVE.sizeof(0);
        }
        else
        {
            size += TypeSizes.NATIVE.sizeof(rsc.rowFilter.size());
            for (IndexExpression expr : rsc.rowFilter)
            {
                size += TypeSizes.NATIVE.sizeofWithShortLength(expr.column);
                size += TypeSizes.NATIVE.sizeof(expr.operator.ordinal());
                size += TypeSizes.NATIVE.sizeofWithShortLength(expr.value);
            }
        }
        size += AbstractBounds.serializer.serializedSize(rsc.keyRange, version);
        size += TypeSizes.NATIVE.sizeof(rsc.maxResults);
        size += TypeSizes.NATIVE.sizeof(rsc.countCQL3Rows);
        size += TypeSizes.NATIVE.sizeof(rsc.isPaging);
        return size;
    public RangeSliceCommand deserialize(DataInput in, int version) throws IOException
    {
        String keyspace = in.readUTF();
        String columnFamily = in.readUTF();

        long timestamp = version < MessagingService.VERSION_20 ? System.currentTimeMillis() : in.readLong();

        CFMetaData metadata = Schema.instance.getCFMetaData(keyspace, columnFamily);
        if (metadata == null)
        {
            String message = String.format("Got range slice command for nonexistent table %s.%s.  If the table was just " +
                    "created, this is likely due to the schema not being fully propagated.  Please wait for schema " +
                    "agreement on table creation." , keyspace, columnFamily);
            throw new UnknownColumnFamilyException(message, null);
        }

        IDiskAtomFilter predicate;
        if (version < MessagingService.VERSION_20)
        {
            int scLength = in.readInt();
            ByteBuffer superColumn = null;
            if (scLength > 0)
            {
                byte[] buf = new byte[scLength];
                in.readFully(buf);
                superColumn = ByteBuffer.wrap(buf);
            }

            AbstractType<?> comparator;
            if (metadata.cfType == ColumnFamilyType.Super)
            {
                CompositeType type = (CompositeType)metadata.comparator;
                comparator = superColumn == null ? type.types.get(0) : type.types.get(1);
            }
            else
            {
                comparator = metadata.comparator;
            }

            predicate = IDiskAtomFilter.Serializer.instance.deserialize(in, version, comparator);

            if (metadata.cfType == ColumnFamilyType.Super)
                predicate = SuperColumns.fromSCFilter((CompositeType)metadata.comparator, superColumn, predicate);
        }
        else
        {
            predicate = IDiskAtomFilter.Serializer.instance.deserialize(in, version, metadata.comparator);
        }

        List<IndexExpression> rowFilter;
        int filterCount = in.readInt();
        rowFilter = new ArrayList<IndexExpression>(filterCount);
        for (int i = 0; i < filterCount; i++)
        {
            IndexExpression expr;
            expr = new IndexExpression(ByteBufferUtil.readWithShortLength(in),
                                       IndexOperator.findByValue(in.readInt()),
                                       ByteBufferUtil.readWithShortLength(in));
            rowFilter.add(expr);
        }
        AbstractBounds<RowPosition> range = AbstractBounds.serializer.deserialize(in, version).toRowBounds();

        int maxResults = in.readInt();
        boolean countCQL3Rows = in.readBoolean();
        boolean isPaging = in.readBoolean();
        return new RangeSliceCommand(keyspace, columnFamily, timestamp, predicate, range, rowFilter, maxResults, countCQL3Rows, isPaging);
    }

+++++++++++++++++++++++
    public RangeSliceCommand deserialize(DataInput in, int version) throws IOException
    {
        String keyspace = in.readUTF();
        String columnFamily = in.readUTF();
        long timestamp = in.readLong();

        CFMetaData metadata = Schema.instance.getCFMetaData(keyspace, columnFamily);

        IDiskAtomFilter predicate = metadata.comparator.diskAtomFilterSerializer().deserialize(in, version);

        List<IndexExpression> rowFilter;
        int filterCount = in.readInt();
        rowFilter = new ArrayList<>(filterCount);
        for (int i = 0; i < filterCount; i++)
        {
            IndexExpression expr;
            expr = new IndexExpression(ByteBufferUtil.readWithShortLength(in),
                                       IndexExpression.Operator.findByOrdinal(in.readInt()),
                                       ByteBufferUtil.readWithShortLength(in));
            rowFilter.add(expr);
        }
        AbstractBounds<RowPosition> range = AbstractBounds.serializer.deserialize(in, version).toRowBounds();

        int maxResults = in.readInt();
        boolean countCQL3Rows = in.readBoolean();
        boolean isPaging = in.readBoolean();
        return new RangeSliceCommand(keyspace, columnFamily, timestamp, predicate, range, rowFilter, maxResults, countCQL3Rows, isPaging);
    }

    public void serialize(ReadCommand command, DataOutput out, int version) throws IOException
    {
        // For super columns, when talking to an older node, we need to translate the filter used.
        // That translation can change the filter type (names -> slice), and so change the command type.
        // Hence we need to detect that early on, before we've written the command type.
        ReadCommand newCommand = command;
        ByteBuffer superColumn = null;
        if (version < MessagingService.VERSION_20)
        {
            CFMetaData metadata = Schema.instance.getCFMetaData(command.ksName, command.cfName);
            if (metadata.cfType == ColumnFamilyType.Super)
            {
                SuperColumns.SCFilter scFilter = SuperColumns.filterToSC((CompositeType)metadata.comparator, command.filter());
                newCommand = ReadCommand.create(command.ksName, command.key, command.cfName, command.timestamp, scFilter.updatedFilter);
                newCommand.setDigestQuery(command.isDigestQuery());
                superColumn = scFilter.scName;
            }
        }

        out.writeByte(newCommand.commandType.serializedValue);
        switch (command.commandType)
        {
            case GET_BY_NAMES:
                SliceByNamesReadCommand.serializer.serialize(newCommand, superColumn, out, version);
                break;
            case GET_SLICES:
                SliceFromReadCommand.serializer.serialize(newCommand, superColumn, out, version);
                break;
            default:
                throw new AssertionError();
        }
    }

+++++++++++++++++++++++
    public void serialize(ReadCommand command, DataOutputPlus out, int version) throws IOException
    {
        out.writeByte(command.commandType.serializedValue);
        switch (command.commandType)
        {
            case GET_BY_NAMES:
                SliceByNamesReadCommand.serializer.serialize(command, out, version);
                break;
            case GET_SLICES:
                SliceFromReadCommand.serializer.serialize(command, out, version);
                break;
            default:
                throw new AssertionError();
        }
    }

    public long serializedSize(ReadCommand command, int version)
    {
        ReadCommand newCommand = command;
        ByteBuffer superColumn = null;
        if (version < MessagingService.VERSION_20)
        {
            CFMetaData metadata = Schema.instance.getCFMetaData(command.ksName, command.cfName);
            if (metadata.cfType == ColumnFamilyType.Super)
            {
                SuperColumns.SCFilter scFilter = SuperColumns.filterToSC((CompositeType)metadata.comparator, command.filter());
                newCommand = ReadCommand.create(command.ksName, command.key, command.cfName, command.timestamp, scFilter.updatedFilter);
                newCommand.setDigestQuery(command.isDigestQuery());
                superColumn = scFilter.scName;
            }
        }

        switch (command.commandType)
        {
            case GET_BY_NAMES:
                return 1 + SliceByNamesReadCommand.serializer.serializedSize(newCommand, superColumn, version);
            case GET_SLICES:
                return 1 + SliceFromReadCommand.serializer.serializedSize(newCommand, superColumn, version);
            default:
                throw new AssertionError();
        }
+++++++++++++++++++++++
    public long serializedSize(ReadCommand command, int version)
    {
        switch (command.commandType)
        {
            case GET_BY_NAMES:
                return 1 + SliceByNamesReadCommand.serializer.serializedSize(command, version);
            case GET_SLICES:
                return 1 + SliceFromReadCommand.serializer.serializedSize(command, version);
            default:
                throw new AssertionError();
        }
    public void serialize(ReadResponse response, DataOutput out, int version) throws IOException
    {
        out.writeInt(response.isDigestQuery() ? response.digest().remaining() : 0);
        ByteBuffer buffer = response.isDigestQuery() ? response.digest() : ByteBufferUtil.EMPTY_BYTE_BUFFER;
        ByteBufferUtil.write(buffer, out);
        out.writeBoolean(response.isDigestQuery());
        if (!response.isDigestQuery())
            Row.serializer.serialize(response.row(), out, version);
    }

+++++++++++++++++++++++
    public void serialize(ReadResponse response, DataOutputPlus out, int version) throws IOException
    {
        out.writeInt(response.isDigestQuery() ? response.digest().remaining() : 0);
        ByteBuffer buffer = response.isDigestQuery() ? response.digest() : ByteBufferUtil.EMPTY_BYTE_BUFFER;
        out.write(buffer);
        out.writeBoolean(response.isDigestQuery());
        if (!response.isDigestQuery())
            Row.serializer.serialize(response.row(), out, version);
    }

    public void serialize(ReadCommand cmd, ByteBuffer superColumn, DataOutput out, int version) throws IOException
    {
        SliceByNamesReadCommand command = (SliceByNamesReadCommand) cmd;
        out.writeBoolean(command.isDigestQuery());
        out.writeUTF(command.ksName);
        ByteBufferUtil.writeWithShortLength(command.key, out);

        if (version < MessagingService.VERSION_20)
            new QueryPath(command.cfName, superColumn).serialize(out);
        else
            out.writeUTF(command.cfName);

        if (version >= MessagingService.VERSION_20)
            out.writeLong(cmd.timestamp);

        NamesQueryFilter.serializer.serialize(command.filter, out, version);
    }

+++++++++++++++++++++++
    public void serialize(ReadCommand cmd, DataOutputPlus out, int version) throws IOException
    {
        SliceByNamesReadCommand command = (SliceByNamesReadCommand) cmd;
        out.writeBoolean(command.isDigestQuery());
        out.writeUTF(command.ksName);
        ByteBufferUtil.writeWithShortLength(command.key, out);
        out.writeUTF(command.cfName);
        out.writeLong(cmd.timestamp);

        CFMetaData metadata = Schema.instance.getCFMetaData(cmd.ksName, cmd.cfName);
        metadata.comparator.namesQueryFilterSerializer().serialize(command.filter, out, version);
    }

    public long serializedSize(ReadCommand cmd, ByteBuffer superColumn, int version)
    {
        TypeSizes sizes = TypeSizes.NATIVE;
        SliceByNamesReadCommand command = (SliceByNamesReadCommand) cmd;
        int size = sizes.sizeof(command.isDigestQuery());
        int keySize = command.key.remaining();

        size += sizes.sizeof(command.ksName);
        size += sizes.sizeof((short)keySize) + keySize;

        if (version < MessagingService.VERSION_20)
            size += new QueryPath(command.cfName, superColumn).serializedSize(sizes);
        else
            size += sizes.sizeof(command.cfName);

        if (version >= MessagingService.VERSION_20)
            size += sizes.sizeof(cmd.timestamp);

        size += NamesQueryFilter.serializer.serializedSize(command.filter, version);

        return size;
+++++++++++++++++++++++
    public long serializedSize(ReadCommand cmd, int version)
    {
        TypeSizes sizes = TypeSizes.NATIVE;
        SliceByNamesReadCommand command = (SliceByNamesReadCommand) cmd;
        int size = sizes.sizeof(command.isDigestQuery());
        int keySize = command.key.remaining();

        CFMetaData metadata = Schema.instance.getCFMetaData(cmd.ksName, cmd.cfName);

        size += sizes.sizeof(command.ksName);
        size += sizes.sizeof((short)keySize) + keySize;
        size += sizes.sizeof(command.cfName);
        size += sizes.sizeof(cmd.timestamp);
        size += metadata.comparator.namesQueryFilterSerializer().serializedSize(command.filter, version);

        return size;
    public ReadCommand deserialize(DataInput in, int version) throws IOException
    {
        boolean isDigest = in.readBoolean();
        String keyspaceName = in.readUTF();
        ByteBuffer key = ByteBufferUtil.readWithShortLength(in);

        String cfName;
        ByteBuffer sc = null;
        if (version < MessagingService.VERSION_20)
        {
            QueryPath path = QueryPath.deserialize(in);
            cfName = path.columnFamilyName;
            sc = path.superColumnName;
        }
        else
        {
            cfName = in.readUTF();
        }

        long timestamp = version < MessagingService.VERSION_20 ? System.currentTimeMillis() : in.readLong();

        CFMetaData metadata = Schema.instance.getCFMetaData(keyspaceName, cfName);
        if (metadata == null)
        {
            String message = String.format("Got slice command for nonexistent table %s.%s.  If the table was just " +
                    "created, this is likely due to the schema not being fully propagated.  Please wait for schema " +
                    "agreement on table creation." , keyspaceName, cfName);
            throw new UnknownColumnFamilyException(message, null);
        }

        ReadCommand command;
        if (version < MessagingService.VERSION_20)
        {
            AbstractType<?> comparator;
            if (metadata.cfType == ColumnFamilyType.Super)
            {
                CompositeType type = (CompositeType)metadata.comparator;
                comparator = sc == null ? type.types.get(0) : type.types.get(1);
            }
            else
            {
                comparator = metadata.comparator;
            }

            IDiskAtomFilter filter = NamesQueryFilter.serializer.deserialize(in, version, comparator);

            if (metadata.cfType == ColumnFamilyType.Super)
                filter = SuperColumns.fromSCFilter((CompositeType)metadata.comparator, sc, filter);

            // Due to SC compat, it's possible we get back a slice filter at this point
            if (filter instanceof NamesQueryFilter)
                command = new SliceByNamesReadCommand(keyspaceName, key, cfName, timestamp, (NamesQueryFilter)filter);
            else
                command = new SliceFromReadCommand(keyspaceName, key, cfName, timestamp, (SliceQueryFilter)filter);
        }
        else
        {
            NamesQueryFilter filter = NamesQueryFilter.serializer.deserialize(in, version, metadata.comparator);
            command = new SliceByNamesReadCommand(keyspaceName, key, cfName, timestamp, filter);
        }

        command.setDigestQuery(isDigest);
        return command;
    }

+++++++++++++++++++++++
    public ReadCommand deserialize(DataInput in, int version) throws IOException
    {
        boolean isDigest = in.readBoolean();
        String keyspaceName = in.readUTF();
        ByteBuffer key = ByteBufferUtil.readWithShortLength(in);
        String cfName = in.readUTF();
        long timestamp = in.readLong();
        CFMetaData metadata = Schema.instance.getCFMetaData(keyspaceName, cfName);
        NamesQueryFilter filter = metadata.comparator.namesQueryFilterSerializer().deserialize(in, version);
        ReadCommand command = new SliceByNamesReadCommand(keyspaceName, key, cfName, timestamp, filter);
        command.setDigestQuery(isDigest);
        return command;
    }

    public void serialize(ReadCommand rm, ByteBuffer superColumn, DataOutput out, int version) throws IOException
    {
        SliceFromReadCommand realRM = (SliceFromReadCommand)rm;
        out.writeBoolean(realRM.isDigestQuery());
        out.writeUTF(realRM.ksName);
        ByteBufferUtil.writeWithShortLength(realRM.key, out);

        if (version < MessagingService.VERSION_20)
            new QueryPath(realRM.cfName, superColumn).serialize(out);
        else
            out.writeUTF(realRM.cfName);

        if (version >= MessagingService.VERSION_20)
            out.writeLong(realRM.timestamp);

        SliceQueryFilter.serializer.serialize(realRM.filter, out, version);
    }

+++++++++++++++++++++++
    public void serialize(ReadCommand rm, DataOutputPlus out, int version) throws IOException
    {
        SliceFromReadCommand realRM = (SliceFromReadCommand)rm;
        out.writeBoolean(realRM.isDigestQuery());
        out.writeUTF(realRM.ksName);
        ByteBufferUtil.writeWithShortLength(realRM.key, out);
        out.writeUTF(realRM.cfName);
        out.writeLong(realRM.timestamp);
        CFMetaData metadata = Schema.instance.getCFMetaData(realRM.ksName, realRM.cfName);
        metadata.comparator.sliceQueryFilterSerializer().serialize(realRM.filter, out, version);
    }

    public long serializedSize(ReadCommand cmd, ByteBuffer superColumn, int version)
    {
        TypeSizes sizes = TypeSizes.NATIVE;
        SliceFromReadCommand command = (SliceFromReadCommand) cmd;
        int keySize = command.key.remaining();

        int size = sizes.sizeof(cmd.isDigestQuery()); // boolean
        size += sizes.sizeof(command.ksName);
        size += sizes.sizeof((short) keySize) + keySize;

        if (version < MessagingService.VERSION_20)
            size += new QueryPath(command.cfName, superColumn).serializedSize(sizes);
        else
            size += sizes.sizeof(command.cfName);

        if (version >= MessagingService.VERSION_20)
            size += sizes.sizeof(cmd.timestamp);

        size += SliceQueryFilter.serializer.serializedSize(command.filter, version);

        return size;
+++++++++++++++++++++++
    public long serializedSize(ReadCommand cmd, int version)
    {
        TypeSizes sizes = TypeSizes.NATIVE;
        SliceFromReadCommand command = (SliceFromReadCommand) cmd;
        int keySize = command.key.remaining();

        CFMetaData metadata = Schema.instance.getCFMetaData(cmd.ksName, cmd.cfName);

        int size = sizes.sizeof(cmd.isDigestQuery()); // boolean
        size += sizes.sizeof(command.ksName);
        size += sizes.sizeof((short) keySize) + keySize;
        size += sizes.sizeof(command.cfName);
        size += sizes.sizeof(cmd.timestamp);
        size += metadata.comparator.sliceQueryFilterSerializer().serializedSize(command.filter, version);

        return size;
    public ReadCommand deserialize(DataInput in, int version) throws IOException
    {
        boolean isDigest = in.readBoolean();
        String keyspaceName = in.readUTF();
        ByteBuffer key = ByteBufferUtil.readWithShortLength(in);

        String cfName;
        ByteBuffer sc = null;
        if (version < MessagingService.VERSION_20)
        {
            QueryPath path = QueryPath.deserialize(in);
            cfName = path.columnFamilyName;
            sc = path.superColumnName;
        }
        else
        {
            cfName = in.readUTF();
        }

        long timestamp = version < MessagingService.VERSION_20 ? System.currentTimeMillis() : in.readLong();

        CFMetaData metadata = Schema.instance.getCFMetaData(keyspaceName, cfName);
        if (metadata == null)
        {
            String message = String.format("Got slice command for nonexistent table %s.%s.  If the table was just " +
                    "created, this is likely due to the schema not being fully propagated.  Please wait for schema " +
                    "agreement on table creation." , keyspaceName, cfName);
            throw new UnknownColumnFamilyException(message, null);
        }

        SliceQueryFilter filter;
        if (version < MessagingService.VERSION_20)
        {
            filter = SliceQueryFilter.serializer.deserialize(in, version);

            if (metadata.cfType == ColumnFamilyType.Super)
                filter = SuperColumns.fromSCSliceFilter((CompositeType)metadata.comparator, sc, filter);
        }
        else
        {
            filter = SliceQueryFilter.serializer.deserialize(in, version);
        }

        ReadCommand command = new SliceFromReadCommand(keyspaceName, key, cfName, timestamp, filter);
        command.setDigestQuery(isDigest);
        return command;
    }

+++++++++++++++++++++++
    public ReadCommand deserialize(DataInput in, int version) throws IOException
    {
        boolean isDigest = in.readBoolean();
        String keyspaceName = in.readUTF();
        ByteBuffer key = ByteBufferUtil.readWithShortLength(in);
        String cfName = in.readUTF();
        long timestamp = in.readLong();
        CFMetaData metadata = Schema.instance.getCFMetaData(keyspaceName, cfName);
        SliceQueryFilter filter = metadata.comparator.sliceQueryFilterSerializer().deserialize(in, version);
        ReadCommand command = new SliceFromReadCommand(keyspaceName, key, cfName, timestamp, filter);
        command.setDigestQuery(isDigest);
        return command;
    }

    public void serialize(SnapshotCommand snapshot_command, DataOutput out, int version) throws IOException
    {
        out.writeUTF(snapshot_command.keyspace);
        out.writeUTF(snapshot_command.column_family);
        out.writeUTF(snapshot_command.snapshot_name);
        out.writeBoolean(snapshot_command.clear_snapshot);
    }

+++++++++++++++++++++++
    public void serialize(SnapshotCommand snapshot_command, DataOutputPlus out, int version) throws IOException
    {
        out.writeUTF(snapshot_command.keyspace);
        out.writeUTF(snapshot_command.column_family);
        out.writeUTF(snapshot_command.snapshot_name);
        out.writeBoolean(snapshot_command.clear_snapshot);
    }

    public static void deserializerSuperColumnFamily(DataInput in, ColumnFamily cf, ColumnSerializer.Flag flag, int version) throws IOException
    {
        // Note that there was no way to insert a range tombstone in a SCF in 1.2
        cf.delete(DeletionInfo.serializer().deserialize(in, version, cf.getComparator()));
        assert !cf.deletionInfo().rangeIterator().hasNext();

        Iterator<OnDiskAtom> iter = onDiskIterator(in, in.readInt(), flag, Integer.MIN_VALUE);
        while (iter.hasNext())
            cf.addAtom(iter.next());
    }

+++++++++++++++++++++++
    public static void deserializerSuperColumnFamily(DataInput in, ColumnFamily cf, ColumnSerializer.Flag flag, int version) throws IOException
    {
        // Note that there was no way to insert a range tombstone in a SCF in 1.2
        cf.delete(cf.getComparator().deletionInfoSerializer().deserialize(in, version));
        assert !cf.deletionInfo().rangeIterator().hasNext();

        Iterator<OnDiskAtom> iter = onDiskIterator(in, in.readInt(), flag, Integer.MIN_VALUE, cf.getComparator());
        while (iter.hasNext())
            cf.addAtom(iter.next());
    }

    private static void serializeSchema(Map<DecoratedKey, RowMutation> mutationMap, String schemaCfName)
    {
        for (Row schemaRow : serializedSchema(schemaCfName))
        {
            if (Schema.ignoredSchemaRow(schemaRow))
                continue;

            RowMutation mutation = mutationMap.get(schemaRow.key);
            if (mutation == null)
            {
                mutation = new RowMutation(Keyspace.SYSTEM_KS, schemaRow.key.key);
                mutationMap.put(schemaRow.key, mutation);
            }

            mutation.add(schemaRow.cf);
        }
    }

+++++++++++++++++++++++
    private static void serializeSchema(Map<DecoratedKey, Mutation> mutationMap, String schemaCfName)
    {
        for (Row schemaRow : serializedSchema(schemaCfName))
        {
            if (Schema.ignoredSchemaRow(schemaRow))
                continue;

            Mutation mutation = mutationMap.get(schemaRow.key);
            if (mutation == null)
            {
                mutation = new Mutation(Keyspace.SYSTEM_KS, schemaRow.key.getKey());
                mutationMap.put(schemaRow.key, mutation);
            }

            mutation.add(schemaRow.cf);
        }
    }

    public void serialize(Truncation t, DataOutput out, int version) throws IOException
    {
        out.writeUTF(t.keyspace);
        out.writeUTF(t.columnFamily);
    }

+++++++++++++++++++++++
    public void serialize(Truncation t, DataOutputPlus out, int version) throws IOException
    {
        out.writeUTF(t.keyspace);
        out.writeUTF(t.columnFamily);
    }

    public void serialize(EndpointState epState, DataOutput out, int version) throws IOException
    {
        /* serialize the HeartBeatState */
        HeartBeatState hbState = epState.getHeartBeatState();
        HeartBeatState.serializer.serialize(hbState, out, version);

        /* serialize the map of ApplicationState objects */
        int size = epState.applicationState.size();
        out.writeInt(size);
        for (Map.Entry<ApplicationState, VersionedValue> entry : epState.applicationState.entrySet())
        {
            VersionedValue value = entry.getValue();
            out.writeInt(entry.getKey().ordinal());
            VersionedValue.serializer.serialize(value, out, version);
        }
    }

+++++++++++++++++++++++
    public void serialize(EndpointState epState, DataOutputPlus out, int version) throws IOException
    {
        /* serialize the HeartBeatState */
        HeartBeatState hbState = epState.getHeartBeatState();
        HeartBeatState.serializer.serialize(hbState, out, version);

        /* serialize the map of ApplicationState objects */
        int size = epState.applicationState.size();
        out.writeInt(size);
        for (Map.Entry<ApplicationState, VersionedValue> entry : epState.applicationState.entrySet())
        {
            VersionedValue value = entry.getValue();
            out.writeInt(entry.getKey().ordinal());
            VersionedValue.serializer.serialize(value, out, version);
        }
    }

    public void serialize(GossipDigest gDigest, DataOutput out, int version) throws IOException
    {
        CompactEndpointSerializationHelper.serialize(gDigest.endpoint, out);
        out.writeInt(gDigest.generation);
        out.writeInt(gDigest.maxVersion);
    }

+++++++++++++++++++++++
    public void serialize(GossipDigest gDigest, DataOutputPlus out, int version) throws IOException
    {
        CompactEndpointSerializationHelper.serialize(gDigest.endpoint, out);
        out.writeInt(gDigest.generation);
        out.writeInt(gDigest.maxVersion);
    }

    public void serialize(GossipDigestAck gDigestAckMessage, DataOutput out, int version) throws IOException
    {
        GossipDigestSerializationHelper.serialize(gDigestAckMessage.gDigestList, out, version);
        out.writeInt(gDigestAckMessage.epStateMap.size());
        for (Map.Entry<InetAddress, EndpointState> entry : gDigestAckMessage.epStateMap.entrySet())
        {
            InetAddress ep = entry.getKey();
            CompactEndpointSerializationHelper.serialize(ep, out);
            EndpointState.serializer.serialize(entry.getValue(), out, version);
        }
    }

+++++++++++++++++++++++
    public void serialize(GossipDigestAck gDigestAckMessage, DataOutputPlus out, int version) throws IOException
    {
        GossipDigestSerializationHelper.serialize(gDigestAckMessage.gDigestList, out, version);
        out.writeInt(gDigestAckMessage.epStateMap.size());
        for (Map.Entry<InetAddress, EndpointState> entry : gDigestAckMessage.epStateMap.entrySet())
        {
            InetAddress ep = entry.getKey();
            CompactEndpointSerializationHelper.serialize(ep, out);
            EndpointState.serializer.serialize(entry.getValue(), out, version);
        }
    }

    public void serialize(GossipDigestAck2 ack2, DataOutput out, int version) throws IOException
    {
        out.writeInt(ack2.epStateMap.size());
        for (Map.Entry<InetAddress, EndpointState> entry : ack2.epStateMap.entrySet())
        {
            InetAddress ep = entry.getKey();
            CompactEndpointSerializationHelper.serialize(ep, out);
            EndpointState.serializer.serialize(entry.getValue(), out, version);
        }
    }

+++++++++++++++++++++++
    public void serialize(GossipDigestAck2 ack2, DataOutputPlus out, int version) throws IOException
    {
        out.writeInt(ack2.epStateMap.size());
        for (Map.Entry<InetAddress, EndpointState> entry : ack2.epStateMap.entrySet())
        {
            InetAddress ep = entry.getKey();
            CompactEndpointSerializationHelper.serialize(ep, out);
            EndpointState.serializer.serialize(entry.getValue(), out, version);
        }
    }

    public void serialize(GossipDigestSyn gDigestSynMessage, DataOutput out, int version) throws IOException
    {
        out.writeUTF(gDigestSynMessage.clusterId);
        out.writeUTF(gDigestSynMessage.partioner);
        GossipDigestSerializationHelper.serialize(gDigestSynMessage.gDigests, out, version);
    }

+++++++++++++++++++++++
    public void serialize(GossipDigestSyn gDigestSynMessage, DataOutputPlus out, int version) throws IOException
    {
        out.writeUTF(gDigestSynMessage.clusterId);
        out.writeUTF(gDigestSynMessage.partioner);
        GossipDigestSerializationHelper.serialize(gDigestSynMessage.gDigests, out, version);
    }

    static void serialize(List<GossipDigest> gDigestList, DataOutput out, int version) throws IOException
    {
        out.writeInt(gDigestList.size());
        for (GossipDigest gDigest : gDigestList)
            GossipDigest.serializer.serialize(gDigest, out, version);
    }

+++++++++++++++++++++++
    static void serialize(List<GossipDigest> gDigestList, DataOutputPlus out, int version) throws IOException
    {
        out.writeInt(gDigestList.size());
        for (GossipDigest gDigest : gDigestList)
            GossipDigest.serializer.serialize(gDigest, out, version);
    }

    public void serialize(HeartBeatState hbState, DataOutput out, int version) throws IOException
    {
        out.writeInt(hbState.getGeneration());
        out.writeInt(hbState.getHeartBeatVersion());
    }

+++++++++++++++++++++++
    public void serialize(HeartBeatState hbState, DataOutputPlus out, int version) throws IOException
    {
        out.writeInt(hbState.getGeneration());
        out.writeInt(hbState.getHeartBeatVersion());
    }

    public static List<IndexInfo> deserializeIndex(FileDataInput in) throws IOException
    {
        int columnIndexSize = in.readInt();
        if (columnIndexSize == 0)
            return Collections.<IndexInfo>emptyList();
        ArrayList<IndexInfo> indexList = new ArrayList<IndexInfo>();
        FileMark mark = in.mark();
        while (in.bytesPastMark(mark) < columnIndexSize)
        {
            indexList.add(IndexInfo.deserialize(in));
        }
        assert in.bytesPastMark(mark) == columnIndexSize;

        return indexList;
    }

    /**
     * The index of the IndexInfo in which a scan starting with @name should begin.
     *
     * @param name
     *         name of the index
     *
     * @param indexList
     *          list of the indexInfo objects
     *
     * @param comparator
     *          comparator type
     *
     * @param reversed
     *          is name reversed
     *
     * @return int index
     */
+++++++++++++++++++++++
    public static List<IndexInfo> deserializeIndex(FileDataInput in, CType type) throws IOException
    {
        int columnIndexSize = in.readInt();
        if (columnIndexSize == 0)
            return Collections.<IndexInfo>emptyList();
        ArrayList<IndexInfo> indexList = new ArrayList<IndexInfo>();
        FileMark mark = in.mark();
        ISerializer<IndexInfo> serializer = type.indexSerializer();
        while (in.bytesPastMark(mark) < columnIndexSize)
        {
            indexList.add(serializer.deserialize(in));
        }
        assert in.bytesPastMark(mark) == columnIndexSize;

        return indexList;
    }

    /**
     * The index of the IndexInfo in which a scan starting with @name should begin.
     *
     * @param name
     *         name of the index
     *
     * @param indexList
     *          list of the indexInfo objects
     *
     * @param comparator
     *          comparator type
     *
     * @param reversed
     *          is name reversed
     *
     * @return int index
     */
    public void serialize(DataOutputStream out, int version) throws IOException
    {
        CompactEndpointSerializationHelper.serialize(from, out);

        out.writeInt(verb.ordinal());
        out.writeInt(parameters.size());
        for (Map.Entry<String, byte[]> entry : parameters.entrySet())
        {
            out.writeUTF(entry.getKey());
            out.writeInt(entry.getValue().length);
            out.write(entry.getValue());
        }

        long longSize = payload == null ? 0 : serializer.serializedSize(payload, version);
        assert longSize <= Integer.MAX_VALUE; // larger values are supported in sstables but not messages
        out.writeInt((int) longSize);
        if (payload != null)
            serializer.serialize(payload, out, version);
    }

+++++++++++++++++++++++
    public void serialize(DataOutputPlus out, int version) throws IOException
    {
        CompactEndpointSerializationHelper.serialize(from, out);

        out.writeInt(verb.ordinal());
        out.writeInt(parameters.size());
        for (Map.Entry<String, byte[]> entry : parameters.entrySet())
        {
            out.writeUTF(entry.getKey());
            out.writeInt(entry.getValue().length);
            out.write(entry.getValue());
        }

        long longSize = payload == null ? 0 : serializer.serializedSize(payload, version);
        assert longSize <= Integer.MAX_VALUE; // larger values are supported in sstables but not messages
        out.writeInt((int) longSize);
        if (payload != null)
            serializer.serialize(payload, out, version);
    }

    public int serializedSize(int version)
    {
        int size = CompactEndpointSerializationHelper.serializedSize(from);

        size += TypeSizes.NATIVE.sizeof(verb.ordinal());
        size += TypeSizes.NATIVE.sizeof(parameters.size());
        for (Map.Entry<String, byte[]> entry : parameters.entrySet())
        {
            size += TypeSizes.NATIVE.sizeof(entry.getKey());
            size += TypeSizes.NATIVE.sizeof(entry.getValue().length);
            size += entry.getValue().length;
        }

        long longSize = payload == null ? 0 : serializer.serializedSize(payload, version);
        assert longSize <= Integer.MAX_VALUE; // larger values are supported in sstables but not messages
        size += TypeSizes.NATIVE.sizeof((int) longSize);
        size += longSize;
        return size;
+++++++++++++++++++++++
    public int serializedSize(int version)
    {
        int size = CompactEndpointSerializationHelper.serializedSize(from);

        size += TypeSizes.NATIVE.sizeof(verb.ordinal());
        size += TypeSizes.NATIVE.sizeof(parameters.size());
        for (Map.Entry<String, byte[]> entry : parameters.entrySet())
        {
            TypeSizes.NATIVE.sizeof(entry.getKey());
            TypeSizes.NATIVE.sizeof(entry.getValue().length);
            size += entry.getValue().length;
        }

        long longSize = payload == null ? 0 : serializer.serializedSize(payload, version);
        assert longSize <= Integer.MAX_VALUE; // larger values are supported in sstables but not messages
        size += TypeSizes.NATIVE.sizeof((int) longSize);
        size += longSize;
        return size;
    public ByteBuffer serialize()
    {
        try
        {
            ByteBuffer result = ByteBuffer.allocate(serializedSize());
            DataOutput out = new DataOutputStream(new ByteBufferOutputStream(result));
            ByteBufferUtil.writeWithShortLength(partitionKey, out);
            ByteBufferUtil.writeWithShortLength(cellName, out);
            out.writeInt(remaining);
            result.flip();
            return result;
        }
        catch (IOException e)
        {
            throw new RuntimeException(e);
        }
    }

+++++++++++++++++++++++
    public ByteBuffer serialize()
    {
        try
        {
            DataOutputBuffer out = new DataOutputBuffer(serializedSize());
            ByteBufferUtil.writeWithShortLength(partitionKey, out);
            ByteBufferUtil.writeWithShortLength(cellName, out);
            out.writeInt(remaining);
            return out.asByteBuffer();
        }
        catch (IOException e)
        {
            throw new RuntimeException(e);
        }
    }

    public static void serialize(StreamMessage message, WritableByteChannel out, int version, StreamSession session) throws IOException
    {
        ByteBuffer buff = ByteBuffer.allocate(1);
        // message type
        buff.put(message.type.type);
        buff.flip();
        while (buff.hasRemaining())
            out.write(buff);
        message.type.outSerializer.serialize(message, out, version, session);
    }

+++++++++++++++++++++++
    public static void serialize(StreamMessage message, DataOutputStreamAndChannel out, int version, StreamSession session) throws IOException
    {
        ByteBuffer buff = ByteBuffer.allocate(1);
        // message type
        buff.put(message.type.type);
        buff.flip();
        out.write(buff);
        message.type.outSerializer.serialize(message, out, version, session);
    }

    public void serialize(ChannelBuffer dest)
    {
        CBUtil.writeEnumValue(type, dest);
        serializeEvent(dest);
    }

+++++++++++++++++++++++
    public void serialize(ByteBuf dest, int version)
    {
        CBUtil.writeEnumValue(type, dest);
        serializeEvent(dest, version);
    }

    protected abstract void serializeEvent(ChannelBuffer dest);
+++++++++++++++++++++++
    protected abstract void serializeEvent(ByteBuf dest, int version);
    public int serializedSize()
    {
        return CBUtil.sizeOfEnumValue(type) + eventSerializedSize();
    }

+++++++++++++++++++++++
    public int serializedSize(int version)
    {
        return CBUtil.sizeOfEnumValue(type) + eventSerializedSize(version);
    }

    public static Event deserialize(ChannelBuffer cb)
    {
        switch (CBUtil.readEnumValue(Type.class, cb))
        {
            case TOPOLOGY_CHANGE:
                return TopologyChange.deserializeEvent(cb);
            case STATUS_CHANGE:
                return StatusChange.deserializeEvent(cb);
            case SCHEMA_CHANGE:
                return SchemaChange.deserializeEvent(cb);
        }
        throw new AssertionError();
    }

+++++++++++++++++++++++
    public static Event deserialize(ByteBuf cb, int version)
    {
        switch (CBUtil.readEnumValue(Type.class, cb))
        {
            case TOPOLOGY_CHANGE:
                return TopologyChange.deserializeEvent(cb, version);
            case STATUS_CHANGE:
                return StatusChange.deserializeEvent(cb, version);
            case SCHEMA_CHANGE:
                return SchemaChange.deserializeEvent(cb, version);
        }
        throw new AssertionError();
    }

    public void serialize(BloomFilter bf, DataOutput out) throws IOException
    {
        out.writeInt(bf.hashCount);
        bf.bitset.serialize(out);
    }

+++++++++++++++++++++++
    public void serialize(BloomFilter bf, DataOutputPlus out) throws IOException
    {
        out.writeInt(bf.hashCount);
        bf.bitset.serialize(out);
    }

    public void serialize(Boolean b, DataOutput out, int version) throws IOException
    {
        out.writeBoolean(b);
    }

+++++++++++++++++++++++
    public void serialize(Boolean b, DataOutputPlus out, int version) throws IOException
    {
        out.writeBoolean(b);
    }

    public static void serialize(IFilter bf, DataOutput output) throws IOException
    {
        Murmur3BloomFilter.serializer.serialize((Murmur3BloomFilter) bf, output);
    }

+++++++++++++++++++++++
    public static void serialize(IFilter bf, DataOutputPlus output) throws IOException
    {
        Murmur3BloomFilter.serializer.serialize((Murmur3BloomFilter) bf, output);
    }

    public static <C, D, I extends Interval<C, D>> Serializer<C, D, I> serializer(ISerializer<C> pointSerializer, ISerializer<D> dataSerializer, Constructor<I> constructor)
    {
        return new Serializer(pointSerializer, dataSerializer, constructor);
    }

    @SuppressWarnings("unchecked")
+++++++++++++++++++++++
    public static <C, D, I extends Interval<C, D>> Serializer<C, D, I> serializer(ISerializer<C> pointSerializer, ISerializer<D> dataSerializer, Constructor<I> constructor)
    {
        return new Serializer<>(pointSerializer, dataSerializer, constructor);
    }

    @SuppressWarnings("unchecked")
    public void serialize(UUID uuid, DataOutput out, int version) throws IOException
    {
        out.writeLong(uuid.getMostSignificantBits());
        out.writeLong(uuid.getLeastSignificantBits());
    }

+++++++++++++++++++++++
    public void serialize(UUID uuid, DataOutputPlus out, int version) throws IOException
    {
        out.writeLong(uuid.getMostSignificantBits());
        out.writeLong(uuid.getLeastSignificantBits());
    }

    public static OffHeapBitSet deserialize(DataInput in) throws IOException
    {
        long byteCount = in.readInt() * 8L;
        Memory memory = RefCountedMemory.allocate(byteCount);
        for (long i = 0; i < byteCount;)
        {
            long v = in.readLong();
            memory.setByte(i++, (byte) (v >>> 0));
            memory.setByte(i++, (byte) (v >>> 8));
            memory.setByte(i++, (byte) (v >>> 16));
            memory.setByte(i++, (byte) (v >>> 24));
            memory.setByte(i++, (byte) (v >>> 32));
            memory.setByte(i++, (byte) (v >>> 40));
            memory.setByte(i++, (byte) (v >>> 48));
            memory.setByte(i++, (byte) (v >>> 56));
        }
        return new OffHeapBitSet(memory);
    }

+++++++++++++++++++++++
    public static OffHeapBitSet deserialize(DataInput in) throws IOException
    {
        long byteCount = in.readInt() * 8L;
        Memory memory = Memory.allocate(byteCount);
        for (long i = 0; i < byteCount;)
        {
            long v = in.readLong();
            memory.setByte(i++, (byte) (v >>> 0));
            memory.setByte(i++, (byte) (v >>> 8));
            memory.setByte(i++, (byte) (v >>> 16));
            memory.setByte(i++, (byte) (v >>> 24));
            memory.setByte(i++, (byte) (v >>> 32));
            memory.setByte(i++, (byte) (v >>> 40));
            memory.setByte(i++, (byte) (v >>> 48));
            memory.setByte(i++, (byte) (v >>> 56));
        }
        return new OffHeapBitSet(memory);
    }

    protected void testSerializeDeserialize(ColumnDefinition cd) throws Exception
    {
        ColumnDefinition newCd = ColumnDefinition.fromThrift(cd.toThrift(), false);
        assert cd != newCd;
        assert cd.hashCode() == newCd.hashCode();
        assert cd.equals(newCd);
+++++++++++++++++++++++
    protected void testSerializeDeserialize(CFMetaData cfm, ColumnDefinition cd) throws Exception
    {
        ColumnDefinition newCd = ColumnDefinition.fromThrift(cfm.ksName, cfm.cfName, cfm.comparator.asAbstractType(), null, cd.toThrift());
        Assert.assertNotSame(cd, newCd);
        Assert.assertEquals(cd.hashCode(), newCd.hashCode());
        Assert.assertEquals(cd, newCd);
add: 269, delete: 126, change: 632, unhandled: 0 size_exceptions: 115 size_serialize: 48
-----------cassandra-2.1.0 vs cassandra-2.1.0-beta1-----------
    private RefCountedMemory serialize(V value)
    {
        long serializedSize = serializer.serializedSize(value, ENCODED_TYPE_SIZES);
        if (serializedSize > Integer.MAX_VALUE)
            throw new IllegalArgumentException("Unable to allocate " + serializedSize + " bytes");

        RefCountedMemory freeableMemory;
        try
        {
            freeableMemory = new RefCountedMemory(serializedSize);
        }
        catch (OutOfMemoryError e)
        {
            return null;
        }

        try
        {
            serializer.serialize(value, new EncodedDataOutputStream(new MemoryOutputStream(freeableMemory)));
        }
        catch (IOException e)
        {
            freeableMemory.unreference();
            throw new RuntimeException(e);
        }
        return freeableMemory;
    }

+++++++++++++++++++++++
    private RefCountedMemory serialize(V value)
    {
        long serializedSize = serializer.serializedSize(value, ENCODED_TYPE_SIZES);
        if (serializedSize > Integer.MAX_VALUE)
            throw new IllegalArgumentException("Unable to allocate " + serializedSize + " bytes");

        RefCountedMemory freeableMemory;
        try
        {
            freeableMemory = new RefCountedMemory(serializedSize);
        }
        catch (OutOfMemoryError e)
        {
            return null;
        }

        try
        {
            serializer.serialize(value, new EncodedDataOutputStream(new MemoryOutputStream(freeableMemory)));
        }
        catch (IOException e)
        {
            throw new RuntimeException(e);
        }
        return freeableMemory;
    }

    private V deserialize(RefCountedMemory mem)
    {
        try
        {
            return serializer.deserialize(new EncodedDataInputStream(new MemoryInputStream(mem)));
        }
        catch (IOException e)
        {
            logger.debug("Cannot fetch in memory data, we will fallback to read from disk ", e);
            return null;
        }
    }

+++++++++++++++++++++++
    private V deserialize(RefCountedMemory mem)
    {
        try
        {
            return serializer.deserialize(new EncodedDataInputStream(new MemoryInputStream(mem)));
        }
        catch (IOException e)
        {
            logger.debug("Cannot fetch in memory data, we will failback to read from disk ", e);
            return null;
        }
    }

    private static ByteBuffer serializeMutations(Collection<Mutation> mutations, int version)
    {
        DataOutputBuffer buf = new DataOutputBuffer();

        try
        {
            buf.writeInt(mutations.size());
            for (Mutation mutation : mutations)
                Mutation.serializer.serialize(mutation, buf, version);
        }
        catch (IOException e)
        {
            throw new AssertionError(); // cannot happen.
        }

        return buf.asByteBuffer();
    }

    @VisibleForTesting
+++++++++++++++++++++++
    private static ByteBuffer serializeMutations(Collection<Mutation> mutations)
    {
        FastByteArrayOutputStream bos = new FastByteArrayOutputStream();
        DataOutputStream out = new DataOutputStream(bos);

        try
        {
            out.writeInt(mutations.size());
            for (Mutation mutation : mutations)
                Mutation.serializer.serialize(mutation, out, VERSION);
        }
        catch (IOException e)
        {
            throw new AssertionError(); // cannot happen.
        }

        return ByteBuffer.wrap(bos.toByteArray());
    }

    @VisibleForTesting
    public void serializeCfId(UUID cfId, DataOutputPlus out, int version) throws IOException
    {
        UUIDSerializer.serializer.serialize(cfId, out, version);
    }

+++++++++++++++++++++++
    public void serializeCfId(UUID cfId, DataOutput out, int version) throws IOException
    {
        UUIDSerializer.serializer.serialize(cfId, out, version);
    }

    public void serializeForSSTable(ColumnFamily cf, DataOutputPlus out)
    {
        // Column families shouldn't be written directly to disk, use ColumnIndex.Builder instead
        throw new UnsupportedOperationException();
    }

+++++++++++++++++++++++
    public void serializeForSSTable(ColumnFamily cf, DataOutput out)
    {
        // Column families shouldn't be written directly to disk, use ColumnIndex.Builder instead
        throw new UnsupportedOperationException();
    }

    public void serialize(ColumnFamily cf, DataOutputPlus out, int version)
    {
        try
        {
            if (cf == null)
            {
                out.writeBoolean(false);
                return;
            }

            out.writeBoolean(true);
            serializeCfId(cf.id(), out, version);
            cf.getComparator().deletionInfoSerializer().serialize(cf.deletionInfo(), out, version);
            ColumnSerializer columnSerializer = cf.getComparator().columnSerializer();
            int count = cf.getColumnCount();
            out.writeInt(count);
            int written = 0;
            for (Cell cell : cf)
            {
                columnSerializer.serialize(cell, out);
                written++;
            }
            assert count == written: "Column family had " + count + " columns, but " + written + " written";
        }
        catch (IOException e)
        {
            throw new RuntimeException(e);
        }
    }

+++++++++++++++++++++++
    public void serialize(ColumnFamily cf, DataOutput out, int version)
    {
        try
        {
            if (cf == null)
            {
                out.writeBoolean(false);
                return;
            }

            out.writeBoolean(true);
            serializeCfId(cf.id(), out, version);
            cf.getComparator().deletionInfoSerializer().serialize(cf.deletionInfo(), out, version);
            ColumnSerializer columnSerializer = cf.getComparator().columnSerializer();
            int count = cf.getColumnCount();
            out.writeInt(count);
            int written = 0;
            for (Cell cell : cf)
            {
                columnSerializer.serialize(cell, out);
                written++;
            }
            assert count == written: "Column family had " + count + " columns, but " + written + " written";
        }
        catch (IOException e)
        {
            throw new RuntimeException(e);
        }
    }

    public void serialize(Cell cell, DataOutputPlus out) throws IOException
    {
        assert !cell.name().isEmpty();
        type.cellSerializer().serialize(cell.name(), out);
        try
        {
            out.writeByte(cell.serializationFlags());
            if (cell instanceof CounterCell)
            {
                out.writeLong(((CounterCell) cell).timestampOfLastDelete());
            }
            else if (cell instanceof ExpiringCell)
            {
                out.writeInt(((ExpiringCell) cell).getTimeToLive());
                out.writeInt(cell.getLocalDeletionTime());
            }
            out.writeLong(cell.timestamp());
            ByteBufferUtil.writeWithLength(cell.value(), out);
        }
        catch (IOException e)
        {
            throw new RuntimeException(e);
        }
    }

+++++++++++++++++++++++
    public void serialize(Cell cell, DataOutput out) throws IOException
    {
        assert !cell.name().isEmpty();
        type.cellSerializer().serialize(cell.name(), out);
        try
        {
            out.writeByte(cell.serializationFlags());
            if (cell instanceof CounterCell)
            {
                out.writeLong(((CounterCell) cell).timestampOfLastDelete());
            }
            else if (cell instanceof ExpiringCell)
            {
                out.writeInt(((ExpiringCell) cell).getTimeToLive());
                out.writeInt(cell.getLocalDeletionTime());
            }
            out.writeLong(cell.timestamp());
            ByteBufferUtil.writeWithLength(cell.value(), out);
        }
        catch (IOException e)
        {
            throw new RuntimeException(e);
        }
    }

    Cell deserializeColumnBody(DataInput in, CellName name, int mask, ColumnSerializer.Flag flag, int expireBefore) throws IOException
    {
        if ((mask & COUNTER_MASK) != 0)
        {
            long timestampOfLastDelete = in.readLong();
            long ts = in.readLong();
            ByteBuffer value = ByteBufferUtil.readWithLength(in);
            return BufferCounterCell.create(name, value, ts, timestampOfLastDelete, flag);
        }
        else if ((mask & EXPIRATION_MASK) != 0)
        {
            int ttl = in.readInt();
            int expiration = in.readInt();
            long ts = in.readLong();
            ByteBuffer value = ByteBufferUtil.readWithLength(in);
            return BufferExpiringCell.create(name, value, ts, ttl, expiration, expireBefore, flag);
        }
        else
        {
            long ts = in.readLong();
            ByteBuffer value = ByteBufferUtil.readWithLength(in);
            return (mask & COUNTER_UPDATE_MASK) != 0
                   ? new BufferCounterUpdateCell(name, value, ts)
                   : ((mask & DELETION_MASK) == 0
                      ? new BufferCell(name, value, ts)
                      : new BufferDeletedCell(name, value, ts));
        }
    }

+++++++++++++++++++++++
    Cell deserializeColumnBody(DataInput in, CellName name, int mask, ColumnSerializer.Flag flag, int expireBefore) throws IOException
    {
        if ((mask & COUNTER_MASK) != 0)
        {
            long timestampOfLastDelete = in.readLong();
            long ts = in.readLong();
            ByteBuffer value = ByteBufferUtil.readWithLength(in);
            return CounterCell.create(name, value, ts, timestampOfLastDelete, flag);
        }
        else if ((mask & EXPIRATION_MASK) != 0)
        {
            int ttl = in.readInt();
            int expiration = in.readInt();
            long ts = in.readLong();
            ByteBuffer value = ByteBufferUtil.readWithLength(in);
            return ExpiringCell.create(name, value, ts, ttl, expiration, expireBefore, flag);
        }
        else
        {
            long ts = in.readLong();
            ByteBuffer value = ByteBufferUtil.readWithLength(in);
            return (mask & COUNTER_UPDATE_MASK) != 0
                   ? new CounterUpdateCell(name, value, ts)
                   : ((mask & DELETION_MASK) == 0
                      ? new Cell(name, value, ts)
                      : new DeletedCell(name, value, ts));
        }
    }

    public void serialize(RangeSliceCommand sliceCommand, DataOutputPlus out, int version) throws IOException
    {
        out.writeUTF(sliceCommand.keyspace);
        out.writeUTF(sliceCommand.columnFamily);
        out.writeLong(sliceCommand.timestamp);

        CFMetaData metadata = Schema.instance.getCFMetaData(sliceCommand.keyspace, sliceCommand.columnFamily);

        metadata.comparator.diskAtomFilterSerializer().serialize(sliceCommand.predicate, out, version);

        if (sliceCommand.rowFilter == null)
        {
            out.writeInt(0);
        }
        else
        {
            out.writeInt(sliceCommand.rowFilter.size());
            for (IndexExpression expr : sliceCommand.rowFilter)
            {
                ByteBufferUtil.writeWithShortLength(expr.column, out);
                out.writeInt(expr.operator.ordinal());
                ByteBufferUtil.writeWithShortLength(expr.value, out);
            }
        }
        AbstractBounds.serializer.serialize(sliceCommand.keyRange, out, version);
        out.writeInt(sliceCommand.maxResults);
        out.writeBoolean(sliceCommand.countCQL3Rows);
        out.writeBoolean(sliceCommand.isPaging);
    }

+++++++++++++++++++++++
    public void serialize(RangeSliceCommand sliceCommand, DataOutput out, int version) throws IOException
    {
        out.writeUTF(sliceCommand.keyspace);
        out.writeUTF(sliceCommand.columnFamily);
        out.writeLong(sliceCommand.timestamp);

        CFMetaData metadata = Schema.instance.getCFMetaData(sliceCommand.keyspace, sliceCommand.columnFamily);

        metadata.comparator.diskAtomFilterSerializer().serialize(sliceCommand.predicate, out, version);

        if (sliceCommand.rowFilter == null)
        {
            out.writeInt(0);
        }
        else
        {
            out.writeInt(sliceCommand.rowFilter.size());
            for (IndexExpression expr : sliceCommand.rowFilter)
            {
                ByteBufferUtil.writeWithShortLength(expr.column, out);
                out.writeInt(expr.operator.ordinal());
                ByteBufferUtil.writeWithShortLength(expr.value, out);
            }
        }
        AbstractBounds.serializer.serialize(sliceCommand.keyRange, out, version);
        out.writeInt(sliceCommand.maxResults);
        out.writeBoolean(sliceCommand.countCQL3Rows);
        out.writeBoolean(sliceCommand.isPaging);
    }

    public void serialize(ReadCommand command, DataOutputPlus out, int version) throws IOException
    {
        out.writeByte(command.commandType.serializedValue);
        switch (command.commandType)
        {
            case GET_BY_NAMES:
                SliceByNamesReadCommand.serializer.serialize(command, out, version);
                break;
            case GET_SLICES:
                SliceFromReadCommand.serializer.serialize(command, out, version);
                break;
            default:
                throw new AssertionError();
        }
    }

+++++++++++++++++++++++
    public void serialize(ReadCommand command, DataOutput out, int version) throws IOException
    {
        out.writeByte(command.commandType.serializedValue);
        switch (command.commandType)
        {
            case GET_BY_NAMES:
                SliceByNamesReadCommand.serializer.serialize(command, out, version);
                break;
            case GET_SLICES:
                SliceFromReadCommand.serializer.serialize(command, out, version);
                break;
            default:
                throw new AssertionError();
        }
    }

    public void serialize(ReadResponse response, DataOutputPlus out, int version) throws IOException
    {
        out.writeInt(response.isDigestQuery() ? response.digest().remaining() : 0);
        ByteBuffer buffer = response.isDigestQuery() ? response.digest() : ByteBufferUtil.EMPTY_BYTE_BUFFER;
        out.write(buffer);
        out.writeBoolean(response.isDigestQuery());
        if (!response.isDigestQuery())
            Row.serializer.serialize(response.row(), out, version);
    }

+++++++++++++++++++++++
    public void serialize(ReadResponse response, DataOutput out, int version) throws IOException
    {
        out.writeInt(response.isDigestQuery() ? response.digest().remaining() : 0);
        ByteBuffer buffer = response.isDigestQuery() ? response.digest() : ByteBufferUtil.EMPTY_BYTE_BUFFER;
        ByteBufferUtil.write(buffer, out);
        out.writeBoolean(response.isDigestQuery());
        if (!response.isDigestQuery())
            Row.serializer.serialize(response.row(), out, version);
    }

    public void serialize(ReadCommand cmd, DataOutputPlus out, int version) throws IOException
    {
        SliceByNamesReadCommand command = (SliceByNamesReadCommand) cmd;
        out.writeBoolean(command.isDigestQuery());
        out.writeUTF(command.ksName);
        ByteBufferUtil.writeWithShortLength(command.key, out);
        out.writeUTF(command.cfName);
        out.writeLong(cmd.timestamp);

        CFMetaData metadata = Schema.instance.getCFMetaData(cmd.ksName, cmd.cfName);
        metadata.comparator.namesQueryFilterSerializer().serialize(command.filter, out, version);
    }

+++++++++++++++++++++++
    public void serialize(ReadCommand cmd, DataOutput out, int version) throws IOException
    {
        SliceByNamesReadCommand command = (SliceByNamesReadCommand) cmd;
        out.writeBoolean(command.isDigestQuery());
        out.writeUTF(command.ksName);
        ByteBufferUtil.writeWithShortLength(command.key, out);
        out.writeUTF(command.cfName);
        out.writeLong(cmd.timestamp);

        CFMetaData metadata = Schema.instance.getCFMetaData(cmd.ksName, cmd.cfName);
        metadata.comparator.namesQueryFilterSerializer().serialize(command.filter, out, version);
    }

    public void serialize(ReadCommand rm, DataOutputPlus out, int version) throws IOException
    {
        SliceFromReadCommand realRM = (SliceFromReadCommand)rm;
        out.writeBoolean(realRM.isDigestQuery());
        out.writeUTF(realRM.ksName);
        ByteBufferUtil.writeWithShortLength(realRM.key, out);
        out.writeUTF(realRM.cfName);
        out.writeLong(realRM.timestamp);
        CFMetaData metadata = Schema.instance.getCFMetaData(realRM.ksName, realRM.cfName);
        metadata.comparator.sliceQueryFilterSerializer().serialize(realRM.filter, out, version);
    }

+++++++++++++++++++++++
    public void serialize(ReadCommand rm, DataOutput out, int version) throws IOException
    {
        SliceFromReadCommand realRM = (SliceFromReadCommand)rm;
        out.writeBoolean(realRM.isDigestQuery());
        out.writeUTF(realRM.ksName);
        ByteBufferUtil.writeWithShortLength(realRM.key, out);
        out.writeUTF(realRM.cfName);
        out.writeLong(realRM.timestamp);
        CFMetaData metadata = Schema.instance.getCFMetaData(realRM.ksName, realRM.cfName);
        metadata.comparator.sliceQueryFilterSerializer().serialize(realRM.filter, out, version);
    }

    public void serialize(SnapshotCommand snapshot_command, DataOutputPlus out, int version) throws IOException
    {
        out.writeUTF(snapshot_command.keyspace);
        out.writeUTF(snapshot_command.column_family);
        out.writeUTF(snapshot_command.snapshot_name);
        out.writeBoolean(snapshot_command.clear_snapshot);
    }

+++++++++++++++++++++++
    public void serialize(SnapshotCommand snapshot_command, DataOutput out, int version) throws IOException
    {
        out.writeUTF(snapshot_command.keyspace);
        out.writeUTF(snapshot_command.column_family);
        out.writeUTF(snapshot_command.snapshot_name);
        out.writeBoolean(snapshot_command.clear_snapshot);
    }

    private static void serializeSchema(Map<DecoratedKey, Mutation> mutationMap, String schemaCfName)
    {
        for (Row schemaRow : serializedSchema(schemaCfName))
        {
            if (Schema.ignoredSchemaRow(schemaRow))
                continue;

            Mutation mutation = mutationMap.get(schemaRow.key);
            if (mutation == null)
            {
                mutation = new Mutation(Keyspace.SYSTEM_KS, schemaRow.key.getKey());
                mutationMap.put(schemaRow.key, mutation);
            }

            mutation.add(schemaRow.cf);
        }
    }

+++++++++++++++++++++++
    private static void serializeSchema(Map<DecoratedKey, Mutation> mutationMap, String schemaCfName)
    {
        for (Row schemaRow : serializedSchema(schemaCfName))
        {
            if (Schema.ignoredSchemaRow(schemaRow))
                continue;

            Mutation mutation = mutationMap.get(schemaRow.key);
            if (mutation == null)
            {
                mutation = new Mutation(Keyspace.SYSTEM_KS, schemaRow.key.key);
                mutationMap.put(schemaRow.key, mutation);
            }

            mutation.add(schemaRow.cf);
        }
    }

    public void serialize(Truncation t, DataOutputPlus out, int version) throws IOException
    {
        out.writeUTF(t.keyspace);
        out.writeUTF(t.columnFamily);
    }

+++++++++++++++++++++++
    public void serialize(Truncation t, DataOutput out, int version) throws IOException
    {
        out.writeUTF(t.keyspace);
        out.writeUTF(t.columnFamily);
    }

    public void serialize(EndpointState epState, DataOutputPlus out, int version) throws IOException
    {
        /* serialize the HeartBeatState */
        HeartBeatState hbState = epState.getHeartBeatState();
        HeartBeatState.serializer.serialize(hbState, out, version);

        /* serialize the map of ApplicationState objects */
        int size = epState.applicationState.size();
        out.writeInt(size);
        for (Map.Entry<ApplicationState, VersionedValue> entry : epState.applicationState.entrySet())
        {
            VersionedValue value = entry.getValue();
            out.writeInt(entry.getKey().ordinal());
            VersionedValue.serializer.serialize(value, out, version);
        }
    }

+++++++++++++++++++++++
    public void serialize(EndpointState epState, DataOutput out, int version) throws IOException
    {
        /* serialize the HeartBeatState */
        HeartBeatState hbState = epState.getHeartBeatState();
        HeartBeatState.serializer.serialize(hbState, out, version);

        /* serialize the map of ApplicationState objects */
        int size = epState.applicationState.size();
        out.writeInt(size);
        for (Map.Entry<ApplicationState, VersionedValue> entry : epState.applicationState.entrySet())
        {
            VersionedValue value = entry.getValue();
            out.writeInt(entry.getKey().ordinal());
            VersionedValue.serializer.serialize(value, out, version);
        }
    }

    public void serialize(GossipDigest gDigest, DataOutputPlus out, int version) throws IOException
    {
        CompactEndpointSerializationHelper.serialize(gDigest.endpoint, out);
        out.writeInt(gDigest.generation);
        out.writeInt(gDigest.maxVersion);
    }

+++++++++++++++++++++++
    public void serialize(GossipDigest gDigest, DataOutput out, int version) throws IOException
    {
        CompactEndpointSerializationHelper.serialize(gDigest.endpoint, out);
        out.writeInt(gDigest.generation);
        out.writeInt(gDigest.maxVersion);
    }

    public void serialize(GossipDigestAck gDigestAckMessage, DataOutputPlus out, int version) throws IOException
    {
        GossipDigestSerializationHelper.serialize(gDigestAckMessage.gDigestList, out, version);
        out.writeInt(gDigestAckMessage.epStateMap.size());
        for (Map.Entry<InetAddress, EndpointState> entry : gDigestAckMessage.epStateMap.entrySet())
        {
            InetAddress ep = entry.getKey();
            CompactEndpointSerializationHelper.serialize(ep, out);
            EndpointState.serializer.serialize(entry.getValue(), out, version);
        }
    }

+++++++++++++++++++++++
    public void serialize(GossipDigestAck gDigestAckMessage, DataOutput out, int version) throws IOException
    {
        GossipDigestSerializationHelper.serialize(gDigestAckMessage.gDigestList, out, version);
        out.writeInt(gDigestAckMessage.epStateMap.size());
        for (Map.Entry<InetAddress, EndpointState> entry : gDigestAckMessage.epStateMap.entrySet())
        {
            InetAddress ep = entry.getKey();
            CompactEndpointSerializationHelper.serialize(ep, out);
            EndpointState.serializer.serialize(entry.getValue(), out, version);
        }
    }

    public void serialize(GossipDigestAck2 ack2, DataOutputPlus out, int version) throws IOException
    {
        out.writeInt(ack2.epStateMap.size());
        for (Map.Entry<InetAddress, EndpointState> entry : ack2.epStateMap.entrySet())
        {
            InetAddress ep = entry.getKey();
            CompactEndpointSerializationHelper.serialize(ep, out);
            EndpointState.serializer.serialize(entry.getValue(), out, version);
        }
    }

+++++++++++++++++++++++
    public void serialize(GossipDigestAck2 ack2, DataOutput out, int version) throws IOException
    {
        out.writeInt(ack2.epStateMap.size());
        for (Map.Entry<InetAddress, EndpointState> entry : ack2.epStateMap.entrySet())
        {
            InetAddress ep = entry.getKey();
            CompactEndpointSerializationHelper.serialize(ep, out);
            EndpointState.serializer.serialize(entry.getValue(), out, version);
        }
    }

    public void serialize(GossipDigestSyn gDigestSynMessage, DataOutputPlus out, int version) throws IOException
    {
        out.writeUTF(gDigestSynMessage.clusterId);
        out.writeUTF(gDigestSynMessage.partioner);
        GossipDigestSerializationHelper.serialize(gDigestSynMessage.gDigests, out, version);
    }

+++++++++++++++++++++++
    public void serialize(GossipDigestSyn gDigestSynMessage, DataOutput out, int version) throws IOException
    {
        out.writeUTF(gDigestSynMessage.clusterId);
        out.writeUTF(gDigestSynMessage.partioner);
        GossipDigestSerializationHelper.serialize(gDigestSynMessage.gDigests, out, version);
    }

    static void serialize(List<GossipDigest> gDigestList, DataOutputPlus out, int version) throws IOException
    {
        out.writeInt(gDigestList.size());
        for (GossipDigest gDigest : gDigestList)
            GossipDigest.serializer.serialize(gDigest, out, version);
    }

+++++++++++++++++++++++
    static void serialize(List<GossipDigest> gDigestList, DataOutput out, int version) throws IOException
    {
        out.writeInt(gDigestList.size());
        for (GossipDigest gDigest : gDigestList)
            GossipDigest.serializer.serialize(gDigest, out, version);
    }

    public void serialize(HeartBeatState hbState, DataOutputPlus out, int version) throws IOException
    {
        out.writeInt(hbState.getGeneration());
        out.writeInt(hbState.getHeartBeatVersion());
    }

+++++++++++++++++++++++
    public void serialize(HeartBeatState hbState, DataOutput out, int version) throws IOException
    {
        out.writeInt(hbState.getGeneration());
        out.writeInt(hbState.getHeartBeatVersion());
    }

    public void serialize(Map<MetadataType, MetadataComponent> components, DataOutputPlus out) throws IOException
    {
        ValidationMetadata validation = (ValidationMetadata) components.get(MetadataType.VALIDATION);
        StatsMetadata stats = (StatsMetadata) components.get(MetadataType.STATS);
        CompactionMetadata compaction = (CompactionMetadata) components.get(MetadataType.COMPACTION);

        assert validation != null && stats != null && compaction != null && validation.partitioner != null;

        EstimatedHistogram.serializer.serialize(stats.estimatedRowSize, out);
        EstimatedHistogram.serializer.serialize(stats.estimatedColumnCount, out);
        ReplayPosition.serializer.serialize(stats.replayPosition, out);
        out.writeLong(stats.minTimestamp);
        out.writeLong(stats.maxTimestamp);
        out.writeInt(stats.maxLocalDeletionTime);
        out.writeDouble(validation.bloomFilterFPChance);
        out.writeDouble(stats.compressionRatio);
        out.writeUTF(validation.partitioner);
        out.writeInt(compaction.ancestors.size());
        for (Integer g : compaction.ancestors)
            out.writeInt(g);
        StreamingHistogram.serializer.serialize(stats.estimatedTombstoneDropTime, out);
        out.writeInt(stats.sstableLevel);
        out.writeInt(stats.minColumnNames.size());
        for (ByteBuffer columnName : stats.minColumnNames)
            ByteBufferUtil.writeWithShortLength(columnName, out);
        out.writeInt(stats.maxColumnNames.size());
        for (ByteBuffer columnName : stats.maxColumnNames)
            ByteBufferUtil.writeWithShortLength(columnName, out);
    }

    /**
     * Legacy serializer deserialize all components no matter what types are specified.
     */
    @Override
+++++++++++++++++++++++
    public void serialize(Map<MetadataType, MetadataComponent> components, DataOutput out) throws IOException
    {
        ValidationMetadata validation = (ValidationMetadata) components.get(MetadataType.VALIDATION);
        StatsMetadata stats = (StatsMetadata) components.get(MetadataType.STATS);
        CompactionMetadata compaction = (CompactionMetadata) components.get(MetadataType.COMPACTION);

        assert validation != null && stats != null && compaction != null && validation.partitioner != null;

        EstimatedHistogram.serializer.serialize(stats.estimatedRowSize, out);
        EstimatedHistogram.serializer.serialize(stats.estimatedColumnCount, out);
        ReplayPosition.serializer.serialize(stats.replayPosition, out);
        out.writeLong(stats.minTimestamp);
        out.writeLong(stats.maxTimestamp);
        out.writeInt(stats.maxLocalDeletionTime);
        out.writeDouble(validation.bloomFilterFPChance);
        out.writeDouble(stats.compressionRatio);
        out.writeUTF(validation.partitioner);
        out.writeInt(compaction.ancestors.size());
        for (Integer g : compaction.ancestors)
            out.writeInt(g);
        StreamingHistogram.serializer.serialize(stats.estimatedTombstoneDropTime, out);
        out.writeInt(stats.sstableLevel);
        out.writeInt(stats.minColumnNames.size());
        for (ByteBuffer columnName : stats.minColumnNames)
            ByteBufferUtil.writeWithShortLength(columnName, out);
        out.writeInt(stats.maxColumnNames.size());
        for (ByteBuffer columnName : stats.maxColumnNames)
            ByteBufferUtil.writeWithShortLength(columnName, out);
    }

    /**
     * Legacy serializer deserialize all components no matter what types are specified.
     */
    @Override
    public Map<MetadataType, MetadataComponent> deserialize(Descriptor descriptor, EnumSet<MetadataType> types) throws IOException
    {
        Map<MetadataType, MetadataComponent> components = Maps.newHashMap();

        File statsFile = new File(descriptor.filenameFor(Component.STATS));
        if (!statsFile.exists() && types.contains(MetadataType.STATS))
        {
            components.put(MetadataType.STATS, MetadataCollector.defaultStatsMetadata());
        }
        else
        {
            try (DataInputStream in = new DataInputStream(new BufferedInputStream(new FileInputStream(statsFile))))
            {
                EstimatedHistogram rowSizes = EstimatedHistogram.serializer.deserialize(in);
                EstimatedHistogram columnCounts = EstimatedHistogram.serializer.deserialize(in);
                ReplayPosition replayPosition = ReplayPosition.serializer.deserialize(in);
                long minTimestamp = in.readLong();
                long maxTimestamp = in.readLong();
                int maxLocalDeletionTime = in.readInt();
                double bloomFilterFPChance = in.readDouble();
                double compressionRatio = in.readDouble();
                String partitioner = in.readUTF();
                int nbAncestors = in.readInt();
                Set<Integer> ancestors = new HashSet<>(nbAncestors);
                for (int i = 0; i < nbAncestors; i++)
                    ancestors.add(in.readInt());
                StreamingHistogram tombstoneHistogram = StreamingHistogram.serializer.deserialize(in);
                int sstableLevel = 0;
                if (in.available() > 0)
                    sstableLevel = in.readInt();

                int colCount = in.readInt();
                List<ByteBuffer> minColumnNames = new ArrayList<>(colCount);
                for (int i = 0; i < colCount; i++)
                    minColumnNames.add(ByteBufferUtil.readWithShortLength(in));

                colCount = in.readInt();
                List<ByteBuffer> maxColumnNames = new ArrayList<>(colCount);
                for (int i = 0; i < colCount; i++)
                    maxColumnNames.add(ByteBufferUtil.readWithShortLength(in));

                if (types.contains(MetadataType.VALIDATION))
                    components.put(MetadataType.VALIDATION,
                                   new ValidationMetadata(partitioner, bloomFilterFPChance));
                if (types.contains(MetadataType.STATS))
                    components.put(MetadataType.STATS,
                                   new StatsMetadata(rowSizes,
                                                     columnCounts,
                                                     replayPosition,
                                                     minTimestamp,
                                                     maxTimestamp,
                                                     maxLocalDeletionTime,
                                                     compressionRatio,
                                                     tombstoneHistogram,
                                                     sstableLevel,
                                                     minColumnNames,
                                                     maxColumnNames,
                                                     true,
                                                     ActiveRepairService.UNREPAIRED_SSTABLE));
                if (types.contains(MetadataType.COMPACTION))
                    components.put(MetadataType.COMPACTION,
                                   new CompactionMetadata(ancestors, null));
            }
        }
        return components;
+++++++++++++++++++++++
    public Map<MetadataType, MetadataComponent> deserialize(Descriptor descriptor, EnumSet<MetadataType> types) throws IOException
    {
        Map<MetadataType, MetadataComponent> components = Maps.newHashMap();

        File statsFile = new File(descriptor.filenameFor(Component.STATS));
        if (!statsFile.exists() && types.contains(MetadataType.STATS))
        {
            components.put(MetadataType.STATS, MetadataCollector.defaultStatsMetadata());
        }
        else
        {
            try (DataInputStream in = new DataInputStream(new BufferedInputStream(new FileInputStream(statsFile))))
            {
                EstimatedHistogram rowSizes = EstimatedHistogram.serializer.deserialize(in);
                EstimatedHistogram columnCounts = EstimatedHistogram.serializer.deserialize(in);
                ReplayPosition replayPosition = ReplayPosition.serializer.deserialize(in);
                long minTimestamp = in.readLong();
                long maxTimestamp = in.readLong();
                int maxLocalDeletionTime = descriptor.version.tracksMaxLocalDeletionTime ? in.readInt() : Integer.MAX_VALUE;
                double bloomFilterFPChance = descriptor.version.hasBloomFilterFPChance ? in.readDouble() : NO_BLOOM_FILTER_FP_CHANCE;
                double compressionRatio = in.readDouble();
                String partitioner = in.readUTF();
                int nbAncestors = in.readInt();
                Set<Integer> ancestors = new HashSet<>(nbAncestors);
                for (int i = 0; i < nbAncestors; i++)
                    ancestors.add(in.readInt());
                StreamingHistogram tombstoneHistogram = StreamingHistogram.serializer.deserialize(in);
                int sstableLevel = 0;
                if (in.available() > 0)
                    sstableLevel = in.readInt();

                List<ByteBuffer> minColumnNames;
                List<ByteBuffer> maxColumnNames;
                if (descriptor.version.tracksMaxMinColumnNames)
                {
                    int colCount = in.readInt();
                    minColumnNames = new ArrayList<>(colCount);
                    for (int i = 0; i < colCount; i++)
                    {
                        minColumnNames.add(ByteBufferUtil.readWithShortLength(in));
                    }
                    colCount = in.readInt();
                    maxColumnNames = new ArrayList<>(colCount);
                    for (int i = 0; i < colCount; i++)
                    {
                        maxColumnNames.add(ByteBufferUtil.readWithShortLength(in));
                    }
                }
                else
                {
                    minColumnNames = Collections.emptyList();
                    maxColumnNames = Collections.emptyList();
                }

                if (types.contains(MetadataType.VALIDATION))
                    components.put(MetadataType.VALIDATION,
                                   new ValidationMetadata(partitioner, bloomFilterFPChance));
                if (types.contains(MetadataType.STATS))
                    components.put(MetadataType.STATS,
                                   new StatsMetadata(rowSizes,
                                                     columnCounts,
                                                     replayPosition,
                                                     minTimestamp,
                                                     maxTimestamp,
                                                     maxLocalDeletionTime,
                                                     compressionRatio,
                                                     tombstoneHistogram,
                                                     sstableLevel,
                                                     minColumnNames,
                                                     maxColumnNames,
                                                     ActiveRepairService.UNREPAIRED_SSTABLE));
                if (types.contains(MetadataType.COMPACTION))
                    components.put(MetadataType.COMPACTION,
                                   new CompactionMetadata(ancestors, null));
            }
        }
        return components;
    public void serialize(Map<MetadataType, MetadataComponent> components, DataOutputPlus out) throws IOException
    {
        // sort components by type
        List<MetadataComponent> sortedComponents = Lists.newArrayList(components.values());
        Collections.sort(sortedComponents);

        // write number of component
        out.writeInt(components.size());
        // build and write toc
        int lastPosition = 4 + (8 * sortedComponents.size());
        for (MetadataComponent component : sortedComponents)
        {
            MetadataType type = component.getType();
            // serialize type
            out.writeInt(type.ordinal());
            // serialize position
            out.writeInt(lastPosition);
            lastPosition += type.serializer.serializedSize(component);
        }
        // serialize components
        for (MetadataComponent component : sortedComponents)
        {
            component.getType().serializer.serialize(component, out);
        }
    }

+++++++++++++++++++++++
    public void serialize(Map<MetadataType, MetadataComponent> components, DataOutput out) throws IOException
    {
        // sort components by type
        List<MetadataComponent> sortedComponents = Lists.newArrayList(components.values());
        Collections.sort(sortedComponents);

        // write number of component
        out.writeInt(components.size());
        // build and write toc
        int lastPosition = 4 + (8 * sortedComponents.size());
        for (MetadataComponent component : sortedComponents)
        {
            MetadataType type = component.getType();
            // serialize type
            out.writeInt(type.ordinal());
            // serialize position
            out.writeInt(lastPosition);
            lastPosition += type.serializer.serializedSize(component);
        }
        // serialize components
        for (MetadataComponent component : sortedComponents)
        {
            component.getType().serializer.serialize(component, out);
        }
    }

    public void serialize(DataOutputPlus out, int version) throws IOException
    {
        CompactEndpointSerializationHelper.serialize(from, out);

        out.writeInt(verb.ordinal());
        out.writeInt(parameters.size());
        for (Map.Entry<String, byte[]> entry : parameters.entrySet())
        {
            out.writeUTF(entry.getKey());
            out.writeInt(entry.getValue().length);
            out.write(entry.getValue());
        }

        long longSize = payload == null ? 0 : serializer.serializedSize(payload, version);
        assert longSize <= Integer.MAX_VALUE; // larger values are supported in sstables but not messages
        out.writeInt((int) longSize);
        if (payload != null)
            serializer.serialize(payload, out, version);
    }

+++++++++++++++++++++++
    public void serialize(DataOutputStream out, int version) throws IOException
    {
        CompactEndpointSerializationHelper.serialize(from, out);

        out.writeInt(verb.ordinal());
        out.writeInt(parameters.size());
        for (Map.Entry<String, byte[]> entry : parameters.entrySet())
        {
            out.writeUTF(entry.getKey());
            out.writeInt(entry.getValue().length);
            out.write(entry.getValue());
        }

        long longSize = payload == null ? 0 : serializer.serializedSize(payload, version);
        assert longSize <= Integer.MAX_VALUE; // larger values are supported in sstables but not messages
        out.writeInt((int) longSize);
        if (payload != null)
            serializer.serialize(payload, out, version);
    }

    public ByteBuffer serialize(BigInteger value)
    {
        return value == null ? ByteBufferUtil.EMPTY_BYTE_BUFFER : ByteBuffer.wrap(value.toByteArray());
    }

+++++++++++++++++++++++
    public ByteBuffer serialize(BigInteger value)
    {
        return ByteBuffer.wrap(value.toByteArray());
    }

    public BigInteger deserialize(ByteBuffer bytes)
    {
        return bytes.hasRemaining() ? new BigInteger(ByteBufferUtil.getArray(bytes)) : null;
    }

+++++++++++++++++++++++
    public BigInteger deserialize(ByteBuffer bytes)
    {
        return new BigInteger(ByteBufferUtil.getArray(bytes));
    }

    public ByteBuffer serialize()
    {
        try
        {
            DataOutputBuffer out = new DataOutputBuffer(serializedSize());
            ByteBufferUtil.writeWithShortLength(partitionKey, out);
            ByteBufferUtil.writeWithShortLength(cellName, out);
            out.writeInt(remaining);
            return out.asByteBuffer();
        }
        catch (IOException e)
        {
            throw new RuntimeException(e);
        }
    }

+++++++++++++++++++++++
    public ByteBuffer serialize()
    {
        try
        {
            ByteBuffer result = ByteBuffer.allocate(serializedSize());
            DataOutput out = new DataOutputStream(new ByteBufferOutputStream(result));
            ByteBufferUtil.writeWithShortLength(partitionKey, out);
            ByteBufferUtil.writeWithShortLength(cellName, out);
            out.writeInt(remaining);
            result.flip();
            return result;
        }
        catch (IOException e)
        {
            throw new RuntimeException(e);
        }
    }

    public static void serialize(StreamMessage message, DataOutputStreamAndChannel out, int version, StreamSession session) throws IOException
    {
        ByteBuffer buff = ByteBuffer.allocate(1);
        // message type
        buff.put(message.type.type);
        buff.flip();
        out.write(buff);
        message.type.outSerializer.serialize(message, out, version, session);
    }

+++++++++++++++++++++++
    public static void serialize(StreamMessage message, WritableByteChannel out, int version, StreamSession session) throws IOException
    {
        ByteBuffer buff = ByteBuffer.allocate(1);
        // message type
        buff.put(message.type.type);
        buff.flip();
        while (buff.hasRemaining())
            out.write(buff);
        message.type.outSerializer.serialize(message, out, version, session);
    }

    public void serialize(ByteBuf dest, int version)
    {
        CBUtil.writeEnumValue(type, dest);
        serializeEvent(dest, version);
    }

+++++++++++++++++++++++
    public void serialize(ChannelBuffer dest)
    {
        CBUtil.writeEnumValue(type, dest);
        serializeEvent(dest);
    }

    protected abstract void serializeEvent(ByteBuf dest, int version);
+++++++++++++++++++++++
    protected abstract void serializeEvent(ChannelBuffer dest);
    public int serializedSize(int version)
    {
        return CBUtil.sizeOfEnumValue(type) + eventSerializedSize(version);
    }

+++++++++++++++++++++++
    public int serializedSize()
    {
        return CBUtil.sizeOfEnumValue(type) + eventSerializedSize();
    }

    public static Event deserialize(ByteBuf cb, int version)
    {
        switch (CBUtil.readEnumValue(Type.class, cb))
        {
            case TOPOLOGY_CHANGE:
                return TopologyChange.deserializeEvent(cb, version);
            case STATUS_CHANGE:
                return StatusChange.deserializeEvent(cb, version);
            case SCHEMA_CHANGE:
                return SchemaChange.deserializeEvent(cb, version);
        }
        throw new AssertionError();
    }

+++++++++++++++++++++++
    public static Event deserialize(ChannelBuffer cb)
    {
        switch (CBUtil.readEnumValue(Type.class, cb))
        {
            case TOPOLOGY_CHANGE:
                return TopologyChange.deserializeEvent(cb);
            case STATUS_CHANGE:
                return StatusChange.deserializeEvent(cb);
            case SCHEMA_CHANGE:
                return SchemaChange.deserializeEvent(cb);
        }
        throw new AssertionError();
    }

    public void serialize(BloomFilter bf, DataOutputPlus out) throws IOException
    {
        out.writeInt(bf.hashCount);
        bf.bitset.serialize(out);
    }

+++++++++++++++++++++++
    public void serialize(BloomFilter bf, DataOutput out) throws IOException
    {
        out.writeInt(bf.hashCount);
        bf.bitset.serialize(out);
    }

    public void serialize(Boolean b, DataOutputPlus out, int version) throws IOException
    {
        out.writeBoolean(b);
    }

+++++++++++++++++++++++
    public void serialize(Boolean b, DataOutput out, int version) throws IOException
    {
        out.writeBoolean(b);
    }

    public static void serialize(IFilter bf, DataOutputPlus output) throws IOException
    {
        Murmur3BloomFilter.serializer.serialize((Murmur3BloomFilter) bf, output);
    }

+++++++++++++++++++++++
    public static void serialize(IFilter bf, DataOutput output) throws IOException
    {
        Murmur3BloomFilter.serializer.serialize((Murmur3BloomFilter) bf, output);
    }

    public static <C, D, I extends Interval<C, D>> Serializer<C, D, I> serializer(ISerializer<C> pointSerializer, ISerializer<D> dataSerializer, Constructor<I> constructor)
    {
        return new Serializer<>(pointSerializer, dataSerializer, constructor);
    }

    @SuppressWarnings("unchecked")
+++++++++++++++++++++++
    public static <C, D, I extends Interval<C, D>> Serializer<C, D, I> serializer(ISerializer<C> pointSerializer, ISerializer<D> dataSerializer, Constructor<I> constructor)
    {
        return new Serializer(pointSerializer, dataSerializer, constructor);
    }

    @SuppressWarnings("unchecked")
    public void serialize(UUID uuid, DataOutputPlus out, int version) throws IOException
    {
        out.writeLong(uuid.getMostSignificantBits());
        out.writeLong(uuid.getLeastSignificantBits());
    }

+++++++++++++++++++++++
    public void serialize(UUID uuid, DataOutput out, int version) throws IOException
    {
        out.writeLong(uuid.getMostSignificantBits());
        out.writeLong(uuid.getLeastSignificantBits());
    }

    public static OffHeapBitSet deserialize(DataInput in) throws IOException
    {
        long byteCount = in.readInt() * 8L;
        Memory memory = Memory.allocate(byteCount);
        for (long i = 0; i < byteCount;)
        {
            long v = in.readLong();
            memory.setByte(i++, (byte) (v >>> 0));
            memory.setByte(i++, (byte) (v >>> 8));
            memory.setByte(i++, (byte) (v >>> 16));
            memory.setByte(i++, (byte) (v >>> 24));
            memory.setByte(i++, (byte) (v >>> 32));
            memory.setByte(i++, (byte) (v >>> 40));
            memory.setByte(i++, (byte) (v >>> 48));
            memory.setByte(i++, (byte) (v >>> 56));
        }
        return new OffHeapBitSet(memory);
    }

+++++++++++++++++++++++
    public static OffHeapBitSet deserialize(DataInput in) throws IOException
    {
        long byteCount = in.readInt() * 8L;
        Memory memory = RefCountedMemory.allocate(byteCount);
        for (long i = 0; i < byteCount;)
        {
            long v = in.readLong();
            memory.setByte(i++, (byte) (v >>> 0));
            memory.setByte(i++, (byte) (v >>> 8));
            memory.setByte(i++, (byte) (v >>> 16));
            memory.setByte(i++, (byte) (v >>> 24));
            memory.setByte(i++, (byte) (v >>> 32));
            memory.setByte(i++, (byte) (v >>> 40));
            memory.setByte(i++, (byte) (v >>> 48));
            memory.setByte(i++, (byte) (v >>> 56));
        }
        return new OffHeapBitSet(memory);
    }

    public static ByteBuffer serializeForSSTable(ColumnFamily cf)
    {
        try
        {
            DataOutputBuffer out = new DataOutputBuffer();
            DeletionTime.serializer.serialize(cf.deletionInfo().getTopLevelDeletion(), out);
            out.writeInt(cf.getColumnCount());
            new ColumnIndex.Builder(cf, ByteBufferUtil.EMPTY_BYTE_BUFFER, out).build(cf);
            return ByteBuffer.wrap(out.toByteArray());
        }
        catch (IOException e)
        {
            throw new RuntimeException(e);
        }
    }

+++++++++++++++++++++++
    public static ByteBuffer serializeForSSTable(ColumnFamily cf)
    {
        try
        {
            ByteArrayOutputStream baos = new ByteArrayOutputStream();
            DataOutputStream out = new DataOutputStream(baos);
            DeletionTime.serializer.serialize(cf.deletionInfo().getTopLevelDeletion(), out);
            out.writeInt(cf.getColumnCount());
            new ColumnIndex.Builder(cf, ByteBufferUtil.EMPTY_BYTE_BUFFER, out).build(cf);
            return ByteBuffer.wrap(baos.toByteArray());
        }
        catch (IOException e)
        {
            throw new RuntimeException(e);
        }
    }

    protected void testSerializeDeserialize(CFMetaData cfm, ColumnDefinition cd) throws Exception
    {
        ColumnDefinition newCd = ColumnDefinition.fromThrift(cfm.ksName, cfm.cfName, cfm.comparator.asAbstractType(), null, cd.toThrift());
        Assert.assertNotSame(cd, newCd);
        Assert.assertEquals(cd.hashCode(), newCd.hashCode());
        Assert.assertEquals(cd, newCd);
+++++++++++++++++++++++
    protected void testSerializeDeserialize(CFMetaData cfm, ColumnDefinition cd) throws Exception
    {
        ColumnDefinition newCd = ColumnDefinition.fromThrift(cfm, cd.toThrift());
        Assert.assertNotSame(cd, newCd);
        Assert.assertEquals(cd.hashCode(), newCd.hashCode());
        Assert.assertEquals(cd, newCd);
add: 69, delete: 161, change: 648, unhandled: 0 size_exceptions: 12 size_serialize: 43
-----------cassandra-2.1.0-beta1 vs cassandra-2.1.0-beta2-----------
    private RefCountedMemory serialize(V value)
    {
        long serializedSize = serializer.serializedSize(value, ENCODED_TYPE_SIZES);
        if (serializedSize > Integer.MAX_VALUE)
            throw new IllegalArgumentException("Unable to allocate " + serializedSize + " bytes");

        RefCountedMemory freeableMemory;
        try
        {
            freeableMemory = new RefCountedMemory(serializedSize);
        }
        catch (OutOfMemoryError e)
        {
            return null;
        }

        try
        {
            serializer.serialize(value, new EncodedDataOutputStream(new MemoryOutputStream(freeableMemory)));
        }
        catch (IOException e)
        {
            throw new RuntimeException(e);
        }
        return freeableMemory;
    }

+++++++++++++++++++++++
    private RefCountedMemory serialize(V value)
    {
        long serializedSize = serializer.serializedSize(value, ENCODED_TYPE_SIZES);
        if (serializedSize > Integer.MAX_VALUE)
            throw new IllegalArgumentException("Unable to allocate " + serializedSize + " bytes");

        RefCountedMemory freeableMemory;
        try
        {
            freeableMemory = new RefCountedMemory(serializedSize);
        }
        catch (OutOfMemoryError e)
        {
            return null;
        }

        try
        {
            serializer.serialize(value, new EncodedDataOutputStream(new MemoryOutputStream(freeableMemory)));
        }
        catch (IOException e)
        {
            freeableMemory.unreference();
            throw new RuntimeException(e);
        }
        return freeableMemory;
    }

    private V deserialize(RefCountedMemory mem)
    {
        try
        {
            return serializer.deserialize(new EncodedDataInputStream(new MemoryInputStream(mem)));
        }
        catch (IOException e)
        {
            logger.debug("Cannot fetch in memory data, we will failback to read from disk ", e);
            return null;
        }
    }

+++++++++++++++++++++++
    private V deserialize(RefCountedMemory mem)
    {
        try
        {
            return serializer.deserialize(new EncodedDataInputStream(new MemoryInputStream(mem)));
        }
        catch (IOException e)
        {
            logger.debug("Cannot fetch in memory data, we will fallback to read from disk ", e);
            return null;
        }
    }

    private static ByteBuffer serializeMutations(Collection<Mutation> mutations)
    {
        FastByteArrayOutputStream bos = new FastByteArrayOutputStream();
        DataOutputStream out = new DataOutputStream(bos);

        try
        {
            out.writeInt(mutations.size());
            for (Mutation mutation : mutations)
                Mutation.serializer.serialize(mutation, out, VERSION);
        }
        catch (IOException e)
        {
            throw new AssertionError(); // cannot happen.
        }

        return ByteBuffer.wrap(bos.toByteArray());
    }

    @VisibleForTesting
+++++++++++++++++++++++
    private static ByteBuffer serializeMutations(Collection<Mutation> mutations, int version)
    {
        DataOutputBuffer buf = new DataOutputBuffer();

        try
        {
            buf.writeInt(mutations.size());
            for (Mutation mutation : mutations)
                Mutation.serializer.serialize(mutation, buf, version);
        }
        catch (IOException e)
        {
            throw new AssertionError(); // cannot happen.
        }

        return buf.asByteBuffer();
    }

    @VisibleForTesting
    public void serializeCfId(UUID cfId, DataOutput out, int version) throws IOException
    {
        UUIDSerializer.serializer.serialize(cfId, out, version);
    }

+++++++++++++++++++++++
    public void serializeCfId(UUID cfId, DataOutputPlus out, int version) throws IOException
    {
        UUIDSerializer.serializer.serialize(cfId, out, version);
    }

    public void serializeForSSTable(ColumnFamily cf, DataOutput out)
    {
        // Column families shouldn't be written directly to disk, use ColumnIndex.Builder instead
        throw new UnsupportedOperationException();
    }

+++++++++++++++++++++++
    public void serializeForSSTable(ColumnFamily cf, DataOutputPlus out)
    {
        // Column families shouldn't be written directly to disk, use ColumnIndex.Builder instead
        throw new UnsupportedOperationException();
    }

    public void serialize(ColumnFamily cf, DataOutput out, int version)
    {
        try
        {
            if (cf == null)
            {
                out.writeBoolean(false);
                return;
            }

            out.writeBoolean(true);
            serializeCfId(cf.id(), out, version);
            cf.getComparator().deletionInfoSerializer().serialize(cf.deletionInfo(), out, version);
            ColumnSerializer columnSerializer = cf.getComparator().columnSerializer();
            int count = cf.getColumnCount();
            out.writeInt(count);
            int written = 0;
            for (Cell cell : cf)
            {
                columnSerializer.serialize(cell, out);
                written++;
            }
            assert count == written: "Column family had " + count + " columns, but " + written + " written";
        }
        catch (IOException e)
        {
            throw new RuntimeException(e);
        }
    }

+++++++++++++++++++++++
    public void serialize(ColumnFamily cf, DataOutputPlus out, int version)
    {
        try
        {
            if (cf == null)
            {
                out.writeBoolean(false);
                return;
            }

            out.writeBoolean(true);
            serializeCfId(cf.id(), out, version);
            cf.getComparator().deletionInfoSerializer().serialize(cf.deletionInfo(), out, version);
            ColumnSerializer columnSerializer = cf.getComparator().columnSerializer();
            int count = cf.getColumnCount();
            out.writeInt(count);
            int written = 0;
            for (Cell cell : cf)
            {
                columnSerializer.serialize(cell, out);
                written++;
            }
            assert count == written: "Column family had " + count + " columns, but " + written + " written";
        }
        catch (IOException e)
        {
            throw new RuntimeException(e);
        }
    }

    public void serialize(Cell cell, DataOutput out) throws IOException
    {
        assert !cell.name().isEmpty();
        type.cellSerializer().serialize(cell.name(), out);
        try
        {
            out.writeByte(cell.serializationFlags());
            if (cell instanceof CounterCell)
            {
                out.writeLong(((CounterCell) cell).timestampOfLastDelete());
            }
            else if (cell instanceof ExpiringCell)
            {
                out.writeInt(((ExpiringCell) cell).getTimeToLive());
                out.writeInt(cell.getLocalDeletionTime());
            }
            out.writeLong(cell.timestamp());
            ByteBufferUtil.writeWithLength(cell.value(), out);
        }
        catch (IOException e)
        {
            throw new RuntimeException(e);
        }
    }

+++++++++++++++++++++++
    public void serialize(Cell cell, DataOutputPlus out) throws IOException
    {
        assert !cell.name().isEmpty();
        type.cellSerializer().serialize(cell.name(), out);
        try
        {
            out.writeByte(cell.serializationFlags());
            if (cell instanceof CounterCell)
            {
                out.writeLong(((CounterCell) cell).timestampOfLastDelete());
            }
            else if (cell instanceof ExpiringCell)
            {
                out.writeInt(((ExpiringCell) cell).getTimeToLive());
                out.writeInt(cell.getLocalDeletionTime());
            }
            out.writeLong(cell.timestamp());
            ByteBufferUtil.writeWithLength(cell.value(), out);
        }
        catch (IOException e)
        {
            throw new RuntimeException(e);
        }
    }

    Cell deserializeColumnBody(DataInput in, CellName name, int mask, ColumnSerializer.Flag flag, int expireBefore) throws IOException
    {
        if ((mask & COUNTER_MASK) != 0)
        {
            long timestampOfLastDelete = in.readLong();
            long ts = in.readLong();
            ByteBuffer value = ByteBufferUtil.readWithLength(in);
            return CounterCell.create(name, value, ts, timestampOfLastDelete, flag);
        }
        else if ((mask & EXPIRATION_MASK) != 0)
        {
            int ttl = in.readInt();
            int expiration = in.readInt();
            long ts = in.readLong();
            ByteBuffer value = ByteBufferUtil.readWithLength(in);
            return ExpiringCell.create(name, value, ts, ttl, expiration, expireBefore, flag);
        }
        else
        {
            long ts = in.readLong();
            ByteBuffer value = ByteBufferUtil.readWithLength(in);
            return (mask & COUNTER_UPDATE_MASK) != 0
                   ? new CounterUpdateCell(name, value, ts)
                   : ((mask & DELETION_MASK) == 0
                      ? new Cell(name, value, ts)
                      : new DeletedCell(name, value, ts));
        }
    }

+++++++++++++++++++++++
    Cell deserializeColumnBody(DataInput in, CellName name, int mask, ColumnSerializer.Flag flag, int expireBefore) throws IOException
    {
        if ((mask & COUNTER_MASK) != 0)
        {
            long timestampOfLastDelete = in.readLong();
            long ts = in.readLong();
            ByteBuffer value = ByteBufferUtil.readWithLength(in);
            return BufferCounterCell.create(name, value, ts, timestampOfLastDelete, flag);
        }
        else if ((mask & EXPIRATION_MASK) != 0)
        {
            int ttl = in.readInt();
            int expiration = in.readInt();
            long ts = in.readLong();
            ByteBuffer value = ByteBufferUtil.readWithLength(in);
            return BufferExpiringCell.create(name, value, ts, ttl, expiration, expireBefore, flag);
        }
        else
        {
            long ts = in.readLong();
            ByteBuffer value = ByteBufferUtil.readWithLength(in);
            return (mask & COUNTER_UPDATE_MASK) != 0
                   ? new BufferCounterUpdateCell(name, value, ts)
                   : ((mask & DELETION_MASK) == 0
                      ? new BufferCell(name, value, ts)
                      : new BufferDeletedCell(name, value, ts));
        }
    }

    public void serialize(RangeSliceCommand sliceCommand, DataOutput out, int version) throws IOException
    {
        out.writeUTF(sliceCommand.keyspace);
        out.writeUTF(sliceCommand.columnFamily);
        out.writeLong(sliceCommand.timestamp);

        CFMetaData metadata = Schema.instance.getCFMetaData(sliceCommand.keyspace, sliceCommand.columnFamily);

        metadata.comparator.diskAtomFilterSerializer().serialize(sliceCommand.predicate, out, version);

        if (sliceCommand.rowFilter == null)
        {
            out.writeInt(0);
        }
        else
        {
            out.writeInt(sliceCommand.rowFilter.size());
            for (IndexExpression expr : sliceCommand.rowFilter)
            {
                ByteBufferUtil.writeWithShortLength(expr.column, out);
                out.writeInt(expr.operator.ordinal());
                ByteBufferUtil.writeWithShortLength(expr.value, out);
            }
        }
        AbstractBounds.serializer.serialize(sliceCommand.keyRange, out, version);
        out.writeInt(sliceCommand.maxResults);
        out.writeBoolean(sliceCommand.countCQL3Rows);
        out.writeBoolean(sliceCommand.isPaging);
    }

+++++++++++++++++++++++
    public void serialize(RangeSliceCommand sliceCommand, DataOutputPlus out, int version) throws IOException
    {
        out.writeUTF(sliceCommand.keyspace);
        out.writeUTF(sliceCommand.columnFamily);
        out.writeLong(sliceCommand.timestamp);

        CFMetaData metadata = Schema.instance.getCFMetaData(sliceCommand.keyspace, sliceCommand.columnFamily);

        metadata.comparator.diskAtomFilterSerializer().serialize(sliceCommand.predicate, out, version);

        if (sliceCommand.rowFilter == null)
        {
            out.writeInt(0);
        }
        else
        {
            out.writeInt(sliceCommand.rowFilter.size());
            for (IndexExpression expr : sliceCommand.rowFilter)
            {
                ByteBufferUtil.writeWithShortLength(expr.column, out);
                out.writeInt(expr.operator.ordinal());
                ByteBufferUtil.writeWithShortLength(expr.value, out);
            }
        }
        AbstractBounds.serializer.serialize(sliceCommand.keyRange, out, version);
        out.writeInt(sliceCommand.maxResults);
        out.writeBoolean(sliceCommand.countCQL3Rows);
        out.writeBoolean(sliceCommand.isPaging);
    }

    public void serialize(ReadCommand command, DataOutput out, int version) throws IOException
    {
        out.writeByte(command.commandType.serializedValue);
        switch (command.commandType)
        {
            case GET_BY_NAMES:
                SliceByNamesReadCommand.serializer.serialize(command, out, version);
                break;
            case GET_SLICES:
                SliceFromReadCommand.serializer.serialize(command, out, version);
                break;
            default:
                throw new AssertionError();
        }
    }

+++++++++++++++++++++++
    public void serialize(ReadCommand command, DataOutputPlus out, int version) throws IOException
    {
        out.writeByte(command.commandType.serializedValue);
        switch (command.commandType)
        {
            case GET_BY_NAMES:
                SliceByNamesReadCommand.serializer.serialize(command, out, version);
                break;
            case GET_SLICES:
                SliceFromReadCommand.serializer.serialize(command, out, version);
                break;
            default:
                throw new AssertionError();
        }
    }

    public void serialize(ReadResponse response, DataOutput out, int version) throws IOException
    {
        out.writeInt(response.isDigestQuery() ? response.digest().remaining() : 0);
        ByteBuffer buffer = response.isDigestQuery() ? response.digest() : ByteBufferUtil.EMPTY_BYTE_BUFFER;
        ByteBufferUtil.write(buffer, out);
        out.writeBoolean(response.isDigestQuery());
        if (!response.isDigestQuery())
            Row.serializer.serialize(response.row(), out, version);
    }

+++++++++++++++++++++++
    public void serialize(ReadResponse response, DataOutputPlus out, int version) throws IOException
    {
        out.writeInt(response.isDigestQuery() ? response.digest().remaining() : 0);
        ByteBuffer buffer = response.isDigestQuery() ? response.digest() : ByteBufferUtil.EMPTY_BYTE_BUFFER;
        out.write(buffer);
        out.writeBoolean(response.isDigestQuery());
        if (!response.isDigestQuery())
            Row.serializer.serialize(response.row(), out, version);
    }

    public void serialize(ReadCommand cmd, DataOutput out, int version) throws IOException
    {
        SliceByNamesReadCommand command = (SliceByNamesReadCommand) cmd;
        out.writeBoolean(command.isDigestQuery());
        out.writeUTF(command.ksName);
        ByteBufferUtil.writeWithShortLength(command.key, out);
        out.writeUTF(command.cfName);
        out.writeLong(cmd.timestamp);

        CFMetaData metadata = Schema.instance.getCFMetaData(cmd.ksName, cmd.cfName);
        metadata.comparator.namesQueryFilterSerializer().serialize(command.filter, out, version);
    }

+++++++++++++++++++++++
    public void serialize(ReadCommand cmd, DataOutputPlus out, int version) throws IOException
    {
        SliceByNamesReadCommand command = (SliceByNamesReadCommand) cmd;
        out.writeBoolean(command.isDigestQuery());
        out.writeUTF(command.ksName);
        ByteBufferUtil.writeWithShortLength(command.key, out);
        out.writeUTF(command.cfName);
        out.writeLong(cmd.timestamp);

        CFMetaData metadata = Schema.instance.getCFMetaData(cmd.ksName, cmd.cfName);
        metadata.comparator.namesQueryFilterSerializer().serialize(command.filter, out, version);
    }

    public void serialize(ReadCommand rm, DataOutput out, int version) throws IOException
    {
        SliceFromReadCommand realRM = (SliceFromReadCommand)rm;
        out.writeBoolean(realRM.isDigestQuery());
        out.writeUTF(realRM.ksName);
        ByteBufferUtil.writeWithShortLength(realRM.key, out);
        out.writeUTF(realRM.cfName);
        out.writeLong(realRM.timestamp);
        CFMetaData metadata = Schema.instance.getCFMetaData(realRM.ksName, realRM.cfName);
        metadata.comparator.sliceQueryFilterSerializer().serialize(realRM.filter, out, version);
    }

+++++++++++++++++++++++
    public void serialize(ReadCommand rm, DataOutputPlus out, int version) throws IOException
    {
        SliceFromReadCommand realRM = (SliceFromReadCommand)rm;
        out.writeBoolean(realRM.isDigestQuery());
        out.writeUTF(realRM.ksName);
        ByteBufferUtil.writeWithShortLength(realRM.key, out);
        out.writeUTF(realRM.cfName);
        out.writeLong(realRM.timestamp);
        CFMetaData metadata = Schema.instance.getCFMetaData(realRM.ksName, realRM.cfName);
        metadata.comparator.sliceQueryFilterSerializer().serialize(realRM.filter, out, version);
    }

    public void serialize(SnapshotCommand snapshot_command, DataOutput out, int version) throws IOException
    {
        out.writeUTF(snapshot_command.keyspace);
        out.writeUTF(snapshot_command.column_family);
        out.writeUTF(snapshot_command.snapshot_name);
        out.writeBoolean(snapshot_command.clear_snapshot);
    }

+++++++++++++++++++++++
    public void serialize(SnapshotCommand snapshot_command, DataOutputPlus out, int version) throws IOException
    {
        out.writeUTF(snapshot_command.keyspace);
        out.writeUTF(snapshot_command.column_family);
        out.writeUTF(snapshot_command.snapshot_name);
        out.writeBoolean(snapshot_command.clear_snapshot);
    }

    private static void serializeSchema(Map<DecoratedKey, Mutation> mutationMap, String schemaCfName)
    {
        for (Row schemaRow : serializedSchema(schemaCfName))
        {
            if (Schema.ignoredSchemaRow(schemaRow))
                continue;

            Mutation mutation = mutationMap.get(schemaRow.key);
            if (mutation == null)
            {
                mutation = new Mutation(Keyspace.SYSTEM_KS, schemaRow.key.key);
                mutationMap.put(schemaRow.key, mutation);
            }

            mutation.add(schemaRow.cf);
        }
    }

+++++++++++++++++++++++
    private static void serializeSchema(Map<DecoratedKey, Mutation> mutationMap, String schemaCfName)
    {
        for (Row schemaRow : serializedSchema(schemaCfName))
        {
            if (Schema.ignoredSchemaRow(schemaRow))
                continue;

            Mutation mutation = mutationMap.get(schemaRow.key);
            if (mutation == null)
            {
                mutation = new Mutation(Keyspace.SYSTEM_KS, schemaRow.key.getKey());
                mutationMap.put(schemaRow.key, mutation);
            }

            mutation.add(schemaRow.cf);
        }
    }

    public void serialize(Truncation t, DataOutput out, int version) throws IOException
    {
        out.writeUTF(t.keyspace);
        out.writeUTF(t.columnFamily);
    }

+++++++++++++++++++++++
    public void serialize(Truncation t, DataOutputPlus out, int version) throws IOException
    {
        out.writeUTF(t.keyspace);
        out.writeUTF(t.columnFamily);
    }

    public void serialize(EndpointState epState, DataOutput out, int version) throws IOException
    {
        /* serialize the HeartBeatState */
        HeartBeatState hbState = epState.getHeartBeatState();
        HeartBeatState.serializer.serialize(hbState, out, version);

        /* serialize the map of ApplicationState objects */
        int size = epState.applicationState.size();
        out.writeInt(size);
        for (Map.Entry<ApplicationState, VersionedValue> entry : epState.applicationState.entrySet())
        {
            VersionedValue value = entry.getValue();
            out.writeInt(entry.getKey().ordinal());
            VersionedValue.serializer.serialize(value, out, version);
        }
    }

+++++++++++++++++++++++
    public void serialize(EndpointState epState, DataOutputPlus out, int version) throws IOException
    {
        /* serialize the HeartBeatState */
        HeartBeatState hbState = epState.getHeartBeatState();
        HeartBeatState.serializer.serialize(hbState, out, version);

        /* serialize the map of ApplicationState objects */
        int size = epState.applicationState.size();
        out.writeInt(size);
        for (Map.Entry<ApplicationState, VersionedValue> entry : epState.applicationState.entrySet())
        {
            VersionedValue value = entry.getValue();
            out.writeInt(entry.getKey().ordinal());
            VersionedValue.serializer.serialize(value, out, version);
        }
    }

    public void serialize(GossipDigest gDigest, DataOutput out, int version) throws IOException
    {
        CompactEndpointSerializationHelper.serialize(gDigest.endpoint, out);
        out.writeInt(gDigest.generation);
        out.writeInt(gDigest.maxVersion);
    }

+++++++++++++++++++++++
    public void serialize(GossipDigest gDigest, DataOutputPlus out, int version) throws IOException
    {
        CompactEndpointSerializationHelper.serialize(gDigest.endpoint, out);
        out.writeInt(gDigest.generation);
        out.writeInt(gDigest.maxVersion);
    }

    public void serialize(GossipDigestAck gDigestAckMessage, DataOutput out, int version) throws IOException
    {
        GossipDigestSerializationHelper.serialize(gDigestAckMessage.gDigestList, out, version);
        out.writeInt(gDigestAckMessage.epStateMap.size());
        for (Map.Entry<InetAddress, EndpointState> entry : gDigestAckMessage.epStateMap.entrySet())
        {
            InetAddress ep = entry.getKey();
            CompactEndpointSerializationHelper.serialize(ep, out);
            EndpointState.serializer.serialize(entry.getValue(), out, version);
        }
    }

+++++++++++++++++++++++
    public void serialize(GossipDigestAck gDigestAckMessage, DataOutputPlus out, int version) throws IOException
    {
        GossipDigestSerializationHelper.serialize(gDigestAckMessage.gDigestList, out, version);
        out.writeInt(gDigestAckMessage.epStateMap.size());
        for (Map.Entry<InetAddress, EndpointState> entry : gDigestAckMessage.epStateMap.entrySet())
        {
            InetAddress ep = entry.getKey();
            CompactEndpointSerializationHelper.serialize(ep, out);
            EndpointState.serializer.serialize(entry.getValue(), out, version);
        }
    }

    public void serialize(GossipDigestAck2 ack2, DataOutput out, int version) throws IOException
    {
        out.writeInt(ack2.epStateMap.size());
        for (Map.Entry<InetAddress, EndpointState> entry : ack2.epStateMap.entrySet())
        {
            InetAddress ep = entry.getKey();
            CompactEndpointSerializationHelper.serialize(ep, out);
            EndpointState.serializer.serialize(entry.getValue(), out, version);
        }
    }

+++++++++++++++++++++++
    public void serialize(GossipDigestAck2 ack2, DataOutputPlus out, int version) throws IOException
    {
        out.writeInt(ack2.epStateMap.size());
        for (Map.Entry<InetAddress, EndpointState> entry : ack2.epStateMap.entrySet())
        {
            InetAddress ep = entry.getKey();
            CompactEndpointSerializationHelper.serialize(ep, out);
            EndpointState.serializer.serialize(entry.getValue(), out, version);
        }
    }

    public void serialize(GossipDigestSyn gDigestSynMessage, DataOutput out, int version) throws IOException
    {
        out.writeUTF(gDigestSynMessage.clusterId);
        out.writeUTF(gDigestSynMessage.partioner);
        GossipDigestSerializationHelper.serialize(gDigestSynMessage.gDigests, out, version);
    }

+++++++++++++++++++++++
    public void serialize(GossipDigestSyn gDigestSynMessage, DataOutputPlus out, int version) throws IOException
    {
        out.writeUTF(gDigestSynMessage.clusterId);
        out.writeUTF(gDigestSynMessage.partioner);
        GossipDigestSerializationHelper.serialize(gDigestSynMessage.gDigests, out, version);
    }

    static void serialize(List<GossipDigest> gDigestList, DataOutput out, int version) throws IOException
    {
        out.writeInt(gDigestList.size());
        for (GossipDigest gDigest : gDigestList)
            GossipDigest.serializer.serialize(gDigest, out, version);
    }

+++++++++++++++++++++++
    static void serialize(List<GossipDigest> gDigestList, DataOutputPlus out, int version) throws IOException
    {
        out.writeInt(gDigestList.size());
        for (GossipDigest gDigest : gDigestList)
            GossipDigest.serializer.serialize(gDigest, out, version);
    }

    public void serialize(HeartBeatState hbState, DataOutput out, int version) throws IOException
    {
        out.writeInt(hbState.getGeneration());
        out.writeInt(hbState.getHeartBeatVersion());
    }

+++++++++++++++++++++++
    public void serialize(HeartBeatState hbState, DataOutputPlus out, int version) throws IOException
    {
        out.writeInt(hbState.getGeneration());
        out.writeInt(hbState.getHeartBeatVersion());
    }

    public void serialize(Map<MetadataType, MetadataComponent> components, DataOutput out) throws IOException
    {
        ValidationMetadata validation = (ValidationMetadata) components.get(MetadataType.VALIDATION);
        StatsMetadata stats = (StatsMetadata) components.get(MetadataType.STATS);
        CompactionMetadata compaction = (CompactionMetadata) components.get(MetadataType.COMPACTION);

        assert validation != null && stats != null && compaction != null && validation.partitioner != null;

        EstimatedHistogram.serializer.serialize(stats.estimatedRowSize, out);
        EstimatedHistogram.serializer.serialize(stats.estimatedColumnCount, out);
        ReplayPosition.serializer.serialize(stats.replayPosition, out);
        out.writeLong(stats.minTimestamp);
        out.writeLong(stats.maxTimestamp);
        out.writeInt(stats.maxLocalDeletionTime);
        out.writeDouble(validation.bloomFilterFPChance);
        out.writeDouble(stats.compressionRatio);
        out.writeUTF(validation.partitioner);
        out.writeInt(compaction.ancestors.size());
        for (Integer g : compaction.ancestors)
            out.writeInt(g);
        StreamingHistogram.serializer.serialize(stats.estimatedTombstoneDropTime, out);
        out.writeInt(stats.sstableLevel);
        out.writeInt(stats.minColumnNames.size());
        for (ByteBuffer columnName : stats.minColumnNames)
            ByteBufferUtil.writeWithShortLength(columnName, out);
        out.writeInt(stats.maxColumnNames.size());
        for (ByteBuffer columnName : stats.maxColumnNames)
            ByteBufferUtil.writeWithShortLength(columnName, out);
    }

    /**
     * Legacy serializer deserialize all components no matter what types are specified.
     */
    @Override
+++++++++++++++++++++++
    public void serialize(Map<MetadataType, MetadataComponent> components, DataOutputPlus out) throws IOException
    {
        ValidationMetadata validation = (ValidationMetadata) components.get(MetadataType.VALIDATION);
        StatsMetadata stats = (StatsMetadata) components.get(MetadataType.STATS);
        CompactionMetadata compaction = (CompactionMetadata) components.get(MetadataType.COMPACTION);

        assert validation != null && stats != null && compaction != null && validation.partitioner != null;

        EstimatedHistogram.serializer.serialize(stats.estimatedRowSize, out);
        EstimatedHistogram.serializer.serialize(stats.estimatedColumnCount, out);
        ReplayPosition.serializer.serialize(stats.replayPosition, out);
        out.writeLong(stats.minTimestamp);
        out.writeLong(stats.maxTimestamp);
        out.writeInt(stats.maxLocalDeletionTime);
        out.writeDouble(validation.bloomFilterFPChance);
        out.writeDouble(stats.compressionRatio);
        out.writeUTF(validation.partitioner);
        out.writeInt(compaction.ancestors.size());
        for (Integer g : compaction.ancestors)
            out.writeInt(g);
        StreamingHistogram.serializer.serialize(stats.estimatedTombstoneDropTime, out);
        out.writeInt(stats.sstableLevel);
        out.writeInt(stats.minColumnNames.size());
        for (ByteBuffer columnName : stats.minColumnNames)
            ByteBufferUtil.writeWithShortLength(columnName, out);
        out.writeInt(stats.maxColumnNames.size());
        for (ByteBuffer columnName : stats.maxColumnNames)
            ByteBufferUtil.writeWithShortLength(columnName, out);
    }

    /**
     * Legacy serializer deserialize all components no matter what types are specified.
     */
    @Override
    public Map<MetadataType, MetadataComponent> deserialize(Descriptor descriptor, EnumSet<MetadataType> types) throws IOException
    {
        Map<MetadataType, MetadataComponent> components = Maps.newHashMap();

        File statsFile = new File(descriptor.filenameFor(Component.STATS));
        if (!statsFile.exists() && types.contains(MetadataType.STATS))
        {
            components.put(MetadataType.STATS, MetadataCollector.defaultStatsMetadata());
        }
        else
        {
            try (DataInputStream in = new DataInputStream(new BufferedInputStream(new FileInputStream(statsFile))))
            {
                EstimatedHistogram rowSizes = EstimatedHistogram.serializer.deserialize(in);
                EstimatedHistogram columnCounts = EstimatedHistogram.serializer.deserialize(in);
                ReplayPosition replayPosition = ReplayPosition.serializer.deserialize(in);
                long minTimestamp = in.readLong();
                long maxTimestamp = in.readLong();
                int maxLocalDeletionTime = descriptor.version.tracksMaxLocalDeletionTime ? in.readInt() : Integer.MAX_VALUE;
                double bloomFilterFPChance = descriptor.version.hasBloomFilterFPChance ? in.readDouble() : NO_BLOOM_FILTER_FP_CHANCE;
                double compressionRatio = in.readDouble();
                String partitioner = in.readUTF();
                int nbAncestors = in.readInt();
                Set<Integer> ancestors = new HashSet<>(nbAncestors);
                for (int i = 0; i < nbAncestors; i++)
                    ancestors.add(in.readInt());
                StreamingHistogram tombstoneHistogram = StreamingHistogram.serializer.deserialize(in);
                int sstableLevel = 0;
                if (in.available() > 0)
                    sstableLevel = in.readInt();

                List<ByteBuffer> minColumnNames;
                List<ByteBuffer> maxColumnNames;
                if (descriptor.version.tracksMaxMinColumnNames)
                {
                    int colCount = in.readInt();
                    minColumnNames = new ArrayList<>(colCount);
                    for (int i = 0; i < colCount; i++)
                    {
                        minColumnNames.add(ByteBufferUtil.readWithShortLength(in));
                    }
                    colCount = in.readInt();
                    maxColumnNames = new ArrayList<>(colCount);
                    for (int i = 0; i < colCount; i++)
                    {
                        maxColumnNames.add(ByteBufferUtil.readWithShortLength(in));
                    }
                }
                else
                {
                    minColumnNames = Collections.emptyList();
                    maxColumnNames = Collections.emptyList();
                }

                if (types.contains(MetadataType.VALIDATION))
                    components.put(MetadataType.VALIDATION,
                                   new ValidationMetadata(partitioner, bloomFilterFPChance));
                if (types.contains(MetadataType.STATS))
                    components.put(MetadataType.STATS,
                                   new StatsMetadata(rowSizes,
                                                     columnCounts,
                                                     replayPosition,
                                                     minTimestamp,
                                                     maxTimestamp,
                                                     maxLocalDeletionTime,
                                                     compressionRatio,
                                                     tombstoneHistogram,
                                                     sstableLevel,
                                                     minColumnNames,
                                                     maxColumnNames,
                                                     ActiveRepairService.UNREPAIRED_SSTABLE));
                if (types.contains(MetadataType.COMPACTION))
                    components.put(MetadataType.COMPACTION,
                                   new CompactionMetadata(ancestors, null));
            }
        }
        return components;
+++++++++++++++++++++++
    public Map<MetadataType, MetadataComponent> deserialize(Descriptor descriptor, EnumSet<MetadataType> types) throws IOException
    {
        Map<MetadataType, MetadataComponent> components = Maps.newHashMap();

        File statsFile = new File(descriptor.filenameFor(Component.STATS));
        if (!statsFile.exists() && types.contains(MetadataType.STATS))
        {
            components.put(MetadataType.STATS, MetadataCollector.defaultStatsMetadata());
        }
        else
        {
            try (DataInputStream in = new DataInputStream(new BufferedInputStream(new FileInputStream(statsFile))))
            {
                EstimatedHistogram rowSizes = EstimatedHistogram.serializer.deserialize(in);
                EstimatedHistogram columnCounts = EstimatedHistogram.serializer.deserialize(in);
                ReplayPosition replayPosition = ReplayPosition.serializer.deserialize(in);
                long minTimestamp = in.readLong();
                long maxTimestamp = in.readLong();
                int maxLocalDeletionTime = in.readInt();
                double bloomFilterFPChance = in.readDouble();
                double compressionRatio = in.readDouble();
                String partitioner = in.readUTF();
                int nbAncestors = in.readInt();
                Set<Integer> ancestors = new HashSet<>(nbAncestors);
                for (int i = 0; i < nbAncestors; i++)
                    ancestors.add(in.readInt());
                StreamingHistogram tombstoneHistogram = StreamingHistogram.serializer.deserialize(in);
                int sstableLevel = 0;
                if (in.available() > 0)
                    sstableLevel = in.readInt();

                int colCount = in.readInt();
                List<ByteBuffer> minColumnNames = new ArrayList<>(colCount);
                for (int i = 0; i < colCount; i++)
                    minColumnNames.add(ByteBufferUtil.readWithShortLength(in));

                colCount = in.readInt();
                List<ByteBuffer> maxColumnNames = new ArrayList<>(colCount);
                for (int i = 0; i < colCount; i++)
                    maxColumnNames.add(ByteBufferUtil.readWithShortLength(in));

                if (types.contains(MetadataType.VALIDATION))
                    components.put(MetadataType.VALIDATION,
                                   new ValidationMetadata(partitioner, bloomFilterFPChance));
                if (types.contains(MetadataType.STATS))
                    components.put(MetadataType.STATS,
                                   new StatsMetadata(rowSizes,
                                                     columnCounts,
                                                     replayPosition,
                                                     minTimestamp,
                                                     maxTimestamp,
                                                     maxLocalDeletionTime,
                                                     compressionRatio,
                                                     tombstoneHistogram,
                                                     sstableLevel,
                                                     minColumnNames,
                                                     maxColumnNames,
                                                     true,
                                                     ActiveRepairService.UNREPAIRED_SSTABLE));
                if (types.contains(MetadataType.COMPACTION))
                    components.put(MetadataType.COMPACTION,
                                   new CompactionMetadata(ancestors, null));
            }
        }
        return components;
    public void serialize(Map<MetadataType, MetadataComponent> components, DataOutput out) throws IOException
    {
        // sort components by type
        List<MetadataComponent> sortedComponents = Lists.newArrayList(components.values());
        Collections.sort(sortedComponents);

        // write number of component
        out.writeInt(components.size());
        // build and write toc
        int lastPosition = 4 + (8 * sortedComponents.size());
        for (MetadataComponent component : sortedComponents)
        {
            MetadataType type = component.getType();
            // serialize type
            out.writeInt(type.ordinal());
            // serialize position
            out.writeInt(lastPosition);
            lastPosition += type.serializer.serializedSize(component);
        }
        // serialize components
        for (MetadataComponent component : sortedComponents)
        {
            component.getType().serializer.serialize(component, out);
        }
    }

+++++++++++++++++++++++
    public void serialize(Map<MetadataType, MetadataComponent> components, DataOutputPlus out) throws IOException
    {
        // sort components by type
        List<MetadataComponent> sortedComponents = Lists.newArrayList(components.values());
        Collections.sort(sortedComponents);

        // write number of component
        out.writeInt(components.size());
        // build and write toc
        int lastPosition = 4 + (8 * sortedComponents.size());
        for (MetadataComponent component : sortedComponents)
        {
            MetadataType type = component.getType();
            // serialize type
            out.writeInt(type.ordinal());
            // serialize position
            out.writeInt(lastPosition);
            lastPosition += type.serializer.serializedSize(component);
        }
        // serialize components
        for (MetadataComponent component : sortedComponents)
        {
            component.getType().serializer.serialize(component, out);
        }
    }

    public void serialize(DataOutputStream out, int version) throws IOException
    {
        CompactEndpointSerializationHelper.serialize(from, out);

        out.writeInt(verb.ordinal());
        out.writeInt(parameters.size());
        for (Map.Entry<String, byte[]> entry : parameters.entrySet())
        {
            out.writeUTF(entry.getKey());
            out.writeInt(entry.getValue().length);
            out.write(entry.getValue());
        }

        long longSize = payload == null ? 0 : serializer.serializedSize(payload, version);
        assert longSize <= Integer.MAX_VALUE; // larger values are supported in sstables but not messages
        out.writeInt((int) longSize);
        if (payload != null)
            serializer.serialize(payload, out, version);
    }

+++++++++++++++++++++++
    public void serialize(DataOutputPlus out, int version) throws IOException
    {
        CompactEndpointSerializationHelper.serialize(from, out);

        out.writeInt(verb.ordinal());
        out.writeInt(parameters.size());
        for (Map.Entry<String, byte[]> entry : parameters.entrySet())
        {
            out.writeUTF(entry.getKey());
            out.writeInt(entry.getValue().length);
            out.write(entry.getValue());
        }

        long longSize = payload == null ? 0 : serializer.serializedSize(payload, version);
        assert longSize <= Integer.MAX_VALUE; // larger values are supported in sstables but not messages
        out.writeInt((int) longSize);
        if (payload != null)
            serializer.serialize(payload, out, version);
    }

    public ByteBuffer serialize()
    {
        try
        {
            ByteBuffer result = ByteBuffer.allocate(serializedSize());
            DataOutput out = new DataOutputStream(new ByteBufferOutputStream(result));
            ByteBufferUtil.writeWithShortLength(partitionKey, out);
            ByteBufferUtil.writeWithShortLength(cellName, out);
            out.writeInt(remaining);
            result.flip();
            return result;
        }
        catch (IOException e)
        {
            throw new RuntimeException(e);
        }
    }

+++++++++++++++++++++++
    public ByteBuffer serialize()
    {
        try
        {
            DataOutputBuffer out = new DataOutputBuffer(serializedSize());
            ByteBufferUtil.writeWithShortLength(partitionKey, out);
            ByteBufferUtil.writeWithShortLength(cellName, out);
            out.writeInt(remaining);
            return out.asByteBuffer();
        }
        catch (IOException e)
        {
            throw new RuntimeException(e);
        }
    }

    public static void serialize(StreamMessage message, WritableByteChannel out, int version, StreamSession session) throws IOException
    {
        ByteBuffer buff = ByteBuffer.allocate(1);
        // message type
        buff.put(message.type.type);
        buff.flip();
        while (buff.hasRemaining())
            out.write(buff);
        message.type.outSerializer.serialize(message, out, version, session);
    }

+++++++++++++++++++++++
    public static void serialize(StreamMessage message, DataOutputStreamAndChannel out, int version, StreamSession session) throws IOException
    {
        ByteBuffer buff = ByteBuffer.allocate(1);
        // message type
        buff.put(message.type.type);
        buff.flip();
        out.write(buff);
        message.type.outSerializer.serialize(message, out, version, session);
    }

    public void serialize(ChannelBuffer dest)
    {
        CBUtil.writeEnumValue(type, dest);
        serializeEvent(dest);
    }

+++++++++++++++++++++++
    public void serialize(ByteBuf dest, int version)
    {
        CBUtil.writeEnumValue(type, dest);
        serializeEvent(dest, version);
    }

    protected abstract void serializeEvent(ChannelBuffer dest);
+++++++++++++++++++++++
    protected abstract void serializeEvent(ByteBuf dest, int version);
    public int serializedSize()
    {
        return CBUtil.sizeOfEnumValue(type) + eventSerializedSize();
    }

+++++++++++++++++++++++
    public int serializedSize(int version)
    {
        return CBUtil.sizeOfEnumValue(type) + eventSerializedSize(version);
    }

    public static Event deserialize(ChannelBuffer cb)
    {
        switch (CBUtil.readEnumValue(Type.class, cb))
        {
            case TOPOLOGY_CHANGE:
                return TopologyChange.deserializeEvent(cb);
            case STATUS_CHANGE:
                return StatusChange.deserializeEvent(cb);
            case SCHEMA_CHANGE:
                return SchemaChange.deserializeEvent(cb);
        }
        throw new AssertionError();
    }

+++++++++++++++++++++++
    public static Event deserialize(ByteBuf cb, int version)
    {
        switch (CBUtil.readEnumValue(Type.class, cb))
        {
            case TOPOLOGY_CHANGE:
                return TopologyChange.deserializeEvent(cb, version);
            case STATUS_CHANGE:
                return StatusChange.deserializeEvent(cb, version);
            case SCHEMA_CHANGE:
                return SchemaChange.deserializeEvent(cb, version);
        }
        throw new AssertionError();
    }

    public void serialize(BloomFilter bf, DataOutput out) throws IOException
    {
        out.writeInt(bf.hashCount);
        bf.bitset.serialize(out);
    }

+++++++++++++++++++++++
    public void serialize(BloomFilter bf, DataOutputPlus out) throws IOException
    {
        out.writeInt(bf.hashCount);
        bf.bitset.serialize(out);
    }

    public void serialize(Boolean b, DataOutput out, int version) throws IOException
    {
        out.writeBoolean(b);
    }

+++++++++++++++++++++++
    public void serialize(Boolean b, DataOutputPlus out, int version) throws IOException
    {
        out.writeBoolean(b);
    }

    public static void serialize(IFilter bf, DataOutput output) throws IOException
    {
        Murmur3BloomFilter.serializer.serialize((Murmur3BloomFilter) bf, output);
    }

+++++++++++++++++++++++
    public static void serialize(IFilter bf, DataOutputPlus output) throws IOException
    {
        Murmur3BloomFilter.serializer.serialize((Murmur3BloomFilter) bf, output);
    }

    public static <C, D, I extends Interval<C, D>> Serializer<C, D, I> serializer(ISerializer<C> pointSerializer, ISerializer<D> dataSerializer, Constructor<I> constructor)
    {
        return new Serializer(pointSerializer, dataSerializer, constructor);
    }

    @SuppressWarnings("unchecked")
+++++++++++++++++++++++
    public static <C, D, I extends Interval<C, D>> Serializer<C, D, I> serializer(ISerializer<C> pointSerializer, ISerializer<D> dataSerializer, Constructor<I> constructor)
    {
        return new Serializer<>(pointSerializer, dataSerializer, constructor);
    }

    @SuppressWarnings("unchecked")
    public void serialize(UUID uuid, DataOutput out, int version) throws IOException
    {
        out.writeLong(uuid.getMostSignificantBits());
        out.writeLong(uuid.getLeastSignificantBits());
    }

+++++++++++++++++++++++
    public void serialize(UUID uuid, DataOutputPlus out, int version) throws IOException
    {
        out.writeLong(uuid.getMostSignificantBits());
        out.writeLong(uuid.getLeastSignificantBits());
    }

    public static OffHeapBitSet deserialize(DataInput in) throws IOException
    {
        long byteCount = in.readInt() * 8L;
        Memory memory = RefCountedMemory.allocate(byteCount);
        for (long i = 0; i < byteCount;)
        {
            long v = in.readLong();
            memory.setByte(i++, (byte) (v >>> 0));
            memory.setByte(i++, (byte) (v >>> 8));
            memory.setByte(i++, (byte) (v >>> 16));
            memory.setByte(i++, (byte) (v >>> 24));
            memory.setByte(i++, (byte) (v >>> 32));
            memory.setByte(i++, (byte) (v >>> 40));
            memory.setByte(i++, (byte) (v >>> 48));
            memory.setByte(i++, (byte) (v >>> 56));
        }
        return new OffHeapBitSet(memory);
    }

+++++++++++++++++++++++
    public static OffHeapBitSet deserialize(DataInput in) throws IOException
    {
        long byteCount = in.readInt() * 8L;
        Memory memory = Memory.allocate(byteCount);
        for (long i = 0; i < byteCount;)
        {
            long v = in.readLong();
            memory.setByte(i++, (byte) (v >>> 0));
            memory.setByte(i++, (byte) (v >>> 8));
            memory.setByte(i++, (byte) (v >>> 16));
            memory.setByte(i++, (byte) (v >>> 24));
            memory.setByte(i++, (byte) (v >>> 32));
            memory.setByte(i++, (byte) (v >>> 40));
            memory.setByte(i++, (byte) (v >>> 48));
            memory.setByte(i++, (byte) (v >>> 56));
        }
        return new OffHeapBitSet(memory);
    }

    public static ByteBuffer serializeForSSTable(ColumnFamily cf)
    {
        try
        {
            ByteArrayOutputStream baos = new ByteArrayOutputStream();
            DataOutputStream out = new DataOutputStream(baos);
            DeletionTime.serializer.serialize(cf.deletionInfo().getTopLevelDeletion(), out);
            out.writeInt(cf.getColumnCount());
            new ColumnIndex.Builder(cf, ByteBufferUtil.EMPTY_BYTE_BUFFER, out).build(cf);
            return ByteBuffer.wrap(baos.toByteArray());
        }
        catch (IOException e)
        {
            throw new RuntimeException(e);
        }
    }

+++++++++++++++++++++++
    public static ByteBuffer serializeForSSTable(ColumnFamily cf)
    {
        try
        {
            DataOutputBuffer out = new DataOutputBuffer();
            DeletionTime.serializer.serialize(cf.deletionInfo().getTopLevelDeletion(), out);
            out.writeInt(cf.getColumnCount());
            new ColumnIndex.Builder(cf, ByteBufferUtil.EMPTY_BYTE_BUFFER, out).build(cf);
            return ByteBuffer.wrap(out.toByteArray());
        }
        catch (IOException e)
        {
            throw new RuntimeException(e);
        }
    }

add: 56, delete: 18, change: 569, unhandled: 0 size_exceptions: 13 size_serialize: 40
-----------cassandra-2.1.0-beta2 vs cassandra-2.1.0-deb-----------
    public ByteBuffer serialize(BigInteger value)
    {
        return ByteBuffer.wrap(value.toByteArray());
    }

+++++++++++++++++++++++
    public ByteBuffer serialize(BigInteger value)
    {
        return value == null ? ByteBufferUtil.EMPTY_BYTE_BUFFER : ByteBuffer.wrap(value.toByteArray());
    }

    public BigInteger deserialize(ByteBuffer bytes)
    {
        return new BigInteger(ByteBufferUtil.getArray(bytes));
    }

+++++++++++++++++++++++
    public BigInteger deserialize(ByteBuffer bytes)
    {
        return bytes.hasRemaining() ? new BigInteger(ByteBufferUtil.getArray(bytes)) : null;
    }

    protected void testSerializeDeserialize(CFMetaData cfm, ColumnDefinition cd) throws Exception
    {
        ColumnDefinition newCd = ColumnDefinition.fromThrift(cfm, cd.toThrift());
        Assert.assertNotSame(cd, newCd);
        Assert.assertEquals(cd.hashCode(), newCd.hashCode());
        Assert.assertEquals(cd, newCd);
+++++++++++++++++++++++
    protected void testSerializeDeserialize(CFMetaData cfm, ColumnDefinition cd) throws Exception
    {
        ColumnDefinition newCd = ColumnDefinition.fromThrift(cfm.ksName, cfm.cfName, cfm.comparator.asAbstractType(), null, cd.toThrift());
        Assert.assertNotSame(cd, newCd);
        Assert.assertEquals(cd.hashCode(), newCd.hashCode());
        Assert.assertEquals(cd, newCd);
add: 105, delete: 51, change: 363, unhandled: 0 size_exceptions: 0 size_serialize: 3
-----------cassandra-2.1.0-deb vs cassandra-2.1.0-rc1-----------
add: 41, delete: 76, change: 258, unhandled: 0 size_exceptions: 0 size_serialize: 0
-----------cassandra-2.1.0-rc1 vs cassandra-2.1.0-rc2-----------
add: 7, delete: 1, change: 133, unhandled: 0 size_exceptions: 0 size_serialize: 0
-----------cassandra-2.1.0-rc2 vs cassandra-2.1.0-rc3-----------
add: 61, delete: 34, change: 77, unhandled: 0 size_exceptions: 0 size_serialize: 0
-----------cassandra-2.1.0-rc3 vs cassandra-2.1.0-rc4-----------
add: 1, delete: 0, change: 24, unhandled: 0 size_exceptions: 0 size_serialize: 0
-----------cassandra-2.1.0-rc4 vs cassandra-2.1.0-rc5-----------
add: 1, delete: 3, change: 48, unhandled: 0 size_exceptions: 0 size_serialize: 0
-----------cassandra-2.1.0-rc5 vs cassandra-2.1.0-rc6-----------
add: 4, delete: 3, change: 58, unhandled: 0 size_exceptions: 0 size_serialize: 0
-----------cassandra-2.1.0-rc6 vs cassandra-2.1.0-rc7-----------
add: 2, delete: 0, change: 36, unhandled: 0 size_exceptions: 0 size_serialize: 0
-----------cassandra-2.1.0-rc7 vs cassandra-2.1.1-----------
    public List<T> deserializeForNativeProtocol(ByteBuffer bytes, int version)
    {
        try
        {
            ByteBuffer input = bytes.duplicate();
            int n = readCollectionSize(input, version);
            List<T> l = new ArrayList<T>(n);
            for (int i = 0; i < n; i++)
            {
                ByteBuffer databb = readValue(input, version);
                elements.validate(databb);
                l.add(elements.deserialize(databb));
            }
            return l;
        }
        catch (BufferUnderflowException e)
        {
            throw new MarshalException("Not enough bytes to read a list");
        }
    }

+++++++++++++++++++++++
    public List<T> deserializeForNativeProtocol(ByteBuffer bytes, int version)
    {
        try
        {
            ByteBuffer input = bytes.duplicate();
            int n = readCollectionSize(input, version);
            List<T> l = new ArrayList<T>(n);
            for (int i = 0; i < n; i++)
            {
                // We can have nulls in lists that are used for IN values
                ByteBuffer databb = readValue(input, version);
                if (databb != null)
                {
                    elements.validate(databb);
                    l.add(elements.deserialize(databb));
                }
                else
                {
                    l.add(null);
                }
            }

            if (input.hasRemaining())
                throw new MarshalException("Unexpected extraneous bytes after list value");

            return l;
        }
        catch (BufferUnderflowException e)
        {
            throw new MarshalException("Not enough bytes to read a list");
        }
    }

    public Map<K, V> deserializeForNativeProtocol(ByteBuffer bytes, int version)
    {
        try
        {
            ByteBuffer input = bytes.duplicate();
            int n = readCollectionSize(input, version);
            Map<K, V> m = new LinkedHashMap<K, V>(n);
            for (int i = 0; i < n; i++)
            {
                ByteBuffer kbb = readValue(input, version);
                keys.validate(kbb);

                ByteBuffer vbb = readValue(input, version);
                values.validate(vbb);

                m.put(keys.deserialize(kbb), values.deserialize(vbb));
            }
            return m;
        }
        catch (BufferUnderflowException e)
        {
            throw new MarshalException("Not enough bytes to read a map");
        }
    }

+++++++++++++++++++++++
    public Map<K, V> deserializeForNativeProtocol(ByteBuffer bytes, int version)
    {
        try
        {
            ByteBuffer input = bytes.duplicate();
            int n = readCollectionSize(input, version);
            Map<K, V> m = new LinkedHashMap<K, V>(n);
            for (int i = 0; i < n; i++)
            {
                ByteBuffer kbb = readValue(input, version);
                keys.validate(kbb);

                ByteBuffer vbb = readValue(input, version);
                values.validate(vbb);

                m.put(keys.deserialize(kbb), values.deserialize(vbb));
            }
            if (input.hasRemaining())
                throw new MarshalException("Unexpected extraneous bytes after map value");
            return m;
        }
        catch (BufferUnderflowException e)
        {
            throw new MarshalException("Not enough bytes to read a map");
        }
    }

    public Set<T> deserializeForNativeProtocol(ByteBuffer bytes, int version)
    {
        try
        {
            ByteBuffer input = bytes.duplicate();
            int n = readCollectionSize(input, version);
            Set<T> l = new LinkedHashSet<T>(n);
            for (int i = 0; i < n; i++)
            {
                ByteBuffer databb = readValue(input, version);
                elements.validate(databb);
                l.add(elements.deserialize(databb));
            }
            return l;
        }
        catch (BufferUnderflowException e)
        {
            throw new MarshalException("Not enough bytes to read a list");
        }
    }

+++++++++++++++++++++++
    public Set<T> deserializeForNativeProtocol(ByteBuffer bytes, int version)
    {
        try
        {
            ByteBuffer input = bytes.duplicate();
            int n = readCollectionSize(input, version);
            Set<T> l = new LinkedHashSet<T>(n);
            for (int i = 0; i < n; i++)
            {
                ByteBuffer databb = readValue(input, version);
                elements.validate(databb);
                l.add(elements.deserialize(databb));
            }
            if (input.hasRemaining())
                throw new MarshalException("Unexpected extraneous bytes after set value");
            return l;
        }
        catch (BufferUnderflowException e)
        {
            throw new MarshalException("Not enough bytes to read a list");
        }
    }

add: 43, delete: 8, change: 292, unhandled: 0 size_exceptions: 9 size_serialize: 3
-----------cassandra-2.1.1 vs cassandra-2.1.2-----------
    public void serialize(RangeSliceCommand sliceCommand, DataOutputPlus out, int version) throws IOException
    {
        out.writeUTF(sliceCommand.keyspace);
        out.writeUTF(sliceCommand.columnFamily);
        out.writeLong(sliceCommand.timestamp);

        CFMetaData metadata = Schema.instance.getCFMetaData(sliceCommand.keyspace, sliceCommand.columnFamily);

        metadata.comparator.diskAtomFilterSerializer().serialize(sliceCommand.predicate, out, version);

        if (sliceCommand.rowFilter == null)
        {
            out.writeInt(0);
        }
        else
        {
            out.writeInt(sliceCommand.rowFilter.size());
            for (IndexExpression expr : sliceCommand.rowFilter)
            {
                ByteBufferUtil.writeWithShortLength(expr.column, out);
                out.writeInt(expr.operator.ordinal());
                ByteBufferUtil.writeWithShortLength(expr.value, out);
            }
        }
        AbstractBounds.serializer.serialize(sliceCommand.keyRange, out, version);
        out.writeInt(sliceCommand.maxResults);
        out.writeBoolean(sliceCommand.countCQL3Rows);
        out.writeBoolean(sliceCommand.isPaging);
    }

+++++++++++++++++++++++
    public void serialize(RangeSliceCommand sliceCommand, DataOutputPlus out, int version) throws IOException
    {
        out.writeUTF(sliceCommand.keyspace);
        out.writeUTF(sliceCommand.columnFamily);
        out.writeLong(sliceCommand.timestamp);

        CFMetaData metadata = Schema.instance.getCFMetaData(sliceCommand.keyspace, sliceCommand.columnFamily);

        metadata.comparator.diskAtomFilterSerializer().serialize(sliceCommand.predicate, out, version);

        if (sliceCommand.rowFilter == null)
        {
            out.writeInt(0);
        }
        else
        {
            out.writeInt(sliceCommand.rowFilter.size());
            for (IndexExpression expr : sliceCommand.rowFilter)
            {
                expr.writeTo(out);
            }
        }
        AbstractBounds.serializer.serialize(sliceCommand.keyRange, out, version);
        out.writeInt(sliceCommand.maxResults);
        out.writeBoolean(sliceCommand.countCQL3Rows);
        out.writeBoolean(sliceCommand.isPaging);
    }

    public RangeSliceCommand deserialize(DataInput in, int version) throws IOException
    {
        String keyspace = in.readUTF();
        String columnFamily = in.readUTF();
        long timestamp = in.readLong();

        CFMetaData metadata = Schema.instance.getCFMetaData(keyspace, columnFamily);

        IDiskAtomFilter predicate = metadata.comparator.diskAtomFilterSerializer().deserialize(in, version);

        List<IndexExpression> rowFilter;
        int filterCount = in.readInt();
        rowFilter = new ArrayList<>(filterCount);
        for (int i = 0; i < filterCount; i++)
        {
            IndexExpression expr;
            expr = new IndexExpression(ByteBufferUtil.readWithShortLength(in),
                                       IndexExpression.Operator.findByOrdinal(in.readInt()),
                                       ByteBufferUtil.readWithShortLength(in));
            rowFilter.add(expr);
        }
        AbstractBounds<RowPosition> range = AbstractBounds.serializer.deserialize(in, version).toRowBounds();

        int maxResults = in.readInt();
        boolean countCQL3Rows = in.readBoolean();
        boolean isPaging = in.readBoolean();
        return new RangeSliceCommand(keyspace, columnFamily, timestamp, predicate, range, rowFilter, maxResults, countCQL3Rows, isPaging);
    }

+++++++++++++++++++++++
    public RangeSliceCommand deserialize(DataInput in, int version) throws IOException
    {
        String keyspace = in.readUTF();
        String columnFamily = in.readUTF();
        long timestamp = in.readLong();

        CFMetaData metadata = Schema.instance.getCFMetaData(keyspace, columnFamily);

        IDiskAtomFilter predicate = metadata.comparator.diskAtomFilterSerializer().deserialize(in, version);

        List<IndexExpression> rowFilter;
        int filterCount = in.readInt();
        rowFilter = new ArrayList<>(filterCount);
        for (int i = 0; i < filterCount; i++)
        {
            rowFilter.add(IndexExpression.readFrom(in));
        }
        AbstractBounds<RowPosition> range = AbstractBounds.serializer.deserialize(in, version).toRowBounds();

        int maxResults = in.readInt();
        boolean countCQL3Rows = in.readBoolean();
        boolean isPaging = in.readBoolean();
        return new RangeSliceCommand(keyspace, columnFamily, timestamp, predicate, range, rowFilter, maxResults, countCQL3Rows, isPaging);
    }

    public static void serialize(IPartitioner partitioner, Collection<Token> tokens, DataOutput out) throws IOException
    {
        for (Token<?> token : tokens)
        {
            byte[] bintoken = partitioner.getTokenFactory().toByteArray(token).array();
            out.writeInt(bintoken.length);
            out.write(bintoken);
        }
        out.writeInt(0);
    }

+++++++++++++++++++++++
    public static void serialize(IPartitioner partitioner, Collection<Token> tokens, DataOutput out) throws IOException
    {
        for (Token token : tokens)
        {
            byte[] bintoken = partitioner.getTokenFactory().toByteArray(token).array();
            out.writeInt(bintoken.length);
            out.write(bintoken);
        }
        out.writeInt(0);
    }

add: 11, delete: 2, change: 172, unhandled: 0 size_exceptions: 2 size_serialize: 3
-----------cassandra-2.1.2 vs cassandra-2.1.3-----------
    public ReadCommand deserialize(DataInput in, int version) throws IOException
    {
        boolean isDigest = in.readBoolean();
        String keyspaceName = in.readUTF();
        ByteBuffer key = ByteBufferUtil.readWithShortLength(in);
        String cfName = in.readUTF();
        long timestamp = in.readLong();
        CFMetaData metadata = Schema.instance.getCFMetaData(keyspaceName, cfName);
        NamesQueryFilter filter = metadata.comparator.namesQueryFilterSerializer().deserialize(in, version);
        ReadCommand command = new SliceByNamesReadCommand(keyspaceName, key, cfName, timestamp, filter);
        command.setDigestQuery(isDigest);
        return command;
    }

+++++++++++++++++++++++
    public ReadCommand deserialize(DataInput in, int version) throws IOException
    {
        boolean isDigest = in.readBoolean();
        String keyspaceName = in.readUTF();
        ByteBuffer key = ByteBufferUtil.readWithShortLength(in);
        String cfName = in.readUTF();
        long timestamp = in.readLong();
        CFMetaData metadata = Schema.instance.getCFMetaData(keyspaceName, cfName);
        NamesQueryFilter filter = metadata.comparator.namesQueryFilterSerializer().deserialize(in, version);
        return new SliceByNamesReadCommand(keyspaceName, key, cfName, timestamp, filter).setIsDigestQuery(isDigest);
    }

    public ReadCommand deserialize(DataInput in, int version) throws IOException
    {
        boolean isDigest = in.readBoolean();
        String keyspaceName = in.readUTF();
        ByteBuffer key = ByteBufferUtil.readWithShortLength(in);
        String cfName = in.readUTF();
        long timestamp = in.readLong();
        CFMetaData metadata = Schema.instance.getCFMetaData(keyspaceName, cfName);
        SliceQueryFilter filter = metadata.comparator.sliceQueryFilterSerializer().deserialize(in, version);
        ReadCommand command = new SliceFromReadCommand(keyspaceName, key, cfName, timestamp, filter);
        command.setDigestQuery(isDigest);
        return command;
    }

+++++++++++++++++++++++
    public ReadCommand deserialize(DataInput in, int version) throws IOException
    {
        boolean isDigest = in.readBoolean();
        String keyspaceName = in.readUTF();
        ByteBuffer key = ByteBufferUtil.readWithShortLength(in);
        String cfName = in.readUTF();
        long timestamp = in.readLong();
        CFMetaData metadata = Schema.instance.getCFMetaData(keyspaceName, cfName);
        SliceQueryFilter filter = metadata.comparator.sliceQueryFilterSerializer().deserialize(in, version);
        return new SliceFromReadCommand(keyspaceName, key, cfName, timestamp, filter).setIsDigestQuery(isDigest);
    }

    public ByteBuffer serializeForNativeProtocol(List<Cell> cells, int version)
    {
        cells = enforceLimit(cells, version);
        List<ByteBuffer> values = serializedValues(cells);
        return CollectionSerializer.pack(values, cells.size(), version);
    }

+++++++++++++++++++++++
    public ByteBuffer serializeForNativeProtocol(List<Cell> cells, int version)
    {
        assert isMultiCell();
        cells = enforceLimit(cells, version);
        List<ByteBuffer> values = serializedValues(cells);
        return CollectionSerializer.pack(values, cells.size(), version);
    }

    @Override
    public List<ByteBuffer> serializedValues(List<Cell> cells)
    {
        List<ByteBuffer> bbs = new ArrayList<ByteBuffer>(cells.size());
        for (Cell c : cells)
            bbs.add(c.value());
        return bbs;
+++++++++++++++++++++++
    public List<ByteBuffer> serializedValues(List<Cell> cells)
    {
        assert isMultiCell;
        List<ByteBuffer> bbs = new ArrayList<ByteBuffer>(cells.size());
        for (Cell c : cells)
            bbs.add(c.value());
        return bbs;
    public List<ByteBuffer> serializedValues(List<Cell> cells)
    {
        List<ByteBuffer> bbs = new ArrayList<ByteBuffer>(cells.size() * 2);
        for (Cell c : cells)
        {
            bbs.add(c.name().collectionElement());
            bbs.add(c.value());
        }
        return bbs;
+++++++++++++++++++++++
    public List<ByteBuffer> serializedValues(List<Cell> cells)
    {
        assert isMultiCell;
        List<ByteBuffer> bbs = new ArrayList<ByteBuffer>(cells.size() * 2);
        for (Cell c : cells)
        {
            bbs.add(c.name().collectionElement());
            bbs.add(c.value());
        }
        return bbs;
    public int serializedSize(int version)
    {
        int size = CompactEndpointSerializationHelper.serializedSize(from);

        size += TypeSizes.NATIVE.sizeof(verb.ordinal());
        size += TypeSizes.NATIVE.sizeof(parameters.size());
        for (Map.Entry<String, byte[]> entry : parameters.entrySet())
        {
            TypeSizes.NATIVE.sizeof(entry.getKey());
            TypeSizes.NATIVE.sizeof(entry.getValue().length);
            size += entry.getValue().length;
        }

        long longSize = payload == null ? 0 : serializer.serializedSize(payload, version);
        assert longSize <= Integer.MAX_VALUE; // larger values are supported in sstables but not messages
        size += TypeSizes.NATIVE.sizeof((int) longSize);
        size += longSize;
        return size;
+++++++++++++++++++++++
    public int serializedSize(int version)
    {
        int size = CompactEndpointSerializationHelper.serializedSize(from);

        size += TypeSizes.NATIVE.sizeof(verb.ordinal());
        size += TypeSizes.NATIVE.sizeof(parameters.size());
        for (Map.Entry<String, byte[]> entry : parameters.entrySet())
        {
            size += TypeSizes.NATIVE.sizeof(entry.getKey());
            size += TypeSizes.NATIVE.sizeof(entry.getValue().length);
            size += entry.getValue().length;
        }

        long longSize = payload == null ? 0 : serializer.serializedSize(payload, version);
        assert longSize <= Integer.MAX_VALUE; // larger values are supported in sstables but not messages
        size += TypeSizes.NATIVE.sizeof((int) longSize);
        size += longSize;
        return size;
    public ByteBuffer serialize(T value)
    {
        List<ByteBuffer> values = serializeValues(value);
        // See deserialize() for why using the protocol v3 variant is the right thing to do.
        return pack(values, getElementCount(value), 3);
    }

+++++++++++++++++++++++
    public ByteBuffer serialize(T value)
    {
        List<ByteBuffer> values = serializeValues(value);
        // See deserialize() for why using the protocol v3 variant is the right thing to do.
        return pack(values, getElementCount(value), Server.VERSION_3);
    }

    public T deserialize(ByteBuffer bytes)
    {
        // The only cases we serialize/deserialize collections internally (i.e. not for the protocol sake),
        // is:
        //  1) when collections are in UDT values
        //  2) for internal calls.
        // In both case, using the protocol 3 version variant is the right thing to do.
        return deserializeForNativeProtocol(bytes, 3);
    }

+++++++++++++++++++++++
    public T deserialize(ByteBuffer bytes)
    {
        // The only cases we serialize/deserialize collections internally (i.e. not for the protocol sake),
        // is:
        //  1) when collections are frozen
        //  2) for internal calls.
        // In both case, using the protocol 3 version variant is the right thing to do.
        return deserializeForNativeProtocol(bytes, Server.VERSION_3);
    }

add: 35, delete: 6, change: 304, unhandled: 0 size_exceptions: 9 size_serialize: 8
-----------cassandra-2.1.3 vs cassandra-2.1.4-----------
add: 1, delete: 0, change: 1, unhandled: 0 size_exceptions: 0 size_serialize: 0
-----------cassandra-2.1.4 vs cassandra-2.1.5-----------
add: 21, delete: 1, change: 196, unhandled: 0 size_exceptions: 0 size_serialize: 0
-----------cassandra-2.1.5 vs cassandra-2.1.6-----------
    public RangeSliceCommand deserialize(DataInput in, int version) throws IOException
    {
        String keyspace = in.readUTF();
        String columnFamily = in.readUTF();
        long timestamp = in.readLong();

        CFMetaData metadata = Schema.instance.getCFMetaData(keyspace, columnFamily);

        IDiskAtomFilter predicate = metadata.comparator.diskAtomFilterSerializer().deserialize(in, version);

        List<IndexExpression> rowFilter;
        int filterCount = in.readInt();
        rowFilter = new ArrayList<>(filterCount);
        for (int i = 0; i < filterCount; i++)
        {
            rowFilter.add(IndexExpression.readFrom(in));
        }
        AbstractBounds<RowPosition> range = AbstractBounds.serializer.deserialize(in, version).toRowBounds();

        int maxResults = in.readInt();
        boolean countCQL3Rows = in.readBoolean();
        boolean isPaging = in.readBoolean();
        return new RangeSliceCommand(keyspace, columnFamily, timestamp, predicate, range, rowFilter, maxResults, countCQL3Rows, isPaging);
    }

+++++++++++++++++++++++
    public RangeSliceCommand deserialize(DataInput in, int version) throws IOException
    {
        String keyspace = in.readUTF();
        String columnFamily = in.readUTF();
        long timestamp = in.readLong();

        CFMetaData metadata = Schema.instance.getCFMetaData(keyspace, columnFamily);
        if (metadata == null)
        {
            String message = String.format("Got range slice command for nonexistent table %s.%s.  If the table was just " +
                    "created, this is likely due to the schema not being fully propagated.  Please wait for schema " +
                    "agreement on table creation." , keyspace, columnFamily);
            throw new UnknownColumnFamilyException(message, null);
        }

        IDiskAtomFilter predicate = metadata.comparator.diskAtomFilterSerializer().deserialize(in, version);

        List<IndexExpression> rowFilter;
        int filterCount = in.readInt();
        rowFilter = new ArrayList<>(filterCount);
        for (int i = 0; i < filterCount; i++)
        {
            rowFilter.add(IndexExpression.readFrom(in));
        }
        AbstractBounds<RowPosition> range = AbstractBounds.serializer.deserialize(in, version).toRowBounds();

        int maxResults = in.readInt();
        boolean countCQL3Rows = in.readBoolean();
        boolean isPaging = in.readBoolean();
        return new RangeSliceCommand(keyspace, columnFamily, timestamp, predicate, range, rowFilter, maxResults, countCQL3Rows, isPaging);
    }

    public ReadCommand deserialize(DataInput in, int version) throws IOException
    {
        boolean isDigest = in.readBoolean();
        String keyspaceName = in.readUTF();
        ByteBuffer key = ByteBufferUtil.readWithShortLength(in);
        String cfName = in.readUTF();
        long timestamp = in.readLong();
        CFMetaData metadata = Schema.instance.getCFMetaData(keyspaceName, cfName);
        NamesQueryFilter filter = metadata.comparator.namesQueryFilterSerializer().deserialize(in, version);
        return new SliceByNamesReadCommand(keyspaceName, key, cfName, timestamp, filter).setIsDigestQuery(isDigest);
    }

+++++++++++++++++++++++
    public ReadCommand deserialize(DataInput in, int version) throws IOException
    {
        boolean isDigest = in.readBoolean();
        String keyspaceName = in.readUTF();
        ByteBuffer key = ByteBufferUtil.readWithShortLength(in);
        String cfName = in.readUTF();
        long timestamp = in.readLong();
        CFMetaData metadata = Schema.instance.getCFMetaData(keyspaceName, cfName);
        if (metadata == null)
        {
            String message = String.format("Got slice command for nonexistent table %s.%s.  If the table was just " +
                    "created, this is likely due to the schema not being fully propagated.  Please wait for schema " +
                    "agreement on table creation.", keyspaceName, cfName);
            throw new UnknownColumnFamilyException(message, null);
        }
        NamesQueryFilter filter = metadata.comparator.namesQueryFilterSerializer().deserialize(in, version);
        return new SliceByNamesReadCommand(keyspaceName, key, cfName, timestamp, filter).setIsDigestQuery(isDigest);
    }

    public ReadCommand deserialize(DataInput in, int version) throws IOException
    {
        boolean isDigest = in.readBoolean();
        String keyspaceName = in.readUTF();
        ByteBuffer key = ByteBufferUtil.readWithShortLength(in);
        String cfName = in.readUTF();
        long timestamp = in.readLong();
        CFMetaData metadata = Schema.instance.getCFMetaData(keyspaceName, cfName);
        SliceQueryFilter filter = metadata.comparator.sliceQueryFilterSerializer().deserialize(in, version);
        return new SliceFromReadCommand(keyspaceName, key, cfName, timestamp, filter).setIsDigestQuery(isDigest);
    }

+++++++++++++++++++++++
    public ReadCommand deserialize(DataInput in, int version) throws IOException
    {
        boolean isDigest = in.readBoolean();
        String keyspaceName = in.readUTF();
        ByteBuffer key = ByteBufferUtil.readWithShortLength(in);
        String cfName = in.readUTF();
        long timestamp = in.readLong();
        CFMetaData metadata = Schema.instance.getCFMetaData(keyspaceName, cfName);
        if (metadata == null)
        {
            String message = String.format("Got slice command for nonexistent table %s.%s.  If the table was just " +
                    "created, this is likely due to the schema not being fully propagated.  Please wait for schema " +
                    "agreement on table creation.", keyspaceName, cfName);
            throw new UnknownColumnFamilyException(message, null);
        }
        SliceQueryFilter filter = metadata.comparator.sliceQueryFilterSerializer().deserialize(in, version);
        return new SliceFromReadCommand(keyspaceName, key, cfName, timestamp, filter).setIsDigestQuery(isDigest);
    }

    public ByteBuffer serializeForNativeProtocol(List<Cell> cells, int version)
    {
        assert isMultiCell();
        cells = enforceLimit(cells, version);
        List<ByteBuffer> values = serializedValues(cells);
        return CollectionSerializer.pack(values, cells.size(), version);
    }

    @Override
+++++++++++++++++++++++
    public ByteBuffer serializeForNativeProtocol(ColumnDefinition def, List<Cell> cells, int version)
    {
        assert isMultiCell();
        cells = enforceLimit(def, cells, version);
        List<ByteBuffer> values = serializedValues(cells);
        return CollectionSerializer.pack(values, cells.size(), version);
    }

    @Override
add: 8, delete: 2, change: 136, unhandled: 0 size_exceptions: 1 size_serialize: 4
-----------cassandra-2.1.6 vs cassandra-2.1.7-----------
add: 4, delete: 2, change: 14, unhandled: 0 size_exceptions: 0 size_serialize: 0
-----------cassandra-2.1.7 vs cassandra-2.1.8-----------
add: 33, delete: 25, change: 54, unhandled: 0 size_exceptions: 1 size_serialize: 0
-----------cassandra-2.1.8 vs cassandra-2.1.9-----------
add: 7, delete: 3, change: 74, unhandled: 0 size_exceptions: 0 size_serialize: 0
-----------cassandra-2.1.9 vs cassandra-2.1.10-----------
    public static StreamMessage deserialize(ReadableByteChannel in, int version, StreamSession session) throws IOException
    {
        ByteBuffer buff = ByteBuffer.allocate(1);
        if (in.read(buff) > 0)
        {
            buff.flip();
            Type type = Type.get(buff.get());
            return type.inSerializer.deserialize(in, version, session);
        }
        else
        {
            // when socket gets closed, there is a chance that buff is empty
            // in that case, just return null
            return null;
        }
    }

    /** StreamMessage serializer */
+++++++++++++++++++++++
    public static StreamMessage deserialize(ReadableByteChannel in, int version, StreamSession session) throws IOException
    {
        ByteBuffer buff = ByteBuffer.allocate(1);
        int readBytes = in.read(buff);
        if (readBytes > 0)
        {
            buff.flip();
            Type type = Type.get(buff.get());
            return type.inSerializer.deserialize(in, version, session);
        }
        else if (readBytes == 0)
        {
            // input socket buffer was not filled yet
            return null;
        }
        else
        {
            // possibly socket gets closed
            throw new SocketException("End-of-stream reached");
        }
    }

    /** StreamMessage serializer */
add: 3, delete: 0, change: 78, unhandled: 0 size_exceptions: 0 size_serialize: 1
-----------cassandra-2.1.10 vs cassandra-2.1.11-----------
add: 1, delete: 0, change: 4, unhandled: 0 size_exceptions: 0 size_serialize: 0
-----------cassandra-2.1.11 vs cassandra-2.1.12-----------
    public void serialize(EndpointState epState, DataOutputPlus out, int version) throws IOException
    {
        /* serialize the HeartBeatState */
        HeartBeatState hbState = epState.getHeartBeatState();
        HeartBeatState.serializer.serialize(hbState, out, version);

        /* serialize the map of ApplicationState objects */
        int size = epState.applicationState.size();
        out.writeInt(size);
        for (Map.Entry<ApplicationState, VersionedValue> entry : epState.applicationState.entrySet())
        {
            VersionedValue value = entry.getValue();
            out.writeInt(entry.getKey().ordinal());
            VersionedValue.serializer.serialize(value, out, version);
        }
    }

+++++++++++++++++++++++
    public void serialize(EndpointState epState, DataOutputPlus out, int version) throws IOException
    {
        /* serialize the HeartBeatState */
        HeartBeatState hbState = epState.getHeartBeatState();
        HeartBeatState.serializer.serialize(hbState, out, version);

        /* serialize the map of ApplicationState objects */
        Set<Map.Entry<ApplicationState, VersionedValue>> states = epState.states();
        out.writeInt(states.size());
        for (Map.Entry<ApplicationState, VersionedValue> state : states)
        {
            VersionedValue value = state.getValue();
            out.writeInt(state.getKey().ordinal());
            VersionedValue.serializer.serialize(value, out, version);
        }
    }

    public long serializedSize(EndpointState epState, int version)
    {
        long size = HeartBeatState.serializer.serializedSize(epState.getHeartBeatState(), version);
        size += TypeSizes.NATIVE.sizeof(epState.applicationState.size());
        for (Map.Entry<ApplicationState, VersionedValue> entry : epState.applicationState.entrySet())
        {
            VersionedValue value = entry.getValue();
            size += TypeSizes.NATIVE.sizeof(entry.getKey().ordinal());
            size += VersionedValue.serializer.serializedSize(value, version);
        }
        return size;
+++++++++++++++++++++++
    public long serializedSize(EndpointState epState, int version)
    {
        long size = HeartBeatState.serializer.serializedSize(epState.getHeartBeatState(), version);
        Set<Map.Entry<ApplicationState, VersionedValue>> states = epState.states();
        size += TypeSizes.NATIVE.sizeof(states.size());
        for (Map.Entry<ApplicationState, VersionedValue> state : states)
        {
            VersionedValue value = state.getValue();
            size += TypeSizes.NATIVE.sizeof(state.getKey().ordinal());
            size += VersionedValue.serializer.serializedSize(value, version);
        }
        return size;
    public EndpointState deserialize(DataInput in, int version) throws IOException
    {
        HeartBeatState hbState = HeartBeatState.serializer.deserialize(in, version);
        EndpointState epState = new EndpointState(hbState);

        int appStateSize = in.readInt();
        for (int i = 0; i < appStateSize; ++i)
        {
            int key = in.readInt();
            VersionedValue value = VersionedValue.serializer.deserialize(in, version);
            epState.addApplicationState(Gossiper.STATES[key], value);
        }
        return epState;
    }

+++++++++++++++++++++++
    public EndpointState deserialize(DataInput in, int version) throws IOException
    {
        HeartBeatState hbState = HeartBeatState.serializer.deserialize(in, version);

        int appStateSize = in.readInt();
        Map<ApplicationState, VersionedValue> states = new EnumMap<>(ApplicationState.class);
        for (int i = 0; i < appStateSize; ++i)
        {
            int key = in.readInt();
            VersionedValue value = VersionedValue.serializer.deserialize(in, version);
            states.put(Gossiper.STATES[key], value);
        }

        return new EndpointState(hbState, states);
    }

    public synchronized void serialize(DataOutputStreamAndChannel out, int version, StreamSession session) throws IOException
    {
        if (completed)
        {
            return;
        }

        FileMessageHeader.serializer.serialize(header, out, version);

        final SSTableReader reader = ref.get();
        StreamWriter writer = header.compressionInfo == null ?
                                      new StreamWriter(reader, header.sections, session) :
                                      new CompressedStreamWriter(reader, header.sections,
                                                                 header.compressionInfo, session);
        writer.write(out.getChannel());
    }

+++++++++++++++++++++++
    public synchronized void serialize(DataOutputStreamAndChannel out, int version, StreamSession session) throws IOException
    {
        if (completed)
        {
            return;
        }

        CompressionInfo compressionInfo = FileMessageHeader.serializer.serialize(header, out, version);

        final SSTableReader reader = ref.get();
        StreamWriter writer = compressionInfo == null ?
                                      new StreamWriter(reader, header.sections, session) :
                                      new CompressedStreamWriter(reader, header.sections, compressionInfo, session);
        writer.write(out.getChannel());
    }

add: 5, delete: 0, change: 97, unhandled: 0 size_exceptions: 1 size_serialize: 4
-----------cassandra-2.1.12 vs cassandra-2.1.13-----------
add: 4, delete: 0, change: 51, unhandled: 0 size_exceptions: 0 size_serialize: 0
-----------cassandra-2.1.13 vs cassandra-2.1.14-----------
add: 3, delete: 0, change: 41, unhandled: 0 size_exceptions: 0 size_serialize: 0
-----------cassandra-2.1.14 vs cassandra-2.1.15-----------
add: 5, delete: 0, change: 63, unhandled: 0 size_exceptions: 0 size_serialize: 0
-----------cassandra-2.1.15 vs cassandra-2.1.16-----------
add: 4, delete: 1, change: 21, unhandled: 0 size_exceptions: 0 size_serialize: 0
-----------cassandra-2.1.16 vs cassandra-2.1.17-----------
add: 1, delete: 0, change: 9, unhandled: 0 size_exceptions: 0 size_serialize: 0
-----------cassandra-2.1.17 vs cassandra-2.1.18-----------
add: 0, delete: 0, change: 4, unhandled: 0 size_exceptions: 0 size_serialize: 0
-----------cassandra-2.1.18 vs cassandra-2.1.19-----------
add: 0, delete: 0, change: 4, unhandled: 0 size_exceptions: 0 size_serialize: 0
-----------cassandra-2.1.19 vs cassandra-2.1.20-----------
add: 1, delete: 0, change: 23, unhandled: 0 size_exceptions: 0 size_serialize: 0
-----------cassandra-2.1.20 vs cassandra-2.1.21-----------
add: 1, delete: 0, change: 4, unhandled: 0 size_exceptions: 0 size_serialize: 0
-----------cassandra-2.1.21 vs cassandra-2.2.0-----------
    public AtomDeserializer getOnDiskDeserializer(DataInput in, Descriptor.Version version)
    {
        return new AtomDeserializer(comparator, in, ColumnSerializer.Flag.LOCAL, Integer.MIN_VALUE, version);
    }

+++++++++++++++++++++++
    public AtomDeserializer getOnDiskDeserializer(DataInput in, Version version)
    {
        return new AtomDeserializer(comparator, in, ColumnSerializer.Flag.LOCAL, Integer.MIN_VALUE, version);
    }

    private static ByteBuffer serializeMutations(Collection<Mutation> mutations, int version)
    {
        DataOutputBuffer buf = new DataOutputBuffer();

        try
        {
            buf.writeInt(mutations.size());
            for (Mutation mutation : mutations)
                Mutation.serializer.serialize(mutation, buf, version);
        }
        catch (IOException e)
        {
            throw new AssertionError(); // cannot happen.
        }

        return buf.asByteBuffer();
    }

+++++++++++++++++++++++
    private static ByteBuffer serializeMutations(Collection<Mutation> mutations, int version)
    {
        try (DataOutputBuffer buf = new DataOutputBuffer())
        {
            buf.writeInt(mutations.size());
            for (Mutation mutation : mutations)
                Mutation.serializer.serialize(mutation, buf, version);
            return buf.buffer();
        }
        catch (IOException e)
        {
            throw new AssertionError(); // cannot happen.
        }
    }

    public ColumnFamily deserializeFromSSTable(DataInput in, Descriptor.Version version)
    {
        throw new UnsupportedOperationException();
    }

+++++++++++++++++++++++
    public ColumnFamily deserializeFromSSTable(DataInput in, Version version)
    {
        throw new UnsupportedOperationException();
    }

    public void serialize(ColumnFamily cf, DataOutputPlus out, int version)
    {
        try
        {
            if (cf == null)
            {
                out.writeBoolean(false);
                return;
            }

            out.writeBoolean(true);
            serializeCfId(cf.id(), out, version);
            cf.getComparator().deletionInfoSerializer().serialize(cf.deletionInfo(), out, version);
            ColumnSerializer columnSerializer = cf.getComparator().columnSerializer();
            int count = cf.getColumnCount();
            out.writeInt(count);
            int written = 0;
            for (Cell cell : cf)
            {
                columnSerializer.serialize(cell, out);
                written++;
            }
            assert count == written: "Column family had " + count + " columns, but " + written + " written";
        }
        catch (IOException e)
        {
            throw new RuntimeException(e);
        }
    }

+++++++++++++++++++++++
    public void serialize(ColumnFamily cf, DataOutputPlus out, int version)
    {
        try
        {
            if (cf == null)
            {
                out.writeBoolean(false);
                return;
            }

            out.writeBoolean(true);
            serializeCfId(cf.id(), out, version);
            cf.getComparator().deletionInfoSerializer().serialize(cf.deletionInfo(), out, version);
            ColumnSerializer columnSerializer = cf.getComparator().columnSerializer();
            int count = cf.getColumnCount();
            out.writeInt(count);
            int written = 0;
            for (Cell cell : cf)
            {
                columnSerializer.serialize(cell, out);
                written++;
            }
            assert count == written: "Table had " + count + " columns, but " + written + " written";
        }
        catch (IOException e)
        {
            throw new RuntimeException(e);
        }
    }

    public void serialize(RangeSliceCommand sliceCommand, DataOutputPlus out, int version) throws IOException
    {
        out.writeUTF(sliceCommand.keyspace);
        out.writeUTF(sliceCommand.columnFamily);
        out.writeLong(sliceCommand.timestamp);

        CFMetaData metadata = Schema.instance.getCFMetaData(sliceCommand.keyspace, sliceCommand.columnFamily);

        metadata.comparator.diskAtomFilterSerializer().serialize(sliceCommand.predicate, out, version);

        if (sliceCommand.rowFilter == null)
        {
            out.writeInt(0);
        }
        else
        {
            out.writeInt(sliceCommand.rowFilter.size());
            for (IndexExpression expr : sliceCommand.rowFilter)
            {
                expr.writeTo(out);
            }
        }
        AbstractBounds.serializer.serialize(sliceCommand.keyRange, out, version);
        out.writeInt(sliceCommand.maxResults);
        out.writeBoolean(sliceCommand.countCQL3Rows);
        out.writeBoolean(sliceCommand.isPaging);
    }

+++++++++++++++++++++++
    public void serialize(RangeSliceCommand sliceCommand, DataOutputPlus out, int version) throws IOException
    {
        out.writeUTF(sliceCommand.keyspace);
        out.writeUTF(sliceCommand.columnFamily);
        out.writeLong(sliceCommand.timestamp);

        CFMetaData metadata = Schema.instance.getCFMetaData(sliceCommand.keyspace, sliceCommand.columnFamily);

        metadata.comparator.diskAtomFilterSerializer().serialize(sliceCommand.predicate, out, version);

        if (sliceCommand.rowFilter == null)
        {
            out.writeInt(0);
        }
        else
        {
            out.writeInt(sliceCommand.rowFilter.size());
            for (IndexExpression expr : sliceCommand.rowFilter)
            {
                expr.writeTo(out);
            }
        }
        MessagingService.validatePartitioner(sliceCommand.keyRange);
        AbstractBounds.rowPositionSerializer.serialize(sliceCommand.keyRange, out, version);
        out.writeInt(sliceCommand.maxResults);
        out.writeBoolean(sliceCommand.countCQL3Rows);
        out.writeBoolean(sliceCommand.isPaging);
    }

    public long serializedSize(RangeSliceCommand rsc, int version)
    {
        long size = TypeSizes.NATIVE.sizeof(rsc.keyspace);
        size += TypeSizes.NATIVE.sizeof(rsc.columnFamily);
        size += TypeSizes.NATIVE.sizeof(rsc.timestamp);

        CFMetaData metadata = Schema.instance.getCFMetaData(rsc.keyspace, rsc.columnFamily);

        IDiskAtomFilter filter = rsc.predicate;

        size += metadata.comparator.diskAtomFilterSerializer().serializedSize(filter, version);

        if (rsc.rowFilter == null)
        {
            size += TypeSizes.NATIVE.sizeof(0);
        }
        else
        {
            size += TypeSizes.NATIVE.sizeof(rsc.rowFilter.size());
            for (IndexExpression expr : rsc.rowFilter)
            {
                size += TypeSizes.NATIVE.sizeofWithShortLength(expr.column);
                size += TypeSizes.NATIVE.sizeof(expr.operator.ordinal());
                size += TypeSizes.NATIVE.sizeofWithShortLength(expr.value);
            }
        }
        size += AbstractBounds.serializer.serializedSize(rsc.keyRange, version);
        size += TypeSizes.NATIVE.sizeof(rsc.maxResults);
        size += TypeSizes.NATIVE.sizeof(rsc.countCQL3Rows);
        size += TypeSizes.NATIVE.sizeof(rsc.isPaging);
        return size;
+++++++++++++++++++++++
    public long serializedSize(RangeSliceCommand rsc, int version)
    {
        long size = TypeSizes.NATIVE.sizeof(rsc.keyspace);
        size += TypeSizes.NATIVE.sizeof(rsc.columnFamily);
        size += TypeSizes.NATIVE.sizeof(rsc.timestamp);

        CFMetaData metadata = Schema.instance.getCFMetaData(rsc.keyspace, rsc.columnFamily);

        IDiskAtomFilter filter = rsc.predicate;

        size += metadata.comparator.diskAtomFilterSerializer().serializedSize(filter, version);

        if (rsc.rowFilter == null)
        {
            size += TypeSizes.NATIVE.sizeof(0);
        }
        else
        {
            size += TypeSizes.NATIVE.sizeof(rsc.rowFilter.size());
            for (IndexExpression expr : rsc.rowFilter)
            {
                size += TypeSizes.NATIVE.sizeofWithShortLength(expr.column);
                size += TypeSizes.NATIVE.sizeof(expr.operator.ordinal());
                size += TypeSizes.NATIVE.sizeofWithShortLength(expr.value);
            }
        }
        size += AbstractBounds.rowPositionSerializer.serializedSize(rsc.keyRange, version);
        size += TypeSizes.NATIVE.sizeof(rsc.maxResults);
        size += TypeSizes.NATIVE.sizeof(rsc.countCQL3Rows);
        size += TypeSizes.NATIVE.sizeof(rsc.isPaging);
        return size;
    public RangeSliceCommand deserialize(DataInput in, int version) throws IOException
    {
        String keyspace = in.readUTF();
        String columnFamily = in.readUTF();
        long timestamp = in.readLong();

        CFMetaData metadata = Schema.instance.getCFMetaData(keyspace, columnFamily);
        if (metadata == null)
        {
            String message = String.format("Got range slice command for nonexistent table %s.%s.  If the table was just " +
                    "created, this is likely due to the schema not being fully propagated.  Please wait for schema " +
                    "agreement on table creation." , keyspace, columnFamily);
            throw new UnknownColumnFamilyException(message, null);
        }

        IDiskAtomFilter predicate = metadata.comparator.diskAtomFilterSerializer().deserialize(in, version);

        List<IndexExpression> rowFilter;
        int filterCount = in.readInt();
        rowFilter = new ArrayList<>(filterCount);
        for (int i = 0; i < filterCount; i++)
        {
            rowFilter.add(IndexExpression.readFrom(in));
        }
        AbstractBounds<RowPosition> range = AbstractBounds.serializer.deserialize(in, version).toRowBounds();

        int maxResults = in.readInt();
        boolean countCQL3Rows = in.readBoolean();
        boolean isPaging = in.readBoolean();
        return new RangeSliceCommand(keyspace, columnFamily, timestamp, predicate, range, rowFilter, maxResults, countCQL3Rows, isPaging);
    }

+++++++++++++++++++++++
    public RangeSliceCommand deserialize(DataInput in, int version) throws IOException
    {
        String keyspace = in.readUTF();
        String columnFamily = in.readUTF();
        long timestamp = in.readLong();

        CFMetaData metadata = Schema.instance.getCFMetaData(keyspace, columnFamily);
        if (metadata == null)
        {
            String message = String.format("Got range slice command for nonexistent table %s.%s.  If the table was just " +
                    "created, this is likely due to the schema not being fully propagated.  Please wait for schema " +
                    "agreement on table creation." , keyspace, columnFamily);
            throw new UnknownColumnFamilyException(message, null);
        }

        IDiskAtomFilter predicate = metadata.comparator.diskAtomFilterSerializer().deserialize(in, version);

        List<IndexExpression> rowFilter;
        int filterCount = in.readInt();
        rowFilter = new ArrayList<>(filterCount);
        for (int i = 0; i < filterCount; i++)
        {
            rowFilter.add(IndexExpression.readFrom(in));
        }
        AbstractBounds<RowPosition> range = AbstractBounds.rowPositionSerializer.deserialize(in, MessagingService.globalPartitioner(), version);

        int maxResults = in.readInt();
        boolean countCQL3Rows = in.readBoolean();
        boolean isPaging = in.readBoolean();
        return new RangeSliceCommand(keyspace, columnFamily, timestamp, predicate, range, rowFilter, maxResults, countCQL3Rows, isPaging);
    }

    private static Collection<Token> deserializeTokens(Collection<String> tokensStrings)
    {
        Token.TokenFactory factory = StorageService.getPartitioner().getTokenFactory();
        List<Token> tokens = new ArrayList<Token>(tokensStrings.size());
        for (String tk : tokensStrings)
            tokens.add(factory.fromString(tk));
        return tokens;
    }

    /**
     * Remove stored tokens being used by another node
     */
+++++++++++++++++++++++
    private static Collection<Token> deserializeTokens(Collection<String> tokensStrings)
    {
        Token.TokenFactory factory = StorageService.getPartitioner().getTokenFactory();
        List<Token> tokens = new ArrayList<>(tokensStrings.size());
        for (String tk : tokensStrings)
            tokens.add(factory.fromString(tk));
        return tokens;
    }

    /**
     * Remove stored tokens being used by another node
     */
    public void serialize(EndpointState epState, DataOutputPlus out, int version) throws IOException
    {
        /* serialize the HeartBeatState */
        HeartBeatState hbState = epState.getHeartBeatState();
        HeartBeatState.serializer.serialize(hbState, out, version);

        /* serialize the map of ApplicationState objects */
        Set<Map.Entry<ApplicationState, VersionedValue>> states = epState.states();
        out.writeInt(states.size());
        for (Map.Entry<ApplicationState, VersionedValue> state : states)
        {
            VersionedValue value = state.getValue();
            out.writeInt(state.getKey().ordinal());
            VersionedValue.serializer.serialize(value, out, version);
        }
    }

+++++++++++++++++++++++
    public void serialize(EndpointState epState, DataOutputPlus out, int version) throws IOException
    {
        /* serialize the HeartBeatState */
        HeartBeatState hbState = epState.getHeartBeatState();
        HeartBeatState.serializer.serialize(hbState, out, version);

        /* serialize the map of ApplicationState objects */
        int size = epState.applicationState.size();
        out.writeInt(size);
        for (Map.Entry<ApplicationState, VersionedValue> entry : epState.applicationState.entrySet())
        {
            VersionedValue value = entry.getValue();
            out.writeInt(entry.getKey().ordinal());
            VersionedValue.serializer.serialize(value, out, version);
        }
    }

    public long serializedSize(EndpointState epState, int version)
    {
        long size = HeartBeatState.serializer.serializedSize(epState.getHeartBeatState(), version);
        Set<Map.Entry<ApplicationState, VersionedValue>> states = epState.states();
        size += TypeSizes.NATIVE.sizeof(states.size());
        for (Map.Entry<ApplicationState, VersionedValue> state : states)
        {
            VersionedValue value = state.getValue();
            size += TypeSizes.NATIVE.sizeof(state.getKey().ordinal());
            size += VersionedValue.serializer.serializedSize(value, version);
        }
        return size;
+++++++++++++++++++++++
    public long serializedSize(EndpointState epState, int version)
    {
        long size = HeartBeatState.serializer.serializedSize(epState.getHeartBeatState(), version);
        size += TypeSizes.NATIVE.sizeof(epState.applicationState.size());
        for (Map.Entry<ApplicationState, VersionedValue> entry : epState.applicationState.entrySet())
        {
            VersionedValue value = entry.getValue();
            size += TypeSizes.NATIVE.sizeof(entry.getKey().ordinal());
            size += VersionedValue.serializer.serializedSize(value, version);
        }
        return size;
    public EndpointState deserialize(DataInput in, int version) throws IOException
    {
        HeartBeatState hbState = HeartBeatState.serializer.deserialize(in, version);

        int appStateSize = in.readInt();
        Map<ApplicationState, VersionedValue> states = new EnumMap<>(ApplicationState.class);
        for (int i = 0; i < appStateSize; ++i)
        {
            int key = in.readInt();
            VersionedValue value = VersionedValue.serializer.deserialize(in, version);
            states.put(Gossiper.STATES[key], value);
        }

        return new EndpointState(hbState, states);
    }

+++++++++++++++++++++++
    public EndpointState deserialize(DataInput in, int version) throws IOException
    {
        HeartBeatState hbState = HeartBeatState.serializer.deserialize(in, version);
        EndpointState epState = new EndpointState(hbState);

        int appStateSize = in.readInt();
        for (int i = 0; i < appStateSize; ++i)
        {
            int key = in.readInt();
            VersionedValue value = VersionedValue.serializer.deserialize(in, version);
            epState.addApplicationState(Gossiper.STATES[key], value);
        }
        return epState;
    }

    public Map<MetadataType, MetadataComponent> deserialize(Descriptor descriptor, FileDataInput in, EnumSet<MetadataType> types) throws IOException
    {
        Map<MetadataType, MetadataComponent> components = Maps.newHashMap();
        // read number of components
        int numComponents = in.readInt();
        // read toc
        Map<MetadataType, Integer> toc = new HashMap<>(numComponents);
        for (int i = 0; i < numComponents; i++)
        {
            toc.put(MetadataType.values()[in.readInt()], in.readInt());
        }
        for (MetadataType type : types)
        {
            MetadataComponent component = null;
            if (toc.containsKey(type))
            {
                in.seek(toc.get(type));
                component = type.serializer.deserialize(descriptor.version, in);
            }
            components.put(type, component);
        }
        return components;
    }

+++++++++++++++++++++++
    public Map<MetadataType, MetadataComponent> deserialize(Descriptor descriptor, FileDataInput in, EnumSet<MetadataType> types) throws IOException
    {
        Map<MetadataType, MetadataComponent> components = Maps.newHashMap();
        // read number of components
        int numComponents = in.readInt();
        // read toc
        Map<MetadataType, Integer> toc = new HashMap<>(numComponents);
        MetadataType[] values = MetadataType.values();
        for (int i = 0; i < numComponents; i++)
        {
            toc.put(values[in.readInt()], in.readInt());
        }
        for (MetadataType type : types)
        {
            MetadataComponent component = null;
            Integer offset = toc.get(type);
            if (offset != null)
            {
                in.seek(offset);
                component = type.serializer.deserialize(descriptor.version, in);
            }
            components.put(type, component);
        }
        return components;
    }

    public void serialize(DataOutputPlus out, int version) throws IOException
    {
        CompactEndpointSerializationHelper.serialize(from, out);

        out.writeInt(verb.ordinal());
        out.writeInt(parameters.size());
        for (Map.Entry<String, byte[]> entry : parameters.entrySet())
        {
            out.writeUTF(entry.getKey());
            out.writeInt(entry.getValue().length);
            out.write(entry.getValue());
        }

        long longSize = payload == null ? 0 : serializer.serializedSize(payload, version);
        assert longSize <= Integer.MAX_VALUE; // larger values are supported in sstables but not messages
        out.writeInt((int) longSize);
        if (payload != null)
            serializer.serialize(payload, out, version);
    }

+++++++++++++++++++++++
    public void serialize(DataOutputPlus out, int version) throws IOException
    {
        CompactEndpointSerializationHelper.serialize(from, out);

        out.writeInt(verb.ordinal());
        out.writeInt(parameters.size());
        for (Map.Entry<String, byte[]> entry : parameters.entrySet())
        {
            out.writeUTF(entry.getKey());
            out.writeInt(entry.getValue().length);
            out.write(entry.getValue());
        }

        long longSize = payloadSize(version);
        assert longSize <= Integer.MAX_VALUE; // larger values are supported in sstables but not messages
        out.writeInt((int) longSize);
        if (payload != null)
            serializer.serialize(payload, out, version);
    }

    public int serializedSize(int version)
    {
        int size = CompactEndpointSerializationHelper.serializedSize(from);

        size += TypeSizes.NATIVE.sizeof(verb.ordinal());
        size += TypeSizes.NATIVE.sizeof(parameters.size());
        for (Map.Entry<String, byte[]> entry : parameters.entrySet())
        {
            size += TypeSizes.NATIVE.sizeof(entry.getKey());
            size += TypeSizes.NATIVE.sizeof(entry.getValue().length);
            size += entry.getValue().length;
        }

        long longSize = payload == null ? 0 : serializer.serializedSize(payload, version);
        assert longSize <= Integer.MAX_VALUE; // larger values are supported in sstables but not messages
        size += TypeSizes.NATIVE.sizeof((int) longSize);
        size += longSize;
        return size;
+++++++++++++++++++++++
    public int serializedSize(int version)
    {
        int size = CompactEndpointSerializationHelper.serializedSize(from);

        size += TypeSizes.NATIVE.sizeof(verb.ordinal());
        size += TypeSizes.NATIVE.sizeof(parameters.size());
        for (Map.Entry<String, byte[]> entry : parameters.entrySet())
        {
            size += TypeSizes.NATIVE.sizeof(entry.getKey());
            size += TypeSizes.NATIVE.sizeof(entry.getValue().length);
            size += entry.getValue().length;
        }

        long longSize = payloadSize(version);
        assert longSize <= Integer.MAX_VALUE; // larger values are supported in sstables but not messages
        size += TypeSizes.NATIVE.sizeof((int) longSize);
        size += longSize;
        return size;
    }

    /**
     * Calculate the size of the payload of this message for the specified protocol version
     * and memoize the result for the specified protocol version. Memoization only covers the protocol
     * version of the first invocation.
     *
     * It is not safe to call payloadSize concurrently from multiple threads unless it has already been invoked
     * once from a single thread and there is a happens before relationship between that invocation and other
     * threads concurrently invoking payloadSize.
     *
     * For instance it would be safe to invokePayload size to make a decision in the thread that created the message
     * and then hand it off to other threads via a thread-safe queue, volatile write, or synchronized/ReentrantLock.
     * @param version Protocol version to use when calculating payload size
     * @return Size of the payload of this message in bytes
     */
    public ByteBuffer serialize()
    {
        try
        {
            DataOutputBuffer out = new DataOutputBuffer(serializedSize());
            ByteBufferUtil.writeWithShortLength(partitionKey, out);
            ByteBufferUtil.writeWithShortLength(cellName, out);
            out.writeInt(remaining);
            return out.asByteBuffer();
        }
        catch (IOException e)
        {
            throw new RuntimeException(e);
        }
    }

+++++++++++++++++++++++
    public ByteBuffer serialize()
    {
        try (DataOutputBuffer out = new DataOutputBufferFixed(serializedSize()))
        {
            ByteBufferUtil.writeWithShortLength(partitionKey, out);
            ByteBufferUtil.writeWithShortLength(cellName, out);
            out.writeInt(remaining);
            return out.buffer();
        }
        catch (IOException e)
        {
            throw new RuntimeException(e);
        }
    }

    public synchronized void serialize(DataOutputStreamAndChannel out, int version, StreamSession session) throws IOException
    {
        if (completed)
        {
            return;
        }

        CompressionInfo compressionInfo = FileMessageHeader.serializer.serialize(header, out, version);

        final SSTableReader reader = ref.get();
        StreamWriter writer = compressionInfo == null ?
                                      new StreamWriter(reader, header.sections, session) :
                                      new CompressedStreamWriter(reader, header.sections, compressionInfo, session);
        writer.write(out.getChannel());
    }

+++++++++++++++++++++++
    public synchronized void serialize(DataOutputStreamPlus out, int version, StreamSession session) throws IOException
    {
        if (completed)
        {
            return;
        }

        FileMessageHeader.serializer.serialize(header, out, version);

        final SSTableReader reader = ref.get();
        StreamWriter writer = header.compressionInfo == null ?
                                      new StreamWriter(reader, header.sections, session) :
                                      new CompressedStreamWriter(reader, header.sections,
                                                                 header.compressionInfo, session);
        writer.write(out);
    }

    public static void serialize(StreamMessage message, DataOutputStreamAndChannel out, int version, StreamSession session) throws IOException
    {
        ByteBuffer buff = ByteBuffer.allocate(1);
        // message type
        buff.put(message.type.type);
        buff.flip();
        out.write(buff);
        message.type.outSerializer.serialize(message, out, version, session);
    }

+++++++++++++++++++++++
    public static void serialize(StreamMessage message, DataOutputStreamPlus out, int version, StreamSession session) throws IOException
    {
        ByteBuffer buff = ByteBuffer.allocate(1);
        // message type
        buff.put(message.type.type);
        buff.flip();
        out.write(buff);
        message.type.outSerializer.serialize(message, out, version, session);
    }

    public static StreamMessage deserialize(ReadableByteChannel in, int version, StreamSession session) throws IOException
    {
        ByteBuffer buff = ByteBuffer.allocate(1);
        int readBytes = in.read(buff);
        if (readBytes > 0)
        {
            buff.flip();
            Type type = Type.get(buff.get());
            return type.inSerializer.deserialize(in, version, session);
        }
        else if (readBytes == 0)
        {
            // input socket buffer was not filled yet
            return null;
        }
        else
        {
            // possibly socket gets closed
            throw new SocketException("End-of-stream reached");
        }
    }

    /** StreamMessage serializer */
+++++++++++++++++++++++
    public static StreamMessage deserialize(ReadableByteChannel in, int version, StreamSession session) throws IOException
    {
        ByteBuffer buff = ByteBuffer.allocate(1);
        if (in.read(buff) > 0)
        {
            buff.flip();
            Type type = Type.get(buff.get());
            return type.inSerializer.deserialize(in, version, session);
        }
        else
        {
            // when socket gets closed, there is a chance that buff is empty
            // in that case, just return null
            return null;
        }
    }

    /** StreamMessage serializer */
    public void serialize(ByteBuf dest, int version)
    {
        CBUtil.writeEnumValue(type, dest);
        serializeEvent(dest, version);
    }

+++++++++++++++++++++++
    public void serialize(ByteBuf dest, int version)
    {
        if (type.minimumVersion > version)
            throw new ProtocolException("Event " + type.name() + " not valid for protocol version " + version);
        CBUtil.writeEnumValue(type, dest);
        serializeEvent(dest, version);
    }

    public static Event deserialize(ByteBuf cb, int version)
    {
        switch (CBUtil.readEnumValue(Type.class, cb))
        {
            case TOPOLOGY_CHANGE:
                return TopologyChange.deserializeEvent(cb, version);
            case STATUS_CHANGE:
                return StatusChange.deserializeEvent(cb, version);
            case SCHEMA_CHANGE:
                return SchemaChange.deserializeEvent(cb, version);
        }
        throw new AssertionError();
    }

+++++++++++++++++++++++
    public static Event deserialize(ByteBuf cb, int version)
    {
        Type eventType = CBUtil.readEnumValue(Type.class, cb);
        if (eventType.minimumVersion > version)
            throw new ProtocolException("Event " + eventType.name() + " not valid for protocol version " + version);
        switch (eventType)
        {
            case TOPOLOGY_CHANGE:
                return TopologyChange.deserializeEvent(cb, version);
            case STATUS_CHANGE:
                return StatusChange.deserializeEvent(cb, version);
            case SCHEMA_CHANGE:
                return SchemaChange.deserializeEvent(cb, version);
        }
        throw new AssertionError();
    }

    public static void serialize(IFilter bf, DataOutputPlus output) throws IOException
    {
        Murmur3BloomFilter.serializer.serialize((Murmur3BloomFilter) bf, output);
    }

+++++++++++++++++++++++
    public static void serialize(IFilter bf, DataOutputPlus output) throws IOException
    {
        BloomFilter.serializer.serialize((BloomFilter) bf, output);
    }

    public static IFilter deserialize(DataInput input, boolean offheap) throws IOException
    {
        return Murmur3BloomFilter.serializer.deserialize(input, offheap);
    }

    /**
     * @return A BloomFilter with the lowest practical false positive
     *         probability for the given number of elements.
     */
+++++++++++++++++++++++
    public static IFilter deserialize(DataInput input, boolean offheap) throws IOException
    {
        return BloomFilter.serializer.deserialize(input, offheap);
    }

    /**
     * @return A BloomFilter with the lowest practical false positive
     *         probability for the given number of elements.
     */
    public static <C, D, I extends Interval<C, D>> Serializer<C, D, I> serializer(ISerializer<C> pointSerializer, ISerializer<D> dataSerializer, Constructor<I> constructor)
    {
        return new Serializer<>(pointSerializer, dataSerializer, constructor);
    }

    @SuppressWarnings("unchecked")
+++++++++++++++++++++++
    public static <C extends Comparable<? super C>, D, I extends Interval<C, D>> Serializer<C, D, I> serializer(ISerializer<C> pointSerializer, ISerializer<D> dataSerializer, Constructor<I> constructor)
    {
        return new Serializer<>(pointSerializer, dataSerializer, constructor);
    }

    @SuppressWarnings("unchecked")
    protected void testSerializeDeserialize(CFMetaData cfm, ColumnDefinition cd) throws Exception
    {
        ColumnDefinition newCd = ColumnDefinition.fromThrift(cfm.ksName, cfm.cfName, cfm.comparator.asAbstractType(), null, cd.toThrift());
        Assert.assertNotSame(cd, newCd);
        Assert.assertEquals(cd.hashCode(), newCd.hashCode());
        Assert.assertEquals(cd, newCd);
+++++++++++++++++++++++
    protected void testSerializeDeserialize(CFMetaData cfm, ColumnDefinition cd) throws Exception
    {
        ColumnDefinition newCd = ThriftConversion.fromThrift(cfm.ksName, cfm.cfName, cfm.comparator.asAbstractType(), null, ThriftConversion.toThrift(cd));
        Assert.assertNotSame(cd, newCd);
        Assert.assertEquals(cd.hashCode(), newCd.hashCode());
        Assert.assertEquals(cd, newCd);
    private void populateAndReserialize(IBitSet bs) throws IOException
    {
        for (long i = 0; i < bs.capacity(); i++)
            if (random.nextBoolean())
                bs.set(i);

        DataOutputBuffer out = new DataOutputBuffer();
        bs.serialize(out);
        DataInputStream in = new DataInputStream(new ByteArrayInputStream(out.getData()));
        OffHeapBitSet newbs = OffHeapBitSet.deserialize(in);
        compare(bs, newbs);
    }

+++++++++++++++++++++++
    private void populateAndReserialize(IBitSet bs) throws IOException
    {
        for (long i = 0; i < bs.capacity(); i++)
            if (random.nextBoolean())
                bs.set(i);

        DataOutputBuffer out = new DataOutputBuffer();
        bs.serialize(out);
        DataInputStream in = new DataInputStream(new ByteArrayInputStream(out.getData()));
        try (OffHeapBitSet newbs = OffHeapBitSet.deserialize(in))
        {
            compare(bs, newbs);
        }
    }

add: 320, delete: 148, change: 798, unhandled: 1 size_exceptions: 105 size_serialize: 25
-----------cassandra-2.2.0 vs cassandra-2.2.0-beta1-----------
    private static ByteBuffer serializeMutations(Collection<Mutation> mutations, int version)
    {
        try (DataOutputBuffer buf = new DataOutputBuffer())
        {
            buf.writeInt(mutations.size());
            for (Mutation mutation : mutations)
                Mutation.serializer.serialize(mutation, buf, version);
            return buf.buffer();
        }
        catch (IOException e)
        {
            throw new AssertionError(); // cannot happen.
        }
    }

+++++++++++++++++++++++
    private static ByteBuffer serializeMutations(Collection<Mutation> mutations, int version)
    {
        DataOutputBuffer buf = new DataOutputBuffer();

        try
        {
            buf.writeInt(mutations.size());
            for (Mutation mutation : mutations)
                Mutation.serializer.serialize(mutation, buf, version);
        }
        catch (IOException e)
        {
            throw new AssertionError(); // cannot happen.
        }

        return buf.buffer();
    }

    public ByteBuffer serialize()
    {
        try (DataOutputBuffer out = new DataOutputBufferFixed(serializedSize()))
        {
            ByteBufferUtil.writeWithShortLength(partitionKey, out);
            ByteBufferUtil.writeWithShortLength(cellName, out);
            out.writeInt(remaining);
            return out.buffer();
        }
        catch (IOException e)
        {
            throw new RuntimeException(e);
        }
    }

+++++++++++++++++++++++
    public ByteBuffer serialize()
    {
        try
        {
            DataOutputBuffer out = new DataOutputBufferFixed(serializedSize());
            ByteBufferUtil.writeWithShortLength(partitionKey, out);
            ByteBufferUtil.writeWithShortLength(cellName, out);
            out.writeInt(remaining);
            return out.buffer();
        }
        catch (IOException e)
        {
            throw new RuntimeException(e);
        }
    }

    public static Event deserialize(ByteBuf cb, int version)
    {
        Type eventType = CBUtil.readEnumValue(Type.class, cb);
        if (eventType.minimumVersion > version)
            throw new ProtocolException("Event " + eventType.name() + " not valid for protocol version " + version);
        switch (eventType)
        {
            case TOPOLOGY_CHANGE:
                return TopologyChange.deserializeEvent(cb, version);
            case STATUS_CHANGE:
                return StatusChange.deserializeEvent(cb, version);
            case SCHEMA_CHANGE:
                return SchemaChange.deserializeEvent(cb, version);
        }
        throw new AssertionError();
    }

+++++++++++++++++++++++
    public static Event deserialize(ByteBuf cb, int version)
    {
        Type eventType = CBUtil.readEnumValue(Type.class, cb);
        if (eventType.minimumVersion > version)
            throw new ProtocolException("Event " + eventType.name() + " not valid for protocol version " + version);
        switch (eventType)
        {
            case TOPOLOGY_CHANGE:
                return TopologyChange.deserializeEvent(cb, version);
            case STATUS_CHANGE:
                return StatusChange.deserializeEvent(cb, version);
            case SCHEMA_CHANGE:
                return SchemaChange.deserializeEvent(cb, version);
            case TRACE_COMPLETE:
                return TraceComplete.deserializeEvent(cb, version);
        }
        throw new AssertionError();
    }

add: 41, delete: 70, change: 310, unhandled: 1 size_exceptions: 3 size_serialize: 3
-----------cassandra-2.2.0-beta1 vs cassandra-2.2.0-rc1-----------
    private static ByteBuffer serializeMutations(Collection<Mutation> mutations, int version)
    {
        DataOutputBuffer buf = new DataOutputBuffer();

        try
        {
            buf.writeInt(mutations.size());
            for (Mutation mutation : mutations)
                Mutation.serializer.serialize(mutation, buf, version);
        }
        catch (IOException e)
        {
            throw new AssertionError(); // cannot happen.
        }

        return buf.buffer();
    }

+++++++++++++++++++++++
    private static ByteBuffer serializeMutations(Collection<Mutation> mutations, int version)
    {
        try (DataOutputBuffer buf = new DataOutputBuffer())
        {
            buf.writeInt(mutations.size());
            for (Mutation mutation : mutations)
                Mutation.serializer.serialize(mutation, buf, version);
            return buf.buffer();
        }
        catch (IOException e)
        {
            throw new AssertionError(); // cannot happen.
        }
    }

    public ByteBuffer serialize()
    {
        try
        {
            DataOutputBuffer out = new DataOutputBufferFixed(serializedSize());
            ByteBufferUtil.writeWithShortLength(partitionKey, out);
            ByteBufferUtil.writeWithShortLength(cellName, out);
            out.writeInt(remaining);
            return out.buffer();
        }
        catch (IOException e)
        {
            throw new RuntimeException(e);
        }
    }

+++++++++++++++++++++++
    public ByteBuffer serialize()
    {
        try (DataOutputBuffer out = new DataOutputBufferFixed(serializedSize()))
        {
            ByteBufferUtil.writeWithShortLength(partitionKey, out);
            ByteBufferUtil.writeWithShortLength(cellName, out);
            out.writeInt(remaining);
            return out.buffer();
        }
        catch (IOException e)
        {
            throw new RuntimeException(e);
        }
    }

    public static Event deserialize(ByteBuf cb, int version)
    {
        Type eventType = CBUtil.readEnumValue(Type.class, cb);
        if (eventType.minimumVersion > version)
            throw new ProtocolException("Event " + eventType.name() + " not valid for protocol version " + version);
        switch (eventType)
        {
            case TOPOLOGY_CHANGE:
                return TopologyChange.deserializeEvent(cb, version);
            case STATUS_CHANGE:
                return StatusChange.deserializeEvent(cb, version);
            case SCHEMA_CHANGE:
                return SchemaChange.deserializeEvent(cb, version);
            case TRACE_COMPLETE:
                return TraceComplete.deserializeEvent(cb, version);
        }
        throw new AssertionError();
    }

+++++++++++++++++++++++
    public static Event deserialize(ByteBuf cb, int version)
    {
        Type eventType = CBUtil.readEnumValue(Type.class, cb);
        if (eventType.minimumVersion > version)
            throw new ProtocolException("Event " + eventType.name() + " not valid for protocol version " + version);
        switch (eventType)
        {
            case TOPOLOGY_CHANGE:
                return TopologyChange.deserializeEvent(cb, version);
            case STATUS_CHANGE:
                return StatusChange.deserializeEvent(cb, version);
            case SCHEMA_CHANGE:
                return SchemaChange.deserializeEvent(cb, version);
        }
        throw new AssertionError();
    }

add: 19, delete: 5, change: 248, unhandled: 1 size_exceptions: 2 size_serialize: 3
-----------cassandra-2.2.0-rc1 vs cassandra-2.2.0-rc2-----------
add: 49, delete: 36, change: 107, unhandled: 0 size_exceptions: 1 size_serialize: 0
-----------cassandra-2.2.0-rc2 vs cassandra-2.2.1-----------
add: 11, delete: 5, change: 138, unhandled: 0 size_exceptions: 0 size_serialize: 0
-----------cassandra-2.2.1 vs cassandra-2.2.2-----------
    private V deserialize(RefCountedMemory mem)
    {
        try
        {
            return serializer.deserialize(new EncodedDataInputStream(new MemoryInputStream(mem)));
        }
        catch (IOException e)
        {
            logger.debug("Cannot fetch in memory data, we will fallback to read from disk ", e);
            return null;
        }
    }

+++++++++++++++++++++++
    private V deserialize(RefCountedMemory mem)
    {
        try
        {
            return serializer.deserialize(new EncodedDataInputStream(new MemoryInputStream(mem)));
        }
        catch (IOException e)
        {
            logger.trace("Cannot fetch in memory data, we will fallback to read from disk ", e);
            return null;
        }
    }

    public static StreamMessage deserialize(ReadableByteChannel in, int version, StreamSession session) throws IOException
    {
        ByteBuffer buff = ByteBuffer.allocate(1);
        if (in.read(buff) > 0)
        {
            buff.flip();
            Type type = Type.get(buff.get());
            return type.inSerializer.deserialize(in, version, session);
        }
        else
        {
            // when socket gets closed, there is a chance that buff is empty
            // in that case, just return null
            return null;
        }
    }

    /** StreamMessage serializer */
+++++++++++++++++++++++
    public static StreamMessage deserialize(ReadableByteChannel in, int version, StreamSession session) throws IOException
    {
        ByteBuffer buff = ByteBuffer.allocate(1);
        int readBytes = in.read(buff);
        if (readBytes > 0)
        {
            buff.flip();
            Type type = Type.get(buff.get());
            return type.inSerializer.deserialize(in, version, session);
        }
        else if (readBytes == 0)
        {
            // input socket buffer was not filled yet
            return null;
        }
        else
        {
            // possibly socket gets closed
            throw new SocketException("End-of-stream reached");
        }
    }

    /** StreamMessage serializer */
add: 6, delete: 0, change: 177, unhandled: 0 size_exceptions: 2 size_serialize: 2
-----------cassandra-2.2.2 vs cassandra-2.2.3-----------
add: 2, delete: 0, change: 19, unhandled: 0 size_exceptions: 0 size_serialize: 0
-----------cassandra-2.2.3 vs cassandra-2.2.4-----------
    public void serialize(EndpointState epState, DataOutputPlus out, int version) throws IOException
    {
        /* serialize the HeartBeatState */
        HeartBeatState hbState = epState.getHeartBeatState();
        HeartBeatState.serializer.serialize(hbState, out, version);

        /* serialize the map of ApplicationState objects */
        int size = epState.applicationState.size();
        out.writeInt(size);
        for (Map.Entry<ApplicationState, VersionedValue> entry : epState.applicationState.entrySet())
        {
            VersionedValue value = entry.getValue();
            out.writeInt(entry.getKey().ordinal());
            VersionedValue.serializer.serialize(value, out, version);
        }
    }

+++++++++++++++++++++++
    public void serialize(EndpointState epState, DataOutputPlus out, int version) throws IOException
    {
        /* serialize the HeartBeatState */
        HeartBeatState hbState = epState.getHeartBeatState();
        HeartBeatState.serializer.serialize(hbState, out, version);

        /* serialize the map of ApplicationState objects */
        Set<Map.Entry<ApplicationState, VersionedValue>> states = epState.states();
        out.writeInt(states.size());
        for (Map.Entry<ApplicationState, VersionedValue> state : states)
        {
            VersionedValue value = state.getValue();
            out.writeInt(state.getKey().ordinal());
            VersionedValue.serializer.serialize(value, out, version);
        }
    }

    public long serializedSize(EndpointState epState, int version)
    {
        long size = HeartBeatState.serializer.serializedSize(epState.getHeartBeatState(), version);
        size += TypeSizes.NATIVE.sizeof(epState.applicationState.size());
        for (Map.Entry<ApplicationState, VersionedValue> entry : epState.applicationState.entrySet())
        {
            VersionedValue value = entry.getValue();
            size += TypeSizes.NATIVE.sizeof(entry.getKey().ordinal());
            size += VersionedValue.serializer.serializedSize(value, version);
        }
        return size;
+++++++++++++++++++++++
    public long serializedSize(EndpointState epState, int version)
    {
        long size = HeartBeatState.serializer.serializedSize(epState.getHeartBeatState(), version);
        Set<Map.Entry<ApplicationState, VersionedValue>> states = epState.states();
        size += TypeSizes.NATIVE.sizeof(states.size());
        for (Map.Entry<ApplicationState, VersionedValue> state : states)
        {
            VersionedValue value = state.getValue();
            size += TypeSizes.NATIVE.sizeof(state.getKey().ordinal());
            size += VersionedValue.serializer.serializedSize(value, version);
        }
        return size;
    public EndpointState deserialize(DataInput in, int version) throws IOException
    {
        HeartBeatState hbState = HeartBeatState.serializer.deserialize(in, version);
        EndpointState epState = new EndpointState(hbState);

        int appStateSize = in.readInt();
        for (int i = 0; i < appStateSize; ++i)
        {
            int key = in.readInt();
            VersionedValue value = VersionedValue.serializer.deserialize(in, version);
            epState.addApplicationState(Gossiper.STATES[key], value);
        }
        return epState;
    }

+++++++++++++++++++++++
    public EndpointState deserialize(DataInput in, int version) throws IOException
    {
        HeartBeatState hbState = HeartBeatState.serializer.deserialize(in, version);

        int appStateSize = in.readInt();
        Map<ApplicationState, VersionedValue> states = new EnumMap<>(ApplicationState.class);
        for (int i = 0; i < appStateSize; ++i)
        {
            int key = in.readInt();
            VersionedValue value = VersionedValue.serializer.deserialize(in, version);
            states.put(Gossiper.STATES[key], value);
        }

        return new EndpointState(hbState, states);
    }

    public synchronized void serialize(DataOutputStreamPlus out, int version, StreamSession session) throws IOException
    {
        if (completed)
        {
            return;
        }

        FileMessageHeader.serializer.serialize(header, out, version);

        final SSTableReader reader = ref.get();
        StreamWriter writer = header.compressionInfo == null ?
                                      new StreamWriter(reader, header.sections, session) :
                                      new CompressedStreamWriter(reader, header.sections,
                                                                 header.compressionInfo, session);
        writer.write(out);
    }

+++++++++++++++++++++++
    public synchronized void serialize(DataOutputStreamPlus out, int version, StreamSession session) throws IOException
    {
        if (completed)
        {
            return;
        }

        CompressionInfo compressionInfo = FileMessageHeader.serializer.serialize(header, out, version);

        final SSTableReader reader = ref.get();
        StreamWriter writer = compressionInfo == null ?
                                      new StreamWriter(reader, header.sections, session) :
                                      new CompressedStreamWriter(reader, header.sections,
                                                                 compressionInfo, session);
        writer.write(out);
    }

add: 8, delete: 0, change: 131, unhandled: 0 size_exceptions: 2 size_serialize: 4
-----------cassandra-2.2.4 vs cassandra-2.2.5-----------
add: 17, delete: 2, change: 100, unhandled: 0 size_exceptions: 0 size_serialize: 0
-----------cassandra-2.2.5 vs cassandra-2.2.6-----------
add: 6, delete: 1, change: 111, unhandled: 0 size_exceptions: 16 size_serialize: 0
-----------cassandra-2.2.6 vs cassandra-2.2.7-----------
    public void serialize(Map<MetadataType, MetadataComponent> components, DataOutputPlus out) throws IOException
    {
        ValidationMetadata validation = (ValidationMetadata) components.get(MetadataType.VALIDATION);
        StatsMetadata stats = (StatsMetadata) components.get(MetadataType.STATS);
        CompactionMetadata compaction = (CompactionMetadata) components.get(MetadataType.COMPACTION);

        assert validation != null && stats != null && compaction != null && validation.partitioner != null;

        EstimatedHistogram.serializer.serialize(stats.estimatedRowSize, out);
        EstimatedHistogram.serializer.serialize(stats.estimatedColumnCount, out);
        ReplayPosition.serializer.serialize(stats.replayPosition, out);
        out.writeLong(stats.minTimestamp);
        out.writeLong(stats.maxTimestamp);
        out.writeInt(stats.maxLocalDeletionTime);
        out.writeDouble(validation.bloomFilterFPChance);
        out.writeDouble(stats.compressionRatio);
        out.writeUTF(validation.partitioner);
        out.writeInt(compaction.ancestors.size());
        for (Integer g : compaction.ancestors)
            out.writeInt(g);
        StreamingHistogram.serializer.serialize(stats.estimatedTombstoneDropTime, out);
        out.writeInt(stats.sstableLevel);
        out.writeInt(stats.minColumnNames.size());
        for (ByteBuffer columnName : stats.minColumnNames)
            ByteBufferUtil.writeWithShortLength(columnName, out);
        out.writeInt(stats.maxColumnNames.size());
        for (ByteBuffer columnName : stats.maxColumnNames)
            ByteBufferUtil.writeWithShortLength(columnName, out);
    }

    /**
     * Legacy serializer deserialize all components no matter what types are specified.
     */
    @Override
+++++++++++++++++++++++
    public void serialize(Map<MetadataType, MetadataComponent> components, Version version, DataOutputPlus out) throws IOException
    {
        ValidationMetadata validation = (ValidationMetadata) components.get(MetadataType.VALIDATION);
        StatsMetadata stats = (StatsMetadata) components.get(MetadataType.STATS);
        CompactionMetadata compaction = (CompactionMetadata) components.get(MetadataType.COMPACTION);

        assert validation != null && stats != null && compaction != null && validation.partitioner != null;

        EstimatedHistogram.serializer.serialize(stats.estimatedRowSize, out);
        EstimatedHistogram.serializer.serialize(stats.estimatedColumnCount, out);
        ReplayPosition.serializer.serialize(stats.commitLogUpperBound, out);
        out.writeLong(stats.minTimestamp);
        out.writeLong(stats.maxTimestamp);
        out.writeInt(stats.maxLocalDeletionTime);
        out.writeDouble(validation.bloomFilterFPChance);
        out.writeDouble(stats.compressionRatio);
        out.writeUTF(validation.partitioner);
        out.writeInt(compaction.ancestors.size());
        for (Integer g : compaction.ancestors)
            out.writeInt(g);
        StreamingHistogram.serializer.serialize(stats.estimatedTombstoneDropTime, out);
        out.writeInt(stats.sstableLevel);
        out.writeInt(stats.minColumnNames.size());
        for (ByteBuffer columnName : stats.minColumnNames)
            ByteBufferUtil.writeWithShortLength(columnName, out);
        out.writeInt(stats.maxColumnNames.size());
        for (ByteBuffer columnName : stats.maxColumnNames)
            ByteBufferUtil.writeWithShortLength(columnName, out);
        if (version.hasCommitLogLowerBound())
            ReplayPosition.serializer.serialize(stats.commitLogLowerBound, out);
    }

    /**
     * Legacy serializer deserialize all components no matter what types are specified.
     */
    @Override
    public Map<MetadataType, MetadataComponent> deserialize(Descriptor descriptor, EnumSet<MetadataType> types) throws IOException
    {
        Map<MetadataType, MetadataComponent> components = Maps.newHashMap();

        File statsFile = new File(descriptor.filenameFor(Component.STATS));
        if (!statsFile.exists() && types.contains(MetadataType.STATS))
        {
            components.put(MetadataType.STATS, MetadataCollector.defaultStatsMetadata());
        }
        else
        {
            try (DataInputStream in = new DataInputStream(new BufferedInputStream(new FileInputStream(statsFile))))
            {
                EstimatedHistogram rowSizes = EstimatedHistogram.serializer.deserialize(in);
                EstimatedHistogram columnCounts = EstimatedHistogram.serializer.deserialize(in);
                ReplayPosition replayPosition = ReplayPosition.serializer.deserialize(in);
                long minTimestamp = in.readLong();
                long maxTimestamp = in.readLong();
                int maxLocalDeletionTime = in.readInt();
                double bloomFilterFPChance = in.readDouble();
                double compressionRatio = in.readDouble();
                String partitioner = in.readUTF();
                int nbAncestors = in.readInt();
                Set<Integer> ancestors = new HashSet<>(nbAncestors);
                for (int i = 0; i < nbAncestors; i++)
                    ancestors.add(in.readInt());
                StreamingHistogram tombstoneHistogram = StreamingHistogram.serializer.deserialize(in);
                int sstableLevel = 0;
                if (in.available() > 0)
                    sstableLevel = in.readInt();

                int colCount = in.readInt();
                List<ByteBuffer> minColumnNames = new ArrayList<>(colCount);
                for (int i = 0; i < colCount; i++)
                    minColumnNames.add(ByteBufferUtil.readWithShortLength(in));

                colCount = in.readInt();
                List<ByteBuffer> maxColumnNames = new ArrayList<>(colCount);
                for (int i = 0; i < colCount; i++)
                    maxColumnNames.add(ByteBufferUtil.readWithShortLength(in));

                if (types.contains(MetadataType.VALIDATION))
                    components.put(MetadataType.VALIDATION,
                                   new ValidationMetadata(partitioner, bloomFilterFPChance));
                if (types.contains(MetadataType.STATS))
                    components.put(MetadataType.STATS,
                                   new StatsMetadata(rowSizes,
                                                     columnCounts,
                                                     replayPosition,
                                                     minTimestamp,
                                                     maxTimestamp,
                                                     maxLocalDeletionTime,
                                                     compressionRatio,
                                                     tombstoneHistogram,
                                                     sstableLevel,
                                                     minColumnNames,
                                                     maxColumnNames,
                                                     true,
                                                     ActiveRepairService.UNREPAIRED_SSTABLE));
                if (types.contains(MetadataType.COMPACTION))
                    components.put(MetadataType.COMPACTION,
                                   new CompactionMetadata(ancestors, null));
            }
        }
        return components;
+++++++++++++++++++++++
    public Map<MetadataType, MetadataComponent> deserialize(Descriptor descriptor, EnumSet<MetadataType> types) throws IOException
    {
        Map<MetadataType, MetadataComponent> components = Maps.newHashMap();

        File statsFile = new File(descriptor.filenameFor(Component.STATS));
        if (!statsFile.exists() && types.contains(MetadataType.STATS))
        {
            components.put(MetadataType.STATS, MetadataCollector.defaultStatsMetadata());
        }
        else
        {
            try (DataInputStream in = new DataInputStream(new BufferedInputStream(new FileInputStream(statsFile))))
            {
                EstimatedHistogram rowSizes = EstimatedHistogram.serializer.deserialize(in);
                EstimatedHistogram columnCounts = EstimatedHistogram.serializer.deserialize(in);
                ReplayPosition commitLogLowerBound = ReplayPosition.NONE;
                ReplayPosition commitLogUpperBound = ReplayPosition.serializer.deserialize(in);
                long minTimestamp = in.readLong();
                long maxTimestamp = in.readLong();
                int maxLocalDeletionTime = in.readInt();
                double bloomFilterFPChance = in.readDouble();
                double compressionRatio = in.readDouble();
                String partitioner = in.readUTF();
                int nbAncestors = in.readInt();
                Set<Integer> ancestors = new HashSet<>(nbAncestors);
                for (int i = 0; i < nbAncestors; i++)
                    ancestors.add(in.readInt());
                StreamingHistogram tombstoneHistogram = StreamingHistogram.serializer.deserialize(in);
                int sstableLevel = 0;
                if (in.available() > 0)
                    sstableLevel = in.readInt();

                int colCount = in.readInt();
                List<ByteBuffer> minColumnNames = new ArrayList<>(colCount);
                for (int i = 0; i < colCount; i++)
                    minColumnNames.add(ByteBufferUtil.readWithShortLength(in));

                colCount = in.readInt();
                List<ByteBuffer> maxColumnNames = new ArrayList<>(colCount);
                for (int i = 0; i < colCount; i++)
                    maxColumnNames.add(ByteBufferUtil.readWithShortLength(in));
                if (descriptor.version.hasCommitLogLowerBound())
                    commitLogLowerBound = ReplayPosition.serializer.deserialize(in);

                if (types.contains(MetadataType.VALIDATION))
                    components.put(MetadataType.VALIDATION,
                                   new ValidationMetadata(partitioner, bloomFilterFPChance));
                if (types.contains(MetadataType.STATS))
                    components.put(MetadataType.STATS,
                                   new StatsMetadata(rowSizes,
                                                     columnCounts,
                                                     commitLogLowerBound,
                                                     commitLogUpperBound,
                                                     minTimestamp,
                                                     maxTimestamp,
                                                     maxLocalDeletionTime,
                                                     compressionRatio,
                                                     tombstoneHistogram,
                                                     sstableLevel,
                                                     minColumnNames,
                                                     maxColumnNames,
                                                     true,
                                                     ActiveRepairService.UNREPAIRED_SSTABLE));
                if (types.contains(MetadataType.COMPACTION))
                    components.put(MetadataType.COMPACTION,
                                   new CompactionMetadata(ancestors, null));
            }
        }
        return components;
    public void serialize(Map<MetadataType, MetadataComponent> components, DataOutputPlus out) throws IOException
    {
        // sort components by type
        List<MetadataComponent> sortedComponents = Lists.newArrayList(components.values());
        Collections.sort(sortedComponents);

        // write number of component
        out.writeInt(components.size());
        // build and write toc
        int lastPosition = 4 + (8 * sortedComponents.size());
        for (MetadataComponent component : sortedComponents)
        {
            MetadataType type = component.getType();
            // serialize type
            out.writeInt(type.ordinal());
            // serialize position
            out.writeInt(lastPosition);
            lastPosition += type.serializer.serializedSize(component);
        }
        // serialize components
        for (MetadataComponent component : sortedComponents)
        {
            component.getType().serializer.serialize(component, out);
        }
    }

+++++++++++++++++++++++
    public void serialize(Map<MetadataType, MetadataComponent> components, Version version, DataOutputPlus out) throws IOException
    {
        // sort components by type
        List<MetadataComponent> sortedComponents = Lists.newArrayList(components.values());
        Collections.sort(sortedComponents);

        // write number of component
        out.writeInt(components.size());
        // build and write toc
        int lastPosition = 4 + (8 * sortedComponents.size());
        for (MetadataComponent component : sortedComponents)
        {
            MetadataType type = component.getType();
            // serialize type
            out.writeInt(type.ordinal());
            // serialize position
            out.writeInt(lastPosition);
            lastPosition += type.serializer.serializedSize(component, version);
        }
        // serialize components
        for (MetadataComponent component : sortedComponents)
        {
            component.getType().serializer.serialize(component, version, out);
        }
    }

add: 6, delete: 1, change: 151, unhandled: 0 size_exceptions: 2 size_serialize: 3
-----------cassandra-2.2.7 vs cassandra-2.2.8-----------
add: 6, delete: 2, change: 83, unhandled: 0 size_exceptions: 0 size_serialize: 0
-----------cassandra-2.2.8 vs cassandra-2.2.9-----------
    public Boolean deserialize(ByteBuffer bytes)
    {
        if (bytes.remaining() == 0)
            return null;

        byte value = bytes.get(bytes.position());
        return value != 0;
    }

+++++++++++++++++++++++
    public Boolean deserialize(ByteBuffer bytes)
    {
        if (bytes == null || bytes.remaining() == 0)
            return null;

        byte value = bytes.get(bytes.position());
        return value != 0;
    }

    public List<T> deserializeForNativeProtocol(ByteBuffer bytes, int version)
    {
        try
        {
            ByteBuffer input = bytes.duplicate();
            int n = readCollectionSize(input, version);
            List<T> l = new ArrayList<T>(n);
            for (int i = 0; i < n; i++)
            {
                // We can have nulls in lists that are used for IN values
                ByteBuffer databb = readValue(input, version);
                if (databb != null)
                {
                    elements.validate(databb);
                    l.add(elements.deserialize(databb));
                }
                else
                {
                    l.add(null);
                }
            }

            if (input.hasRemaining())
                throw new MarshalException("Unexpected extraneous bytes after list value");

            return l;
        }
        catch (BufferUnderflowException e)
        {
            throw new MarshalException("Not enough bytes to read a list");
        }
    }

    /**
     * Returns the element at the given index in a list.
     * @param serializedList a serialized list
     * @param index the index to get
     * @return the serialized element at the given index, or null if the index exceeds the list size
     */
+++++++++++++++++++++++
    public List<T> deserializeForNativeProtocol(ByteBuffer bytes, int version)
    {
        try
        {
            ByteBuffer input = bytes.duplicate();
            int n = readCollectionSize(input, version);

            if (n < 0)
                throw new MarshalException("The data cannot be deserialized as a list");

            // If the received bytes are not corresponding to a list, n might be a huge number.
            // In such a case we do not want to initialize the list with that size as it can result
            // in an OOM (see CASSANDRA-12618). On the other hand we do not want to have to resize the list
            // if we can avoid it, so we put a reasonable limit on the initialCapacity.
            List<T> l = new ArrayList<T>(Math.min(n, 256));
            for (int i = 0; i < n; i++)
            {
                // We can have nulls in lists that are used for IN values
                ByteBuffer databb = readValue(input, version);
                if (databb != null)
                {
                    elements.validate(databb);
                    l.add(elements.deserialize(databb));
                }
                else
                {
                    l.add(null);
                }
            }

            if (input.hasRemaining())
                throw new MarshalException("Unexpected extraneous bytes after list value");

            return l;
        }
        catch (BufferUnderflowException e)
        {
            throw new MarshalException("Not enough bytes to read a list");
        }
    }

    /**
     * Returns the element at the given index in a list.
     * @param serializedList a serialized list
     * @param index the index to get
     * @return the serialized element at the given index, or null if the index exceeds the list size
     */
    public Map<K, V> deserializeForNativeProtocol(ByteBuffer bytes, int version)
    {
        try
        {
            ByteBuffer input = bytes.duplicate();
            int n = readCollectionSize(input, version);
            Map<K, V> m = new LinkedHashMap<K, V>(n);
            for (int i = 0; i < n; i++)
            {
                ByteBuffer kbb = readValue(input, version);
                keys.validate(kbb);

                ByteBuffer vbb = readValue(input, version);
                values.validate(vbb);

                m.put(keys.deserialize(kbb), values.deserialize(vbb));
            }
            if (input.hasRemaining())
                throw new MarshalException("Unexpected extraneous bytes after map value");
            return m;
        }
        catch (BufferUnderflowException e)
        {
            throw new MarshalException("Not enough bytes to read a map");
        }
    }

    /**
     * Given a serialized map, gets the value associated with a given key.
     * @param serializedMap a serialized map
     * @param serializedKey a serialized key
     * @param keyType the key type for the map
     * @return the value associated with the key if one exists, null otherwise
     */
+++++++++++++++++++++++
    public Map<K, V> deserializeForNativeProtocol(ByteBuffer bytes, int version)
    {
        try
        {
            ByteBuffer input = bytes.duplicate();
            int n = readCollectionSize(input, version);

            if (n < 0)
                throw new MarshalException("The data cannot be deserialized as a map");

            // If the received bytes are not corresponding to a map, n might be a huge number.
            // In such a case we do not want to initialize the map with that initialCapacity as it can result
            // in an OOM when put is called (see CASSANDRA-12618). On the other hand we do not want to have to resize
            // the map if we can avoid it, so we put a reasonable limit on the initialCapacity.
            Map<K, V> m = new LinkedHashMap<K, V>(Math.min(n, 256));
            for (int i = 0; i < n; i++)
            {
                ByteBuffer kbb = readValue(input, version);
                keys.validate(kbb);

                ByteBuffer vbb = readValue(input, version);
                values.validate(vbb);

                m.put(keys.deserialize(kbb), values.deserialize(vbb));
            }
            if (input.hasRemaining())
                throw new MarshalException("Unexpected extraneous bytes after map value");
            return m;
        }
        catch (BufferUnderflowException e)
        {
            throw new MarshalException("Not enough bytes to read a map");
        }
    }

    /**
     * Given a serialized map, gets the value associated with a given key.
     * @param serializedMap a serialized map
     * @param serializedKey a serialized key
     * @param keyType the key type for the map
     * @return the value associated with the key if one exists, null otherwise
     */
    public Set<T> deserializeForNativeProtocol(ByteBuffer bytes, int version)
    {
        try
        {
            ByteBuffer input = bytes.duplicate();
            int n = readCollectionSize(input, version);
            Set<T> l = new LinkedHashSet<T>(n);
            for (int i = 0; i < n; i++)
            {
                ByteBuffer databb = readValue(input, version);
                elements.validate(databb);
                l.add(elements.deserialize(databb));
            }
            if (input.hasRemaining())
                throw new MarshalException("Unexpected extraneous bytes after set value");
            return l;
        }
        catch (BufferUnderflowException e)
        {
            throw new MarshalException("Not enough bytes to read a list");
        }
    }

+++++++++++++++++++++++
    public Set<T> deserializeForNativeProtocol(ByteBuffer bytes, int version)
    {
        try
        {
            ByteBuffer input = bytes.duplicate();
            int n = readCollectionSize(input, version);

            if (n < 0)
                throw new MarshalException("The data cannot be deserialized as a set");

            // If the received bytes are not corresponding to a set, n might be a huge number.
            // In such a case we do not want to initialize the set with that initialCapacity as it can result
            // in an OOM when add is called (see CASSANDRA-12618). On the other hand we do not want to have to resize
            // the set if we can avoid it, so we put a reasonable limit on the initialCapacity.
            Set<T> l = new LinkedHashSet<T>(Math.min(n, 256));

            for (int i = 0; i < n; i++)
            {
                ByteBuffer databb = readValue(input, version);
                elements.validate(databb);
                l.add(elements.deserialize(databb));
            }
            if (input.hasRemaining())
                throw new MarshalException("Unexpected extraneous bytes after set value");
            return l;
        }
        catch (BufferUnderflowException e)
        {
            throw new MarshalException("Not enough bytes to read a set");
        }
    }

add: 4, delete: 0, change: 91, unhandled: 0 size_exceptions: 0 size_serialize: 4
-----------cassandra-2.2.9 vs cassandra-2.2.10-----------
add: 0, delete: 0, change: 33, unhandled: 0 size_exceptions: 1 size_serialize: 0
-----------cassandra-2.2.10 vs cassandra-2.2.11-----------
add: 7, delete: 0, change: 34, unhandled: 0 size_exceptions: 1 size_serialize: 0
-----------cassandra-2.2.11 vs cassandra-2.2.12-----------
add: 4, delete: 0, change: 25, unhandled: 0 size_exceptions: 0 size_serialize: 0
-----------cassandra-2.2.12 vs cassandra-2.2.13-----------
add: 1, delete: 0, change: 17, unhandled: 0 size_exceptions: 0 size_serialize: 0
-----------cassandra-2.2.13 vs cassandra-2.2.14-----------
add: 19, delete: 0, change: 57, unhandled: 0 size_exceptions: 0 size_serialize: 0
-----------cassandra-2.2.14 vs cassandra-2.2.15-----------
add: 38, delete: 13, change: 40, unhandled: 1 size_exceptions: 4 size_serialize: 0
-----------cassandra-2.2.15 vs cassandra-2.2.16-----------
add: 8, delete: 0, change: 33, unhandled: 0 size_exceptions: 0 size_serialize: 0
-----------cassandra-2.2.16 vs cassandra-3.0.0-----------
    private RefCountedMemory serialize(V value)
    {
        long serializedSize = serializer.serializedSize(value, ENCODED_TYPE_SIZES);
        if (serializedSize > Integer.MAX_VALUE)
            throw new IllegalArgumentException("Unable to allocate " + serializedSize + " bytes");

        RefCountedMemory freeableMemory;
        try
        {
            freeableMemory = new RefCountedMemory(serializedSize);
        }
        catch (OutOfMemoryError e)
        {
            return null;
        }

        try
        {
            serializer.serialize(value, new EncodedDataOutputStream(new MemoryOutputStream(freeableMemory)));
        }
        catch (IOException e)
        {
            freeableMemory.unreference();
            throw new RuntimeException(e);
        }
        return freeableMemory;
    }

+++++++++++++++++++++++
    private RefCountedMemory serialize(V value)
    {
        long serializedSize = serializer.serializedSize(value);
        if (serializedSize > Integer.MAX_VALUE)
            throw new IllegalArgumentException("Unable to allocate " + serializedSize + " bytes");

        RefCountedMemory freeableMemory;
        try
        {
            freeableMemory = new RefCountedMemory(serializedSize);
        }
        catch (OutOfMemoryError e)
        {
            return null;
        }

        try
        {
            serializer.serialize(value, new WrappedDataOutputStreamPlus(new MemoryOutputStream(freeableMemory)));
        }
        catch (IOException e)
        {
            freeableMemory.unreference();
            throw new RuntimeException(e);
        }
        return freeableMemory;
    }

    private V deserialize(RefCountedMemory mem)
    {
        try
        {
            return serializer.deserialize(new EncodedDataInputStream(new MemoryInputStream(mem)));
        }
        catch (IOException e)
        {
            logger.trace("Cannot fetch in memory data, we will fallback to read from disk ", e);
            return null;
        }
    }

+++++++++++++++++++++++
    private V deserialize(RefCountedMemory mem)
    {
        try
        {
            return serializer.deserialize(new MemoryInputStream(mem));
        }
        catch (IOException e)
        {
            logger.trace("Cannot fetch in memory data, we will fallback to read from disk ", e);
            return null;
        }
    }

    private static Token deserializeToken(ByteBuffer buffer)
    {
        return partitioner.getTokenFactory().fromByteArray(buffer);
    }

+++++++++++++++++++++++
    private Token deserializeToken(ByteBuffer buffer)
    {
        return partitioner.getTokenFactory().fromByteArray(buffer);
    }

    public long serializedSize(SnapshotCommand sc, int version)
    {
        return TypeSizes.NATIVE.sizeof(sc.keyspace)
             + TypeSizes.NATIVE.sizeof(sc.column_family)
             + TypeSizes.NATIVE.sizeof(sc.snapshot_name)
             + TypeSizes.NATIVE.sizeof(sc.clear_snapshot);
+++++++++++++++++++++++
    public long serializedSize(SnapshotCommand sc, int version)
    {
        return TypeSizes.sizeof(sc.keyspace)
             + TypeSizes.sizeof(sc.column_family)
             + TypeSizes.sizeof(sc.snapshot_name)
             + TypeSizes.sizeof(sc.clear_snapshot);
    public SnapshotCommand deserialize(DataInput in, int version) throws IOException
    {
        String keyspace = in.readUTF();
        String column_family = in.readUTF();
        String snapshot_name = in.readUTF();
        boolean clear_snapshot = in.readBoolean();
        return new SnapshotCommand(keyspace, column_family, snapshot_name, clear_snapshot);
    }

+++++++++++++++++++++++
    public SnapshotCommand deserialize(DataInputPlus in, int version) throws IOException
    {
        String keyspace = in.readUTF();
        String column_family = in.readUTF();
        String snapshot_name = in.readUTF();
        boolean clear_snapshot = in.readBoolean();
        return new SnapshotCommand(keyspace, column_family, snapshot_name, clear_snapshot);
    }

    private static Collection<Token> deserializeTokens(Collection<String> tokensStrings)
    {
        Token.TokenFactory factory = StorageService.getPartitioner().getTokenFactory();
        List<Token> tokens = new ArrayList<>(tokensStrings.size());
        for (String tk : tokensStrings)
            tokens.add(factory.fromString(tk));
        return tokens;
    }

    /**
     * Remove stored tokens being used by another node
     */
+++++++++++++++++++++++
    private static Collection<Token> deserializeTokens(Collection<String> tokensStrings)
    {
        Token.TokenFactory factory = StorageService.instance.getTokenFactory();
        List<Token> tokens = new ArrayList<>(tokensStrings.size());
        for (String tk : tokensStrings)
            tokens.add(factory.fromString(tk));
        return tokens;
    }

    /**
     * Remove stored tokens being used by another node
     */
    public long serializedSize(Truncation truncation, int version)
    {
        return TypeSizes.NATIVE.sizeof(truncation.keyspace) + TypeSizes.NATIVE.sizeof(truncation.columnFamily);
+++++++++++++++++++++++
    public long serializedSize(Truncation truncation, int version)
    {
        return TypeSizes.sizeof(truncation.keyspace) + TypeSizes.sizeof(truncation.columnFamily);
    public Truncation deserialize(DataInput in, int version) throws IOException
    {
        String keyspace = in.readUTF();
        String columnFamily = in.readUTF();
        return new Truncation(keyspace, columnFamily);
    }

+++++++++++++++++++++++
    public Truncation deserialize(DataInputPlus in, int version) throws IOException
    {
        String keyspace = in.readUTF();
        String columnFamily = in.readUTF();
        return new Truncation(keyspace, columnFamily);
    }

    public abstract List<ByteBuffer> serializedValues(List<Cell> cells);

+++++++++++++++++++++++
    protected abstract List<ByteBuffer> serializedValues(Iterator<Cell> cells);

    @Override
    public ByteBuffer serializeForNativeProtocol(ColumnDefinition def, List<Cell> cells, int version)
    {
        assert isMultiCell();
        cells = enforceLimit(def, cells, version);
        List<ByteBuffer> values = serializedValues(cells);
        return CollectionSerializer.pack(values, cells.size(), version);
    }

    @Override
+++++++++++++++++++++++
    public ByteBuffer serializeForNativeProtocol(ColumnDefinition def, Iterator<Cell> cells, int version)
    {
        assert isMultiCell();
        List<ByteBuffer> values = serializedValues(cells);
        int size = collectionSize(values);
        return CollectionSerializer.pack(values, size, version);
    }

    @Override
    public List<ByteBuffer> serializedValues(List<Cell> cells)
    {
        assert isMultiCell;
        List<ByteBuffer> bbs = new ArrayList<ByteBuffer>(cells.size());
        for (Cell c : cells)
            bbs.add(c.value());
        return bbs;
    }

    @Override
+++++++++++++++++++++++
    public List<ByteBuffer> serializedValues(Iterator<Cell> cells)
    {
        assert isMultiCell;
        List<ByteBuffer> bbs = new ArrayList<ByteBuffer>();
        while (cells.hasNext())
            bbs.add(cells.next().value());
        return bbs;
    }

    @Override
    public List<ByteBuffer> serializedValues(List<Cell> cells)
    {
        assert isMultiCell;
        List<ByteBuffer> bbs = new ArrayList<ByteBuffer>(cells.size() * 2);
        for (Cell c : cells)
        {
            bbs.add(c.name().collectionElement());
            bbs.add(c.value());
        }
        return bbs;
    }

    @Override
+++++++++++++++++++++++
    public List<ByteBuffer> serializedValues(Iterator<Cell> cells)
    {
        assert isMultiCell;
        List<ByteBuffer> bbs = new ArrayList<ByteBuffer>();
        while (cells.hasNext())
        {
            Cell c = cells.next();
            bbs.add(c.path().get(0));
            bbs.add(c.value());
        }
        return bbs;
    }

    @Override
    public List<ByteBuffer> serializedValues(List<Cell> cells)
    {
        List<ByteBuffer> bbs = new ArrayList<ByteBuffer>(cells.size());
        for (Cell c : cells)
            bbs.add(c.name().collectionElement());
        return bbs;
    }

    @Override
+++++++++++++++++++++++
    public List<ByteBuffer> serializedValues(Iterator<Cell> cells)
    {
        List<ByteBuffer> bbs = new ArrayList<ByteBuffer>();
        while (cells.hasNext())
            bbs.add(cells.next().path().get(0));
        return bbs;
    }

    @Override
    public void serialize(EndpointState epState, DataOutputPlus out, int version) throws IOException
    {
        /* serialize the HeartBeatState */
        HeartBeatState hbState = epState.getHeartBeatState();
        HeartBeatState.serializer.serialize(hbState, out, version);

        /* serialize the map of ApplicationState objects */
        Set<Map.Entry<ApplicationState, VersionedValue>> states = epState.states();
        out.writeInt(states.size());
        for (Map.Entry<ApplicationState, VersionedValue> state : states)
        {
            VersionedValue value = state.getValue();
            out.writeInt(state.getKey().ordinal());
            VersionedValue.serializer.serialize(value, out, version);
        }
    }

+++++++++++++++++++++++
    public void serialize(EndpointState epState, DataOutputPlus out, int version) throws IOException
    {
        /* serialize the HeartBeatState */
        HeartBeatState hbState = epState.getHeartBeatState();
        HeartBeatState.serializer.serialize(hbState, out, version);

        /* serialize the map of ApplicationState objects */
        int size = epState.applicationState.size();
        out.writeInt(size);
        for (Map.Entry<ApplicationState, VersionedValue> entry : epState.applicationState.entrySet())
        {
            VersionedValue value = entry.getValue();
            out.writeInt(entry.getKey().ordinal());
            VersionedValue.serializer.serialize(value, out, version);
        }
    }

    public long serializedSize(EndpointState epState, int version)
    {
        long size = HeartBeatState.serializer.serializedSize(epState.getHeartBeatState(), version);
        Set<Map.Entry<ApplicationState, VersionedValue>> states = epState.states();
        size += TypeSizes.NATIVE.sizeof(states.size());
        for (Map.Entry<ApplicationState, VersionedValue> state : states)
        {
            VersionedValue value = state.getValue();
            size += TypeSizes.NATIVE.sizeof(state.getKey().ordinal());
            size += VersionedValue.serializer.serializedSize(value, version);
        }
        return size;
+++++++++++++++++++++++
    public long serializedSize(EndpointState epState, int version)
    {
        long size = HeartBeatState.serializer.serializedSize(epState.getHeartBeatState(), version);
        size += TypeSizes.sizeof(epState.applicationState.size());
        for (Map.Entry<ApplicationState, VersionedValue> entry : epState.applicationState.entrySet())
        {
            VersionedValue value = entry.getValue();
            size += TypeSizes.sizeof(entry.getKey().ordinal());
            size += VersionedValue.serializer.serializedSize(value, version);
        }
        return size;
    public EndpointState deserialize(DataInput in, int version) throws IOException
    {
        HeartBeatState hbState = HeartBeatState.serializer.deserialize(in, version);

        int appStateSize = in.readInt();
        Map<ApplicationState, VersionedValue> states = new EnumMap<>(ApplicationState.class);
        for (int i = 0; i < appStateSize; ++i)
        {
            int key = in.readInt();
            VersionedValue value = VersionedValue.serializer.deserialize(in, version);
            states.put(Gossiper.STATES[key], value);
        }

        return new EndpointState(hbState, states);
    }

+++++++++++++++++++++++
    public EndpointState deserialize(DataInputPlus in, int version) throws IOException
    {
        HeartBeatState hbState = HeartBeatState.serializer.deserialize(in, version);
        EndpointState epState = new EndpointState(hbState);

        int appStateSize = in.readInt();
        for (int i = 0; i < appStateSize; ++i)
        {
            int key = in.readInt();
            VersionedValue value = VersionedValue.serializer.deserialize(in, version);
            epState.addApplicationState(Gossiper.STATES[key], value);
        }
        return epState;
    }

    public long serializedSize(GossipDigest gDigest, int version)
    {
        long size = CompactEndpointSerializationHelper.serializedSize(gDigest.endpoint);
        size += TypeSizes.NATIVE.sizeof(gDigest.generation);
        size += TypeSizes.NATIVE.sizeof(gDigest.maxVersion);
        return size;
+++++++++++++++++++++++
    public long serializedSize(GossipDigest gDigest, int version)
    {
        long size = CompactEndpointSerializationHelper.serializedSize(gDigest.endpoint);
        size += TypeSizes.sizeof(gDigest.generation);
        size += TypeSizes.sizeof(gDigest.maxVersion);
        return size;
    public GossipDigest deserialize(DataInput in, int version) throws IOException
    {
        InetAddress endpoint = CompactEndpointSerializationHelper.deserialize(in);
        int generation = in.readInt();
        int maxVersion = in.readInt();
        return new GossipDigest(endpoint, generation, maxVersion);
    }

+++++++++++++++++++++++
    public GossipDigest deserialize(DataInputPlus in, int version) throws IOException
    {
        InetAddress endpoint = CompactEndpointSerializationHelper.deserialize(in);
        int generation = in.readInt();
        int maxVersion = in.readInt();
        return new GossipDigest(endpoint, generation, maxVersion);
    }

    public long serializedSize(GossipDigestAck ack, int version)
    {
        int size = GossipDigestSerializationHelper.serializedSize(ack.gDigestList, version);
        size += TypeSizes.NATIVE.sizeof(ack.epStateMap.size());
        for (Map.Entry<InetAddress, EndpointState> entry : ack.epStateMap.entrySet())
            size += CompactEndpointSerializationHelper.serializedSize(entry.getKey())
                    + EndpointState.serializer.serializedSize(entry.getValue(), version);
        return size;
+++++++++++++++++++++++
    public long serializedSize(GossipDigestAck ack, int version)
    {
        int size = GossipDigestSerializationHelper.serializedSize(ack.gDigestList, version);
        size += TypeSizes.sizeof(ack.epStateMap.size());
        for (Map.Entry<InetAddress, EndpointState> entry : ack.epStateMap.entrySet())
            size += CompactEndpointSerializationHelper.serializedSize(entry.getKey())
                    + EndpointState.serializer.serializedSize(entry.getValue(), version);
        return size;
    public GossipDigestAck deserialize(DataInput in, int version) throws IOException
    {
        List<GossipDigest> gDigestList = GossipDigestSerializationHelper.deserialize(in, version);
        int size = in.readInt();
        Map<InetAddress, EndpointState> epStateMap = new HashMap<InetAddress, EndpointState>(size);

        for (int i = 0; i < size; ++i)
        {
            InetAddress ep = CompactEndpointSerializationHelper.deserialize(in);
            EndpointState epState = EndpointState.serializer.deserialize(in, version);
            epStateMap.put(ep, epState);
        }
        return new GossipDigestAck(gDigestList, epStateMap);
    }

+++++++++++++++++++++++
    public GossipDigestAck deserialize(DataInputPlus in, int version) throws IOException
    {
        List<GossipDigest> gDigestList = GossipDigestSerializationHelper.deserialize(in, version);
        int size = in.readInt();
        Map<InetAddress, EndpointState> epStateMap = new HashMap<InetAddress, EndpointState>(size);

        for (int i = 0; i < size; ++i)
        {
            InetAddress ep = CompactEndpointSerializationHelper.deserialize(in);
            EndpointState epState = EndpointState.serializer.deserialize(in, version);
            epStateMap.put(ep, epState);
        }
        return new GossipDigestAck(gDigestList, epStateMap);
    }

    public long serializedSize(GossipDigestAck2 ack2, int version)
    {
        long size = TypeSizes.NATIVE.sizeof(ack2.epStateMap.size());
        for (Map.Entry<InetAddress, EndpointState> entry : ack2.epStateMap.entrySet())
            size += CompactEndpointSerializationHelper.serializedSize(entry.getKey())
                    + EndpointState.serializer.serializedSize(entry.getValue(), version);
        return size;
+++++++++++++++++++++++
    public long serializedSize(GossipDigestAck2 ack2, int version)
    {
        long size = TypeSizes.sizeof(ack2.epStateMap.size());
        for (Map.Entry<InetAddress, EndpointState> entry : ack2.epStateMap.entrySet())
            size += CompactEndpointSerializationHelper.serializedSize(entry.getKey())
                    + EndpointState.serializer.serializedSize(entry.getValue(), version);
        return size;
    public GossipDigestAck2 deserialize(DataInput in, int version) throws IOException
    {
        int size = in.readInt();
        Map<InetAddress, EndpointState> epStateMap = new HashMap<InetAddress, EndpointState>(size);

        for (int i = 0; i < size; ++i)
        {
            InetAddress ep = CompactEndpointSerializationHelper.deserialize(in);
            EndpointState epState = EndpointState.serializer.deserialize(in, version);
            epStateMap.put(ep, epState);
        }
        return new GossipDigestAck2(epStateMap);
    }

+++++++++++++++++++++++
    public GossipDigestAck2 deserialize(DataInputPlus in, int version) throws IOException
    {
        int size = in.readInt();
        Map<InetAddress, EndpointState> epStateMap = new HashMap<InetAddress, EndpointState>(size);

        for (int i = 0; i < size; ++i)
        {
            InetAddress ep = CompactEndpointSerializationHelper.deserialize(in);
            EndpointState epState = EndpointState.serializer.deserialize(in, version);
            epStateMap.put(ep, epState);
        }
        return new GossipDigestAck2(epStateMap);
    }

    public long serializedSize(GossipDigestSyn syn, int version)
    {
        long size = TypeSizes.NATIVE.sizeof(syn.clusterId);
        size += TypeSizes.NATIVE.sizeof(syn.partioner);
        size += GossipDigestSerializationHelper.serializedSize(syn.gDigests, version);
        return size;
+++++++++++++++++++++++
    public long serializedSize(GossipDigestSyn syn, int version)
    {
        long size = TypeSizes.sizeof(syn.clusterId);
        size += TypeSizes.sizeof(syn.partioner);
        size += GossipDigestSerializationHelper.serializedSize(syn.gDigests, version);
        return size;
    public GossipDigestSyn deserialize(DataInput in, int version) throws IOException
    {
        String clusterId = in.readUTF();
        String partioner = null;
        partioner = in.readUTF();
        List<GossipDigest> gDigests = GossipDigestSerializationHelper.deserialize(in, version);
        return new GossipDigestSyn(clusterId, partioner, gDigests);
    }

+++++++++++++++++++++++
    public GossipDigestSyn deserialize(DataInputPlus in, int version) throws IOException
    {
        String clusterId = in.readUTF();
        String partioner = null;
        partioner = in.readUTF();
        List<GossipDigest> gDigests = GossipDigestSerializationHelper.deserialize(in, version);
        return new GossipDigestSyn(clusterId, partioner, gDigests);
    }

    static int serializedSize(List<GossipDigest> digests, int version)
    {
        int size = TypeSizes.NATIVE.sizeof(digests.size());
        for (GossipDigest digest : digests)
            size += GossipDigest.serializer.serializedSize(digest, version);
        return size;
    }
}

class GossipDigestSynSerializer implements IVersionedSerializer<GossipDigestSyn>
{
    public void serialize(GossipDigestSyn gDigestSynMessage, DataOutputPlus out, int version) throws IOException
    {
        out.writeUTF(gDigestSynMessage.clusterId);
        out.writeUTF(gDigestSynMessage.partioner);
        GossipDigestSerializationHelper.serialize(gDigestSynMessage.gDigests, out, version);
    }

    public GossipDigestSyn deserialize(DataInput in, int version) throws IOException
    {
        String clusterId = in.readUTF();
        String partioner = null;
        partioner = in.readUTF();
        List<GossipDigest> gDigests = GossipDigestSerializationHelper.deserialize(in, version);
        return new GossipDigestSyn(clusterId, partioner, gDigests);
    }

    public long serializedSize(GossipDigestSyn syn, int version)
    {
        long size = TypeSizes.NATIVE.sizeof(syn.clusterId);
        size += TypeSizes.NATIVE.sizeof(syn.partioner);
        size += GossipDigestSerializationHelper.serializedSize(syn.gDigests, version);
        return size;
+++++++++++++++++++++++
    static int serializedSize(List<GossipDigest> digests, int version)
    {
        int size = TypeSizes.sizeof(digests.size());
        for (GossipDigest digest : digests)
            size += GossipDigest.serializer.serializedSize(digest, version);
        return size;
    }
}

class GossipDigestSynSerializer implements IVersionedSerializer<GossipDigestSyn>
{
    public void serialize(GossipDigestSyn gDigestSynMessage, DataOutputPlus out, int version) throws IOException
    {
        out.writeUTF(gDigestSynMessage.clusterId);
        out.writeUTF(gDigestSynMessage.partioner);
        GossipDigestSerializationHelper.serialize(gDigestSynMessage.gDigests, out, version);
    }

    public GossipDigestSyn deserialize(DataInputPlus in, int version) throws IOException
    {
        String clusterId = in.readUTF();
        String partioner = null;
        partioner = in.readUTF();
        List<GossipDigest> gDigests = GossipDigestSerializationHelper.deserialize(in, version);
        return new GossipDigestSyn(clusterId, partioner, gDigests);
    }

    public long serializedSize(GossipDigestSyn syn, int version)
    {
        long size = TypeSizes.sizeof(syn.clusterId);
        size += TypeSizes.sizeof(syn.partioner);
        size += GossipDigestSerializationHelper.serializedSize(syn.gDigests, version);
        return size;
    static List<GossipDigest> deserialize(DataInput in, int version) throws IOException
    {
        int size = in.readInt();
        List<GossipDigest> gDigests = new ArrayList<GossipDigest>(size);
        for (int i = 0; i < size; ++i)
            gDigests.add(GossipDigest.serializer.deserialize(in, version));
        return gDigests;
    }

+++++++++++++++++++++++
    static List<GossipDigest> deserialize(DataInputPlus in, int version) throws IOException
    {
        int size = in.readInt();
        List<GossipDigest> gDigests = new ArrayList<GossipDigest>(size);
        for (int i = 0; i < size; ++i)
            gDigests.add(GossipDigest.serializer.deserialize(in, version));
        return gDigests;
    }

    public long serializedSize(HeartBeatState state, int version)
    {
        return TypeSizes.NATIVE.sizeof(state.getGeneration()) + TypeSizes.NATIVE.sizeof(state.getHeartBeatVersion());
+++++++++++++++++++++++
    public long serializedSize(HeartBeatState state, int version)
    {
        return TypeSizes.sizeof(state.getGeneration()) + TypeSizes.sizeof(state.getHeartBeatVersion());
    public HeartBeatState deserialize(DataInput in, int version) throws IOException
    {
        return new HeartBeatState(in.readInt(), in.readInt());
    }

+++++++++++++++++++++++
    public HeartBeatState deserialize(DataInputPlus in, int version) throws IOException
    {
        return new HeartBeatState(in.readInt(), in.readInt());
    }

    public void serialize(Map<MetadataType, MetadataComponent> components, Version version, DataOutputPlus out) throws IOException
    {
        ValidationMetadata validation = (ValidationMetadata) components.get(MetadataType.VALIDATION);
        StatsMetadata stats = (StatsMetadata) components.get(MetadataType.STATS);
        CompactionMetadata compaction = (CompactionMetadata) components.get(MetadataType.COMPACTION);

        assert validation != null && stats != null && compaction != null && validation.partitioner != null;

        EstimatedHistogram.serializer.serialize(stats.estimatedRowSize, out);
        EstimatedHistogram.serializer.serialize(stats.estimatedColumnCount, out);
        ReplayPosition.serializer.serialize(stats.commitLogUpperBound, out);
        out.writeLong(stats.minTimestamp);
        out.writeLong(stats.maxTimestamp);
        out.writeInt(stats.maxLocalDeletionTime);
        out.writeDouble(validation.bloomFilterFPChance);
        out.writeDouble(stats.compressionRatio);
        out.writeUTF(validation.partitioner);
        out.writeInt(compaction.ancestors.size());
        for (Integer g : compaction.ancestors)
            out.writeInt(g);
        StreamingHistogram.serializer.serialize(stats.estimatedTombstoneDropTime, out);
        out.writeInt(stats.sstableLevel);
        out.writeInt(stats.minColumnNames.size());
        for (ByteBuffer columnName : stats.minColumnNames)
            ByteBufferUtil.writeWithShortLength(columnName, out);
        out.writeInt(stats.maxColumnNames.size());
        for (ByteBuffer columnName : stats.maxColumnNames)
            ByteBufferUtil.writeWithShortLength(columnName, out);
        if (version.hasCommitLogLowerBound())
            ReplayPosition.serializer.serialize(stats.commitLogLowerBound, out);
    }

    /**
     * Legacy serializer deserialize all components no matter what types are specified.
     */
    @Override
+++++++++++++++++++++++
    public void serialize(Map<MetadataType, MetadataComponent> components, DataOutputPlus out, Version version) throws IOException
    {
        ValidationMetadata validation = (ValidationMetadata) components.get(MetadataType.VALIDATION);
        StatsMetadata stats = (StatsMetadata) components.get(MetadataType.STATS);
        CompactionMetadata compaction = (CompactionMetadata) components.get(MetadataType.COMPACTION);

        assert validation != null && stats != null && compaction != null && validation.partitioner != null;

        EstimatedHistogram.serializer.serialize(stats.estimatedPartitionSize, out);
        EstimatedHistogram.serializer.serialize(stats.estimatedColumnCount, out);
        ReplayPosition.serializer.serialize(stats.replayPosition, out);
        out.writeLong(stats.minTimestamp);
        out.writeLong(stats.maxTimestamp);
        out.writeInt(stats.maxLocalDeletionTime);
        out.writeDouble(validation.bloomFilterFPChance);
        out.writeDouble(stats.compressionRatio);
        out.writeUTF(validation.partitioner);
        out.writeInt(0); // compaction ancestors
        StreamingHistogram.serializer.serialize(stats.estimatedTombstoneDropTime, out);
        out.writeInt(stats.sstableLevel);
        out.writeInt(stats.minClusteringValues.size());
        for (ByteBuffer value : stats.minClusteringValues)
            ByteBufferUtil.writeWithShortLength(value, out);
        out.writeInt(stats.maxClusteringValues.size());
        for (ByteBuffer value : stats.maxClusteringValues)
            ByteBufferUtil.writeWithShortLength(value, out);
    }

    /**
     * Legacy serializer deserialize all components no matter what types are specified.
     */
    @Override
    public Map<MetadataType, MetadataComponent> deserialize(Descriptor descriptor, EnumSet<MetadataType> types) throws IOException
    {
        Map<MetadataType, MetadataComponent> components = Maps.newHashMap();

        File statsFile = new File(descriptor.filenameFor(Component.STATS));
        if (!statsFile.exists() && types.contains(MetadataType.STATS))
        {
            components.put(MetadataType.STATS, MetadataCollector.defaultStatsMetadata());
        }
        else
        {
            try (DataInputStream in = new DataInputStream(new BufferedInputStream(new FileInputStream(statsFile))))
            {
                EstimatedHistogram rowSizes = EstimatedHistogram.serializer.deserialize(in);
                EstimatedHistogram columnCounts = EstimatedHistogram.serializer.deserialize(in);
                ReplayPosition commitLogLowerBound = ReplayPosition.NONE;
                ReplayPosition commitLogUpperBound = ReplayPosition.serializer.deserialize(in);
                long minTimestamp = in.readLong();
                long maxTimestamp = in.readLong();
                int maxLocalDeletionTime = in.readInt();
                double bloomFilterFPChance = in.readDouble();
                double compressionRatio = in.readDouble();
                String partitioner = in.readUTF();
                int nbAncestors = in.readInt();
                Set<Integer> ancestors = new HashSet<>(nbAncestors);
                for (int i = 0; i < nbAncestors; i++)
                    ancestors.add(in.readInt());
                StreamingHistogram tombstoneHistogram = StreamingHistogram.serializer.deserialize(in);
                int sstableLevel = 0;
                if (in.available() > 0)
                    sstableLevel = in.readInt();

                int colCount = in.readInt();
                List<ByteBuffer> minColumnNames = new ArrayList<>(colCount);
                for (int i = 0; i < colCount; i++)
                    minColumnNames.add(ByteBufferUtil.readWithShortLength(in));

                colCount = in.readInt();
                List<ByteBuffer> maxColumnNames = new ArrayList<>(colCount);
                for (int i = 0; i < colCount; i++)
                    maxColumnNames.add(ByteBufferUtil.readWithShortLength(in));
                if (descriptor.version.hasCommitLogLowerBound())
                    commitLogLowerBound = ReplayPosition.serializer.deserialize(in);

                if (types.contains(MetadataType.VALIDATION))
                    components.put(MetadataType.VALIDATION,
                                   new ValidationMetadata(partitioner, bloomFilterFPChance));
                if (types.contains(MetadataType.STATS))
                    components.put(MetadataType.STATS,
                                   new StatsMetadata(rowSizes,
                                                     columnCounts,
                                                     commitLogLowerBound,
                                                     commitLogUpperBound,
                                                     minTimestamp,
                                                     maxTimestamp,
                                                     maxLocalDeletionTime,
                                                     compressionRatio,
                                                     tombstoneHistogram,
                                                     sstableLevel,
                                                     minColumnNames,
                                                     maxColumnNames,
                                                     true,
                                                     ActiveRepairService.UNREPAIRED_SSTABLE));
                if (types.contains(MetadataType.COMPACTION))
                    components.put(MetadataType.COMPACTION,
                                   new CompactionMetadata(ancestors, null));
            }
        }
        return components;
+++++++++++++++++++++++
    public Map<MetadataType, MetadataComponent> deserialize(Descriptor descriptor, EnumSet<MetadataType> types) throws IOException
    {
        Map<MetadataType, MetadataComponent> components = Maps.newHashMap();

        File statsFile = new File(descriptor.filenameFor(Component.STATS));
        if (!statsFile.exists() && types.contains(MetadataType.STATS))
        {
            components.put(MetadataType.STATS, MetadataCollector.defaultStatsMetadata());
        }
        else
        {
            try (DataInputStreamPlus in = new DataInputStreamPlus(new BufferedInputStream(new FileInputStream(statsFile))))
            {
                EstimatedHistogram partitionSizes = EstimatedHistogram.serializer.deserialize(in);
                EstimatedHistogram columnCounts = EstimatedHistogram.serializer.deserialize(in);
                ReplayPosition replayPosition = ReplayPosition.serializer.deserialize(in);
                long minTimestamp = in.readLong();
                long maxTimestamp = in.readLong();
                int maxLocalDeletionTime = in.readInt();
                double bloomFilterFPChance = in.readDouble();
                double compressionRatio = in.readDouble();
                String partitioner = in.readUTF();
                int nbAncestors = in.readInt(); //skip compaction ancestors
                in.skipBytes(nbAncestors * TypeSizes.sizeof(nbAncestors));
                StreamingHistogram tombstoneHistogram = StreamingHistogram.serializer.deserialize(in);
                int sstableLevel = 0;
                if (in.available() > 0)
                    sstableLevel = in.readInt();

                int colCount = in.readInt();
                List<ByteBuffer> minColumnNames = new ArrayList<>(colCount);
                for (int i = 0; i < colCount; i++)
                    minColumnNames.add(ByteBufferUtil.readWithShortLength(in));

                colCount = in.readInt();
                List<ByteBuffer> maxColumnNames = new ArrayList<>(colCount);
                for (int i = 0; i < colCount; i++)
                    maxColumnNames.add(ByteBufferUtil.readWithShortLength(in));

                if (types.contains(MetadataType.VALIDATION))
                    components.put(MetadataType.VALIDATION,
                                   new ValidationMetadata(partitioner, bloomFilterFPChance));
                if (types.contains(MetadataType.STATS))
                    components.put(MetadataType.STATS,
                                   new StatsMetadata(partitionSizes,
                                                     columnCounts,
                                                     replayPosition,
                                                     minTimestamp,
                                                     maxTimestamp,
                                                     Integer.MAX_VALUE,
                                                     maxLocalDeletionTime,
                                                     0,
                                                     Integer.MAX_VALUE,
                                                     compressionRatio,
                                                     tombstoneHistogram,
                                                     sstableLevel,
                                                     minColumnNames,
                                                     maxColumnNames,
                                                     true,
                                                     ActiveRepairService.UNREPAIRED_SSTABLE,
                                                     -1,
                                                     -1));
                if (types.contains(MetadataType.COMPACTION))
                    components.put(MetadataType.COMPACTION,
                                   new CompactionMetadata(null));
            }
        }
        return components;
    public void serialize(Map<MetadataType, MetadataComponent> components, Version version, DataOutputPlus out) throws IOException
    {
        // sort components by type
        List<MetadataComponent> sortedComponents = Lists.newArrayList(components.values());
        Collections.sort(sortedComponents);

        // write number of component
        out.writeInt(components.size());
        // build and write toc
        int lastPosition = 4 + (8 * sortedComponents.size());
        for (MetadataComponent component : sortedComponents)
        {
            MetadataType type = component.getType();
            // serialize type
            out.writeInt(type.ordinal());
            // serialize position
            out.writeInt(lastPosition);
            lastPosition += type.serializer.serializedSize(component, version);
        }
        // serialize components
        for (MetadataComponent component : sortedComponents)
        {
            component.getType().serializer.serialize(component, version, out);
        }
    }

+++++++++++++++++++++++
    public void serialize(Map<MetadataType, MetadataComponent> components, DataOutputPlus out, Version version) throws IOException
    {
        // sort components by type
        List<MetadataComponent> sortedComponents = Lists.newArrayList(components.values());
        Collections.sort(sortedComponents);

        // write number of component
        out.writeInt(components.size());
        // build and write toc
        int lastPosition = 4 + (8 * sortedComponents.size());
        for (MetadataComponent component : sortedComponents)
        {
            MetadataType type = component.getType();
            // serialize type
            out.writeInt(type.ordinal());
            // serialize position
            out.writeInt(lastPosition);
            lastPosition += type.serializer.serializedSize(version, component);
        }
        // serialize components
        for (MetadataComponent component : sortedComponents)
        {
            component.getType().serializer.serialize(version, component, out);
        }
    }

    public Map<MetadataType, MetadataComponent> deserialize(Descriptor descriptor, FileDataInput in, EnumSet<MetadataType> types) throws IOException
    {
        Map<MetadataType, MetadataComponent> components = Maps.newHashMap();
        // read number of components
        int numComponents = in.readInt();
        // read toc
        Map<MetadataType, Integer> toc = new HashMap<>(numComponents);
        MetadataType[] values = MetadataType.values();
        for (int i = 0; i < numComponents; i++)
        {
            toc.put(values[in.readInt()], in.readInt());
        }
        for (MetadataType type : types)
        {
            MetadataComponent component = null;
            Integer offset = toc.get(type);
            if (offset != null)
            {
                in.seek(offset);
                component = type.serializer.deserialize(descriptor.version, in);
            }
            components.put(type, component);
        }
        return components;
    }

+++++++++++++++++++++++
    public Map<MetadataType, MetadataComponent> deserialize(Descriptor descriptor, FileDataInput in, EnumSet<MetadataType> types) throws IOException
    {
        Map<MetadataType, MetadataComponent> components = Maps.newHashMap();
        // read number of components
        int numComponents = in.readInt();
        // read toc
        Map<MetadataType, Integer> toc = new HashMap<>(numComponents);
        MetadataType[] values = MetadataType.values();
        for (int i = 0; i < numComponents; i++)
        {
            toc.put(values[in.readInt()], in.readInt());
        }
        for (MetadataType type : types)
        {
            Integer offset = toc.get(type);
            if (offset != null)
            {
                in.seek(offset);
                MetadataComponent component = type.serializer.deserialize(descriptor.version, in);
                components.put(type, component);
            }
        }
        return components;
    }

    public int serializedSize(int version)
    {
        int size = CompactEndpointSerializationHelper.serializedSize(from);

        size += TypeSizes.NATIVE.sizeof(verb.ordinal());
        size += TypeSizes.NATIVE.sizeof(parameters.size());
        for (Map.Entry<String, byte[]> entry : parameters.entrySet())
        {
            size += TypeSizes.NATIVE.sizeof(entry.getKey());
            size += TypeSizes.NATIVE.sizeof(entry.getValue().length);
            size += entry.getValue().length;
        }

        long longSize = payloadSize(version);
        assert longSize <= Integer.MAX_VALUE; // larger values are supported in sstables but not messages
        size += TypeSizes.NATIVE.sizeof((int) longSize);
        size += longSize;
        return size;
    }

    /**
     * Calculate the size of the payload of this message for the specified protocol version
     * and memoize the result for the specified protocol version. Memoization only covers the protocol
     * version of the first invocation.
     *
     * It is not safe to call payloadSize concurrently from multiple threads unless it has already been invoked
     * once from a single thread and there is a happens before relationship between that invocation and other
     * threads concurrently invoking payloadSize.
     *
     * For instance it would be safe to invokePayload size to make a decision in the thread that created the message
     * and then hand it off to other threads via a thread-safe queue, volatile write, or synchronized/ReentrantLock.
     * @param version Protocol version to use when calculating payload size
     * @return Size of the payload of this message in bytes
     */
+++++++++++++++++++++++
    public int serializedSize(int version)
    {
        int size = CompactEndpointSerializationHelper.serializedSize(from);

        size += TypeSizes.sizeof(verb.ordinal());
        size += TypeSizes.sizeof(parameters.size());
        for (Map.Entry<String, byte[]> entry : parameters.entrySet())
        {
            size += TypeSizes.sizeof(entry.getKey());
            size += TypeSizes.sizeof(entry.getValue().length);
            size += entry.getValue().length;
        }

        long longSize = payloadSize(version);
        assert longSize <= Integer.MAX_VALUE; // larger values are supported in sstables but not messages
        size += TypeSizes.sizeof((int) longSize);
        size += longSize;
        return size;
    }

    /**
     * Calculate the size of the payload of this message for the specified protocol version
     * and memoize the result for the specified protocol version. Memoization only covers the protocol
     * version of the first invocation.
     *
     * It is not safe to call payloadSize concurrently from multiple threads unless it has already been invoked
     * once from a single thread and there is a happens before relationship between that invocation and other
     * threads concurrently invoking payloadSize.
     *
     * For instance it would be safe to invokePayload size to make a decision in the thread that created the message
     * and then hand it off to other threads via a thread-safe queue, volatile write, or synchronized/ReentrantLock.
     * @param version Protocol version to use when calculating payload size
     * @return Size of the payload of this message in bytes
     */
    public Boolean deserialize(ByteBuffer bytes)
    {
        if (bytes == null || bytes.remaining() == 0)
            return null;

        byte value = bytes.get(bytes.position());
        return value != 0;
    }

+++++++++++++++++++++++
    public Boolean deserialize(ByteBuffer bytes)
    {
        if (bytes.remaining() == 0)
            return null;

        byte value = bytes.get(bytes.position());
        return value != 0;
    }

    public List<T> deserializeForNativeProtocol(ByteBuffer bytes, int version)
    {
        try
        {
            ByteBuffer input = bytes.duplicate();
            int n = readCollectionSize(input, version);

            if (n < 0)
                throw new MarshalException("The data cannot be deserialized as a list");

            // If the received bytes are not corresponding to a list, n might be a huge number.
            // In such a case we do not want to initialize the list with that size as it can result
            // in an OOM (see CASSANDRA-12618). On the other hand we do not want to have to resize the list
            // if we can avoid it, so we put a reasonable limit on the initialCapacity.
            List<T> l = new ArrayList<T>(Math.min(n, 256));
            for (int i = 0; i < n; i++)
            {
                // We can have nulls in lists that are used for IN values
                ByteBuffer databb = readValue(input, version);
                if (databb != null)
                {
                    elements.validate(databb);
                    l.add(elements.deserialize(databb));
                }
                else
                {
                    l.add(null);
                }
            }

            if (input.hasRemaining())
                throw new MarshalException("Unexpected extraneous bytes after list value");

            return l;
        }
        catch (BufferUnderflowException e)
        {
            throw new MarshalException("Not enough bytes to read a list");
        }
    }

    /**
     * Returns the element at the given index in a list.
     * @param serializedList a serialized list
     * @param index the index to get
     * @return the serialized element at the given index, or null if the index exceeds the list size
     */
+++++++++++++++++++++++
    public List<T> deserializeForNativeProtocol(ByteBuffer bytes, int version)
    {
        try
        {
            ByteBuffer input = bytes.duplicate();
            int n = readCollectionSize(input, version);
            List<T> l = new ArrayList<T>(n);
            for (int i = 0; i < n; i++)
            {
                // We can have nulls in lists that are used for IN values
                ByteBuffer databb = readValue(input, version);
                if (databb != null)
                {
                    elements.validate(databb);
                    l.add(elements.deserialize(databb));
                }
                else
                {
                    l.add(null);
                }
            }

            if (input.hasRemaining())
                throw new MarshalException("Unexpected extraneous bytes after list value");

            return l;
        }
        catch (BufferUnderflowException e)
        {
            throw new MarshalException("Not enough bytes to read a list");
        }
    }

    /**
     * Returns the element at the given index in a list.
     * @param serializedList a serialized list
     * @param index the index to get
     * @return the serialized element at the given index, or null if the index exceeds the list size
     */
    public List<ByteBuffer> serializeValues(Map<K, V> map)
    {
        List<ByteBuffer> buffers = new ArrayList<>(map.size() * 2);
        for (Map.Entry<K, V> entry : map.entrySet())
        {
            buffers.add(keys.serialize(entry.getKey()));
            buffers.add(values.serialize(entry.getValue()));
        }
        return buffers;
    }

+++++++++++++++++++++++
    public List<ByteBuffer> serializeValues(Map<K, V> map)
    {
        List<Pair<ByteBuffer, ByteBuffer>> pairs = new ArrayList<>(map.size());
        for (Map.Entry<K, V> entry : map.entrySet())
            pairs.add(Pair.create(keys.serialize(entry.getKey()), values.serialize(entry.getValue())));
        Collections.sort(pairs, comparator);
        List<ByteBuffer> buffers = new ArrayList<>(pairs.size() * 2);
        for (Pair<ByteBuffer, ByteBuffer> p : pairs)
        {
            buffers.add(p.left);
            buffers.add(p.right);
        }
        return buffers;
    }

    public Map<K, V> deserializeForNativeProtocol(ByteBuffer bytes, int version)
    {
        try
        {
            ByteBuffer input = bytes.duplicate();
            int n = readCollectionSize(input, version);

            if (n < 0)
                throw new MarshalException("The data cannot be deserialized as a map");

            // If the received bytes are not corresponding to a map, n might be a huge number.
            // In such a case we do not want to initialize the map with that initialCapacity as it can result
            // in an OOM when put is called (see CASSANDRA-12618). On the other hand we do not want to have to resize
            // the map if we can avoid it, so we put a reasonable limit on the initialCapacity.
            Map<K, V> m = new LinkedHashMap<K, V>(Math.min(n, 256));
            for (int i = 0; i < n; i++)
            {
                ByteBuffer kbb = readValue(input, version);
                keys.validate(kbb);

                ByteBuffer vbb = readValue(input, version);
                values.validate(vbb);

                m.put(keys.deserialize(kbb), values.deserialize(vbb));
            }
            if (input.hasRemaining())
                throw new MarshalException("Unexpected extraneous bytes after map value");
            return m;
        }
        catch (BufferUnderflowException e)
        {
            throw new MarshalException("Not enough bytes to read a map");
        }
    }

    /**
     * Given a serialized map, gets the value associated with a given key.
     * @param serializedMap a serialized map
     * @param serializedKey a serialized key
     * @param keyType the key type for the map
     * @return the value associated with the key if one exists, null otherwise
     */
+++++++++++++++++++++++
    public Map<K, V> deserializeForNativeProtocol(ByteBuffer bytes, int version)
    {
        try
        {
            ByteBuffer input = bytes.duplicate();
            int n = readCollectionSize(input, version);
            Map<K, V> m = new LinkedHashMap<K, V>(n);
            for (int i = 0; i < n; i++)
            {
                ByteBuffer kbb = readValue(input, version);
                keys.validate(kbb);

                ByteBuffer vbb = readValue(input, version);
                values.validate(vbb);

                m.put(keys.deserialize(kbb), values.deserialize(vbb));
            }
            if (input.hasRemaining())
                throw new MarshalException("Unexpected extraneous bytes after map value");
            return m;
        }
        catch (BufferUnderflowException e)
        {
            throw new MarshalException("Not enough bytes to read a map");
        }
    }

    /**
     * Given a serialized map, gets the value associated with a given key.
     * @param serializedMap a serialized map
     * @param serializedKey a serialized key
     * @param keyType the key type for the map
     * @return the value associated with the key if one exists, null otherwise
     */
    public List<ByteBuffer> serializeValues(Set<T> values)
    {
        List<ByteBuffer> buffers = new ArrayList<>(values.size());
        for (T value : values)
            buffers.add(elements.serialize(value));
        return buffers;
    }

+++++++++++++++++++++++
    public List<ByteBuffer> serializeValues(Set<T> values)
    {
        List<ByteBuffer> buffers = new ArrayList<>(values.size());
        for (T value : values)
            buffers.add(elements.serialize(value));
        Collections.sort(buffers, comparator);
        return buffers;
    }

    public Set<T> deserializeForNativeProtocol(ByteBuffer bytes, int version)
    {
        try
        {
            ByteBuffer input = bytes.duplicate();
            int n = readCollectionSize(input, version);

            if (n < 0)
                throw new MarshalException("The data cannot be deserialized as a set");

            // If the received bytes are not corresponding to a set, n might be a huge number.
            // In such a case we do not want to initialize the set with that initialCapacity as it can result
            // in an OOM when add is called (see CASSANDRA-12618). On the other hand we do not want to have to resize
            // the set if we can avoid it, so we put a reasonable limit on the initialCapacity.
            Set<T> l = new LinkedHashSet<T>(Math.min(n, 256));

            for (int i = 0; i < n; i++)
            {
                ByteBuffer databb = readValue(input, version);
                elements.validate(databb);
                l.add(elements.deserialize(databb));
            }
            if (input.hasRemaining())
                throw new MarshalException("Unexpected extraneous bytes after set value");
            return l;
        }
        catch (BufferUnderflowException e)
        {
            throw new MarshalException("Not enough bytes to read a set");
        }
    }

+++++++++++++++++++++++
    public Set<T> deserializeForNativeProtocol(ByteBuffer bytes, int version)
    {
        try
        {
            ByteBuffer input = bytes.duplicate();
            int n = readCollectionSize(input, version);
            Set<T> l = new LinkedHashSet<T>(n);
            for (int i = 0; i < n; i++)
            {
                ByteBuffer databb = readValue(input, version);
                elements.validate(databb);
                l.add(elements.deserialize(databb));
            }
            if (input.hasRemaining())
                throw new MarshalException("Unexpected extraneous bytes after set value");
            return l;
        }
        catch (BufferUnderflowException e)
        {
            throw new MarshalException("Not enough bytes to read a set");
        }
    }

    public ByteBuffer serialize()
    {
        try (DataOutputBuffer out = new DataOutputBufferFixed(serializedSize()))
        {
            ByteBufferUtil.writeWithShortLength(partitionKey, out);
            ByteBufferUtil.writeWithShortLength(cellName, out);
            out.writeInt(remaining);
            return out.buffer();
        }
        catch (IOException e)
        {
            throw new RuntimeException(e);
        }
    }

+++++++++++++++++++++++
    public ByteBuffer serialize(int protocolVersion)
    {
        assert rowMark == null || protocolVersion == rowMark.protocolVersion;
        try (DataOutputBuffer out = new DataOutputBufferFixed(serializedSize(protocolVersion)))
        {
            ByteBuffer pk = partitionKey == null ? ByteBufferUtil.EMPTY_BYTE_BUFFER : partitionKey;
            ByteBuffer mark = rowMark == null ? ByteBufferUtil.EMPTY_BYTE_BUFFER : rowMark.mark;
            if (protocolVersion <= Server.VERSION_3)
            {
                ByteBufferUtil.writeWithShortLength(pk, out);
                ByteBufferUtil.writeWithShortLength(mark, out);
                out.writeInt(remaining);
                out.writeInt(remainingInPartition);
            }
            else
            {
                ByteBufferUtil.writeWithVIntLength(pk, out);
                ByteBufferUtil.writeWithVIntLength(mark, out);
                out.writeUnsignedVInt(remaining);
                out.writeUnsignedVInt(remainingInPartition);
            }
            return out.buffer();
        }
        catch (IOException e)
        {
            throw new RuntimeException(e);
        }
    }

    private int serializedSize()
    {
        return 2 + partitionKey.remaining()
             + 2 + cellName.remaining()
             + 4;
    }

    @Override
+++++++++++++++++++++++
    public int serializedSize(int protocolVersion)
    {
        assert rowMark == null || protocolVersion == rowMark.protocolVersion;
        ByteBuffer pk = partitionKey == null ? ByteBufferUtil.EMPTY_BYTE_BUFFER : partitionKey;
        ByteBuffer mark = rowMark == null ? ByteBufferUtil.EMPTY_BYTE_BUFFER : rowMark.mark;
        if (protocolVersion <= Server.VERSION_3)
        {
            return ByteBufferUtil.serializedSizeWithShortLength(pk)
                 + ByteBufferUtil.serializedSizeWithShortLength(mark)
                 + 8; // remaining & remainingInPartition
        }
        else
        {
            return ByteBufferUtil.serializedSizeWithVIntLength(pk)
                 + ByteBufferUtil.serializedSizeWithVIntLength(mark)
                 + TypeSizes.sizeofUnsignedVInt(remaining)
                 + TypeSizes.sizeofUnsignedVInt(remainingInPartition);
        }
    }

    @Override
    public static PagingState deserialize(ByteBuffer bytes)
    {
        if (bytes == null)
            return null;

        try
        {
            DataInputStream in = new DataInputStream(ByteBufferUtil.inputStream(bytes));
            ByteBuffer pk = ByteBufferUtil.readWithShortLength(in);
            ByteBuffer cn = ByteBufferUtil.readWithShortLength(in);
            int remaining = in.readInt();
            return new PagingState(pk, cn, remaining);
        }
        catch (IOException e)
        {
            throw new ProtocolException("Invalid value for the paging state");
        }
    }

+++++++++++++++++++++++
    public static PagingState deserialize(ByteBuffer bytes, int protocolVersion)
    {
        if (bytes == null)
            return null;

        try (DataInputBuffer in = new DataInputBuffer(bytes, true))
        {
            ByteBuffer pk;
            RowMark mark;
            int remaining, remainingInPartition;
            if (protocolVersion <= Server.VERSION_3)
            {
                pk = ByteBufferUtil.readWithShortLength(in);
                mark = new RowMark(ByteBufferUtil.readWithShortLength(in), protocolVersion);
                remaining = in.readInt();
                // Note that while 'in.available()' is theoretically an estimate of how many bytes are available
                // without blocking, we know that since we're reading a ByteBuffer it will be exactly how many
                // bytes remain to be read. And the reason we want to condition this is for backward compatility
                // as we used to not set this.
                remainingInPartition = in.available() > 0 ? in.readInt() : Integer.MAX_VALUE;
            }
            else
            {
                pk = ByteBufferUtil.readWithVIntLength(in);
                mark = new RowMark(ByteBufferUtil.readWithVIntLength(in), protocolVersion);
                remaining = (int)in.readUnsignedVInt();
                remainingInPartition = (int)in.readUnsignedVInt();
            }
            return new PagingState(pk.hasRemaining() ? pk : null,
                                   mark.mark.hasRemaining() ? mark : null,
                                   remaining,
                                   remainingInPartition);
        }
        catch (IOException e)
        {
            throw new ProtocolException("Invalid value for the paging state");
        }
    }

    public synchronized void serialize(DataOutputStreamPlus out, int version, StreamSession session) throws IOException
    {
        if (completed)
        {
            return;
        }

        CompressionInfo compressionInfo = FileMessageHeader.serializer.serialize(header, out, version);

        final SSTableReader reader = ref.get();
        StreamWriter writer = compressionInfo == null ?
                                      new StreamWriter(reader, header.sections, session) :
                                      new CompressedStreamWriter(reader, header.sections,
                                                                 compressionInfo, session);
        writer.write(out);
    }

    @VisibleForTesting
+++++++++++++++++++++++
    public synchronized void serialize(DataOutputStreamPlus out, int version, StreamSession session) throws IOException
    {
        if (completed)
        {
            return;
        }

        FileMessageHeader.serializer.serialize(header, out, version);

        final SSTableReader reader = ref.get();
        StreamWriter writer = header.compressionInfo == null ?
                                      new StreamWriter(reader, header.sections, session) :
                                      new CompressedStreamWriter(reader, header.sections,
                                                                 header.compressionInfo, session);
        writer.write(out);
    }

    public long serializedSize()
    {
        return serializer.serializedSize(this, TypeSizes.NATIVE);
    }

    // Murmur is faster than an SHA-based approach and provides as-good collision
    // resistance.  The combinatorial generation approach described in
    // http://www.eecs.harvard.edu/~kirsch/pubs/bbbf/esa06.pdf
    // does prove to work in actual tests, and is obviously faster
    // than performing further iterations of murmur.

    // tests ask for ridiculous numbers of hashes so here is a special case for them
    // rather than using the threadLocal like we do in production
    @VisibleForTesting
+++++++++++++++++++++++
    public long serializedSize()
    {
        return BloomFilterSerializer.serializedSize(this);
    }

    // Murmur is faster than an SHA-based approach and provides as-good collision
    // resistance.  The combinatorial generation approach described in
    // http://www.eecs.harvard.edu/~kirsch/pubs/bbbf/esa06.pdf
    // does prove to work in actual tests, and is obviously faster
    // than performing further iterations of murmur.

    // tests ask for ridiculous numbers of hashes so here is a special case for them
    // rather than using the threadLocal like we do in production
    @VisibleForTesting
    public void serialize(BloomFilter bf, DataOutputPlus out) throws IOException
    {
        out.writeInt(bf.hashCount);
        bf.bitset.serialize(out);
    }

+++++++++++++++++++++++
    public static void serialize(BloomFilter bf, DataOutputPlus out) throws IOException
    {
        out.writeInt(bf.hashCount);
        bf.bitset.serialize(out);
    }

    public long serializedSize(BloomFilter bf, TypeSizes typeSizes)
    {
        int size = typeSizes.sizeof(bf.hashCount); // hash count
        size += bf.bitset.serializedSize(typeSizes);
        return size;
+++++++++++++++++++++++
    public static long serializedSize(BloomFilter bf)
    {
        int size = TypeSizes.sizeof(bf.hashCount); // hash count
        size += bf.bitset.serializedSize();
        return size;
    public BloomFilter deserialize(DataInput in, boolean offheap) throws IOException
    {
        int hashes = in.readInt();
        IBitSet bs = offheap ? OffHeapBitSet.deserialize(in) : OpenBitSet.deserialize(in);
        return createFilter(hashes, bs);
    }

+++++++++++++++++++++++
    public static BloomFilter deserialize(DataInput in, boolean offheap, boolean oldBfHashOrder) throws IOException
    {
        int hashes = in.readInt();
        IBitSet bs = offheap ? OffHeapBitSet.deserialize(in) : OpenBitSet.deserialize(in);

        return new BloomFilter(hashes, bs, oldBfHashOrder);
    }

    /**
     * Calculates a serialized size of the given Bloom Filter
     * @param bf Bloom filter to calculate serialized size
     * @see org.apache.cassandra.io.ISerializer#serialize(Object, org.apache.cassandra.io.util.DataOutputPlus)
     *
     * @return serialized size of the given bloom filter
     */
    public Boolean deserialize(DataInput in, int version) throws IOException
    {
        return in.readBoolean();
    }

+++++++++++++++++++++++
    public Boolean deserialize(DataInputPlus in, int version) throws IOException
    {
        return in.readBoolean();
    }

    public static void serialize(IFilter bf, DataOutputPlus output) throws IOException
    {
        BloomFilter.serializer.serialize((BloomFilter) bf, output);
    }

+++++++++++++++++++++++
    public static void serialize(IFilter bf, DataOutputPlus output) throws IOException
    {
        BloomFilterSerializer.serialize((BloomFilter) bf, output);
    }

    public static IFilter deserialize(DataInput input, boolean offheap) throws IOException
    {
        return BloomFilter.serializer.deserialize(input, offheap);
    }

    /**
     * @return A BloomFilter with the lowest practical false positive
     *         probability for the given number of elements.
     */
+++++++++++++++++++++++
    public static IFilter deserialize(DataInput input, boolean offheap, boolean oldBfHashOrder) throws IOException
    {
        return BloomFilterSerializer.deserialize(input, offheap, oldBfHashOrder);
    }

    /**
     * @return A BloomFilter with the lowest practical false positive
     *         probability for the given number of elements.
     */
    public long serializedSize(UUID uuid, int version)
    {
        return TypeSizes.NATIVE.sizeof(uuid.getMostSignificantBits()) + TypeSizes.NATIVE.sizeof(uuid.getLeastSignificantBits());
+++++++++++++++++++++++
    public long serializedSize(UUID uuid, int version)
    {
        return TypeSizes.sizeof(uuid.getMostSignificantBits()) + TypeSizes.sizeof(uuid.getLeastSignificantBits());
    public UUID deserialize(DataInput in, int version) throws IOException
    {
        return new UUID(in.readLong(), in.readLong());
    }

+++++++++++++++++++++++
    public UUID deserialize(DataInputPlus in, int version) throws IOException
    {
        return new UUID(in.readLong(), in.readLong());
    }

    public long serializedSize(TypeSizes type)
    {
        return type.sizeof((int) bytes.size()) + bytes.size();
    }

    @SuppressWarnings("resource")
+++++++++++++++++++++++
    public long serializedSize()
    {
        return TypeSizes.sizeof((int) bytes.size()) + bytes.size();
    }

    @SuppressWarnings("resource")
  public long serializedSize(TypeSizes type) {
    int bitLength = getNumWords();
    int pageSize = getPageSize();
    int pageCount = getPageCount();

    long size = type.sizeof(bitLength); // length
    for (int p = 0; p < pageCount; p++) {
      long[] bits = getPage(p);
      for (int i = 0; i < pageSize && bitLength-- > 0; i++)
        size += type.sizeof(bits[i]); // bucket
    }
    return size;
  }

+++++++++++++++++++++++
  public long serializedSize() {
    int bitLength = getNumWords();
    int pageSize = getPageSize();
    int pageCount = getPageCount();

    long size = TypeSizes.sizeof(bitLength); // length
    for (int p = 0; p < pageCount; p++) {
      long[] bits = getPage(p);
      for (int i = 0; i < pageSize && bitLength-- > 0; i++)
        size += TypeSizes.sizeof(bits[i]); // bucket
    }
    return size;
  }

    protected void testSerializeDeserialize(CFMetaData cfm, ColumnDefinition cd) throws Exception
    {
        ColumnDefinition newCd = ThriftConversion.fromThrift(cfm.ksName, cfm.cfName, cfm.comparator.asAbstractType(), null, ThriftConversion.toThrift(cd));
        Assert.assertNotSame(cd, newCd);
        Assert.assertEquals(cd.hashCode(), newCd.hashCode());
        Assert.assertEquals(cd, newCd);
+++++++++++++++++++++++
    protected void testSerializeDeserialize(CFMetaData cfm, ColumnDefinition cd) throws Exception
    {
        ColumnDefinition newCd = ThriftConversion.fromThrift(cfm.ksName, cfm.cfName, cfm.comparator.subtype(0), null, ThriftConversion.toThrift(cfm, cd));
        Assert.assertNotSame(cd, newCd);
        Assert.assertEquals(cd.hashCode(), newCd.hashCode());
        Assert.assertEquals(cd, newCd);
    private ReadCommand serializeAndDeserializeReadMessage(ReadCommand rm) throws IOException
    {
        ReadCommandSerializer rms = ReadCommand.serializer;
        DataOutputBuffer out = new DataOutputBuffer();
        ByteArrayInputStream bis;

        rms.serialize(rm, out, MessagingService.current_version);
        bis = new ByteArrayInputStream(out.getData(), 0, out.getLength());
        return rms.deserialize(new DataInputStream(bis), MessagingService.current_version);
    }

    @Test
+++++++++++++++++++++++
    private ReadCommand serializeAndDeserializeReadMessage(ReadCommand rm) throws IOException
    {
        IVersionedSerializer<ReadCommand> rms = ReadCommand.serializer;
        DataOutputBuffer out = new DataOutputBuffer();

        rms.serialize(rm, out, MessagingService.current_version);

        DataInputPlus dis = new DataInputBuffer(out.getData());
        return rms.deserialize(dis, MessagingService.current_version);
    }


    @Test
    private void populateAndReserialize(IBitSet bs) throws IOException
    {
        for (long i = 0; i < bs.capacity(); i++)
            if (random.nextBoolean())
                bs.set(i);

        DataOutputBuffer out = new DataOutputBuffer();
        bs.serialize(out);
        DataInputStream in = new DataInputStream(new ByteArrayInputStream(out.getData()));
        try (OffHeapBitSet newbs = OffHeapBitSet.deserialize(in))
        {
            compare(bs, newbs);
        }
    }

+++++++++++++++++++++++
    private static void populateAndReserialize(IBitSet bs) throws IOException
    {
        for (long i = 0; i < bs.capacity(); i++)
            if (random.nextBoolean())
                bs.set(i);

        DataOutputBuffer out = new DataOutputBuffer();
        bs.serialize(out);
        DataInputStream in = new DataInputStream(new ByteArrayInputStream(out.getData()));
        try (OffHeapBitSet newbs = OffHeapBitSet.deserialize(in))
        {
            compare(bs, newbs);
        }
    }

add: 354, delete: 325, change: 796, unhandled: 3 size_exceptions: 78 size_serialize: 57
-----------cassandra-3.0.0 vs cassandra-3.0.0-alpha1-----------
    private V deserialize(RefCountedMemory mem)
    {
        try
        {
            return serializer.deserialize(new MemoryInputStream(mem));
        }
        catch (IOException e)
        {
            logger.trace("Cannot fetch in memory data, we will fallback to read from disk ", e);
            return null;
        }
    }

+++++++++++++++++++++++
    private V deserialize(RefCountedMemory mem)
    {
        try
        {
            return serializer.deserialize(new MemoryInputStream(mem));
        }
        catch (IOException e)
        {
            logger.debug("Cannot fetch in memory data, we will fallback to read from disk ", e);
            return null;
        }
    }

    private Token deserializeToken(ByteBuffer buffer)
    {
        return partitioner.getTokenFactory().fromByteArray(buffer);
    }

+++++++++++++++++++++++
    private static Token deserializeToken(ByteBuffer buffer)
    {
        return partitioner.getTokenFactory().fromByteArray(buffer);
    }

    private static Collection<Token> deserializeTokens(Collection<String> tokensStrings)
    {
        Token.TokenFactory factory = StorageService.instance.getTokenFactory();
        List<Token> tokens = new ArrayList<>(tokensStrings.size());
        for (String tk : tokensStrings)
            tokens.add(factory.fromString(tk));
        return tokens;
    }

    /**
     * Remove stored tokens being used by another node
     */
+++++++++++++++++++++++
    private static Collection<Token> deserializeTokens(Collection<String> tokensStrings)
    {
        Token.TokenFactory factory = StorageService.getPartitioner().getTokenFactory();
        List<Token> tokens = new ArrayList<>(tokensStrings.size());
        for (String tk : tokensStrings)
            tokens.add(factory.fromString(tk));
        return tokens;
    }

    /**
     * Remove stored tokens being used by another node
     */
    protected long serializedSizeInternal(int version)
    {
        ClusteringComparator comparator = (ClusteringComparator)clusterings.comparator();
        long size = TypeSizes.sizeofUnsignedVInt(clusterings.size());
        for (Clustering clustering : clusterings)
            size += Clustering.serializer.serializedSize(clustering, version, comparator.subtypes());
        return size;
    }

+++++++++++++++++++++++
    protected long serializedSizeInternal(int version)
    {
        ClusteringComparator comparator = (ClusteringComparator)clusterings.comparator();
        long size = TypeSizes.sizeofVInt(clusterings.size());
        for (Clustering clustering : clusterings)
            size += Clustering.serializer.serializedSize(clustering, version, comparator.subtypes());
        return size;
    }

    protected void serializeInternal(DataOutputPlus out, int version) throws IOException
    {
        ClusteringComparator comparator = (ClusteringComparator)clusterings.comparator();
        out.writeUnsignedVInt(clusterings.size());
        for (Clustering clustering : clusterings)
            Clustering.serializer.serialize(clustering, out, version, comparator.subtypes());
    }

+++++++++++++++++++++++
    protected void serializeInternal(DataOutputPlus out, int version) throws IOException
    {
        ClusteringComparator comparator = (ClusteringComparator)clusterings.comparator();
        out.writeVInt(clusterings.size());
        for (Clustering clustering : clusterings)
            Clustering.serializer.serialize(clustering, out, version, comparator.subtypes());
    }

    public ByteBuffer serializeForNativeProtocol(ColumnDefinition def, Iterator<Cell> cells, int version)
    {
        assert isMultiCell();
        List<ByteBuffer> values = serializedValues(cells);
        int size = collectionSize(values);
        return CollectionSerializer.pack(values, size, version);
    }

    @Override
+++++++++++++++++++++++
    public ByteBuffer serializeForNativeProtocol(ColumnDefinition def, Iterator<Cell> cells, int version)
    {
        assert isMultiCell();
        List<ByteBuffer> values = serializedValues(cells);
        int size = enforceLimit(def, values, version);
        return CollectionSerializer.pack(values, size, version);
    }

    @Override
    public void serialize(UnfilteredRowIterator iterator, SerializationHeader header, ColumnFilter selection, DataOutputPlus out, int version, int rowEstimate) throws IOException
    {
        assert !header.isForSSTable();

        ByteBufferUtil.writeWithVIntLength(iterator.partitionKey().getKey(), out);

        int flags = 0;
        if (iterator.isReverseOrder())
            flags |= IS_REVERSED;

        if (iterator.isEmpty())
        {
            out.writeByte((byte)(flags | IS_EMPTY));
            return;
        }

        DeletionTime partitionDeletion = iterator.partitionLevelDeletion();
        if (!partitionDeletion.isLive())
            flags |= HAS_PARTITION_DELETION;
        Row staticRow = iterator.staticRow();
        boolean hasStatic = staticRow != Rows.EMPTY_STATIC_ROW;
        if (hasStatic)
            flags |= HAS_STATIC_ROW;

        if (rowEstimate >= 0)
            flags |= HAS_ROW_ESTIMATE;

        out.writeByte((byte)flags);

        SerializationHeader.serializer.serializeForMessaging(header, selection, out, hasStatic);

        if (!partitionDeletion.isLive())
            header.writeDeletionTime(partitionDeletion, out);

        if (hasStatic)
            UnfilteredSerializer.serializer.serialize(staticRow, header, out, version);

        if (rowEstimate >= 0)
            out.writeUnsignedVInt(rowEstimate);

        while (iterator.hasNext())
            UnfilteredSerializer.serializer.serialize(iterator.next(), header, out, version);
        UnfilteredSerializer.serializer.writeEndOfPartition(out);
    }

    // Please note that this consume the iterator, and as such should not be called unless we have a simple way to
    // recreate an iterator for both serialize and serializedSize, which is mostly only PartitionUpdate/ArrayBackedCachedPartition.
+++++++++++++++++++++++
    public void serialize(UnfilteredRowIterator iterator, DataOutputPlus out, SerializationHeader header, int version, int rowEstimate) throws IOException
    {
        ByteBufferUtil.writeWithVIntLength(iterator.partitionKey().getKey(), out);

        int flags = 0;
        if (iterator.isReverseOrder())
            flags |= IS_REVERSED;

        if (iterator.isEmpty())
        {
            out.writeByte((byte)(flags | IS_EMPTY));
            return;
        }

        DeletionTime partitionDeletion = iterator.partitionLevelDeletion();
        if (!partitionDeletion.isLive())
            flags |= HAS_PARTITION_DELETION;
        Row staticRow = iterator.staticRow();
        boolean hasStatic = staticRow != Rows.EMPTY_STATIC_ROW;
        if (hasStatic)
            flags |= HAS_STATIC_ROW;

        if (rowEstimate >= 0)
            flags |= HAS_ROW_ESTIMATE;

        out.writeByte((byte)flags);

        SerializationHeader.serializer.serializeForMessaging(header, out, hasStatic);

        if (!partitionDeletion.isLive())
            header.writeDeletionTime(partitionDeletion, out);

        if (hasStatic)
            UnfilteredSerializer.serializer.serialize(staticRow, header, out, version);

        if (rowEstimate >= 0)
            out.writeVInt(rowEstimate);

        while (iterator.hasNext())
            UnfilteredSerializer.serializer.serialize(iterator.next(), header, out, version);
        UnfilteredSerializer.serializer.writeEndOfPartition(out);
    }

    // Please note that this consume the iterator, and as such should not be called unless we have a simple way to
    // recreate an iterator for both serialize and serializedSize, which is mostly only PartitionUpdate/ArrayBackedCachedPartition.
    public Header deserializeHeader(CFMetaData metadata, ColumnFilter selection, DataInputPlus in, int version, SerializationHelper.Flag flag) throws IOException
    {
        DecoratedKey key = metadata.decorateKey(ByteBufferUtil.readWithVIntLength(in));
        int flags = in.readUnsignedByte();
        boolean isReversed = (flags & IS_REVERSED) != 0;
        if ((flags & IS_EMPTY) != 0)
        {
            SerializationHeader sh = new SerializationHeader(false, metadata, PartitionColumns.NONE, EncodingStats.NO_STATS);
            return new Header(sh, key, isReversed, true, null, null, 0);
        }

        boolean hasPartitionDeletion = (flags & HAS_PARTITION_DELETION) != 0;
        boolean hasStatic = (flags & HAS_STATIC_ROW) != 0;
        boolean hasRowEstimate = (flags & HAS_ROW_ESTIMATE) != 0;

        SerializationHeader header = SerializationHeader.serializer.deserializeForMessaging(in, metadata, selection, hasStatic);

        DeletionTime partitionDeletion = hasPartitionDeletion ? header.readDeletionTime(in) : DeletionTime.LIVE;

        Row staticRow = Rows.EMPTY_STATIC_ROW;
        if (hasStatic)
            staticRow = UnfilteredSerializer.serializer.deserializeStaticRow(in, header, new SerializationHelper(metadata, version, flag));

        int rowEstimate = hasRowEstimate ? (int)in.readUnsignedVInt() : -1;
        return new Header(header, key, isReversed, false, partitionDeletion, staticRow, rowEstimate);
    }

+++++++++++++++++++++++
    public Header deserializeHeader(DataInputPlus in, int version, CFMetaData metadata, SerializationHelper.Flag flag) throws IOException
    {
        DecoratedKey key = StorageService.getPartitioner().decorateKey(ByteBufferUtil.readWithVIntLength(in));
        int flags = in.readUnsignedByte();
        boolean isReversed = (flags & IS_REVERSED) != 0;
        if ((flags & IS_EMPTY) != 0)
        {
            SerializationHeader sh = new SerializationHeader(metadata, PartitionColumns.NONE, EncodingStats.NO_STATS);
            return new Header(sh, key, isReversed, true, null, null, 0);
        }

        boolean hasPartitionDeletion = (flags & HAS_PARTITION_DELETION) != 0;
        boolean hasStatic = (flags & HAS_STATIC_ROW) != 0;
        boolean hasRowEstimate = (flags & HAS_ROW_ESTIMATE) != 0;

        SerializationHeader header = SerializationHeader.serializer.deserializeForMessaging(in, metadata, hasStatic);

        DeletionTime partitionDeletion = hasPartitionDeletion ? header.readDeletionTime(in) : DeletionTime.LIVE;

        Row staticRow = Rows.EMPTY_STATIC_ROW;
        if (hasStatic)
            staticRow = UnfilteredSerializer.serializer.deserializeStaticRow(in, header, new SerializationHelper(metadata, version, flag));

        int rowEstimate = hasRowEstimate ? (int)in.readVInt() : -1;
        return new Header(header, key, isReversed, false, partitionDeletion, staticRow, rowEstimate);
    }

    public long serializedSize(UnfilteredRowIterator iterator, ColumnFilter selection, int version, int rowEstimate)
    {
        SerializationHeader header = new SerializationHeader(false,
                                                             iterator.metadata(),
                                                             iterator.columns(),
                                                             iterator.stats());

        assert rowEstimate >= 0;

        long size = ByteBufferUtil.serializedSizeWithVIntLength(iterator.partitionKey().getKey())
                  + 1; // flags

        if (iterator.isEmpty())
            return size;

        DeletionTime partitionDeletion = iterator.partitionLevelDeletion();
        Row staticRow = iterator.staticRow();
        boolean hasStatic = staticRow != Rows.EMPTY_STATIC_ROW;

        size += SerializationHeader.serializer.serializedSizeForMessaging(header, selection, hasStatic);

        if (!partitionDeletion.isLive())
            size += header.deletionTimeSerializedSize(partitionDeletion);

        if (hasStatic)
            size += UnfilteredSerializer.serializer.serializedSize(staticRow, header, version);

        if (rowEstimate >= 0)
            size += TypeSizes.sizeofUnsignedVInt(rowEstimate);

        while (iterator.hasNext())
            size += UnfilteredSerializer.serializer.serializedSize(iterator.next(), header, version);
        size += UnfilteredSerializer.serializer.serializedSizeEndOfPartition();

        return size;
    }

+++++++++++++++++++++++
    public long serializedSize(UnfilteredRowIterator iterator, int version, int rowEstimate)
    {
        SerializationHeader header = new SerializationHeader(iterator.metadata(),
                                                             iterator.columns(),
                                                             iterator.stats());

        assert rowEstimate >= 0;

        long size = ByteBufferUtil.serializedSizeWithVIntLength(iterator.partitionKey().getKey())
                  + 1; // flags

        if (iterator.isEmpty())
            return size;

        DeletionTime partitionDeletion = iterator.partitionLevelDeletion();
        Row staticRow = iterator.staticRow();
        boolean hasStatic = staticRow != Rows.EMPTY_STATIC_ROW;

        size += SerializationHeader.serializer.serializedSizeForMessaging(header, hasStatic);

        if (!partitionDeletion.isLive())
            size += header.deletionTimeSerializedSize(partitionDeletion);

        if (hasStatic)
            size += UnfilteredSerializer.serializer.serializedSize(staticRow, header, version);

        if (rowEstimate >= 0)
            size += TypeSizes.sizeofVInt(rowEstimate);

        while (iterator.hasNext())
            size += UnfilteredSerializer.serializer.serializedSize(iterator.next(), header, version);
        size += UnfilteredSerializer.serializer.serializedSizeEndOfPartition();

        return size;
    }

    public UnfilteredRowIterator deserialize(DataInputPlus in, int version, CFMetaData metadata, ColumnFilter selection, SerializationHelper.Flag flag) throws IOException
    {
        return deserialize(in, version, metadata, flag, deserializeHeader(metadata, selection, in, version, flag));
    }

+++++++++++++++++++++++
    public UnfilteredRowIterator deserialize(DataInputPlus in, int version, CFMetaData metadata, SerializationHelper.Flag flag) throws IOException
    {
        return deserialize(in, version, metadata, flag, deserializeHeader(in, version, metadata, flag));
    }

    public Row deserializeRowBody(DataInputPlus in,
                                  SerializationHeader header,
                                  SerializationHelper helper,
                                  int flags,
                                  int extendedFlags,
                                  Row.Builder builder)
    throws IOException
    {
        try
        {
            boolean isStatic = isStatic(extendedFlags);
            boolean hasTimestamp = (flags & HAS_TIMESTAMP) != 0;
            boolean hasTTL = (flags & HAS_TTL) != 0;
            boolean hasDeletion = (flags & HAS_DELETION) != 0;
            boolean deletionIsShadowable = (extendedFlags & HAS_SHADOWABLE_DELETION) != 0;
            boolean hasComplexDeletion = (flags & HAS_COMPLEX_DELETION) != 0;
            boolean hasAllColumns = (flags & HAS_ALL_COLUMNS) != 0;
            Columns headerColumns = header.columns(isStatic);

            if (header.isForSSTable())
            {
                in.readUnsignedVInt(); // Skip row size
                in.readUnsignedVInt(); // previous unfiltered size
            }

            LivenessInfo rowLiveness = LivenessInfo.EMPTY;
            if (hasTimestamp)
            {
                long timestamp = header.readTimestamp(in);
                int ttl = hasTTL ? header.readTTL(in) : LivenessInfo.NO_TTL;
                int localDeletionTime = hasTTL ? header.readLocalDeletionTime(in) : LivenessInfo.NO_EXPIRATION_TIME;
                rowLiveness = LivenessInfo.create(timestamp, ttl, localDeletionTime);
            }

            builder.addPrimaryKeyLivenessInfo(rowLiveness);
            builder.addRowDeletion(hasDeletion ? new Row.Deletion(header.readDeletionTime(in), deletionIsShadowable) : Row.Deletion.LIVE);

            Columns columns = hasAllColumns ? headerColumns : Columns.serializer.deserializeSubset(headerColumns, in);
            for (ColumnDefinition column : columns)
            {
                if (column.isSimple())
                    readSimpleColumn(column, in, header, helper, builder, rowLiveness);
                else
                    readComplexColumn(column, in, header, helper, hasComplexDeletion, builder, rowLiveness);
            }

            return builder.build();
        }
        catch (RuntimeException | AssertionError e)
        {
            // Corrupted data could be such that it triggers an assertion in the row Builder, or break one of its assumption.
            // Of course, a bug in said builder could also trigger this, but it's impossible a priori to always make the distinction
            // between a real bug and data corrupted in just the bad way. Besides, re-throwing as an IOException doesn't hide the
            // exception, it just make we catch it properly and mark the sstable as corrupted.
            throw new IOException("Error building row with data deserialized from " + in, e);
        }
    }

+++++++++++++++++++++++
    public Row deserializeRowBody(DataInputPlus in,
                                  SerializationHeader header,
                                  SerializationHelper helper,
                                  int flags,
                                  Row.Builder builder)
    throws IOException
    {
        try
        {
            boolean isStatic = isStatic(flags);
            boolean hasTimestamp = (flags & HAS_TIMESTAMP) != 0;
            boolean hasTTL = (flags & HAS_TTL) != 0;
            boolean hasDeletion = (flags & HAS_DELETION) != 0;
            boolean hasComplexDeletion = (flags & HAS_COMPLEX_DELETION) != 0;

            LivenessInfo rowLiveness = LivenessInfo.EMPTY;
            if (hasTimestamp)
            {
                long timestamp = header.readTimestamp(in);
                int ttl = hasTTL ? header.readTTL(in) : LivenessInfo.NO_TTL;
                int localDeletionTime = hasTTL ? header.readLocalDeletionTime(in) : LivenessInfo.NO_EXPIRATION_TIME;
                rowLiveness = LivenessInfo.create(timestamp, ttl, localDeletionTime);
            }

            builder.addPrimaryKeyLivenessInfo(rowLiveness);
            builder.addRowDeletion(hasDeletion ? header.readDeletionTime(in) : DeletionTime.LIVE);

            Columns columns = header.columns(isStatic);
            if (header.useSparseColumnLayout(isStatic))
            {
                int count = columns.columnCount();
                int simpleCount = columns.simpleColumnCount();
                int i;
                while ((i = (int)in.readVInt()) >= 0)
                {
                    if (i > count)
                        throw new IOException(String.format("Impossible column index %d, the header has only %d columns defined", i, count));

                    if (i < simpleCount)
                        readSimpleColumn(columns.getSimple(i), in, header, helper, builder, rowLiveness);
                    else
                        readComplexColumn(columns.getComplex(i - simpleCount), in, header, helper, hasComplexDeletion, builder, rowLiveness);
                }
            }
            else
            {
                for (int i = 0; i < columns.simpleColumnCount(); i++)
                    readSimpleColumn(columns.getSimple(i), in, header, helper, builder, rowLiveness);

                for (int i = 0; i < columns.complexColumnCount(); i++)
                    readComplexColumn(columns.getComplex(i), in, header, helper, hasComplexDeletion, builder, rowLiveness);
            }

                return builder.build();
        }
        catch (RuntimeException | AssertionError e)
        {
            // Corrupted data could be such that it triggers an assertion in the row Builder, or break one of its assumption.
            // Of course, a bug in said builder could also trigger this, but it's impossible a priori to always make the distinction
            // between a real bug and data corrupted in just the bad way. Besides, re-throwing as an IOException doesn't hide the
            // exception, it just make we catch it properly and mark the sstable as corrupted.
            throw new IOException("Error building row with data deserialized from " + in, e);
        }
    }

    private void serialize(RangeTombstoneMarker marker, SerializationHeader header, DataOutputPlus out, long previousUnfilteredSize, int version)
    throws IOException
    {
        out.writeByte((byte)IS_MARKER);
        RangeTombstone.Bound.serializer.serialize(marker.clustering(), out, version, header.clusteringTypes());

        if (header.isForSSTable())
        {
            out.writeUnsignedVInt(serializedMarkerBodySize(marker, header, previousUnfilteredSize, version));
            out.writeUnsignedVInt(previousUnfilteredSize);
        }

        if (marker.isBoundary())
        {
            RangeTombstoneBoundaryMarker bm = (RangeTombstoneBoundaryMarker)marker;
            header.writeDeletionTime(bm.endDeletionTime(), out);
            header.writeDeletionTime(bm.startDeletionTime(), out);
        }
        else
        {
            header.writeDeletionTime(((RangeTombstoneBoundMarker)marker).deletionTime(), out);
        }
    }

+++++++++++++++++++++++
    public void serialize(RangeTombstoneMarker marker, SerializationHeader header, DataOutputPlus out, int version)
    throws IOException
    {
        out.writeByte((byte)IS_MARKER);
        RangeTombstone.Bound.serializer.serialize(marker.clustering(), out, version, header.clusteringTypes());

        if (marker.isBoundary())
        {
            RangeTombstoneBoundaryMarker bm = (RangeTombstoneBoundaryMarker)marker;
            header.writeDeletionTime(bm.endDeletionTime(), out);
            header.writeDeletionTime(bm.startDeletionTime(), out);
        }
        else
        {
            header.writeDeletionTime(((RangeTombstoneBoundMarker)marker).deletionTime(), out);
        }
    }

    private long serializedSize(RangeTombstoneMarker marker, SerializationHeader header, long previousUnfilteredSize, int version)
    {
        assert !header.isForSSTable();
        return 1 // flags
             + RangeTombstone.Bound.serializer.serializedSize(marker.clustering(), version, header.clusteringTypes())
             + serializedMarkerBodySize(marker, header, previousUnfilteredSize, version);
    }

+++++++++++++++++++++++
    public long serializedSize(RangeTombstoneMarker marker, SerializationHeader header, int version)
    {
        long size = 1 // flags
                  + RangeTombstone.Bound.serializer.serializedSize(marker.clustering(), version, header.clusteringTypes());

        if (marker.isBoundary())
        {
            RangeTombstoneBoundaryMarker bm = (RangeTombstoneBoundaryMarker)marker;
            size += header.deletionTimeSerializedSize(bm.endDeletionTime());
            size += header.deletionTimeSerializedSize(bm.startDeletionTime());
        }
        else
        {
           size += header.deletionTimeSerializedSize(((RangeTombstoneBoundMarker)marker).deletionTime());
        }
        return size;
    }

    public Unfiltered deserialize(DataInputPlus in, SerializationHeader header, SerializationHelper helper, Row.Builder builder)
    throws IOException
    {
        // It wouldn't be wrong per-se to use an unsorted builder, but it would be inefficient so make sure we don't do it by mistake
        assert builder.isSorted();

        int flags = in.readUnsignedByte();
        if (isEndOfPartition(flags))
            return null;

        int extendedFlags = readExtendedFlags(in, flags);

        if (kind(flags) == Unfiltered.Kind.RANGE_TOMBSTONE_MARKER)
        {
            RangeTombstone.Bound bound = RangeTombstone.Bound.serializer.deserialize(in, helper.version, header.clusteringTypes());
            return deserializeMarkerBody(in, header, bound);
        }
        else
        {
            assert !isStatic(extendedFlags); // deserializeStaticRow should be used for that.
            builder.newRow(Clustering.serializer.deserialize(in, helper.version, header.clusteringTypes()));
            return deserializeRowBody(in, header, helper, flags, extendedFlags, builder);
        }
    }

+++++++++++++++++++++++
    public Unfiltered deserialize(DataInputPlus in, SerializationHeader header, SerializationHelper helper, Row.Builder builder)
    throws IOException
    {
        // It wouldn't be wrong per-se to use an unsorted builder, but it would be inefficient so make sure we don't do it by mistake
        assert builder.isSorted();

        int flags = in.readUnsignedByte();
        if (isEndOfPartition(flags))
            return null;

        if (kind(flags) == Unfiltered.Kind.RANGE_TOMBSTONE_MARKER)
        {
            RangeTombstone.Bound bound = RangeTombstone.Bound.serializer.deserialize(in, helper.version, header.clusteringTypes());
            return deserializeMarkerBody(in, header, bound);
        }
        else
        {
            assert !isStatic(flags); // deserializeStaticRow should be used for that.
            builder.newRow(Clustering.serializer.deserialize(in, helper.version, header.clusteringTypes()));
            return deserializeRowBody(in, header, helper, flags, builder);
        }
    }

    public Row deserializeStaticRow(DataInputPlus in, SerializationHeader header, SerializationHelper helper)
    throws IOException
    {
        int flags = in.readUnsignedByte();
        assert !isEndOfPartition(flags) && kind(flags) == Unfiltered.Kind.ROW && isExtended(flags) : flags;
        int extendedFlags = in.readUnsignedByte();
        Row.Builder builder = BTreeRow.sortedBuilder();
        builder.newRow(Clustering.STATIC_CLUSTERING);
        return deserializeRowBody(in, header, helper, flags, extendedFlags, builder);
    }

+++++++++++++++++++++++
    public Row deserializeStaticRow(DataInputPlus in, SerializationHeader header, SerializationHelper helper)
    throws IOException
    {
        int flags = in.readUnsignedByte();
        assert !isEndOfPartition(flags) && kind(flags) == Unfiltered.Kind.ROW && isStatic(flags) : flags;
        Row.Builder builder = BTreeBackedRow.sortedBuilder(helper.fetchedStaticColumns(header));
        builder.newRow(Clustering.STATIC_CLUSTERING);
        return deserializeRowBody(in, header, helper, flags, builder);
    }

    public RangeTombstoneMarker deserializeMarkerBody(DataInputPlus in, SerializationHeader header, RangeTombstone.Bound bound)
    throws IOException
    {
        if (header.isForSSTable())
        {
            in.readUnsignedVInt(); // marker size
            in.readUnsignedVInt(); // previous unfiltered size
        }

        if (bound.isBoundary())
            return new RangeTombstoneBoundaryMarker(bound, header.readDeletionTime(in), header.readDeletionTime(in));
        else
            return new RangeTombstoneBoundMarker(bound, header.readDeletionTime(in));
    }

+++++++++++++++++++++++
    public RangeTombstoneMarker deserializeMarkerBody(DataInputPlus in, SerializationHeader header, RangeTombstone.Bound bound)
    throws IOException
    {
        if (bound.isBoundary())
            return new RangeTombstoneBoundaryMarker(bound, header.readDeletionTime(in), header.readDeletionTime(in));
        else
            return new RangeTombstoneBoundMarker(bound, header.readDeletionTime(in));
    }

    public void serialize(Map<MetadataType, MetadataComponent> components, DataOutputPlus out, Version version) throws IOException
    {
        ValidationMetadata validation = (ValidationMetadata) components.get(MetadataType.VALIDATION);
        StatsMetadata stats = (StatsMetadata) components.get(MetadataType.STATS);
        CompactionMetadata compaction = (CompactionMetadata) components.get(MetadataType.COMPACTION);

        assert validation != null && stats != null && compaction != null && validation.partitioner != null;

        EstimatedHistogram.serializer.serialize(stats.estimatedPartitionSize, out);
        EstimatedHistogram.serializer.serialize(stats.estimatedColumnCount, out);
        ReplayPosition.serializer.serialize(stats.replayPosition, out);
        out.writeLong(stats.minTimestamp);
        out.writeLong(stats.maxTimestamp);
        out.writeInt(stats.maxLocalDeletionTime);
        out.writeDouble(validation.bloomFilterFPChance);
        out.writeDouble(stats.compressionRatio);
        out.writeUTF(validation.partitioner);
        out.writeInt(0); // compaction ancestors
        StreamingHistogram.serializer.serialize(stats.estimatedTombstoneDropTime, out);
        out.writeInt(stats.sstableLevel);
        out.writeInt(stats.minClusteringValues.size());
        for (ByteBuffer value : stats.minClusteringValues)
            ByteBufferUtil.writeWithShortLength(value, out);
        out.writeInt(stats.maxClusteringValues.size());
        for (ByteBuffer value : stats.maxClusteringValues)
            ByteBufferUtil.writeWithShortLength(value, out);
    }

    /**
     * Legacy serializer deserialize all components no matter what types are specified.
     */
    @Override
+++++++++++++++++++++++
    public void serialize(Map<MetadataType, MetadataComponent> components, DataOutputPlus out) throws IOException
    {
        ValidationMetadata validation = (ValidationMetadata) components.get(MetadataType.VALIDATION);
        StatsMetadata stats = (StatsMetadata) components.get(MetadataType.STATS);
        CompactionMetadata compaction = (CompactionMetadata) components.get(MetadataType.COMPACTION);

        assert validation != null && stats != null && compaction != null && validation.partitioner != null;

        EstimatedHistogram.serializer.serialize(stats.estimatedPartitionSize, out);
        EstimatedHistogram.serializer.serialize(stats.estimatedColumnCount, out);
        ReplayPosition.serializer.serialize(stats.replayPosition, out);
        out.writeLong(stats.minTimestamp);
        out.writeLong(stats.maxTimestamp);
        out.writeInt(stats.maxLocalDeletionTime);
        out.writeDouble(validation.bloomFilterFPChance);
        out.writeDouble(stats.compressionRatio);
        out.writeUTF(validation.partitioner);
        out.writeInt(compaction.ancestors.size());
        for (Integer g : compaction.ancestors)
            out.writeInt(g);
        StreamingHistogram.serializer.serialize(stats.estimatedTombstoneDropTime, out);
        out.writeInt(stats.sstableLevel);
        out.writeInt(stats.minClusteringValues.size());
        for (ByteBuffer value : stats.minClusteringValues)
            ByteBufferUtil.writeWithShortLength(value, out);
        out.writeInt(stats.maxClusteringValues.size());
        for (ByteBuffer value : stats.maxClusteringValues)
            ByteBufferUtil.writeWithShortLength(value, out);
    }

    /**
     * Legacy serializer deserialize all components no matter what types are specified.
     */
    @Override
    public Map<MetadataType, MetadataComponent> deserialize(Descriptor descriptor, EnumSet<MetadataType> types) throws IOException
    {
        Map<MetadataType, MetadataComponent> components = Maps.newHashMap();

        File statsFile = new File(descriptor.filenameFor(Component.STATS));
        if (!statsFile.exists() && types.contains(MetadataType.STATS))
        {
            components.put(MetadataType.STATS, MetadataCollector.defaultStatsMetadata());
        }
        else
        {
            try (DataInputStreamPlus in = new DataInputStreamPlus(new BufferedInputStream(new FileInputStream(statsFile))))
            {
                EstimatedHistogram partitionSizes = EstimatedHistogram.serializer.deserialize(in);
                EstimatedHistogram columnCounts = EstimatedHistogram.serializer.deserialize(in);
                ReplayPosition replayPosition = ReplayPosition.serializer.deserialize(in);
                long minTimestamp = in.readLong();
                long maxTimestamp = in.readLong();
                int maxLocalDeletionTime = in.readInt();
                double bloomFilterFPChance = in.readDouble();
                double compressionRatio = in.readDouble();
                String partitioner = in.readUTF();
                int nbAncestors = in.readInt(); //skip compaction ancestors
                in.skipBytes(nbAncestors * TypeSizes.sizeof(nbAncestors));
                StreamingHistogram tombstoneHistogram = StreamingHistogram.serializer.deserialize(in);
                int sstableLevel = 0;
                if (in.available() > 0)
                    sstableLevel = in.readInt();

                int colCount = in.readInt();
                List<ByteBuffer> minColumnNames = new ArrayList<>(colCount);
                for (int i = 0; i < colCount; i++)
                    minColumnNames.add(ByteBufferUtil.readWithShortLength(in));

                colCount = in.readInt();
                List<ByteBuffer> maxColumnNames = new ArrayList<>(colCount);
                for (int i = 0; i < colCount; i++)
                    maxColumnNames.add(ByteBufferUtil.readWithShortLength(in));

                if (types.contains(MetadataType.VALIDATION))
                    components.put(MetadataType.VALIDATION,
                                   new ValidationMetadata(partitioner, bloomFilterFPChance));
                if (types.contains(MetadataType.STATS))
                    components.put(MetadataType.STATS,
                                   new StatsMetadata(partitionSizes,
                                                     columnCounts,
                                                     replayPosition,
                                                     minTimestamp,
                                                     maxTimestamp,
                                                     Integer.MAX_VALUE,
                                                     maxLocalDeletionTime,
                                                     0,
                                                     Integer.MAX_VALUE,
                                                     compressionRatio,
                                                     tombstoneHistogram,
                                                     sstableLevel,
                                                     minColumnNames,
                                                     maxColumnNames,
                                                     true,
                                                     ActiveRepairService.UNREPAIRED_SSTABLE,
                                                     -1,
                                                     -1));
                if (types.contains(MetadataType.COMPACTION))
                    components.put(MetadataType.COMPACTION,
                                   new CompactionMetadata(null));
            }
        }
        return components;
+++++++++++++++++++++++
    public Map<MetadataType, MetadataComponent> deserialize(Descriptor descriptor, EnumSet<MetadataType> types) throws IOException
    {
        Map<MetadataType, MetadataComponent> components = Maps.newHashMap();

        File statsFile = new File(descriptor.filenameFor(Component.STATS));
        if (!statsFile.exists() && types.contains(MetadataType.STATS))
        {
            components.put(MetadataType.STATS, MetadataCollector.defaultStatsMetadata());
        }
        else
        {
            try (DataInputStreamPlus in = new DataInputStreamPlus(new BufferedInputStream(new FileInputStream(statsFile))))
            {
                EstimatedHistogram partitionSizes = EstimatedHistogram.serializer.deserialize(in);
                EstimatedHistogram columnCounts = EstimatedHistogram.serializer.deserialize(in);
                ReplayPosition replayPosition = ReplayPosition.serializer.deserialize(in);
                long minTimestamp = in.readLong();
                long maxTimestamp = in.readLong();
                int maxLocalDeletionTime = in.readInt();
                double bloomFilterFPChance = in.readDouble();
                double compressionRatio = in.readDouble();
                String partitioner = in.readUTF();
                int nbAncestors = in.readInt();
                Set<Integer> ancestors = new HashSet<>(nbAncestors);
                for (int i = 0; i < nbAncestors; i++)
                    ancestors.add(in.readInt());
                StreamingHistogram tombstoneHistogram = StreamingHistogram.serializer.deserialize(in);
                int sstableLevel = 0;
                if (in.available() > 0)
                    sstableLevel = in.readInt();

                int colCount = in.readInt();
                List<ByteBuffer> minColumnNames = new ArrayList<>(colCount);
                for (int i = 0; i < colCount; i++)
                    minColumnNames.add(ByteBufferUtil.readWithShortLength(in));

                colCount = in.readInt();
                List<ByteBuffer> maxColumnNames = new ArrayList<>(colCount);
                for (int i = 0; i < colCount; i++)
                    maxColumnNames.add(ByteBufferUtil.readWithShortLength(in));

                if (types.contains(MetadataType.VALIDATION))
                    components.put(MetadataType.VALIDATION,
                                   new ValidationMetadata(partitioner, bloomFilterFPChance));
                if (types.contains(MetadataType.STATS))
                    components.put(MetadataType.STATS,
                                   new StatsMetadata(partitionSizes,
                                                     columnCounts,
                                                     replayPosition,
                                                     minTimestamp,
                                                     maxTimestamp,
                                                     Integer.MAX_VALUE,
                                                     maxLocalDeletionTime,
                                                     0,
                                                     Integer.MAX_VALUE,
                                                     compressionRatio,
                                                     tombstoneHistogram,
                                                     sstableLevel,
                                                     minColumnNames,
                                                     maxColumnNames,
                                                     true,
                                                     ActiveRepairService.UNREPAIRED_SSTABLE,
                                                     -1,
                                                     -1));
                if (types.contains(MetadataType.COMPACTION))
                    components.put(MetadataType.COMPACTION,
                                   new CompactionMetadata(ancestors, null));
            }
        }
        return components;
    public void serialize(Map<MetadataType, MetadataComponent> components, DataOutputPlus out, Version version) throws IOException
    {
        // sort components by type
        List<MetadataComponent> sortedComponents = Lists.newArrayList(components.values());
        Collections.sort(sortedComponents);

        // write number of component
        out.writeInt(components.size());
        // build and write toc
        int lastPosition = 4 + (8 * sortedComponents.size());
        for (MetadataComponent component : sortedComponents)
        {
            MetadataType type = component.getType();
            // serialize type
            out.writeInt(type.ordinal());
            // serialize position
            out.writeInt(lastPosition);
            lastPosition += type.serializer.serializedSize(version, component);
        }
        // serialize components
        for (MetadataComponent component : sortedComponents)
        {
            component.getType().serializer.serialize(version, component, out);
        }
    }

+++++++++++++++++++++++
    public void serialize(Map<MetadataType, MetadataComponent> components, DataOutputPlus out) throws IOException
    {
        // sort components by type
        List<MetadataComponent> sortedComponents = Lists.newArrayList(components.values());
        Collections.sort(sortedComponents);

        // write number of component
        out.writeInt(components.size());
        // build and write toc
        int lastPosition = 4 + (8 * sortedComponents.size());
        for (MetadataComponent component : sortedComponents)
        {
            MetadataType type = component.getType();
            // serialize type
            out.writeInt(type.ordinal());
            // serialize position
            out.writeInt(lastPosition);
            lastPosition += type.serializer.serializedSize(component);
        }
        // serialize components
        for (MetadataComponent component : sortedComponents)
        {
            component.getType().serializer.serialize(component, out);
        }
    }

    public Map<MetadataType, MetadataComponent> deserialize(Descriptor descriptor, FileDataInput in, EnumSet<MetadataType> types) throws IOException
    {
        Map<MetadataType, MetadataComponent> components = Maps.newHashMap();
        // read number of components
        int numComponents = in.readInt();
        // read toc
        Map<MetadataType, Integer> toc = new HashMap<>(numComponents);
        MetadataType[] values = MetadataType.values();
        for (int i = 0; i < numComponents; i++)
        {
            toc.put(values[in.readInt()], in.readInt());
        }
        for (MetadataType type : types)
        {
            Integer offset = toc.get(type);
            if (offset != null)
            {
                in.seek(offset);
                MetadataComponent component = type.serializer.deserialize(descriptor.version, in);
                components.put(type, component);
            }
        }
        return components;
    }

+++++++++++++++++++++++
    public Map<MetadataType, MetadataComponent> deserialize(Descriptor descriptor, FileDataInput in, EnumSet<MetadataType> types) throws IOException
    {
        Map<MetadataType, MetadataComponent> components = Maps.newHashMap();
        // read number of components
        int numComponents = in.readInt();
        // read toc
        Map<MetadataType, Integer> toc = new HashMap<>(numComponents);
        MetadataType[] values = MetadataType.values();
        for (int i = 0; i < numComponents; i++)
        {
            toc.put(values[in.readInt()], in.readInt());
        }
        for (MetadataType type : types)
        {
            MetadataComponent component = null;
            Integer offset = toc.get(type);
            if (offset != null)
            {
                in.seek(offset);
                component = type.serializer.deserialize(descriptor.version, in);
            }
            components.put(type, component);
        }
        return components;
    }

    public List<ByteBuffer> serializeValues(Map<K, V> map)
    {
        List<Pair<ByteBuffer, ByteBuffer>> pairs = new ArrayList<>(map.size());
        for (Map.Entry<K, V> entry : map.entrySet())
            pairs.add(Pair.create(keys.serialize(entry.getKey()), values.serialize(entry.getValue())));
        Collections.sort(pairs, comparator);
        List<ByteBuffer> buffers = new ArrayList<>(pairs.size() * 2);
        for (Pair<ByteBuffer, ByteBuffer> p : pairs)
        {
            buffers.add(p.left);
            buffers.add(p.right);
        }
        return buffers;
    }

+++++++++++++++++++++++
    public List<ByteBuffer> serializeValues(Map<K, V> map)
    {
        List<ByteBuffer> buffers = new ArrayList<>(map.size() * 2);
        for (Map.Entry<K, V> entry : map.entrySet())
        {
            buffers.add(keys.serialize(entry.getKey()));
            buffers.add(values.serialize(entry.getValue()));
        }
        return buffers;
    }

    public List<ByteBuffer> serializeValues(Set<T> values)
    {
        List<ByteBuffer> buffers = new ArrayList<>(values.size());
        for (T value : values)
            buffers.add(elements.serialize(value));
        Collections.sort(buffers, comparator);
        return buffers;
    }

+++++++++++++++++++++++
    public List<ByteBuffer> serializeValues(Set<T> values)
    {
        List<ByteBuffer> buffers = new ArrayList<>(values.size());
        for (T value : values)
            buffers.add(elements.serialize(value));
        return buffers;
    }

    public ByteBuffer serialize(int protocolVersion)
    {
        assert rowMark == null || protocolVersion == rowMark.protocolVersion;
        try (DataOutputBuffer out = new DataOutputBufferFixed(serializedSize(protocolVersion)))
        {
            ByteBuffer pk = partitionKey == null ? ByteBufferUtil.EMPTY_BYTE_BUFFER : partitionKey;
            ByteBuffer mark = rowMark == null ? ByteBufferUtil.EMPTY_BYTE_BUFFER : rowMark.mark;
            if (protocolVersion <= Server.VERSION_3)
            {
                ByteBufferUtil.writeWithShortLength(pk, out);
                ByteBufferUtil.writeWithShortLength(mark, out);
                out.writeInt(remaining);
                out.writeInt(remainingInPartition);
            }
            else
            {
                ByteBufferUtil.writeWithVIntLength(pk, out);
                ByteBufferUtil.writeWithVIntLength(mark, out);
                out.writeUnsignedVInt(remaining);
                out.writeUnsignedVInt(remainingInPartition);
            }
            return out.buffer();
        }
        catch (IOException e)
        {
            throw new RuntimeException(e);
        }
    }

+++++++++++++++++++++++
    public ByteBuffer serialize()
    {
        try (DataOutputBuffer out = new DataOutputBufferFixed(serializedSize()))
        {
            ByteBufferUtil.writeWithShortLength(partitionKey, out);
            ByteBufferUtil.writeWithShortLength(cellName, out);
            out.writeInt(remaining);
            out.writeInt(remainingInPartition);
            return out.buffer();
        }
        catch (IOException e)
        {
            throw new RuntimeException(e);
        }
    }

    public int serializedSize(int protocolVersion)
    {
        assert rowMark == null || protocolVersion == rowMark.protocolVersion;
        ByteBuffer pk = partitionKey == null ? ByteBufferUtil.EMPTY_BYTE_BUFFER : partitionKey;
        ByteBuffer mark = rowMark == null ? ByteBufferUtil.EMPTY_BYTE_BUFFER : rowMark.mark;
        if (protocolVersion <= Server.VERSION_3)
        {
            return ByteBufferUtil.serializedSizeWithShortLength(pk)
                 + ByteBufferUtil.serializedSizeWithShortLength(mark)
                 + 8; // remaining & remainingInPartition
        }
        else
        {
            return ByteBufferUtil.serializedSizeWithVIntLength(pk)
                 + ByteBufferUtil.serializedSizeWithVIntLength(mark)
                 + TypeSizes.sizeofUnsignedVInt(remaining)
                 + TypeSizes.sizeofUnsignedVInt(remainingInPartition);
        }
    }

    @Override
+++++++++++++++++++++++
    private int serializedSize()
    {
        return 2 + partitionKey.remaining()
             + 2 + cellName.remaining()
             + 8; // remaining & remainingInPartition
    }

    @Override
    public static PagingState deserialize(ByteBuffer bytes, int protocolVersion)
    {
        if (bytes == null)
            return null;

        try (DataInputBuffer in = new DataInputBuffer(bytes, true))
        {
            ByteBuffer pk;
            RowMark mark;
            int remaining, remainingInPartition;
            if (protocolVersion <= Server.VERSION_3)
            {
                pk = ByteBufferUtil.readWithShortLength(in);
                mark = new RowMark(ByteBufferUtil.readWithShortLength(in), protocolVersion);
                remaining = in.readInt();
                // Note that while 'in.available()' is theoretically an estimate of how many bytes are available
                // without blocking, we know that since we're reading a ByteBuffer it will be exactly how many
                // bytes remain to be read. And the reason we want to condition this is for backward compatility
                // as we used to not set this.
                remainingInPartition = in.available() > 0 ? in.readInt() : Integer.MAX_VALUE;
            }
            else
            {
                pk = ByteBufferUtil.readWithVIntLength(in);
                mark = new RowMark(ByteBufferUtil.readWithVIntLength(in), protocolVersion);
                remaining = (int)in.readUnsignedVInt();
                remainingInPartition = (int)in.readUnsignedVInt();
            }
            return new PagingState(pk.hasRemaining() ? pk : null,
                                   mark.mark.hasRemaining() ? mark : null,
                                   remaining,
                                   remainingInPartition);
        }
        catch (IOException e)
        {
            throw new ProtocolException("Invalid value for the paging state");
        }
    }

+++++++++++++++++++++++
    public static PagingState deserialize(ByteBuffer bytes)
    {
        if (bytes == null)
            return null;

        try
        {
            DataInputStream in = new DataInputStream(ByteBufferUtil.inputStream(bytes));
            ByteBuffer pk = ByteBufferUtil.readWithShortLength(in);
            ByteBuffer cn = ByteBufferUtil.readWithShortLength(in);
            int remaining = in.readInt();
            // Note that while 'in.available()' is theoretically an estimate of how many bytes are available
            // without blocking, we know that since we're reading a ByteBuffer it will be exactly how many
            // bytes remain to be read. And the reason we want to condition this is for backward compatility
            // as we used to not set this.
            int remainingInPartition = in.available() > 0 ? in.readInt() : Integer.MAX_VALUE;
            return new PagingState(pk, cn, remaining, remainingInPartition);
        }
        catch (IOException e)
        {
            throw new ProtocolException("Invalid value for the paging state");
        }
    }

    public static StreamMessage deserialize(ReadableByteChannel in, int version, StreamSession session) throws IOException
    {
        ByteBuffer buff = ByteBuffer.allocate(1);
        int readBytes = in.read(buff);
        if (readBytes > 0)
        {
            buff.flip();
            Type type = Type.get(buff.get());
            return type.inSerializer.deserialize(in, version, session);
        }
        else if (readBytes == 0)
        {
            // input socket buffer was not filled yet
            return null;
        }
        else
        {
            // possibly socket gets closed
            throw new SocketException("End-of-stream reached");
        }
    }

    /** StreamMessage serializer */
+++++++++++++++++++++++
    public static StreamMessage deserialize(ReadableByteChannel in, int version, StreamSession session) throws IOException
    {
        ByteBuffer buff = ByteBuffer.allocate(1);
        if (in.read(buff) > 0)
        {
            buff.flip();
            Type type = Type.get(buff.get());
            return type.inSerializer.deserialize(in, version, session);
        }
        else
        {
            // when socket gets closed, there is a chance that buff is empty
            // in that case, just return null
            return null;
        }
    }

    /** StreamMessage serializer */
    public static int serializedSizeWithVIntLength(ByteBuffer buffer)
    {
        int size = buffer.remaining();
        return TypeSizes.sizeofUnsignedVInt(size) + size;
    }

+++++++++++++++++++++++
    public static int serializedSizeWithVIntLength(ByteBuffer buffer)
    {
        int size = buffer.remaining();
        return TypeSizes.sizeofVInt(size) + size;
    }

    protected void testSerializeDeserialize(CFMetaData cfm, ColumnDefinition cd) throws Exception
    {
        ColumnDefinition newCd = ThriftConversion.fromThrift(cfm.ksName, cfm.cfName, cfm.comparator.subtype(0), null, ThriftConversion.toThrift(cfm, cd));
        Assert.assertNotSame(cd, newCd);
        Assert.assertEquals(cd.hashCode(), newCd.hashCode());
        Assert.assertEquals(cd, newCd);
+++++++++++++++++++++++
    protected void testSerializeDeserialize(CFMetaData cfm, ColumnDefinition cd) throws Exception
    {
        ColumnDefinition newCd = ThriftConversion.fromThrift(cfm.ksName, cfm.cfName, cfm.comparator.subtype(0), null, ThriftConversion.toThrift(cd));
        Assert.assertNotSame(cd, newCd);
        Assert.assertEquals(cd.hashCode(), newCd.hashCode());
        Assert.assertEquals(cd, newCd);
add: 82, delete: 201, change: 647, unhandled: 1 size_exceptions: 27 size_serialize: 28
-----------cassandra-3.0.0-alpha1 vs cassandra-3.0.0-beta1-----------
    private static Token deserializeToken(ByteBuffer buffer)
    {
        return partitioner.getTokenFactory().fromByteArray(buffer);
    }

+++++++++++++++++++++++
    private Token deserializeToken(ByteBuffer buffer)
    {
        return partitioner.getTokenFactory().fromByteArray(buffer);
    }

    private static ByteBuffer serializeMutations(Collection<Mutation> mutations, int version)
    {
        try (DataOutputBuffer buf = new DataOutputBuffer())
        {
            buf.writeInt(mutations.size());
            for (Mutation mutation : mutations)
                Mutation.serializer.serialize(mutation, buf, version);
            return buf.buffer();
        }
        catch (IOException e)
        {
            throw new AssertionError(); // cannot happen.
        }
    }

+++++++++++++++++++++++
    static ByteBuffer serializeMutations(Collection<Mutation> mutations, int version)
    {
        try (DataOutputBuffer buf = new DataOutputBuffer())
        {
            buf.writeInt(mutations.size());
            for (Mutation mutation : mutations)
                Mutation.serializer.serialize(mutation, buf, version);
            return buf.buffer();
        }
        catch (IOException e)
        {
            throw new AssertionError(); // cannot happen.
        }
    }

    private static Collection<Token> deserializeTokens(Collection<String> tokensStrings)
    {
        Token.TokenFactory factory = StorageService.getPartitioner().getTokenFactory();
        List<Token> tokens = new ArrayList<>(tokensStrings.size());
        for (String tk : tokensStrings)
            tokens.add(factory.fromString(tk));
        return tokens;
    }

    /**
     * Remove stored tokens being used by another node
     */
+++++++++++++++++++++++
    private static Collection<Token> deserializeTokens(Collection<String> tokensStrings)
    {
        Token.TokenFactory factory = StorageService.instance.getTokenFactory();
        List<Token> tokens = new ArrayList<>(tokensStrings.size());
        for (String tk : tokensStrings)
            tokens.add(factory.fromString(tk));
        return tokens;
    }

    /**
     * Remove stored tokens being used by another node
     */
    public void serialize(UnfilteredRowIterator iterator, DataOutputPlus out, SerializationHeader header, int version, int rowEstimate) throws IOException
    {
        ByteBufferUtil.writeWithVIntLength(iterator.partitionKey().getKey(), out);

        int flags = 0;
        if (iterator.isReverseOrder())
            flags |= IS_REVERSED;

        if (iterator.isEmpty())
        {
            out.writeByte((byte)(flags | IS_EMPTY));
            return;
        }

        DeletionTime partitionDeletion = iterator.partitionLevelDeletion();
        if (!partitionDeletion.isLive())
            flags |= HAS_PARTITION_DELETION;
        Row staticRow = iterator.staticRow();
        boolean hasStatic = staticRow != Rows.EMPTY_STATIC_ROW;
        if (hasStatic)
            flags |= HAS_STATIC_ROW;

        if (rowEstimate >= 0)
            flags |= HAS_ROW_ESTIMATE;

        out.writeByte((byte)flags);

        SerializationHeader.serializer.serializeForMessaging(header, out, hasStatic);

        if (!partitionDeletion.isLive())
            header.writeDeletionTime(partitionDeletion, out);

        if (hasStatic)
            UnfilteredSerializer.serializer.serialize(staticRow, header, out, version);

        if (rowEstimate >= 0)
            out.writeVInt(rowEstimate);

        while (iterator.hasNext())
            UnfilteredSerializer.serializer.serialize(iterator.next(), header, out, version);
        UnfilteredSerializer.serializer.writeEndOfPartition(out);
    }

    // Please note that this consume the iterator, and as such should not be called unless we have a simple way to
    // recreate an iterator for both serialize and serializedSize, which is mostly only PartitionUpdate/ArrayBackedCachedPartition.
+++++++++++++++++++++++
    public void serialize(UnfilteredRowIterator iterator, SerializationHeader header, ColumnFilter selection, DataOutputPlus out, int version, int rowEstimate) throws IOException
    {
        ByteBufferUtil.writeWithVIntLength(iterator.partitionKey().getKey(), out);

        int flags = 0;
        if (iterator.isReverseOrder())
            flags |= IS_REVERSED;

        if (iterator.isEmpty())
        {
            out.writeByte((byte)(flags | IS_EMPTY));
            return;
        }

        DeletionTime partitionDeletion = iterator.partitionLevelDeletion();
        if (!partitionDeletion.isLive())
            flags |= HAS_PARTITION_DELETION;
        Row staticRow = iterator.staticRow();
        boolean hasStatic = staticRow != Rows.EMPTY_STATIC_ROW;
        if (hasStatic)
            flags |= HAS_STATIC_ROW;

        if (rowEstimate >= 0)
            flags |= HAS_ROW_ESTIMATE;

        out.writeByte((byte)flags);

        SerializationHeader.serializer.serializeForMessaging(header, selection, out, hasStatic);

        if (!partitionDeletion.isLive())
            header.writeDeletionTime(partitionDeletion, out);

        if (hasStatic)
            UnfilteredSerializer.serializer.serialize(staticRow, header, out, version);

        if (rowEstimate >= 0)
            out.writeVInt(rowEstimate);

        while (iterator.hasNext())
            UnfilteredSerializer.serializer.serialize(iterator.next(), header, out, version);
        UnfilteredSerializer.serializer.writeEndOfPartition(out);
    }

    // Please note that this consume the iterator, and as such should not be called unless we have a simple way to
    // recreate an iterator for both serialize and serializedSize, which is mostly only PartitionUpdate/ArrayBackedCachedPartition.
    public Header deserializeHeader(DataInputPlus in, int version, CFMetaData metadata, SerializationHelper.Flag flag) throws IOException
    {
        DecoratedKey key = StorageService.getPartitioner().decorateKey(ByteBufferUtil.readWithVIntLength(in));
        int flags = in.readUnsignedByte();
        boolean isReversed = (flags & IS_REVERSED) != 0;
        if ((flags & IS_EMPTY) != 0)
        {
            SerializationHeader sh = new SerializationHeader(metadata, PartitionColumns.NONE, EncodingStats.NO_STATS);
            return new Header(sh, key, isReversed, true, null, null, 0);
        }

        boolean hasPartitionDeletion = (flags & HAS_PARTITION_DELETION) != 0;
        boolean hasStatic = (flags & HAS_STATIC_ROW) != 0;
        boolean hasRowEstimate = (flags & HAS_ROW_ESTIMATE) != 0;

        SerializationHeader header = SerializationHeader.serializer.deserializeForMessaging(in, metadata, hasStatic);

        DeletionTime partitionDeletion = hasPartitionDeletion ? header.readDeletionTime(in) : DeletionTime.LIVE;

        Row staticRow = Rows.EMPTY_STATIC_ROW;
        if (hasStatic)
            staticRow = UnfilteredSerializer.serializer.deserializeStaticRow(in, header, new SerializationHelper(metadata, version, flag));

        int rowEstimate = hasRowEstimate ? (int)in.readVInt() : -1;
        return new Header(header, key, isReversed, false, partitionDeletion, staticRow, rowEstimate);
    }

+++++++++++++++++++++++
    public Header deserializeHeader(CFMetaData metadata, ColumnFilter selection, DataInputPlus in, int version, SerializationHelper.Flag flag) throws IOException
    {
        DecoratedKey key = metadata.decorateKey(ByteBufferUtil.readWithVIntLength(in));
        int flags = in.readUnsignedByte();
        boolean isReversed = (flags & IS_REVERSED) != 0;
        if ((flags & IS_EMPTY) != 0)
        {
            SerializationHeader sh = new SerializationHeader(metadata, PartitionColumns.NONE, EncodingStats.NO_STATS);
            return new Header(sh, key, isReversed, true, null, null, 0);
        }

        boolean hasPartitionDeletion = (flags & HAS_PARTITION_DELETION) != 0;
        boolean hasStatic = (flags & HAS_STATIC_ROW) != 0;
        boolean hasRowEstimate = (flags & HAS_ROW_ESTIMATE) != 0;

        SerializationHeader header = SerializationHeader.serializer.deserializeForMessaging(in, metadata, selection, hasStatic);

        DeletionTime partitionDeletion = hasPartitionDeletion ? header.readDeletionTime(in) : DeletionTime.LIVE;

        Row staticRow = Rows.EMPTY_STATIC_ROW;
        if (hasStatic)
            staticRow = UnfilteredSerializer.serializer.deserializeStaticRow(in, header, new SerializationHelper(metadata, version, flag));

        int rowEstimate = hasRowEstimate ? (int)in.readVInt() : -1;
        return new Header(header, key, isReversed, false, partitionDeletion, staticRow, rowEstimate);
    }

    public long serializedSize(UnfilteredRowIterator iterator, int version, int rowEstimate)
    {
        SerializationHeader header = new SerializationHeader(iterator.metadata(),
                                                             iterator.columns(),
                                                             iterator.stats());

        assert rowEstimate >= 0;

        long size = ByteBufferUtil.serializedSizeWithVIntLength(iterator.partitionKey().getKey())
                  + 1; // flags

        if (iterator.isEmpty())
            return size;

        DeletionTime partitionDeletion = iterator.partitionLevelDeletion();
        Row staticRow = iterator.staticRow();
        boolean hasStatic = staticRow != Rows.EMPTY_STATIC_ROW;

        size += SerializationHeader.serializer.serializedSizeForMessaging(header, hasStatic);

        if (!partitionDeletion.isLive())
            size += header.deletionTimeSerializedSize(partitionDeletion);

        if (hasStatic)
            size += UnfilteredSerializer.serializer.serializedSize(staticRow, header, version);

        if (rowEstimate >= 0)
            size += TypeSizes.sizeofVInt(rowEstimate);

        while (iterator.hasNext())
            size += UnfilteredSerializer.serializer.serializedSize(iterator.next(), header, version);
        size += UnfilteredSerializer.serializer.serializedSizeEndOfPartition();

        return size;
    }

+++++++++++++++++++++++
    public long serializedSize(UnfilteredRowIterator iterator, ColumnFilter selection, int version, int rowEstimate)
    {
        SerializationHeader header = new SerializationHeader(iterator.metadata(),
                                                             iterator.columns(),
                                                             iterator.stats());

        assert rowEstimate >= 0;

        long size = ByteBufferUtil.serializedSizeWithVIntLength(iterator.partitionKey().getKey())
                  + 1; // flags

        if (iterator.isEmpty())
            return size;

        DeletionTime partitionDeletion = iterator.partitionLevelDeletion();
        Row staticRow = iterator.staticRow();
        boolean hasStatic = staticRow != Rows.EMPTY_STATIC_ROW;

        size += SerializationHeader.serializer.serializedSizeForMessaging(header, selection, hasStatic);

        if (!partitionDeletion.isLive())
            size += header.deletionTimeSerializedSize(partitionDeletion);

        if (hasStatic)
            size += UnfilteredSerializer.serializer.serializedSize(staticRow, header, version);

        if (rowEstimate >= 0)
            size += TypeSizes.sizeofVInt(rowEstimate);

        while (iterator.hasNext())
            size += UnfilteredSerializer.serializer.serializedSize(iterator.next(), header, version);
        size += UnfilteredSerializer.serializer.serializedSizeEndOfPartition();

        return size;
    }

    public UnfilteredRowIterator deserialize(DataInputPlus in, int version, CFMetaData metadata, SerializationHelper.Flag flag) throws IOException
    {
        return deserialize(in, version, metadata, flag, deserializeHeader(in, version, metadata, flag));
    }

+++++++++++++++++++++++
    public UnfilteredRowIterator deserialize(DataInputPlus in, int version, CFMetaData metadata, ColumnFilter selection, SerializationHelper.Flag flag) throws IOException
    {
        return deserialize(in, version, metadata, flag, deserializeHeader(metadata, selection, in, version, flag));
    }

    public Row deserializeStaticRow(DataInputPlus in, SerializationHeader header, SerializationHelper helper)
    throws IOException
    {
        int flags = in.readUnsignedByte();
        assert !isEndOfPartition(flags) && kind(flags) == Unfiltered.Kind.ROW && isStatic(flags) : flags;
        Row.Builder builder = BTreeBackedRow.sortedBuilder(helper.fetchedStaticColumns(header));
        builder.newRow(Clustering.STATIC_CLUSTERING);
        return deserializeRowBody(in, header, helper, flags, builder);
    }

+++++++++++++++++++++++
    public Row deserializeStaticRow(DataInputPlus in, SerializationHeader header, SerializationHelper helper)
    throws IOException
    {
        int flags = in.readUnsignedByte();
        assert !isEndOfPartition(flags) && kind(flags) == Unfiltered.Kind.ROW && isStatic(flags) : flags;
        Row.Builder builder = BTreeRow.sortedBuilder(helper.fetchedStaticColumns(header));
        builder.newRow(Clustering.STATIC_CLUSTERING);
        return deserializeRowBody(in, header, helper, flags, builder);
    }

    public void serialize(Map<MetadataType, MetadataComponent> components, DataOutputPlus out) throws IOException
    {
        ValidationMetadata validation = (ValidationMetadata) components.get(MetadataType.VALIDATION);
        StatsMetadata stats = (StatsMetadata) components.get(MetadataType.STATS);
        CompactionMetadata compaction = (CompactionMetadata) components.get(MetadataType.COMPACTION);

        assert validation != null && stats != null && compaction != null && validation.partitioner != null;

        EstimatedHistogram.serializer.serialize(stats.estimatedPartitionSize, out);
        EstimatedHistogram.serializer.serialize(stats.estimatedColumnCount, out);
        ReplayPosition.serializer.serialize(stats.replayPosition, out);
        out.writeLong(stats.minTimestamp);
        out.writeLong(stats.maxTimestamp);
        out.writeInt(stats.maxLocalDeletionTime);
        out.writeDouble(validation.bloomFilterFPChance);
        out.writeDouble(stats.compressionRatio);
        out.writeUTF(validation.partitioner);
        out.writeInt(compaction.ancestors.size());
        for (Integer g : compaction.ancestors)
            out.writeInt(g);
        StreamingHistogram.serializer.serialize(stats.estimatedTombstoneDropTime, out);
        out.writeInt(stats.sstableLevel);
        out.writeInt(stats.minClusteringValues.size());
        for (ByteBuffer value : stats.minClusteringValues)
            ByteBufferUtil.writeWithShortLength(value, out);
        out.writeInt(stats.maxClusteringValues.size());
        for (ByteBuffer value : stats.maxClusteringValues)
            ByteBufferUtil.writeWithShortLength(value, out);
    }

    /**
     * Legacy serializer deserialize all components no matter what types are specified.
     */
    @Override
+++++++++++++++++++++++
    public void serialize(Map<MetadataType, MetadataComponent> components, DataOutputPlus out) throws IOException
    {
        ValidationMetadata validation = (ValidationMetadata) components.get(MetadataType.VALIDATION);
        StatsMetadata stats = (StatsMetadata) components.get(MetadataType.STATS);
        CompactionMetadata compaction = (CompactionMetadata) components.get(MetadataType.COMPACTION);

        assert validation != null && stats != null && compaction != null && validation.partitioner != null;

        EstimatedHistogram.serializer.serialize(stats.estimatedPartitionSize, out);
        EstimatedHistogram.serializer.serialize(stats.estimatedColumnCount, out);
        ReplayPosition.serializer.serialize(stats.replayPosition, out);
        out.writeLong(stats.minTimestamp);
        out.writeLong(stats.maxTimestamp);
        out.writeInt(stats.maxLocalDeletionTime);
        out.writeDouble(validation.bloomFilterFPChance);
        out.writeDouble(stats.compressionRatio);
        out.writeUTF(validation.partitioner);
        out.writeInt(0); // compaction ancestors
        StreamingHistogram.serializer.serialize(stats.estimatedTombstoneDropTime, out);
        out.writeInt(stats.sstableLevel);
        out.writeInt(stats.minClusteringValues.size());
        for (ByteBuffer value : stats.minClusteringValues)
            ByteBufferUtil.writeWithShortLength(value, out);
        out.writeInt(stats.maxClusteringValues.size());
        for (ByteBuffer value : stats.maxClusteringValues)
            ByteBufferUtil.writeWithShortLength(value, out);
    }

    /**
     * Legacy serializer deserialize all components no matter what types are specified.
     */
    @Override
    public Map<MetadataType, MetadataComponent> deserialize(Descriptor descriptor, EnumSet<MetadataType> types) throws IOException
    {
        Map<MetadataType, MetadataComponent> components = Maps.newHashMap();

        File statsFile = new File(descriptor.filenameFor(Component.STATS));
        if (!statsFile.exists() && types.contains(MetadataType.STATS))
        {
            components.put(MetadataType.STATS, MetadataCollector.defaultStatsMetadata());
        }
        else
        {
            try (DataInputStreamPlus in = new DataInputStreamPlus(new BufferedInputStream(new FileInputStream(statsFile))))
            {
                EstimatedHistogram partitionSizes = EstimatedHistogram.serializer.deserialize(in);
                EstimatedHistogram columnCounts = EstimatedHistogram.serializer.deserialize(in);
                ReplayPosition replayPosition = ReplayPosition.serializer.deserialize(in);
                long minTimestamp = in.readLong();
                long maxTimestamp = in.readLong();
                int maxLocalDeletionTime = in.readInt();
                double bloomFilterFPChance = in.readDouble();
                double compressionRatio = in.readDouble();
                String partitioner = in.readUTF();
                int nbAncestors = in.readInt();
                Set<Integer> ancestors = new HashSet<>(nbAncestors);
                for (int i = 0; i < nbAncestors; i++)
                    ancestors.add(in.readInt());
                StreamingHistogram tombstoneHistogram = StreamingHistogram.serializer.deserialize(in);
                int sstableLevel = 0;
                if (in.available() > 0)
                    sstableLevel = in.readInt();

                int colCount = in.readInt();
                List<ByteBuffer> minColumnNames = new ArrayList<>(colCount);
                for (int i = 0; i < colCount; i++)
                    minColumnNames.add(ByteBufferUtil.readWithShortLength(in));

                colCount = in.readInt();
                List<ByteBuffer> maxColumnNames = new ArrayList<>(colCount);
                for (int i = 0; i < colCount; i++)
                    maxColumnNames.add(ByteBufferUtil.readWithShortLength(in));

                if (types.contains(MetadataType.VALIDATION))
                    components.put(MetadataType.VALIDATION,
                                   new ValidationMetadata(partitioner, bloomFilterFPChance));
                if (types.contains(MetadataType.STATS))
                    components.put(MetadataType.STATS,
                                   new StatsMetadata(partitionSizes,
                                                     columnCounts,
                                                     replayPosition,
                                                     minTimestamp,
                                                     maxTimestamp,
                                                     Integer.MAX_VALUE,
                                                     maxLocalDeletionTime,
                                                     0,
                                                     Integer.MAX_VALUE,
                                                     compressionRatio,
                                                     tombstoneHistogram,
                                                     sstableLevel,
                                                     minColumnNames,
                                                     maxColumnNames,
                                                     true,
                                                     ActiveRepairService.UNREPAIRED_SSTABLE,
                                                     -1,
                                                     -1));
                if (types.contains(MetadataType.COMPACTION))
                    components.put(MetadataType.COMPACTION,
                                   new CompactionMetadata(ancestors, null));
            }
        }
        return components;
+++++++++++++++++++++++
    public Map<MetadataType, MetadataComponent> deserialize(Descriptor descriptor, EnumSet<MetadataType> types) throws IOException
    {
        Map<MetadataType, MetadataComponent> components = Maps.newHashMap();

        File statsFile = new File(descriptor.filenameFor(Component.STATS));
        if (!statsFile.exists() && types.contains(MetadataType.STATS))
        {
            components.put(MetadataType.STATS, MetadataCollector.defaultStatsMetadata());
        }
        else
        {
            try (DataInputStreamPlus in = new DataInputStreamPlus(new BufferedInputStream(new FileInputStream(statsFile))))
            {
                EstimatedHistogram partitionSizes = EstimatedHistogram.serializer.deserialize(in);
                EstimatedHistogram columnCounts = EstimatedHistogram.serializer.deserialize(in);
                ReplayPosition replayPosition = ReplayPosition.serializer.deserialize(in);
                long minTimestamp = in.readLong();
                long maxTimestamp = in.readLong();
                int maxLocalDeletionTime = in.readInt();
                double bloomFilterFPChance = in.readDouble();
                double compressionRatio = in.readDouble();
                String partitioner = in.readUTF();
                int nbAncestors = in.readInt(); //skip compaction ancestors
                in.skipBytes(nbAncestors * TypeSizes.sizeof(nbAncestors));
                StreamingHistogram tombstoneHistogram = StreamingHistogram.serializer.deserialize(in);
                int sstableLevel = 0;
                if (in.available() > 0)
                    sstableLevel = in.readInt();

                int colCount = in.readInt();
                List<ByteBuffer> minColumnNames = new ArrayList<>(colCount);
                for (int i = 0; i < colCount; i++)
                    minColumnNames.add(ByteBufferUtil.readWithShortLength(in));

                colCount = in.readInt();
                List<ByteBuffer> maxColumnNames = new ArrayList<>(colCount);
                for (int i = 0; i < colCount; i++)
                    maxColumnNames.add(ByteBufferUtil.readWithShortLength(in));

                if (types.contains(MetadataType.VALIDATION))
                    components.put(MetadataType.VALIDATION,
                                   new ValidationMetadata(partitioner, bloomFilterFPChance));
                if (types.contains(MetadataType.STATS))
                    components.put(MetadataType.STATS,
                                   new StatsMetadata(partitionSizes,
                                                     columnCounts,
                                                     replayPosition,
                                                     minTimestamp,
                                                     maxTimestamp,
                                                     Integer.MAX_VALUE,
                                                     maxLocalDeletionTime,
                                                     0,
                                                     Integer.MAX_VALUE,
                                                     compressionRatio,
                                                     tombstoneHistogram,
                                                     sstableLevel,
                                                     minColumnNames,
                                                     maxColumnNames,
                                                     true,
                                                     ActiveRepairService.UNREPAIRED_SSTABLE,
                                                     -1,
                                                     -1));
                if (types.contains(MetadataType.COMPACTION))
                    components.put(MetadataType.COMPACTION,
                                   new CompactionMetadata(null));
            }
        }
        return components;
    protected void testSerializeDeserialize(CFMetaData cfm, ColumnDefinition cd) throws Exception
    {
        ColumnDefinition newCd = ThriftConversion.fromThrift(cfm.ksName, cfm.cfName, cfm.comparator.subtype(0), null, ThriftConversion.toThrift(cd));
        Assert.assertNotSame(cd, newCd);
        Assert.assertEquals(cd.hashCode(), newCd.hashCode());
        Assert.assertEquals(cd, newCd);
+++++++++++++++++++++++
    protected void testSerializeDeserialize(CFMetaData cfm, ColumnDefinition cd) throws Exception
    {
        ColumnDefinition newCd = ThriftConversion.fromThrift(cfm.ksName, cfm.cfName, cfm.comparator.subtype(0), null, ThriftConversion.toThrift(cfm, cd));
        Assert.assertNotSame(cd, newCd);
        Assert.assertEquals(cd.hashCode(), newCd.hashCode());
        Assert.assertEquals(cd, newCd);
add: 108, delete: 36, change: 403, unhandled: 1 size_exceptions: 6 size_serialize: 11
-----------cassandra-3.0.0-beta1 vs cassandra-3.0.0-beta2-----------
    public Row deserializeRowBody(DataInputPlus in,
                                  SerializationHeader header,
                                  SerializationHelper helper,
                                  int flags,
                                  Row.Builder builder)
    throws IOException
    {
        try
        {
            boolean isStatic = isStatic(flags);
            boolean hasTimestamp = (flags & HAS_TIMESTAMP) != 0;
            boolean hasTTL = (flags & HAS_TTL) != 0;
            boolean hasDeletion = (flags & HAS_DELETION) != 0;
            boolean hasComplexDeletion = (flags & HAS_COMPLEX_DELETION) != 0;

            LivenessInfo rowLiveness = LivenessInfo.EMPTY;
            if (hasTimestamp)
            {
                long timestamp = header.readTimestamp(in);
                int ttl = hasTTL ? header.readTTL(in) : LivenessInfo.NO_TTL;
                int localDeletionTime = hasTTL ? header.readLocalDeletionTime(in) : LivenessInfo.NO_EXPIRATION_TIME;
                rowLiveness = LivenessInfo.create(timestamp, ttl, localDeletionTime);
            }

            builder.addPrimaryKeyLivenessInfo(rowLiveness);
            builder.addRowDeletion(hasDeletion ? header.readDeletionTime(in) : DeletionTime.LIVE);

            Columns columns = header.columns(isStatic);
            if (header.useSparseColumnLayout(isStatic))
            {
                int count = columns.columnCount();
                int simpleCount = columns.simpleColumnCount();
                int i;
                while ((i = (int)in.readVInt()) >= 0)
                {
                    if (i > count)
                        throw new IOException(String.format("Impossible column index %d, the header has only %d columns defined", i, count));

                    if (i < simpleCount)
                        readSimpleColumn(columns.getSimple(i), in, header, helper, builder, rowLiveness);
                    else
                        readComplexColumn(columns.getComplex(i - simpleCount), in, header, helper, hasComplexDeletion, builder, rowLiveness);
                }
            }
            else
            {
                for (int i = 0; i < columns.simpleColumnCount(); i++)
                    readSimpleColumn(columns.getSimple(i), in, header, helper, builder, rowLiveness);

                for (int i = 0; i < columns.complexColumnCount(); i++)
                    readComplexColumn(columns.getComplex(i), in, header, helper, hasComplexDeletion, builder, rowLiveness);
            }

                return builder.build();
        }
        catch (RuntimeException | AssertionError e)
        {
            // Corrupted data could be such that it triggers an assertion in the row Builder, or break one of its assumption.
            // Of course, a bug in said builder could also trigger this, but it's impossible a priori to always make the distinction
            // between a real bug and data corrupted in just the bad way. Besides, re-throwing as an IOException doesn't hide the
            // exception, it just make we catch it properly and mark the sstable as corrupted.
            throw new IOException("Error building row with data deserialized from " + in, e);
        }
    }

+++++++++++++++++++++++
    public Row deserializeRowBody(DataInputPlus in,
                                  SerializationHeader header,
                                  SerializationHelper helper,
                                  int flags,
                                  Row.Builder builder)
    throws IOException
    {
        try
        {
            boolean isStatic = isStatic(flags);
            boolean hasTimestamp = (flags & HAS_TIMESTAMP) != 0;
            boolean hasTTL = (flags & HAS_TTL) != 0;
            boolean hasDeletion = (flags & HAS_DELETION) != 0;
            boolean hasComplexDeletion = (flags & HAS_COMPLEX_DELETION) != 0;
            boolean hasAllColumns = (flags & HAS_ALL_COLUMNS) != 0;
            Columns headerColumns = header.columns(isStatic);

            LivenessInfo rowLiveness = LivenessInfo.EMPTY;
            if (hasTimestamp)
            {
                long timestamp = header.readTimestamp(in);
                int ttl = hasTTL ? header.readTTL(in) : LivenessInfo.NO_TTL;
                int localDeletionTime = hasTTL ? header.readLocalDeletionTime(in) : LivenessInfo.NO_EXPIRATION_TIME;
                rowLiveness = LivenessInfo.create(timestamp, ttl, localDeletionTime);
            }

            builder.addPrimaryKeyLivenessInfo(rowLiveness);
            builder.addRowDeletion(hasDeletion ? header.readDeletionTime(in) : DeletionTime.LIVE);

            Columns columns = hasAllColumns ? headerColumns : Columns.serializer.deserializeSubset(headerColumns, in);
            for (ColumnDefinition column : columns)
            {
                if (column.isSimple())
                    readSimpleColumn(column, in, header, helper, builder, rowLiveness);
                else
                    readComplexColumn(column, in, header, helper, hasComplexDeletion, builder, rowLiveness);
            }

            return builder.build();
        }
        catch (RuntimeException | AssertionError e)
        {
            // Corrupted data could be such that it triggers an assertion in the row Builder, or break one of its assumption.
            // Of course, a bug in said builder could also trigger this, but it's impossible a priori to always make the distinction
            // between a real bug and data corrupted in just the bad way. Besides, re-throwing as an IOException doesn't hide the
            // exception, it just make we catch it properly and mark the sstable as corrupted.
            throw new IOException("Error building row with data deserialized from " + in, e);
        }
    }

    public Row deserializeStaticRow(DataInputPlus in, SerializationHeader header, SerializationHelper helper)
    throws IOException
    {
        int flags = in.readUnsignedByte();
        assert !isEndOfPartition(flags) && kind(flags) == Unfiltered.Kind.ROW && isStatic(flags) : flags;
        Row.Builder builder = BTreeRow.sortedBuilder(helper.fetchedStaticColumns(header));
        builder.newRow(Clustering.STATIC_CLUSTERING);
        return deserializeRowBody(in, header, helper, flags, builder);
    }

+++++++++++++++++++++++
    public Row deserializeStaticRow(DataInputPlus in, SerializationHeader header, SerializationHelper helper)
    throws IOException
    {
        int flags = in.readUnsignedByte();
        assert !isEndOfPartition(flags) && kind(flags) == Unfiltered.Kind.ROW && isStatic(flags) : flags;
        Row.Builder builder = BTreeRow.sortedBuilder();
        builder.newRow(Clustering.STATIC_CLUSTERING);
        return deserializeRowBody(in, header, helper, flags, builder);
    }

    public List<ByteBuffer> serializeValues(Map<K, V> map)
    {
        List<ByteBuffer> buffers = new ArrayList<>(map.size() * 2);
        for (Map.Entry<K, V> entry : map.entrySet())
        {
            buffers.add(keys.serialize(entry.getKey()));
            buffers.add(values.serialize(entry.getValue()));
        }
        return buffers;
    }

+++++++++++++++++++++++
    public List<ByteBuffer> serializeValues(Map<K, V> map)
    {
        List<Pair<ByteBuffer, ByteBuffer>> pairs = new ArrayList<>(map.size());
        for (Map.Entry<K, V> entry : map.entrySet())
            pairs.add(Pair.create(keys.serialize(entry.getKey()), values.serialize(entry.getValue())));
        Collections.sort(pairs, comparator);
        List<ByteBuffer> buffers = new ArrayList<>(pairs.size() * 2);
        for (Pair<ByteBuffer, ByteBuffer> p : pairs)
        {
            buffers.add(p.left);
            buffers.add(p.right);
        }
        return buffers;
    }

    public List<ByteBuffer> serializeValues(Set<T> values)
    {
        List<ByteBuffer> buffers = new ArrayList<>(values.size());
        for (T value : values)
            buffers.add(elements.serialize(value));
        return buffers;
    }

+++++++++++++++++++++++
    public List<ByteBuffer> serializeValues(Set<T> values)
    {
        List<ByteBuffer> buffers = new ArrayList<>(values.size());
        for (T value : values)
            buffers.add(elements.serialize(value));
        Collections.sort(buffers, comparator);
        return buffers;
    }

    public static StreamMessage deserialize(ReadableByteChannel in, int version, StreamSession session) throws IOException
    {
        ByteBuffer buff = ByteBuffer.allocate(1);
        if (in.read(buff) > 0)
        {
            buff.flip();
            Type type = Type.get(buff.get());
            return type.inSerializer.deserialize(in, version, session);
        }
        else
        {
            // when socket gets closed, there is a chance that buff is empty
            // in that case, just return null
            return null;
        }
    }

    /** StreamMessage serializer */
+++++++++++++++++++++++
    public static StreamMessage deserialize(ReadableByteChannel in, int version, StreamSession session) throws IOException
    {
        ByteBuffer buff = ByteBuffer.allocate(1);
        int readBytes = in.read(buff);
        if (readBytes > 0)
        {
            buff.flip();
            Type type = Type.get(buff.get());
            return type.inSerializer.deserialize(in, version, session);
        }
        else if (readBytes == 0)
        {
            // input socket buffer was not filled yet
            return null;
        }
        else
        {
            // possibly socket gets closed
            throw new SocketException("End-of-stream reached");
        }
    }

    /** StreamMessage serializer */
add: 33, delete: 10, change: 232, unhandled: 1 size_exceptions: 20 size_serialize: 5
-----------cassandra-3.0.0-beta2 vs cassandra-3.0.0-rc1-----------
    protected long serializedSizeInternal(int version)
    {
        ClusteringComparator comparator = (ClusteringComparator)clusterings.comparator();
        long size = TypeSizes.sizeofVInt(clusterings.size());
        for (Clustering clustering : clusterings)
            size += Clustering.serializer.serializedSize(clustering, version, comparator.subtypes());
        return size;
    }

+++++++++++++++++++++++
    protected long serializedSizeInternal(int version)
    {
        ClusteringComparator comparator = (ClusteringComparator)clusterings.comparator();
        long size = TypeSizes.sizeofUnsignedVInt(clusterings.size());
        for (Clustering clustering : clusterings)
            size += Clustering.serializer.serializedSize(clustering, version, comparator.subtypes());
        return size;
    }

    protected void serializeInternal(DataOutputPlus out, int version) throws IOException
    {
        ClusteringComparator comparator = (ClusteringComparator)clusterings.comparator();
        out.writeVInt(clusterings.size());
        for (Clustering clustering : clusterings)
            Clustering.serializer.serialize(clustering, out, version, comparator.subtypes());
    }

+++++++++++++++++++++++
    protected void serializeInternal(DataOutputPlus out, int version) throws IOException
    {
        ClusteringComparator comparator = (ClusteringComparator)clusterings.comparator();
        out.writeUnsignedVInt(clusterings.size());
        for (Clustering clustering : clusterings)
            Clustering.serializer.serialize(clustering, out, version, comparator.subtypes());
    }

    public ByteBuffer serializeForNativeProtocol(ColumnDefinition def, Iterator<Cell> cells, int version)
    {
        assert isMultiCell();
        List<ByteBuffer> values = serializedValues(cells);
        int size = enforceLimit(def, values, version);
        return CollectionSerializer.pack(values, size, version);
    }

    @Override
+++++++++++++++++++++++
    public ByteBuffer serializeForNativeProtocol(ColumnDefinition def, Iterator<Cell> cells, int version)
    {
        assert isMultiCell();
        List<ByteBuffer> values = serializedValues(cells);
        int size = collectionSize(values);
        return CollectionSerializer.pack(values, size, version);
    }

    @Override
    public void serialize(UnfilteredRowIterator iterator, SerializationHeader header, ColumnFilter selection, DataOutputPlus out, int version, int rowEstimate) throws IOException
    {
        ByteBufferUtil.writeWithVIntLength(iterator.partitionKey().getKey(), out);

        int flags = 0;
        if (iterator.isReverseOrder())
            flags |= IS_REVERSED;

        if (iterator.isEmpty())
        {
            out.writeByte((byte)(flags | IS_EMPTY));
            return;
        }

        DeletionTime partitionDeletion = iterator.partitionLevelDeletion();
        if (!partitionDeletion.isLive())
            flags |= HAS_PARTITION_DELETION;
        Row staticRow = iterator.staticRow();
        boolean hasStatic = staticRow != Rows.EMPTY_STATIC_ROW;
        if (hasStatic)
            flags |= HAS_STATIC_ROW;

        if (rowEstimate >= 0)
            flags |= HAS_ROW_ESTIMATE;

        out.writeByte((byte)flags);

        SerializationHeader.serializer.serializeForMessaging(header, selection, out, hasStatic);

        if (!partitionDeletion.isLive())
            header.writeDeletionTime(partitionDeletion, out);

        if (hasStatic)
            UnfilteredSerializer.serializer.serialize(staticRow, header, out, version);

        if (rowEstimate >= 0)
            out.writeVInt(rowEstimate);

        while (iterator.hasNext())
            UnfilteredSerializer.serializer.serialize(iterator.next(), header, out, version);
        UnfilteredSerializer.serializer.writeEndOfPartition(out);
    }

    // Please note that this consume the iterator, and as such should not be called unless we have a simple way to
    // recreate an iterator for both serialize and serializedSize, which is mostly only PartitionUpdate/ArrayBackedCachedPartition.
+++++++++++++++++++++++
    public void serialize(UnfilteredRowIterator iterator, SerializationHeader header, ColumnFilter selection, DataOutputPlus out, int version, int rowEstimate) throws IOException
    {
        ByteBufferUtil.writeWithVIntLength(iterator.partitionKey().getKey(), out);

        int flags = 0;
        if (iterator.isReverseOrder())
            flags |= IS_REVERSED;

        if (iterator.isEmpty())
        {
            out.writeByte((byte)(flags | IS_EMPTY));
            return;
        }

        DeletionTime partitionDeletion = iterator.partitionLevelDeletion();
        if (!partitionDeletion.isLive())
            flags |= HAS_PARTITION_DELETION;
        Row staticRow = iterator.staticRow();
        boolean hasStatic = staticRow != Rows.EMPTY_STATIC_ROW;
        if (hasStatic)
            flags |= HAS_STATIC_ROW;

        if (rowEstimate >= 0)
            flags |= HAS_ROW_ESTIMATE;

        out.writeByte((byte)flags);

        SerializationHeader.serializer.serializeForMessaging(header, selection, out, hasStatic);

        if (!partitionDeletion.isLive())
            header.writeDeletionTime(partitionDeletion, out);

        if (hasStatic)
            UnfilteredSerializer.serializer.serialize(staticRow, header, out, version);

        if (rowEstimate >= 0)
            out.writeUnsignedVInt(rowEstimate);

        while (iterator.hasNext())
            UnfilteredSerializer.serializer.serialize(iterator.next(), header, out, version);
        UnfilteredSerializer.serializer.writeEndOfPartition(out);
    }

    // Please note that this consume the iterator, and as such should not be called unless we have a simple way to
    // recreate an iterator for both serialize and serializedSize, which is mostly only PartitionUpdate/ArrayBackedCachedPartition.
    public Header deserializeHeader(CFMetaData metadata, ColumnFilter selection, DataInputPlus in, int version, SerializationHelper.Flag flag) throws IOException
    {
        DecoratedKey key = metadata.decorateKey(ByteBufferUtil.readWithVIntLength(in));
        int flags = in.readUnsignedByte();
        boolean isReversed = (flags & IS_REVERSED) != 0;
        if ((flags & IS_EMPTY) != 0)
        {
            SerializationHeader sh = new SerializationHeader(metadata, PartitionColumns.NONE, EncodingStats.NO_STATS);
            return new Header(sh, key, isReversed, true, null, null, 0);
        }

        boolean hasPartitionDeletion = (flags & HAS_PARTITION_DELETION) != 0;
        boolean hasStatic = (flags & HAS_STATIC_ROW) != 0;
        boolean hasRowEstimate = (flags & HAS_ROW_ESTIMATE) != 0;

        SerializationHeader header = SerializationHeader.serializer.deserializeForMessaging(in, metadata, selection, hasStatic);

        DeletionTime partitionDeletion = hasPartitionDeletion ? header.readDeletionTime(in) : DeletionTime.LIVE;

        Row staticRow = Rows.EMPTY_STATIC_ROW;
        if (hasStatic)
            staticRow = UnfilteredSerializer.serializer.deserializeStaticRow(in, header, new SerializationHelper(metadata, version, flag));

        int rowEstimate = hasRowEstimate ? (int)in.readVInt() : -1;
        return new Header(header, key, isReversed, false, partitionDeletion, staticRow, rowEstimate);
    }

+++++++++++++++++++++++
    public Header deserializeHeader(CFMetaData metadata, ColumnFilter selection, DataInputPlus in, int version, SerializationHelper.Flag flag) throws IOException
    {
        DecoratedKey key = metadata.decorateKey(ByteBufferUtil.readWithVIntLength(in));
        int flags = in.readUnsignedByte();
        boolean isReversed = (flags & IS_REVERSED) != 0;
        if ((flags & IS_EMPTY) != 0)
        {
            SerializationHeader sh = new SerializationHeader(metadata, PartitionColumns.NONE, EncodingStats.NO_STATS);
            return new Header(sh, key, isReversed, true, null, null, 0);
        }

        boolean hasPartitionDeletion = (flags & HAS_PARTITION_DELETION) != 0;
        boolean hasStatic = (flags & HAS_STATIC_ROW) != 0;
        boolean hasRowEstimate = (flags & HAS_ROW_ESTIMATE) != 0;

        SerializationHeader header = SerializationHeader.serializer.deserializeForMessaging(in, metadata, selection, hasStatic);

        DeletionTime partitionDeletion = hasPartitionDeletion ? header.readDeletionTime(in) : DeletionTime.LIVE;

        Row staticRow = Rows.EMPTY_STATIC_ROW;
        if (hasStatic)
            staticRow = UnfilteredSerializer.serializer.deserializeStaticRow(in, header, new SerializationHelper(metadata, version, flag));

        int rowEstimate = hasRowEstimate ? (int)in.readUnsignedVInt() : -1;
        return new Header(header, key, isReversed, false, partitionDeletion, staticRow, rowEstimate);
    }

    public long serializedSize(UnfilteredRowIterator iterator, ColumnFilter selection, int version, int rowEstimate)
    {
        SerializationHeader header = new SerializationHeader(iterator.metadata(),
                                                             iterator.columns(),
                                                             iterator.stats());

        assert rowEstimate >= 0;

        long size = ByteBufferUtil.serializedSizeWithVIntLength(iterator.partitionKey().getKey())
                  + 1; // flags

        if (iterator.isEmpty())
            return size;

        DeletionTime partitionDeletion = iterator.partitionLevelDeletion();
        Row staticRow = iterator.staticRow();
        boolean hasStatic = staticRow != Rows.EMPTY_STATIC_ROW;

        size += SerializationHeader.serializer.serializedSizeForMessaging(header, selection, hasStatic);

        if (!partitionDeletion.isLive())
            size += header.deletionTimeSerializedSize(partitionDeletion);

        if (hasStatic)
            size += UnfilteredSerializer.serializer.serializedSize(staticRow, header, version);

        if (rowEstimate >= 0)
            size += TypeSizes.sizeofVInt(rowEstimate);

        while (iterator.hasNext())
            size += UnfilteredSerializer.serializer.serializedSize(iterator.next(), header, version);
        size += UnfilteredSerializer.serializer.serializedSizeEndOfPartition();

        return size;
    }

+++++++++++++++++++++++
    public long serializedSize(UnfilteredRowIterator iterator, ColumnFilter selection, int version, int rowEstimate)
    {
        SerializationHeader header = new SerializationHeader(iterator.metadata(),
                                                             iterator.columns(),
                                                             iterator.stats());

        assert rowEstimate >= 0;

        long size = ByteBufferUtil.serializedSizeWithVIntLength(iterator.partitionKey().getKey())
                  + 1; // flags

        if (iterator.isEmpty())
            return size;

        DeletionTime partitionDeletion = iterator.partitionLevelDeletion();
        Row staticRow = iterator.staticRow();
        boolean hasStatic = staticRow != Rows.EMPTY_STATIC_ROW;

        size += SerializationHeader.serializer.serializedSizeForMessaging(header, selection, hasStatic);

        if (!partitionDeletion.isLive())
            size += header.deletionTimeSerializedSize(partitionDeletion);

        if (hasStatic)
            size += UnfilteredSerializer.serializer.serializedSize(staticRow, header, version);

        if (rowEstimate >= 0)
            size += TypeSizes.sizeofUnsignedVInt(rowEstimate);

        while (iterator.hasNext())
            size += UnfilteredSerializer.serializer.serializedSize(iterator.next(), header, version);
        size += UnfilteredSerializer.serializer.serializedSizeEndOfPartition();

        return size;
    }

    public Row deserializeRowBody(DataInputPlus in,
                                  SerializationHeader header,
                                  SerializationHelper helper,
                                  int flags,
                                  Row.Builder builder)
    throws IOException
    {
        try
        {
            boolean isStatic = isStatic(flags);
            boolean hasTimestamp = (flags & HAS_TIMESTAMP) != 0;
            boolean hasTTL = (flags & HAS_TTL) != 0;
            boolean hasDeletion = (flags & HAS_DELETION) != 0;
            boolean hasComplexDeletion = (flags & HAS_COMPLEX_DELETION) != 0;
            boolean hasAllColumns = (flags & HAS_ALL_COLUMNS) != 0;
            Columns headerColumns = header.columns(isStatic);

            LivenessInfo rowLiveness = LivenessInfo.EMPTY;
            if (hasTimestamp)
            {
                long timestamp = header.readTimestamp(in);
                int ttl = hasTTL ? header.readTTL(in) : LivenessInfo.NO_TTL;
                int localDeletionTime = hasTTL ? header.readLocalDeletionTime(in) : LivenessInfo.NO_EXPIRATION_TIME;
                rowLiveness = LivenessInfo.create(timestamp, ttl, localDeletionTime);
            }

            builder.addPrimaryKeyLivenessInfo(rowLiveness);
            builder.addRowDeletion(hasDeletion ? header.readDeletionTime(in) : DeletionTime.LIVE);

            Columns columns = hasAllColumns ? headerColumns : Columns.serializer.deserializeSubset(headerColumns, in);
            for (ColumnDefinition column : columns)
            {
                if (column.isSimple())
                    readSimpleColumn(column, in, header, helper, builder, rowLiveness);
                else
                    readComplexColumn(column, in, header, helper, hasComplexDeletion, builder, rowLiveness);
            }

            return builder.build();
        }
        catch (RuntimeException | AssertionError e)
        {
            // Corrupted data could be such that it triggers an assertion in the row Builder, or break one of its assumption.
            // Of course, a bug in said builder could also trigger this, but it's impossible a priori to always make the distinction
            // between a real bug and data corrupted in just the bad way. Besides, re-throwing as an IOException doesn't hide the
            // exception, it just make we catch it properly and mark the sstable as corrupted.
            throw new IOException("Error building row with data deserialized from " + in, e);
        }
    }

+++++++++++++++++++++++
    public Row deserializeRowBody(DataInputPlus in,
                                  SerializationHeader header,
                                  SerializationHelper helper,
                                  int flags,
                                  int extendedFlags,
                                  Row.Builder builder)
    throws IOException
    {
        try
        {
            boolean isStatic = isStatic(extendedFlags);
            boolean hasTimestamp = (flags & HAS_TIMESTAMP) != 0;
            boolean hasTTL = (flags & HAS_TTL) != 0;
            boolean hasDeletion = (flags & HAS_DELETION) != 0;
            boolean deletionIsShadowable = (extendedFlags & HAS_SHADOWABLE_DELETION) != 0;
            boolean hasComplexDeletion = (flags & HAS_COMPLEX_DELETION) != 0;
            boolean hasAllColumns = (flags & HAS_ALL_COLUMNS) != 0;
            Columns headerColumns = header.columns(isStatic);

            LivenessInfo rowLiveness = LivenessInfo.EMPTY;
            if (hasTimestamp)
            {
                long timestamp = header.readTimestamp(in);
                int ttl = hasTTL ? header.readTTL(in) : LivenessInfo.NO_TTL;
                int localDeletionTime = hasTTL ? header.readLocalDeletionTime(in) : LivenessInfo.NO_EXPIRATION_TIME;
                rowLiveness = LivenessInfo.create(timestamp, ttl, localDeletionTime);
            }

            builder.addPrimaryKeyLivenessInfo(rowLiveness);
            builder.addRowDeletion(hasDeletion ? new Row.Deletion(header.readDeletionTime(in), deletionIsShadowable) : Row.Deletion.LIVE);

            Columns columns = hasAllColumns ? headerColumns : Columns.serializer.deserializeSubset(headerColumns, in);
            for (ColumnDefinition column : columns)
            {
                if (column.isSimple())
                    readSimpleColumn(column, in, header, helper, builder, rowLiveness);
                else
                    readComplexColumn(column, in, header, helper, hasComplexDeletion, builder, rowLiveness);
            }

            return builder.build();
        }
        catch (RuntimeException | AssertionError e)
        {
            // Corrupted data could be such that it triggers an assertion in the row Builder, or break one of its assumption.
            // Of course, a bug in said builder could also trigger this, but it's impossible a priori to always make the distinction
            // between a real bug and data corrupted in just the bad way. Besides, re-throwing as an IOException doesn't hide the
            // exception, it just make we catch it properly and mark the sstable as corrupted.
            throw new IOException("Error building row with data deserialized from " + in, e);
        }
    }

    public Unfiltered deserialize(DataInputPlus in, SerializationHeader header, SerializationHelper helper, Row.Builder builder)
    throws IOException
    {
        // It wouldn't be wrong per-se to use an unsorted builder, but it would be inefficient so make sure we don't do it by mistake
        assert builder.isSorted();

        int flags = in.readUnsignedByte();
        if (isEndOfPartition(flags))
            return null;

        if (kind(flags) == Unfiltered.Kind.RANGE_TOMBSTONE_MARKER)
        {
            RangeTombstone.Bound bound = RangeTombstone.Bound.serializer.deserialize(in, helper.version, header.clusteringTypes());
            return deserializeMarkerBody(in, header, bound);
        }
        else
        {
            assert !isStatic(flags); // deserializeStaticRow should be used for that.
            builder.newRow(Clustering.serializer.deserialize(in, helper.version, header.clusteringTypes()));
            return deserializeRowBody(in, header, helper, flags, builder);
        }
    }

+++++++++++++++++++++++
    public Unfiltered deserialize(DataInputPlus in, SerializationHeader header, SerializationHelper helper, Row.Builder builder)
    throws IOException
    {
        // It wouldn't be wrong per-se to use an unsorted builder, but it would be inefficient so make sure we don't do it by mistake
        assert builder.isSorted();

        int flags = in.readUnsignedByte();
        if (isEndOfPartition(flags))
            return null;

        int extendedFlags = isExtended(flags) ? in.readUnsignedByte() : 0;

        if (kind(flags) == Unfiltered.Kind.RANGE_TOMBSTONE_MARKER)
        {
            RangeTombstone.Bound bound = RangeTombstone.Bound.serializer.deserialize(in, helper.version, header.clusteringTypes());
            return deserializeMarkerBody(in, header, bound);
        }
        else
        {
            assert !isStatic(extendedFlags); // deserializeStaticRow should be used for that.
            builder.newRow(Clustering.serializer.deserialize(in, helper.version, header.clusteringTypes()));
            return deserializeRowBody(in, header, helper, flags, extendedFlags, builder);
        }
    }

    public Row deserializeStaticRow(DataInputPlus in, SerializationHeader header, SerializationHelper helper)
    throws IOException
    {
        int flags = in.readUnsignedByte();
        assert !isEndOfPartition(flags) && kind(flags) == Unfiltered.Kind.ROW && isStatic(flags) : flags;
        Row.Builder builder = BTreeRow.sortedBuilder();
        builder.newRow(Clustering.STATIC_CLUSTERING);
        return deserializeRowBody(in, header, helper, flags, builder);
    }

+++++++++++++++++++++++
    public Row deserializeStaticRow(DataInputPlus in, SerializationHeader header, SerializationHelper helper)
    throws IOException
    {
        int flags = in.readUnsignedByte();
        assert !isEndOfPartition(flags) && kind(flags) == Unfiltered.Kind.ROW && isExtended(flags) : flags;
        int extendedFlags = in.readUnsignedByte();
        Row.Builder builder = BTreeRow.sortedBuilder();
        builder.newRow(Clustering.STATIC_CLUSTERING);
        return deserializeRowBody(in, header, helper, flags, extendedFlags, builder);
    }

    public void serialize(Map<MetadataType, MetadataComponent> components, DataOutputPlus out) throws IOException
    {
        ValidationMetadata validation = (ValidationMetadata) components.get(MetadataType.VALIDATION);
        StatsMetadata stats = (StatsMetadata) components.get(MetadataType.STATS);
        CompactionMetadata compaction = (CompactionMetadata) components.get(MetadataType.COMPACTION);

        assert validation != null && stats != null && compaction != null && validation.partitioner != null;

        EstimatedHistogram.serializer.serialize(stats.estimatedPartitionSize, out);
        EstimatedHistogram.serializer.serialize(stats.estimatedColumnCount, out);
        ReplayPosition.serializer.serialize(stats.replayPosition, out);
        out.writeLong(stats.minTimestamp);
        out.writeLong(stats.maxTimestamp);
        out.writeInt(stats.maxLocalDeletionTime);
        out.writeDouble(validation.bloomFilterFPChance);
        out.writeDouble(stats.compressionRatio);
        out.writeUTF(validation.partitioner);
        out.writeInt(0); // compaction ancestors
        StreamingHistogram.serializer.serialize(stats.estimatedTombstoneDropTime, out);
        out.writeInt(stats.sstableLevel);
        out.writeInt(stats.minClusteringValues.size());
        for (ByteBuffer value : stats.minClusteringValues)
            ByteBufferUtil.writeWithShortLength(value, out);
        out.writeInt(stats.maxClusteringValues.size());
        for (ByteBuffer value : stats.maxClusteringValues)
            ByteBufferUtil.writeWithShortLength(value, out);
    }

    /**
     * Legacy serializer deserialize all components no matter what types are specified.
     */
    @Override
+++++++++++++++++++++++
    public void serialize(Map<MetadataType, MetadataComponent> components, DataOutputPlus out, Version version) throws IOException
    {
        ValidationMetadata validation = (ValidationMetadata) components.get(MetadataType.VALIDATION);
        StatsMetadata stats = (StatsMetadata) components.get(MetadataType.STATS);
        CompactionMetadata compaction = (CompactionMetadata) components.get(MetadataType.COMPACTION);

        assert validation != null && stats != null && compaction != null && validation.partitioner != null;

        EstimatedHistogram.serializer.serialize(stats.estimatedPartitionSize, out);
        EstimatedHistogram.serializer.serialize(stats.estimatedColumnCount, out);
        ReplayPosition.serializer.serialize(stats.replayPosition, out);
        out.writeLong(stats.minTimestamp);
        out.writeLong(stats.maxTimestamp);
        out.writeInt(stats.maxLocalDeletionTime);
        out.writeDouble(validation.bloomFilterFPChance);
        out.writeDouble(stats.compressionRatio);
        out.writeUTF(validation.partitioner);
        out.writeInt(0); // compaction ancestors
        StreamingHistogram.serializer.serialize(stats.estimatedTombstoneDropTime, out);
        out.writeInt(stats.sstableLevel);
        out.writeInt(stats.minClusteringValues.size());
        for (ByteBuffer value : stats.minClusteringValues)
            ByteBufferUtil.writeWithShortLength(value, out);
        out.writeInt(stats.maxClusteringValues.size());
        for (ByteBuffer value : stats.maxClusteringValues)
            ByteBufferUtil.writeWithShortLength(value, out);
    }

    /**
     * Legacy serializer deserialize all components no matter what types are specified.
     */
    @Override
    public void serialize(Map<MetadataType, MetadataComponent> components, DataOutputPlus out) throws IOException
    {
        // sort components by type
        List<MetadataComponent> sortedComponents = Lists.newArrayList(components.values());
        Collections.sort(sortedComponents);

        // write number of component
        out.writeInt(components.size());
        // build and write toc
        int lastPosition = 4 + (8 * sortedComponents.size());
        for (MetadataComponent component : sortedComponents)
        {
            MetadataType type = component.getType();
            // serialize type
            out.writeInt(type.ordinal());
            // serialize position
            out.writeInt(lastPosition);
            lastPosition += type.serializer.serializedSize(component);
        }
        // serialize components
        for (MetadataComponent component : sortedComponents)
        {
            component.getType().serializer.serialize(component, out);
        }
    }

+++++++++++++++++++++++
    public void serialize(Map<MetadataType, MetadataComponent> components, DataOutputPlus out, Version version) throws IOException
    {
        // sort components by type
        List<MetadataComponent> sortedComponents = Lists.newArrayList(components.values());
        Collections.sort(sortedComponents);

        // write number of component
        out.writeInt(components.size());
        // build and write toc
        int lastPosition = 4 + (8 * sortedComponents.size());
        for (MetadataComponent component : sortedComponents)
        {
            MetadataType type = component.getType();
            // serialize type
            out.writeInt(type.ordinal());
            // serialize position
            out.writeInt(lastPosition);
            lastPosition += type.serializer.serializedSize(version, component);
        }
        // serialize components
        for (MetadataComponent component : sortedComponents)
        {
            component.getType().serializer.serialize(version, component, out);
        }
    }

    public Map<MetadataType, MetadataComponent> deserialize(Descriptor descriptor, FileDataInput in, EnumSet<MetadataType> types) throws IOException
    {
        Map<MetadataType, MetadataComponent> components = Maps.newHashMap();
        // read number of components
        int numComponents = in.readInt();
        // read toc
        Map<MetadataType, Integer> toc = new HashMap<>(numComponents);
        MetadataType[] values = MetadataType.values();
        for (int i = 0; i < numComponents; i++)
        {
            toc.put(values[in.readInt()], in.readInt());
        }
        for (MetadataType type : types)
        {
            MetadataComponent component = null;
            Integer offset = toc.get(type);
            if (offset != null)
            {
                in.seek(offset);
                component = type.serializer.deserialize(descriptor.version, in);
            }
            components.put(type, component);
        }
        return components;
    }

+++++++++++++++++++++++
    public Map<MetadataType, MetadataComponent> deserialize(Descriptor descriptor, FileDataInput in, EnumSet<MetadataType> types) throws IOException
    {
        Map<MetadataType, MetadataComponent> components = Maps.newHashMap();
        // read number of components
        int numComponents = in.readInt();
        // read toc
        Map<MetadataType, Integer> toc = new HashMap<>(numComponents);
        MetadataType[] values = MetadataType.values();
        for (int i = 0; i < numComponents; i++)
        {
            toc.put(values[in.readInt()], in.readInt());
        }
        for (MetadataType type : types)
        {
            Integer offset = toc.get(type);
            if (offset != null)
            {
                in.seek(offset);
                MetadataComponent component = type.serializer.deserialize(descriptor.version, in);
                components.put(type, component);
            }
        }
        return components;
    }

    public ByteBuffer serialize()
    {
        try (DataOutputBuffer out = new DataOutputBufferFixed(serializedSize()))
        {
            ByteBufferUtil.writeWithShortLength(partitionKey, out);
            ByteBufferUtil.writeWithShortLength(cellName, out);
            out.writeInt(remaining);
            out.writeInt(remainingInPartition);
            return out.buffer();
        }
        catch (IOException e)
        {
            throw new RuntimeException(e);
        }
    }

+++++++++++++++++++++++
    public ByteBuffer serialize(int protocolVersion)
    {
        assert rowMark == null || protocolVersion == rowMark.protocolVersion;
        try (DataOutputBuffer out = new DataOutputBufferFixed(serializedSize(protocolVersion)))
        {
            ByteBuffer pk = partitionKey == null ? ByteBufferUtil.EMPTY_BYTE_BUFFER : partitionKey;
            ByteBuffer mark = rowMark == null ? ByteBufferUtil.EMPTY_BYTE_BUFFER : rowMark.mark;
            if (protocolVersion <= Server.VERSION_3)
            {
                ByteBufferUtil.writeWithShortLength(pk, out);
                ByteBufferUtil.writeWithShortLength(mark, out);
                out.writeInt(remaining);
                out.writeInt(remainingInPartition);
            }
            else
            {
                ByteBufferUtil.writeWithVIntLength(pk, out);
                ByteBufferUtil.writeWithVIntLength(mark, out);
                out.writeUnsignedVInt(remaining);
                out.writeUnsignedVInt(remainingInPartition);
            }
            return out.buffer();
        }
        catch (IOException e)
        {
            throw new RuntimeException(e);
        }
    }

    private int serializedSize()
    {
        return 2 + partitionKey.remaining()
             + 2 + cellName.remaining()
             + 8; // remaining & remainingInPartition
    }

    @Override
+++++++++++++++++++++++
    public int serializedSize(int protocolVersion)
    {
        assert rowMark == null || protocolVersion == rowMark.protocolVersion;
        ByteBuffer pk = partitionKey == null ? ByteBufferUtil.EMPTY_BYTE_BUFFER : partitionKey;
        ByteBuffer mark = rowMark == null ? ByteBufferUtil.EMPTY_BYTE_BUFFER : rowMark.mark;
        if (protocolVersion <= Server.VERSION_3)
        {
            return ByteBufferUtil.serializedSizeWithShortLength(pk)
                 + ByteBufferUtil.serializedSizeWithShortLength(mark)
                 + 8; // remaining & remainingInPartition
        }
        else
        {
            return ByteBufferUtil.serializedSizeWithVIntLength(pk)
                 + ByteBufferUtil.serializedSizeWithVIntLength(mark)
                 + TypeSizes.sizeofUnsignedVInt(remaining)
                 + TypeSizes.sizeofUnsignedVInt(remainingInPartition);
        }
    }

    @Override
    public static PagingState deserialize(ByteBuffer bytes)
    {
        if (bytes == null)
            return null;

        try
        {
            DataInputStream in = new DataInputStream(ByteBufferUtil.inputStream(bytes));
            ByteBuffer pk = ByteBufferUtil.readWithShortLength(in);
            ByteBuffer cn = ByteBufferUtil.readWithShortLength(in);
            int remaining = in.readInt();
            // Note that while 'in.available()' is theoretically an estimate of how many bytes are available
            // without blocking, we know that since we're reading a ByteBuffer it will be exactly how many
            // bytes remain to be read. And the reason we want to condition this is for backward compatility
            // as we used to not set this.
            int remainingInPartition = in.available() > 0 ? in.readInt() : Integer.MAX_VALUE;
            return new PagingState(pk, cn, remaining, remainingInPartition);
        }
        catch (IOException e)
        {
            throw new ProtocolException("Invalid value for the paging state");
        }
    }

+++++++++++++++++++++++
    public static PagingState deserialize(ByteBuffer bytes, int protocolVersion)
    {
        if (bytes == null)
            return null;

        try
        {
            DataInputBuffer in = new DataInputBuffer(bytes, true);
            ByteBuffer pk;
            RowMark mark;
            int remaining, remainingInPartition;
            if (protocolVersion <= Server.VERSION_3)
            {
                pk = ByteBufferUtil.readWithShortLength(in);
                mark = new RowMark(ByteBufferUtil.readWithShortLength(in), protocolVersion);
                remaining = in.readInt();
                // Note that while 'in.available()' is theoretically an estimate of how many bytes are available
                // without blocking, we know that since we're reading a ByteBuffer it will be exactly how many
                // bytes remain to be read. And the reason we want to condition this is for backward compatility
                // as we used to not set this.
                remainingInPartition = in.available() > 0 ? in.readInt() : Integer.MAX_VALUE;
            }
            else
            {
                pk = ByteBufferUtil.readWithVIntLength(in);
                mark = new RowMark(ByteBufferUtil.readWithVIntLength(in), protocolVersion);
                remaining = (int)in.readUnsignedVInt();
                remainingInPartition = (int)in.readUnsignedVInt();
            }
            return new PagingState(pk.hasRemaining() ? pk : null,
                                   mark.mark.hasRemaining() ? mark : null,
                                   remaining,
                                   remainingInPartition);
        }
        catch (IOException e)
        {
            throw new ProtocolException("Invalid value for the paging state");
        }
    }

    public static int serializedSizeWithVIntLength(ByteBuffer buffer)
    {
        int size = buffer.remaining();
        return TypeSizes.sizeofVInt(size) + size;
    }

+++++++++++++++++++++++
    public static int serializedSizeWithVIntLength(ByteBuffer buffer)
    {
        int size = buffer.remaining();
        return TypeSizes.sizeofUnsignedVInt(size) + size;
    }

add: 34, delete: 17, change: 292, unhandled: 1 size_exceptions: 3 size_serialize: 16
-----------cassandra-3.0.0-rc1 vs cassandra-3.0.0-rc2-----------
    private V deserialize(RefCountedMemory mem)
    {
        try
        {
            return serializer.deserialize(new MemoryInputStream(mem));
        }
        catch (IOException e)
        {
            logger.debug("Cannot fetch in memory data, we will fallback to read from disk ", e);
            return null;
        }
    }

+++++++++++++++++++++++
    private V deserialize(RefCountedMemory mem)
    {
        try
        {
            return serializer.deserialize(new MemoryInputStream(mem));
        }
        catch (IOException e)
        {
            logger.trace("Cannot fetch in memory data, we will fallback to read from disk ", e);
            return null;
        }
    }

    public void serialize(UnfilteredRowIterator iterator, SerializationHeader header, ColumnFilter selection, DataOutputPlus out, int version, int rowEstimate) throws IOException
    {
        ByteBufferUtil.writeWithVIntLength(iterator.partitionKey().getKey(), out);

        int flags = 0;
        if (iterator.isReverseOrder())
            flags |= IS_REVERSED;

        if (iterator.isEmpty())
        {
            out.writeByte((byte)(flags | IS_EMPTY));
            return;
        }

        DeletionTime partitionDeletion = iterator.partitionLevelDeletion();
        if (!partitionDeletion.isLive())
            flags |= HAS_PARTITION_DELETION;
        Row staticRow = iterator.staticRow();
        boolean hasStatic = staticRow != Rows.EMPTY_STATIC_ROW;
        if (hasStatic)
            flags |= HAS_STATIC_ROW;

        if (rowEstimate >= 0)
            flags |= HAS_ROW_ESTIMATE;

        out.writeByte((byte)flags);

        SerializationHeader.serializer.serializeForMessaging(header, selection, out, hasStatic);

        if (!partitionDeletion.isLive())
            header.writeDeletionTime(partitionDeletion, out);

        if (hasStatic)
            UnfilteredSerializer.serializer.serialize(staticRow, header, out, version);

        if (rowEstimate >= 0)
            out.writeUnsignedVInt(rowEstimate);

        while (iterator.hasNext())
            UnfilteredSerializer.serializer.serialize(iterator.next(), header, out, version);
        UnfilteredSerializer.serializer.writeEndOfPartition(out);
    }

    // Please note that this consume the iterator, and as such should not be called unless we have a simple way to
    // recreate an iterator for both serialize and serializedSize, which is mostly only PartitionUpdate/ArrayBackedCachedPartition.
+++++++++++++++++++++++
    public void serialize(UnfilteredRowIterator iterator, SerializationHeader header, ColumnFilter selection, DataOutputPlus out, int version, int rowEstimate) throws IOException
    {
        assert !header.isForSSTable();

        ByteBufferUtil.writeWithVIntLength(iterator.partitionKey().getKey(), out);

        int flags = 0;
        if (iterator.isReverseOrder())
            flags |= IS_REVERSED;

        if (iterator.isEmpty())
        {
            out.writeByte((byte)(flags | IS_EMPTY));
            return;
        }

        DeletionTime partitionDeletion = iterator.partitionLevelDeletion();
        if (!partitionDeletion.isLive())
            flags |= HAS_PARTITION_DELETION;
        Row staticRow = iterator.staticRow();
        boolean hasStatic = staticRow != Rows.EMPTY_STATIC_ROW;
        if (hasStatic)
            flags |= HAS_STATIC_ROW;

        if (rowEstimate >= 0)
            flags |= HAS_ROW_ESTIMATE;

        out.writeByte((byte)flags);

        SerializationHeader.serializer.serializeForMessaging(header, selection, out, hasStatic);

        if (!partitionDeletion.isLive())
            header.writeDeletionTime(partitionDeletion, out);

        if (hasStatic)
            UnfilteredSerializer.serializer.serialize(staticRow, header, out, version);

        if (rowEstimate >= 0)
            out.writeUnsignedVInt(rowEstimate);

        while (iterator.hasNext())
            UnfilteredSerializer.serializer.serialize(iterator.next(), header, out, version);
        UnfilteredSerializer.serializer.writeEndOfPartition(out);
    }

    // Please note that this consume the iterator, and as such should not be called unless we have a simple way to
    // recreate an iterator for both serialize and serializedSize, which is mostly only PartitionUpdate/ArrayBackedCachedPartition.
    public Header deserializeHeader(CFMetaData metadata, ColumnFilter selection, DataInputPlus in, int version, SerializationHelper.Flag flag) throws IOException
    {
        DecoratedKey key = metadata.decorateKey(ByteBufferUtil.readWithVIntLength(in));
        int flags = in.readUnsignedByte();
        boolean isReversed = (flags & IS_REVERSED) != 0;
        if ((flags & IS_EMPTY) != 0)
        {
            SerializationHeader sh = new SerializationHeader(metadata, PartitionColumns.NONE, EncodingStats.NO_STATS);
            return new Header(sh, key, isReversed, true, null, null, 0);
        }

        boolean hasPartitionDeletion = (flags & HAS_PARTITION_DELETION) != 0;
        boolean hasStatic = (flags & HAS_STATIC_ROW) != 0;
        boolean hasRowEstimate = (flags & HAS_ROW_ESTIMATE) != 0;

        SerializationHeader header = SerializationHeader.serializer.deserializeForMessaging(in, metadata, selection, hasStatic);

        DeletionTime partitionDeletion = hasPartitionDeletion ? header.readDeletionTime(in) : DeletionTime.LIVE;

        Row staticRow = Rows.EMPTY_STATIC_ROW;
        if (hasStatic)
            staticRow = UnfilteredSerializer.serializer.deserializeStaticRow(in, header, new SerializationHelper(metadata, version, flag));

        int rowEstimate = hasRowEstimate ? (int)in.readUnsignedVInt() : -1;
        return new Header(header, key, isReversed, false, partitionDeletion, staticRow, rowEstimate);
    }

+++++++++++++++++++++++
    public Header deserializeHeader(CFMetaData metadata, ColumnFilter selection, DataInputPlus in, int version, SerializationHelper.Flag flag) throws IOException
    {
        DecoratedKey key = metadata.decorateKey(ByteBufferUtil.readWithVIntLength(in));
        int flags = in.readUnsignedByte();
        boolean isReversed = (flags & IS_REVERSED) != 0;
        if ((flags & IS_EMPTY) != 0)
        {
            SerializationHeader sh = new SerializationHeader(false, metadata, PartitionColumns.NONE, EncodingStats.NO_STATS);
            return new Header(sh, key, isReversed, true, null, null, 0);
        }

        boolean hasPartitionDeletion = (flags & HAS_PARTITION_DELETION) != 0;
        boolean hasStatic = (flags & HAS_STATIC_ROW) != 0;
        boolean hasRowEstimate = (flags & HAS_ROW_ESTIMATE) != 0;

        SerializationHeader header = SerializationHeader.serializer.deserializeForMessaging(in, metadata, selection, hasStatic);

        DeletionTime partitionDeletion = hasPartitionDeletion ? header.readDeletionTime(in) : DeletionTime.LIVE;

        Row staticRow = Rows.EMPTY_STATIC_ROW;
        if (hasStatic)
            staticRow = UnfilteredSerializer.serializer.deserializeStaticRow(in, header, new SerializationHelper(metadata, version, flag));

        int rowEstimate = hasRowEstimate ? (int)in.readUnsignedVInt() : -1;
        return new Header(header, key, isReversed, false, partitionDeletion, staticRow, rowEstimate);
    }

    public long serializedSize(UnfilteredRowIterator iterator, ColumnFilter selection, int version, int rowEstimate)
    {
        SerializationHeader header = new SerializationHeader(iterator.metadata(),
                                                             iterator.columns(),
                                                             iterator.stats());

        assert rowEstimate >= 0;

        long size = ByteBufferUtil.serializedSizeWithVIntLength(iterator.partitionKey().getKey())
                  + 1; // flags

        if (iterator.isEmpty())
            return size;

        DeletionTime partitionDeletion = iterator.partitionLevelDeletion();
        Row staticRow = iterator.staticRow();
        boolean hasStatic = staticRow != Rows.EMPTY_STATIC_ROW;

        size += SerializationHeader.serializer.serializedSizeForMessaging(header, selection, hasStatic);

        if (!partitionDeletion.isLive())
            size += header.deletionTimeSerializedSize(partitionDeletion);

        if (hasStatic)
            size += UnfilteredSerializer.serializer.serializedSize(staticRow, header, version);

        if (rowEstimate >= 0)
            size += TypeSizes.sizeofUnsignedVInt(rowEstimate);

        while (iterator.hasNext())
            size += UnfilteredSerializer.serializer.serializedSize(iterator.next(), header, version);
        size += UnfilteredSerializer.serializer.serializedSizeEndOfPartition();

        return size;
    }

+++++++++++++++++++++++
    public long serializedSize(UnfilteredRowIterator iterator, ColumnFilter selection, int version, int rowEstimate)
    {
        SerializationHeader header = new SerializationHeader(false,
                                                             iterator.metadata(),
                                                             iterator.columns(),
                                                             iterator.stats());

        assert rowEstimate >= 0;

        long size = ByteBufferUtil.serializedSizeWithVIntLength(iterator.partitionKey().getKey())
                  + 1; // flags

        if (iterator.isEmpty())
            return size;

        DeletionTime partitionDeletion = iterator.partitionLevelDeletion();
        Row staticRow = iterator.staticRow();
        boolean hasStatic = staticRow != Rows.EMPTY_STATIC_ROW;

        size += SerializationHeader.serializer.serializedSizeForMessaging(header, selection, hasStatic);

        if (!partitionDeletion.isLive())
            size += header.deletionTimeSerializedSize(partitionDeletion);

        if (hasStatic)
            size += UnfilteredSerializer.serializer.serializedSize(staticRow, header, version);

        if (rowEstimate >= 0)
            size += TypeSizes.sizeofUnsignedVInt(rowEstimate);

        while (iterator.hasNext())
            size += UnfilteredSerializer.serializer.serializedSize(iterator.next(), header, version);
        size += UnfilteredSerializer.serializer.serializedSizeEndOfPartition();

        return size;
    }

    public long serializedSize(RangeTombstoneMarker marker, SerializationHeader header, int version)
    {
        long size = 1 // flags
                  + RangeTombstone.Bound.serializer.serializedSize(marker.clustering(), version, header.clusteringTypes());

        if (marker.isBoundary())
        {
            RangeTombstoneBoundaryMarker bm = (RangeTombstoneBoundaryMarker)marker;
            size += header.deletionTimeSerializedSize(bm.endDeletionTime());
            size += header.deletionTimeSerializedSize(bm.startDeletionTime());
        }
        else
        {
           size += header.deletionTimeSerializedSize(((RangeTombstoneBoundMarker)marker).deletionTime());
        }
        return size;
    }

+++++++++++++++++++++++
    private long serializedSize(RangeTombstoneMarker marker, SerializationHeader header, long previousUnfilteredSize, int version)
    {
        assert !header.isForSSTable();
        return 1 // flags
             + RangeTombstone.Bound.serializer.serializedSize(marker.clustering(), version, header.clusteringTypes())
             + serializedMarkerBodySize(marker, header, previousUnfilteredSize, version);
    }

    public Row deserializeRowBody(DataInputPlus in,
                                  SerializationHeader header,
                                  SerializationHelper helper,
                                  int flags,
                                  int extendedFlags,
                                  Row.Builder builder)
    throws IOException
    {
        try
        {
            boolean isStatic = isStatic(extendedFlags);
            boolean hasTimestamp = (flags & HAS_TIMESTAMP) != 0;
            boolean hasTTL = (flags & HAS_TTL) != 0;
            boolean hasDeletion = (flags & HAS_DELETION) != 0;
            boolean deletionIsShadowable = (extendedFlags & HAS_SHADOWABLE_DELETION) != 0;
            boolean hasComplexDeletion = (flags & HAS_COMPLEX_DELETION) != 0;
            boolean hasAllColumns = (flags & HAS_ALL_COLUMNS) != 0;
            Columns headerColumns = header.columns(isStatic);

            LivenessInfo rowLiveness = LivenessInfo.EMPTY;
            if (hasTimestamp)
            {
                long timestamp = header.readTimestamp(in);
                int ttl = hasTTL ? header.readTTL(in) : LivenessInfo.NO_TTL;
                int localDeletionTime = hasTTL ? header.readLocalDeletionTime(in) : LivenessInfo.NO_EXPIRATION_TIME;
                rowLiveness = LivenessInfo.create(timestamp, ttl, localDeletionTime);
            }

            builder.addPrimaryKeyLivenessInfo(rowLiveness);
            builder.addRowDeletion(hasDeletion ? new Row.Deletion(header.readDeletionTime(in), deletionIsShadowable) : Row.Deletion.LIVE);

            Columns columns = hasAllColumns ? headerColumns : Columns.serializer.deserializeSubset(headerColumns, in);
            for (ColumnDefinition column : columns)
            {
                if (column.isSimple())
                    readSimpleColumn(column, in, header, helper, builder, rowLiveness);
                else
                    readComplexColumn(column, in, header, helper, hasComplexDeletion, builder, rowLiveness);
            }

            return builder.build();
        }
        catch (RuntimeException | AssertionError e)
        {
            // Corrupted data could be such that it triggers an assertion in the row Builder, or break one of its assumption.
            // Of course, a bug in said builder could also trigger this, but it's impossible a priori to always make the distinction
            // between a real bug and data corrupted in just the bad way. Besides, re-throwing as an IOException doesn't hide the
            // exception, it just make we catch it properly and mark the sstable as corrupted.
            throw new IOException("Error building row with data deserialized from " + in, e);
        }
    }

+++++++++++++++++++++++
    public Row deserializeRowBody(DataInputPlus in,
                                  SerializationHeader header,
                                  SerializationHelper helper,
                                  int flags,
                                  int extendedFlags,
                                  Row.Builder builder)
    throws IOException
    {
        try
        {
            boolean isStatic = isStatic(extendedFlags);
            boolean hasTimestamp = (flags & HAS_TIMESTAMP) != 0;
            boolean hasTTL = (flags & HAS_TTL) != 0;
            boolean hasDeletion = (flags & HAS_DELETION) != 0;
            boolean deletionIsShadowable = (extendedFlags & HAS_SHADOWABLE_DELETION) != 0;
            boolean hasComplexDeletion = (flags & HAS_COMPLEX_DELETION) != 0;
            boolean hasAllColumns = (flags & HAS_ALL_COLUMNS) != 0;
            Columns headerColumns = header.columns(isStatic);

            if (header.isForSSTable())
            {
                in.readUnsignedVInt(); // Skip row size
                in.readUnsignedVInt(); // previous unfiltered size
            }

            LivenessInfo rowLiveness = LivenessInfo.EMPTY;
            if (hasTimestamp)
            {
                long timestamp = header.readTimestamp(in);
                int ttl = hasTTL ? header.readTTL(in) : LivenessInfo.NO_TTL;
                int localDeletionTime = hasTTL ? header.readLocalDeletionTime(in) : LivenessInfo.NO_EXPIRATION_TIME;
                rowLiveness = LivenessInfo.create(timestamp, ttl, localDeletionTime);
            }

            builder.addPrimaryKeyLivenessInfo(rowLiveness);
            builder.addRowDeletion(hasDeletion ? new Row.Deletion(header.readDeletionTime(in), deletionIsShadowable) : Row.Deletion.LIVE);

            Columns columns = hasAllColumns ? headerColumns : Columns.serializer.deserializeSubset(headerColumns, in);
            for (ColumnDefinition column : columns)
            {
                if (column.isSimple())
                    readSimpleColumn(column, in, header, helper, builder, rowLiveness);
                else
                    readComplexColumn(column, in, header, helper, hasComplexDeletion, builder, rowLiveness);
            }

            return builder.build();
        }
        catch (RuntimeException | AssertionError e)
        {
            // Corrupted data could be such that it triggers an assertion in the row Builder, or break one of its assumption.
            // Of course, a bug in said builder could also trigger this, but it's impossible a priori to always make the distinction
            // between a real bug and data corrupted in just the bad way. Besides, re-throwing as an IOException doesn't hide the
            // exception, it just make we catch it properly and mark the sstable as corrupted.
            throw new IOException("Error building row with data deserialized from " + in, e);
        }
    }

    public RangeTombstoneMarker deserializeMarkerBody(DataInputPlus in, SerializationHeader header, RangeTombstone.Bound bound)
    throws IOException
    {
        if (bound.isBoundary())
            return new RangeTombstoneBoundaryMarker(bound, header.readDeletionTime(in), header.readDeletionTime(in));
        else
            return new RangeTombstoneBoundMarker(bound, header.readDeletionTime(in));
    }

+++++++++++++++++++++++
    public RangeTombstoneMarker deserializeMarkerBody(DataInputPlus in, SerializationHeader header, RangeTombstone.Bound bound)
    throws IOException
    {
        if (header.isForSSTable())
        {
            in.readUnsignedVInt(); // marker size
            in.readUnsignedVInt(); // previous unfiltered size
        }

        if (bound.isBoundary())
            return new RangeTombstoneBoundaryMarker(bound, header.readDeletionTime(in), header.readDeletionTime(in));
        else
            return new RangeTombstoneBoundMarker(bound, header.readDeletionTime(in));
    }

    public void serialize(RangeTombstoneMarker marker, SerializationHeader header, DataOutputPlus out, int version)
    throws IOException
    {
        out.writeByte((byte)IS_MARKER);
        RangeTombstone.Bound.serializer.serialize(marker.clustering(), out, version, header.clusteringTypes());

        if (marker.isBoundary())
        {
            RangeTombstoneBoundaryMarker bm = (RangeTombstoneBoundaryMarker)marker;
            header.writeDeletionTime(bm.endDeletionTime(), out);
            header.writeDeletionTime(bm.startDeletionTime(), out);
        }
        else
        {
            header.writeDeletionTime(((RangeTombstoneBoundMarker)marker).deletionTime(), out);
        }
    }

+++++++++++++++++++++++
    private void serialize(RangeTombstoneMarker marker, SerializationHeader header, DataOutputPlus out, long previousUnfilteredSize, int version)
    throws IOException
    {
        out.writeByte((byte)IS_MARKER);
        RangeTombstone.Bound.serializer.serialize(marker.clustering(), out, version, header.clusteringTypes());

        if (header.isForSSTable())
        {
            out.writeUnsignedVInt(serializedMarkerBodySize(marker, header, previousUnfilteredSize, version));
            out.writeUnsignedVInt(previousUnfilteredSize);
        }

        if (marker.isBoundary())
        {
            RangeTombstoneBoundaryMarker bm = (RangeTombstoneBoundaryMarker)marker;
            header.writeDeletionTime(bm.endDeletionTime(), out);
            header.writeDeletionTime(bm.startDeletionTime(), out);
        }
        else
        {
            header.writeDeletionTime(((RangeTombstoneBoundMarker)marker).deletionTime(), out);
        }
    }

    public Unfiltered deserialize(DataInputPlus in, SerializationHeader header, SerializationHelper helper, Row.Builder builder)
    throws IOException
    {
        // It wouldn't be wrong per-se to use an unsorted builder, but it would be inefficient so make sure we don't do it by mistake
        assert builder.isSorted();

        int flags = in.readUnsignedByte();
        if (isEndOfPartition(flags))
            return null;

        int extendedFlags = isExtended(flags) ? in.readUnsignedByte() : 0;

        if (kind(flags) == Unfiltered.Kind.RANGE_TOMBSTONE_MARKER)
        {
            RangeTombstone.Bound bound = RangeTombstone.Bound.serializer.deserialize(in, helper.version, header.clusteringTypes());
            return deserializeMarkerBody(in, header, bound);
        }
        else
        {
            assert !isStatic(extendedFlags); // deserializeStaticRow should be used for that.
            builder.newRow(Clustering.serializer.deserialize(in, helper.version, header.clusteringTypes()));
            return deserializeRowBody(in, header, helper, flags, extendedFlags, builder);
        }
    }

+++++++++++++++++++++++
    public Unfiltered deserialize(DataInputPlus in, SerializationHeader header, SerializationHelper helper, Row.Builder builder)
    throws IOException
    {
        // It wouldn't be wrong per-se to use an unsorted builder, but it would be inefficient so make sure we don't do it by mistake
        assert builder.isSorted();

        int flags = in.readUnsignedByte();
        if (isEndOfPartition(flags))
            return null;

        int extendedFlags = readExtendedFlags(in, flags);

        if (kind(flags) == Unfiltered.Kind.RANGE_TOMBSTONE_MARKER)
        {
            RangeTombstone.Bound bound = RangeTombstone.Bound.serializer.deserialize(in, helper.version, header.clusteringTypes());
            return deserializeMarkerBody(in, header, bound);
        }
        else
        {
            assert !isStatic(extendedFlags); // deserializeStaticRow should be used for that.
            builder.newRow(Clustering.serializer.deserialize(in, helper.version, header.clusteringTypes()));
            return deserializeRowBody(in, header, helper, flags, extendedFlags, builder);
        }
    }

    private static Mutation deserializeLegacyMutation(UntypedResultSet.Row row)
    {
        try
        {
            Mutation mutation = Mutation.serializer.deserialize(new DataInputBuffer(row.getBlob("mutation"), true),
                                                                row.getInt("message_version"));
            mutation.getPartitionUpdates().forEach(PartitionUpdate::validate);
            return mutation;
        }
        catch (IOException e)
        {
            logger.error("Failed to migrate a hint for {} from legacy {}.{} table: {}",
                         row.getUUID("target_id"),
                         SystemKeyspace.NAME,
                         SystemKeyspace.LEGACY_HINTS,
                         e);
            return null;
        }
        catch (MarshalException e)
        {
            logger.warn("Failed to validate a hint for {} (table id {}) from legacy {}.{} table - skipping: {})",
                        row.getUUID("target_id"),
                        SystemKeyspace.NAME,
                        SystemKeyspace.LEGACY_HINTS,
                        e);
            return null;
        }
    }

+++++++++++++++++++++++
    private static Mutation deserializeLegacyMutation(UntypedResultSet.Row row)
    {
        try (DataInputBuffer dib = new DataInputBuffer(row.getBlob("mutation"), true))
        {
            Mutation mutation = Mutation.serializer.deserialize(dib,
                                                                row.getInt("message_version"));
            mutation.getPartitionUpdates().forEach(PartitionUpdate::validate);
            return mutation;
        }
        catch (IOException e)
        {
            logger.error("Failed to migrate a hint for {} from legacy {}.{} table: {}",
                         row.getUUID("target_id"),
                         SystemKeyspace.NAME,
                         SystemKeyspace.LEGACY_HINTS,
                         e);
            return null;
        }
        catch (MarshalException e)
        {
            logger.warn("Failed to validate a hint for {} (table id {}) from legacy {}.{} table - skipping: {})",
                        row.getUUID("target_id"),
                        SystemKeyspace.NAME,
                        SystemKeyspace.LEGACY_HINTS,
                        e);
            return null;
        }
    }

    public static PagingState deserialize(ByteBuffer bytes, int protocolVersion)
    {
        if (bytes == null)
            return null;

        try
        {
            DataInputBuffer in = new DataInputBuffer(bytes, true);
            ByteBuffer pk;
            RowMark mark;
            int remaining, remainingInPartition;
            if (protocolVersion <= Server.VERSION_3)
            {
                pk = ByteBufferUtil.readWithShortLength(in);
                mark = new RowMark(ByteBufferUtil.readWithShortLength(in), protocolVersion);
                remaining = in.readInt();
                // Note that while 'in.available()' is theoretically an estimate of how many bytes are available
                // without blocking, we know that since we're reading a ByteBuffer it will be exactly how many
                // bytes remain to be read. And the reason we want to condition this is for backward compatility
                // as we used to not set this.
                remainingInPartition = in.available() > 0 ? in.readInt() : Integer.MAX_VALUE;
            }
            else
            {
                pk = ByteBufferUtil.readWithVIntLength(in);
                mark = new RowMark(ByteBufferUtil.readWithVIntLength(in), protocolVersion);
                remaining = (int)in.readUnsignedVInt();
                remainingInPartition = (int)in.readUnsignedVInt();
            }
            return new PagingState(pk.hasRemaining() ? pk : null,
                                   mark.mark.hasRemaining() ? mark : null,
                                   remaining,
                                   remainingInPartition);
        }
        catch (IOException e)
        {
            throw new ProtocolException("Invalid value for the paging state");
        }
    }

+++++++++++++++++++++++
    public static PagingState deserialize(ByteBuffer bytes, int protocolVersion)
    {
        if (bytes == null)
            return null;

        try (DataInputBuffer in = new DataInputBuffer(bytes, true))
        {
            ByteBuffer pk;
            RowMark mark;
            int remaining, remainingInPartition;
            if (protocolVersion <= Server.VERSION_3)
            {
                pk = ByteBufferUtil.readWithShortLength(in);
                mark = new RowMark(ByteBufferUtil.readWithShortLength(in), protocolVersion);
                remaining = in.readInt();
                // Note that while 'in.available()' is theoretically an estimate of how many bytes are available
                // without blocking, we know that since we're reading a ByteBuffer it will be exactly how many
                // bytes remain to be read. And the reason we want to condition this is for backward compatility
                // as we used to not set this.
                remainingInPartition = in.available() > 0 ? in.readInt() : Integer.MAX_VALUE;
            }
            else
            {
                pk = ByteBufferUtil.readWithVIntLength(in);
                mark = new RowMark(ByteBufferUtil.readWithVIntLength(in), protocolVersion);
                remaining = (int)in.readUnsignedVInt();
                remainingInPartition = (int)in.readUnsignedVInt();
            }
            return new PagingState(pk.hasRemaining() ? pk : null,
                                   mark.mark.hasRemaining() ? mark : null,
                                   remaining,
                                   remainingInPartition);
        }
        catch (IOException e)
        {
            throw new ProtocolException("Invalid value for the paging state");
        }
    }

add: 3, delete: 12, change: 216, unhandled: 1 size_exceptions: 1 size_serialize: 11
-----------cassandra-3.0.0-rc2 vs cassandra-3.0.1-----------
    public static void serializeAsLegacyPartition(UnfilteredRowIterator partition, DataOutputPlus out, int version) throws IOException
    {
        assert version < MessagingService.VERSION_30;

        out.writeBoolean(true);

        LegacyLayout.LegacyUnfilteredPartition legacyPartition = LegacyLayout.fromUnfilteredRowIterator(partition);

        UUIDSerializer.serializer.serialize(partition.metadata().cfId, out, version);
        DeletionTime.serializer.serialize(legacyPartition.partitionDeletion, out);

        legacyPartition.rangeTombstones.serialize(out, partition.metadata());

        // begin cell serialization
        out.writeInt(legacyPartition.cells.size());
        for (LegacyLayout.LegacyCell cell : legacyPartition.cells)
        {
            ByteBufferUtil.writeWithShortLength(cell.name.encode(partition.metadata()), out);
            if (cell.kind == LegacyLayout.LegacyCell.Kind.EXPIRING)
            {
                out.writeByte(LegacyLayout.EXPIRATION_MASK);  // serialization flags
                out.writeInt(cell.ttl);
                out.writeInt(cell.localDeletionTime);
            }
            else if (cell.kind == LegacyLayout.LegacyCell.Kind.DELETED)
            {
                out.writeByte(LegacyLayout.DELETION_MASK);  // serialization flags
                out.writeLong(cell.timestamp);
                out.writeInt(TypeSizes.sizeof(cell.localDeletionTime));
                out.writeInt(cell.localDeletionTime);
                continue;
            }
            else if (cell.kind == LegacyLayout.LegacyCell.Kind.COUNTER)
            {
                out.writeByte(LegacyLayout.COUNTER_MASK);  // serialization flags
                out.writeLong(Long.MIN_VALUE);  // timestampOfLastDelete (not used, and MIN_VALUE is the default)
            }
            else
            {
                // normal cell
                out.writeByte(0);  // serialization flags
            }

            out.writeLong(cell.timestamp);
            ByteBufferUtil.writeWithLength(cell.value, out);
        }
    }

    // For the old wire format
    // Note: this can return null if an empty partition is serialized!
+++++++++++++++++++++++
    public static void serializeAsLegacyPartition(UnfilteredRowIterator partition, DataOutputPlus out, int version) throws IOException
    {
        assert version < MessagingService.VERSION_30;

        out.writeBoolean(true);

        LegacyLayout.LegacyUnfilteredPartition legacyPartition = LegacyLayout.fromUnfilteredRowIterator(partition);

        UUIDSerializer.serializer.serialize(partition.metadata().cfId, out, version);
        DeletionTime.serializer.serialize(legacyPartition.partitionDeletion, out);

        legacyPartition.rangeTombstones.serialize(out, partition.metadata());

        // begin cell serialization
        out.writeInt(legacyPartition.cells.size());
        for (LegacyLayout.LegacyCell cell : legacyPartition.cells)
        {
            ByteBufferUtil.writeWithShortLength(cell.name.encode(partition.metadata()), out);
            out.writeByte(cell.serializationFlags());
            if (cell.isExpiring())
            {
                out.writeInt(cell.ttl);
                out.writeInt(cell.localDeletionTime);
            }
            else if (cell.isTombstone())
            {
                out.writeLong(cell.timestamp);
                out.writeInt(TypeSizes.sizeof(cell.localDeletionTime));
                out.writeInt(cell.localDeletionTime);
                continue;
            }
            else if (cell.isCounterUpdate())
            {
                out.writeLong(cell.timestamp);
                long count = CounterContext.instance().getLocalCount(cell.value);
                ByteBufferUtil.writeWithLength(ByteBufferUtil.bytes(count), out);
                continue;
            }
            else if (cell.isCounter())
            {
                out.writeLong(Long.MIN_VALUE);  // timestampOfLastDelete (not used, and MIN_VALUE is the default)
            }

            out.writeLong(cell.timestamp);
            ByteBufferUtil.writeWithLength(cell.value, out);
        }
    }

    // For the old wire format
    // Note: this can return null if an empty partition is serialized!
    public void serialize(EndpointState epState, DataOutputPlus out, int version) throws IOException
    {
        /* serialize the HeartBeatState */
        HeartBeatState hbState = epState.getHeartBeatState();
        HeartBeatState.serializer.serialize(hbState, out, version);

        /* serialize the map of ApplicationState objects */
        int size = epState.applicationState.size();
        out.writeInt(size);
        for (Map.Entry<ApplicationState, VersionedValue> entry : epState.applicationState.entrySet())
        {
            VersionedValue value = entry.getValue();
            out.writeInt(entry.getKey().ordinal());
            VersionedValue.serializer.serialize(value, out, version);
        }
    }

+++++++++++++++++++++++
    public void serialize(EndpointState epState, DataOutputPlus out, int version) throws IOException
    {
        /* serialize the HeartBeatState */
        HeartBeatState hbState = epState.getHeartBeatState();
        HeartBeatState.serializer.serialize(hbState, out, version);

        /* serialize the map of ApplicationState objects */
        Set<Map.Entry<ApplicationState, VersionedValue>> states = epState.states();
        out.writeInt(states.size());
        for (Map.Entry<ApplicationState, VersionedValue> state : states)
        {
            VersionedValue value = state.getValue();
            out.writeInt(state.getKey().ordinal());
            VersionedValue.serializer.serialize(value, out, version);
        }
    }

    public long serializedSize(EndpointState epState, int version)
    {
        long size = HeartBeatState.serializer.serializedSize(epState.getHeartBeatState(), version);
        size += TypeSizes.sizeof(epState.applicationState.size());
        for (Map.Entry<ApplicationState, VersionedValue> entry : epState.applicationState.entrySet())
        {
            VersionedValue value = entry.getValue();
            size += TypeSizes.sizeof(entry.getKey().ordinal());
            size += VersionedValue.serializer.serializedSize(value, version);
        }
        return size;
+++++++++++++++++++++++
    public long serializedSize(EndpointState epState, int version)
    {
        long size = HeartBeatState.serializer.serializedSize(epState.getHeartBeatState(), version);
        Set<Map.Entry<ApplicationState, VersionedValue>> states = epState.states();
        size += TypeSizes.sizeof(states.size());
        for (Map.Entry<ApplicationState, VersionedValue> state : states)
        {
            VersionedValue value = state.getValue();
            size += TypeSizes.sizeof(state.getKey().ordinal());
            size += VersionedValue.serializer.serializedSize(value, version);
        }
        return size;
    public EndpointState deserialize(DataInputPlus in, int version) throws IOException
    {
        HeartBeatState hbState = HeartBeatState.serializer.deserialize(in, version);
        EndpointState epState = new EndpointState(hbState);

        int appStateSize = in.readInt();
        for (int i = 0; i < appStateSize; ++i)
        {
            int key = in.readInt();
            VersionedValue value = VersionedValue.serializer.deserialize(in, version);
            epState.addApplicationState(Gossiper.STATES[key], value);
        }
        return epState;
    }

+++++++++++++++++++++++
    public EndpointState deserialize(DataInputPlus in, int version) throws IOException
    {
        HeartBeatState hbState = HeartBeatState.serializer.deserialize(in, version);

        int appStateSize = in.readInt();
        Map<ApplicationState, VersionedValue> states = new EnumMap<>(ApplicationState.class);
        for (int i = 0; i < appStateSize; ++i)
        {
            int key = in.readInt();
            VersionedValue value = VersionedValue.serializer.deserialize(in, version);
            states.put(Gossiper.STATES[key], value);
        }

        return new EndpointState(hbState, states);
    }

    public synchronized void serialize(DataOutputStreamPlus out, int version, StreamSession session) throws IOException
    {
        if (completed)
        {
            return;
        }

        FileMessageHeader.serializer.serialize(header, out, version);

        final SSTableReader reader = ref.get();
        StreamWriter writer = header.compressionInfo == null ?
                                      new StreamWriter(reader, header.sections, session) :
                                      new CompressedStreamWriter(reader, header.sections,
                                                                 header.compressionInfo, session);
        writer.write(out);
    }

+++++++++++++++++++++++
    public synchronized void serialize(DataOutputStreamPlus out, int version, StreamSession session) throws IOException
    {
        if (completed)
        {
            return;
        }

        CompressionInfo compressionInfo = FileMessageHeader.serializer.serialize(header, out, version);

        final SSTableReader reader = ref.get();
        StreamWriter writer = compressionInfo == null ?
                                      new StreamWriter(reader, header.sections, session) :
                                      new CompressedStreamWriter(reader, header.sections,
                                                                 compressionInfo, session);
        writer.write(out);
    }

add: 37, delete: 13, change: 270, unhandled: 1 size_exceptions: 5 size_serialize: 5
-----------cassandra-3.0.1 vs cassandra-3.0.2-----------
add: 0, delete: 0, change: 2, unhandled: 0 size_exceptions: 0 size_serialize: 0
-----------cassandra-3.0.2 vs cassandra-3.0.3-----------
    public static long serializedSizeAsLegacyPartition(UnfilteredRowIterator partition, int version)
    {
        assert version < MessagingService.VERSION_30;

        if (partition.isEmpty())
            return TypeSizes.sizeof(false);

        long size = TypeSizes.sizeof(true);

        LegacyLayout.LegacyUnfilteredPartition legacyPartition = LegacyLayout.fromUnfilteredRowIterator(partition);

        size += UUIDSerializer.serializer.serializedSize(partition.metadata().cfId, version);
        size += DeletionTime.serializer.serializedSize(legacyPartition.partitionDeletion);
        size += legacyPartition.rangeTombstones.serializedSize(partition.metadata());

        // begin cell serialization
        size += TypeSizes.sizeof(legacyPartition.cells.size());
        for (LegacyLayout.LegacyCell cell : legacyPartition.cells)
        {
            size += ByteBufferUtil.serializedSizeWithShortLength(cell.name.encode(partition.metadata()));
            size += 1;  // serialization flags
            if (cell.kind == LegacyLayout.LegacyCell.Kind.EXPIRING)
            {
                size += TypeSizes.sizeof(cell.ttl);
                size += TypeSizes.sizeof(cell.localDeletionTime);
            }
            else if (cell.kind == LegacyLayout.LegacyCell.Kind.DELETED)
            {
                size += TypeSizes.sizeof(cell.timestamp);
                // localDeletionTime replaces cell.value as the body
                size += TypeSizes.sizeof(TypeSizes.sizeof(cell.localDeletionTime));
                size += TypeSizes.sizeof(cell.localDeletionTime);
                continue;
            }
            else if (cell.kind == LegacyLayout.LegacyCell.Kind.COUNTER)
            {
                size += TypeSizes.sizeof(Long.MIN_VALUE);  // timestampOfLastDelete
            }

            size += TypeSizes.sizeof(cell.timestamp);
            size += ByteBufferUtil.serializedSizeWithLength(cell.value);
        }

        return size;
    }

    // For thrift sake
+++++++++++++++++++++++
    public static long serializedSizeAsLegacyPartition(ReadCommand command, UnfilteredRowIterator partition, int version)
    {
        assert version < MessagingService.VERSION_30;

        if (partition.isEmpty())
            return TypeSizes.sizeof(false);

        long size = TypeSizes.sizeof(true);

        LegacyLayout.LegacyUnfilteredPartition legacyPartition = LegacyLayout.fromUnfilteredRowIterator(command, partition);

        size += UUIDSerializer.serializer.serializedSize(partition.metadata().cfId, version);
        size += DeletionTime.serializer.serializedSize(legacyPartition.partitionDeletion);
        size += legacyPartition.rangeTombstones.serializedSize(partition.metadata());

        // begin cell serialization
        size += TypeSizes.sizeof(legacyPartition.cells.size());
        for (LegacyLayout.LegacyCell cell : legacyPartition.cells)
        {
            size += ByteBufferUtil.serializedSizeWithShortLength(cell.name.encode(partition.metadata()));
            size += 1;  // serialization flags
            if (cell.kind == LegacyLayout.LegacyCell.Kind.EXPIRING)
            {
                size += TypeSizes.sizeof(cell.ttl);
                size += TypeSizes.sizeof(cell.localDeletionTime);
            }
            else if (cell.kind == LegacyLayout.LegacyCell.Kind.DELETED)
            {
                size += TypeSizes.sizeof(cell.timestamp);
                // localDeletionTime replaces cell.value as the body
                size += TypeSizes.sizeof(TypeSizes.sizeof(cell.localDeletionTime));
                size += TypeSizes.sizeof(cell.localDeletionTime);
                continue;
            }
            else if (cell.kind == LegacyLayout.LegacyCell.Kind.COUNTER)
            {
                size += TypeSizes.sizeof(Long.MIN_VALUE);  // timestampOfLastDelete
            }

            size += TypeSizes.sizeof(cell.timestamp);
            size += ByteBufferUtil.serializedSizeWithLength(cell.value);
        }

        return size;
    }

    // For thrift sake
    public static void serializeAsLegacyPartition(UnfilteredRowIterator partition, DataOutputPlus out, int version) throws IOException
    {
        assert version < MessagingService.VERSION_30;

        out.writeBoolean(true);

        LegacyLayout.LegacyUnfilteredPartition legacyPartition = LegacyLayout.fromUnfilteredRowIterator(partition);

        UUIDSerializer.serializer.serialize(partition.metadata().cfId, out, version);
        DeletionTime.serializer.serialize(legacyPartition.partitionDeletion, out);

        legacyPartition.rangeTombstones.serialize(out, partition.metadata());

        // begin cell serialization
        out.writeInt(legacyPartition.cells.size());
        for (LegacyLayout.LegacyCell cell : legacyPartition.cells)
        {
            ByteBufferUtil.writeWithShortLength(cell.name.encode(partition.metadata()), out);
            out.writeByte(cell.serializationFlags());
            if (cell.isExpiring())
            {
                out.writeInt(cell.ttl);
                out.writeInt(cell.localDeletionTime);
            }
            else if (cell.isTombstone())
            {
                out.writeLong(cell.timestamp);
                out.writeInt(TypeSizes.sizeof(cell.localDeletionTime));
                out.writeInt(cell.localDeletionTime);
                continue;
            }
            else if (cell.isCounterUpdate())
            {
                out.writeLong(cell.timestamp);
                long count = CounterContext.instance().getLocalCount(cell.value);
                ByteBufferUtil.writeWithLength(ByteBufferUtil.bytes(count), out);
                continue;
            }
            else if (cell.isCounter())
            {
                out.writeLong(Long.MIN_VALUE);  // timestampOfLastDelete (not used, and MIN_VALUE is the default)
            }

            out.writeLong(cell.timestamp);
            ByteBufferUtil.writeWithLength(cell.value, out);
        }
    }

    // For the old wire format
    // Note: this can return null if an empty partition is serialized!
+++++++++++++++++++++++
    public static void serializeAsLegacyPartition(ReadCommand command, UnfilteredRowIterator partition, DataOutputPlus out, int version) throws IOException
    {
        assert version < MessagingService.VERSION_30;

        out.writeBoolean(true);

        LegacyLayout.LegacyUnfilteredPartition legacyPartition = LegacyLayout.fromUnfilteredRowIterator(command, partition);

        UUIDSerializer.serializer.serialize(partition.metadata().cfId, out, version);
        DeletionTime.serializer.serialize(legacyPartition.partitionDeletion, out);

        legacyPartition.rangeTombstones.serialize(out, partition.metadata());

        // begin cell serialization
        out.writeInt(legacyPartition.cells.size());
        for (LegacyLayout.LegacyCell cell : legacyPartition.cells)
        {
            ByteBufferUtil.writeWithShortLength(cell.name.encode(partition.metadata()), out);
            out.writeByte(cell.serializationFlags());
            if (cell.isExpiring())
            {
                out.writeInt(cell.ttl);
                out.writeInt(cell.localDeletionTime);
            }
            else if (cell.isTombstone())
            {
                out.writeLong(cell.timestamp);
                out.writeInt(TypeSizes.sizeof(cell.localDeletionTime));
                out.writeInt(cell.localDeletionTime);
                continue;
            }
            else if (cell.isCounterUpdate())
            {
                out.writeLong(cell.timestamp);
                long count = CounterContext.instance().getLocalCount(cell.value);
                ByteBufferUtil.writeWithLength(ByteBufferUtil.bytes(count), out);
                continue;
            }
            else if (cell.isCounter())
            {
                out.writeLong(Long.MIN_VALUE);  // timestampOfLastDelete (not used, and MIN_VALUE is the default)
            }

            out.writeLong(cell.timestamp);
            ByteBufferUtil.writeWithLength(cell.value, out);
        }
    }

    // For the old wire format
    // Note: this can return null if an empty partition is serialized!
add: 23, delete: 2, change: 159, unhandled: 0 size_exceptions: 1 size_serialize: 2
-----------cassandra-3.0.3 vs cassandra-3.0.4-----------
add: 6, delete: 0, change: 134, unhandled: 0 size_exceptions: 0 size_serialize: 0
-----------cassandra-3.0.4 vs cassandra-3.0.5-----------
add: 14, delete: 2, change: 111, unhandled: 0 size_exceptions: 0 size_serialize: 0
-----------cassandra-3.0.5 vs cassandra-3.0.6-----------
    private void serializePartition(UnfilteredRowIterator partition)
    {
        String key = metadata.getKeyValidator().getString(partition.partitionKey().getKey());
        try
        {
            json.writeStartObject();

            json.writeFieldName("partition");
            json.writeStartObject();
            json.writeFieldName("key");
            serializePartitionKey(partition.partitionKey());
            json.writeNumberField("position", this.currentScanner.getCurrentPosition());

            if (!partition.partitionLevelDeletion().isLive())
            {
                json.writeFieldName("deletion_info");
                objectIndenter.setCompact(true);
                json.writeStartObject();
                json.writeFieldName("deletion_time");
                json.writeNumber(partition.partitionLevelDeletion().markedForDeleteAt());
                json.writeFieldName("tstamp");
                json.writeNumber(partition.partitionLevelDeletion().localDeletionTime());
                json.writeEndObject();
                objectIndenter.setCompact(false);
                json.writeEndObject();
            }
            else
            {
                json.writeEndObject();
                json.writeFieldName("rows");
                json.writeStartArray();
                updatePosition();
                if (!partition.staticRow().isEmpty())
                {
                    serializeRow(partition.staticRow());
                }
                Unfiltered unfiltered;
                updatePosition();
                while (partition.hasNext())
                {
                    unfiltered = partition.next();
                    if (unfiltered instanceof Row)
                    {
                        serializeRow((Row) unfiltered);
                    }
                    else if (unfiltered instanceof RangeTombstoneMarker)
                    {
                        serializeTombstone((RangeTombstoneMarker) unfiltered);
                    }
                    updatePosition();
                }
                json.writeEndArray();
            }

            json.writeEndObject();
        }
        catch (IOException e)
        {
            logger.error("Fatal error parsing partition: {}", key, e);
        }
    }

+++++++++++++++++++++++
    private void serializePartition(UnfilteredRowIterator partition)
    {
        String key = metadata.getKeyValidator().getString(partition.partitionKey().getKey());
        try
        {
            json.writeStartObject();

            json.writeFieldName("partition");
            json.writeStartObject();
            json.writeFieldName("key");
            serializePartitionKey(partition.partitionKey());
            json.writeNumberField("position", this.currentScanner.getCurrentPosition());

            if (!partition.partitionLevelDeletion().isLive())
            {
                serializeDeletion(partition.partitionLevelDeletion());
            }
            else
            {
                json.writeEndObject();
                json.writeFieldName("rows");
                json.writeStartArray();
                updatePosition();
                if (!partition.staticRow().isEmpty())
                {
                    serializeRow(partition.staticRow());
                }
                Unfiltered unfiltered;
                updatePosition();
                while (partition.hasNext())
                {
                    unfiltered = partition.next();
                    if (unfiltered instanceof Row)
                    {
                        serializeRow((Row) unfiltered);
                    }
                    else if (unfiltered instanceof RangeTombstoneMarker)
                    {
                        serializeTombstone((RangeTombstoneMarker) unfiltered);
                    }
                    updatePosition();
                }
                json.writeEndArray();
            }

            json.writeEndObject();
        }
        catch (IOException e)
        {
            logger.error("Fatal error parsing partition: {}", key, e);
        }
    }

    private void serializeRow(Row row)
    {
        try
        {
            json.writeStartObject();
            String rowType = row.isStatic() ? "static_block" : "row";
            json.writeFieldName("type");
            json.writeString(rowType);
            json.writeNumberField("position", this.currentPosition);

            // Only print clustering information for non-static rows.
            if (!row.isStatic())
            {
                serializeClustering(row.clustering());
            }

            LivenessInfo liveInfo = row.primaryKeyLivenessInfo();
            if (!liveInfo.isEmpty())
            {
                objectIndenter.setCompact(false);
                json.writeFieldName("liveness_info");
                objectIndenter.setCompact(true);
                json.writeStartObject();
                json.writeFieldName("tstamp");
                json.writeNumber(liveInfo.timestamp());
                if (liveInfo.isExpiring())
                {
                    json.writeFieldName("ttl");
                    json.writeNumber(liveInfo.ttl());
                    json.writeFieldName("expires_at");
                    json.writeNumber(liveInfo.localExpirationTime());
                    json.writeFieldName("expired");
                    json.writeBoolean(liveInfo.localExpirationTime() < (System.currentTimeMillis() / 1000));
                }
                json.writeEndObject();
                objectIndenter.setCompact(false);
            }

            // If this is a deletion, indicate that, otherwise write cells.
            if (!row.deletion().isLive())
            {
                json.writeFieldName("deletion_info");
                objectIndenter.setCompact(true);
                json.writeStartObject();
                json.writeFieldName("deletion_time");
                json.writeNumber(row.deletion().time().markedForDeleteAt());
                json.writeFieldName("tstamp");
                json.writeNumber(row.deletion().time().localDeletionTime());
                json.writeEndObject();
                objectIndenter.setCompact(false);
            }
            else
            {
                json.writeFieldName("cells");
                json.writeStartArray();
                row.cells().forEach(c -> serializeCell(c, liveInfo));
                json.writeEndArray();
            }
            json.writeEndObject();
        }
        catch (IOException e)
        {
            logger.error("Fatal error parsing row.", e);
        }
    }

+++++++++++++++++++++++
    private void serializeRow(Row row)
    {
        try
        {
            json.writeStartObject();
            String rowType = row.isStatic() ? "static_block" : "row";
            json.writeFieldName("type");
            json.writeString(rowType);
            json.writeNumberField("position", this.currentPosition);

            // Only print clustering information for non-static rows.
            if (!row.isStatic())
            {
                serializeClustering(row.clustering());
            }

            LivenessInfo liveInfo = row.primaryKeyLivenessInfo();
            if (!liveInfo.isEmpty())
            {
                objectIndenter.setCompact(false);
                json.writeFieldName("liveness_info");
                objectIndenter.setCompact(true);
                json.writeStartObject();
                json.writeFieldName("tstamp");
                json.writeString(dateString(TimeUnit.MICROSECONDS, liveInfo.timestamp()));
                if (liveInfo.isExpiring())
                {
                    json.writeNumberField("ttl", liveInfo.ttl());
                    json.writeFieldName("expires_at");
                    json.writeString(dateString(TimeUnit.SECONDS, liveInfo.localExpirationTime()));
                    json.writeFieldName("expired");
                    json.writeBoolean(liveInfo.localExpirationTime() < (System.currentTimeMillis() / 1000));
                }
                json.writeEndObject();
                objectIndenter.setCompact(false);
            }

            // If this is a deletion, indicate that, otherwise write cells.
            if (!row.deletion().isLive())
            {
                serializeDeletion(row.deletion().time());
            }
            json.writeFieldName("cells");
            json.writeStartArray();
            for (ColumnData cd : row)
            {
                serializeColumnData(cd, liveInfo);
            }
            json.writeEndArray();
            json.writeEndObject();
        }
        catch (IOException e)
        {
            logger.error("Fatal error parsing row.", e);
        }
    }

    private void serializeCell(Cell cell, LivenessInfo liveInfo)
    {
        try
        {
            json.writeStartObject();
            objectIndenter.setCompact(true);
            json.writeFieldName("name");
            AbstractType<?> type = cell.column().type;
            json.writeString(cell.column().name.toCQLString());

            if (cell.path() != null && cell.path().size() > 0)
            {
                CollectionType ct = (CollectionType) type;
                json.writeFieldName("path");
                arrayIndenter.setCompact(true);
                json.writeStartArray();
                for (int i = 0; i < cell.path().size(); i++)
                {
                    json.writeString(ct.nameComparator().getString(cell.path().get(i)));
                }
                json.writeEndArray();
                arrayIndenter.setCompact(false);
            }
            if (cell.isTombstone())
            {
                json.writeFieldName("deletion_time");
                json.writeNumber(cell.localDeletionTime());
            }
            else
            {
                json.writeFieldName("value");
                json.writeString(cell.column().cellValueType().getString(cell.value()));
            }
            if (liveInfo.isEmpty() || cell.timestamp() != liveInfo.timestamp())
            {
                json.writeFieldName("tstamp");
                json.writeNumber(cell.timestamp());
            }
            if (cell.isExpiring() && (liveInfo.isEmpty() || cell.ttl() != liveInfo.ttl()))
            {
                json.writeFieldName("ttl");
                json.writeNumber(cell.ttl());
                json.writeFieldName("expires_at");
                json.writeNumber(cell.localDeletionTime());
                json.writeFieldName("expired");
                json.writeBoolean(!cell.isLive((int) (System.currentTimeMillis() / 1000)));
            }
            json.writeEndObject();
            objectIndenter.setCompact(false);
        }
        catch (IOException e)
        {
            logger.error("Failure parsing cell.", e);
        }
    }

    /**
     * A specialized {@link Indenter} that enables a 'compact' mode which puts all subsequent json values on the same
     * line. This is manipulated via {@link CompactIndenter#setCompact(boolean)}
     */
+++++++++++++++++++++++
    private void serializeCell(Cell cell, LivenessInfo liveInfo)
    {
        try
        {
            json.writeStartObject();
            objectIndenter.setCompact(true);
            json.writeFieldName("name");
            AbstractType<?> type = cell.column().type;
            json.writeString(cell.column().name.toCQLString());

            if (cell.path() != null && cell.path().size() > 0)
            {
                CollectionType ct = (CollectionType) type;
                json.writeFieldName("path");
                arrayIndenter.setCompact(true);
                json.writeStartArray();
                for (int i = 0; i < cell.path().size(); i++)
                {
                    json.writeString(ct.nameComparator().getString(cell.path().get(i)));
                }
                json.writeEndArray();
                arrayIndenter.setCompact(false);
            }
            if (cell.isTombstone())
            {
                json.writeFieldName("deletion_info");
                objectIndenter.setCompact(true);
                json.writeStartObject();
                json.writeFieldName("local_delete_time");
                json.writeString(dateString(TimeUnit.SECONDS, cell.localDeletionTime()));
                json.writeEndObject();
                objectIndenter.setCompact(false);
            }
            else
            {
                json.writeFieldName("value");
                json.writeString(cell.column().cellValueType().getString(cell.value()));
            }
            if (liveInfo.isEmpty() || cell.timestamp() != liveInfo.timestamp())
            {
                json.writeFieldName("tstamp");
                json.writeString(dateString(TimeUnit.MICROSECONDS, cell.timestamp()));
            }
            if (cell.isExpiring() && (liveInfo.isEmpty() || cell.ttl() != liveInfo.ttl()))
            {
                json.writeFieldName("ttl");
                json.writeNumber(cell.ttl());
                json.writeFieldName("expires_at");
                json.writeString(dateString(TimeUnit.SECONDS, cell.localDeletionTime()));
                json.writeFieldName("expired");
                json.writeBoolean(!cell.isLive((int) (System.currentTimeMillis() / 1000)));
            }
            json.writeEndObject();
            objectIndenter.setCompact(false);
        }
        catch (IOException e)
        {
            logger.error("Failure parsing cell.", e);
        }
    }

    private void serializeDeletion(DeletionTime deletion) throws IOException
    {
        json.writeFieldName("deletion_info");
        objectIndenter.setCompact(true);
        json.writeStartObject();
        json.writeFieldName("deletion_time");
        json.writeNumber(deletion.markedForDeleteAt());
        json.writeFieldName("tstamp");
        json.writeNumber(deletion.localDeletionTime());
        json.writeEndObject();
        objectIndenter.setCompact(false);
    }

+++++++++++++++++++++++
    private void serializeDeletion(DeletionTime deletion) throws IOException
    {
        json.writeFieldName("deletion_info");
        objectIndenter.setCompact(true);
        json.writeStartObject();
        json.writeFieldName("marked_deleted");
        json.writeString(dateString(TimeUnit.MICROSECONDS, deletion.markedForDeleteAt()));
        json.writeFieldName("local_delete_time");
        json.writeString(dateString(TimeUnit.SECONDS, deletion.localDeletionTime()));
        json.writeEndObject();
        objectIndenter.setCompact(false);
    }

add: 3, delete: 2, change: 111, unhandled: 1 size_exceptions: 0 size_serialize: 4
-----------cassandra-3.0.6 vs cassandra-3.0.7-----------
    public Unfiltered deserialize(DataInputPlus in, SerializationHeader header, SerializationHelper helper, Row.Builder builder)
    throws IOException
    {
        // It wouldn't be wrong per-se to use an unsorted builder, but it would be inefficient so make sure we don't do it by mistake
        assert builder.isSorted();

        int flags = in.readUnsignedByte();
        if (isEndOfPartition(flags))
            return null;

        int extendedFlags = readExtendedFlags(in, flags);

        if (kind(flags) == Unfiltered.Kind.RANGE_TOMBSTONE_MARKER)
        {
            RangeTombstone.Bound bound = RangeTombstone.Bound.serializer.deserialize(in, helper.version, header.clusteringTypes());
            return deserializeMarkerBody(in, header, bound);
        }
        else
        {
            assert !isStatic(extendedFlags); // deserializeStaticRow should be used for that.
            builder.newRow(Clustering.serializer.deserialize(in, helper.version, header.clusteringTypes()));
            return deserializeRowBody(in, header, helper, flags, extendedFlags, builder);
        }
    }

+++++++++++++++++++++++
    public Unfiltered deserialize(DataInputPlus in, SerializationHeader header, SerializationHelper helper, Row.Builder builder)
    throws IOException
    {
        // It wouldn't be wrong per-se to use an unsorted builder, but it would be inefficient so make sure we don't do it by mistake
        assert builder.isSorted();

        int flags = in.readUnsignedByte();
        if (isEndOfPartition(flags))
            return null;

        int extendedFlags = readExtendedFlags(in, flags);

        if (kind(flags) == Unfiltered.Kind.RANGE_TOMBSTONE_MARKER)
        {
            RangeTombstone.Bound bound = RangeTombstone.Bound.serializer.deserialize(in, helper.version, header.clusteringTypes());
            return deserializeMarkerBody(in, header, bound);
        }
        else
        {
            // deserializeStaticRow should be used for that.
            if (isStatic(extendedFlags))
                throw new IOException("Corrupt flags value for unfiltered partition (isStatic flag set): " + flags);

            builder.newRow(Clustering.serializer.deserialize(in, helper.version, header.clusteringTypes()));
            Row row = deserializeRowBody(in, header, helper, flags, extendedFlags, builder);
            // we do not write empty rows because Rows.collectStats(), called by BTW.applyToRow(), asserts that rows are not empty
            // if we don't throw here, then later the very same assertion in Rows.collectStats() will fail compactions
            // see BlackListingCompactionsTest and CASSANDRA-9530 for details
            if (row.isEmpty())
                throw new IOException("Corrupt empty row found in unfiltered partition");
            return row;
        }
    }

    public void serialize(Map<MetadataType, MetadataComponent> components, DataOutputPlus out, Version version) throws IOException
    {
        ValidationMetadata validation = (ValidationMetadata) components.get(MetadataType.VALIDATION);
        StatsMetadata stats = (StatsMetadata) components.get(MetadataType.STATS);
        CompactionMetadata compaction = (CompactionMetadata) components.get(MetadataType.COMPACTION);

        assert validation != null && stats != null && compaction != null && validation.partitioner != null;

        EstimatedHistogram.serializer.serialize(stats.estimatedPartitionSize, out);
        EstimatedHistogram.serializer.serialize(stats.estimatedColumnCount, out);
        ReplayPosition.serializer.serialize(stats.replayPosition, out);
        out.writeLong(stats.minTimestamp);
        out.writeLong(stats.maxTimestamp);
        out.writeInt(stats.maxLocalDeletionTime);
        out.writeDouble(validation.bloomFilterFPChance);
        out.writeDouble(stats.compressionRatio);
        out.writeUTF(validation.partitioner);
        out.writeInt(0); // compaction ancestors
        StreamingHistogram.serializer.serialize(stats.estimatedTombstoneDropTime, out);
        out.writeInt(stats.sstableLevel);
        out.writeInt(stats.minClusteringValues.size());
        for (ByteBuffer value : stats.minClusteringValues)
            ByteBufferUtil.writeWithShortLength(value, out);
        out.writeInt(stats.maxClusteringValues.size());
        for (ByteBuffer value : stats.maxClusteringValues)
            ByteBufferUtil.writeWithShortLength(value, out);
    }

    /**
     * Legacy serializer deserialize all components no matter what types are specified.
     */
    @Override
+++++++++++++++++++++++
    public void serialize(Map<MetadataType, MetadataComponent> components, DataOutputPlus out, Version version) throws IOException
    {
        ValidationMetadata validation = (ValidationMetadata) components.get(MetadataType.VALIDATION);
        StatsMetadata stats = (StatsMetadata) components.get(MetadataType.STATS);
        CompactionMetadata compaction = (CompactionMetadata) components.get(MetadataType.COMPACTION);

        assert validation != null && stats != null && compaction != null && validation.partitioner != null;

        EstimatedHistogram.serializer.serialize(stats.estimatedPartitionSize, out);
        EstimatedHistogram.serializer.serialize(stats.estimatedColumnCount, out);
        ReplayPosition.serializer.serialize(stats.commitLogUpperBound, out);
        out.writeLong(stats.minTimestamp);
        out.writeLong(stats.maxTimestamp);
        out.writeInt(stats.maxLocalDeletionTime);
        out.writeDouble(validation.bloomFilterFPChance);
        out.writeDouble(stats.compressionRatio);
        out.writeUTF(validation.partitioner);
        out.writeInt(0); // compaction ancestors
        StreamingHistogram.serializer.serialize(stats.estimatedTombstoneDropTime, out);
        out.writeInt(stats.sstableLevel);
        out.writeInt(stats.minClusteringValues.size());
        for (ByteBuffer value : stats.minClusteringValues)
            ByteBufferUtil.writeWithShortLength(value, out);
        out.writeInt(stats.maxClusteringValues.size());
        for (ByteBuffer value : stats.maxClusteringValues)
            ByteBufferUtil.writeWithShortLength(value, out);
        if (version.hasCommitLogLowerBound())
            ReplayPosition.serializer.serialize(stats.commitLogLowerBound, out);
    }

    /**
     * Legacy serializer deserialize all components no matter what types are specified.
     */
    @Override
    public Map<MetadataType, MetadataComponent> deserialize(Descriptor descriptor, EnumSet<MetadataType> types) throws IOException
    {
        Map<MetadataType, MetadataComponent> components = Maps.newHashMap();

        File statsFile = new File(descriptor.filenameFor(Component.STATS));
        if (!statsFile.exists() && types.contains(MetadataType.STATS))
        {
            components.put(MetadataType.STATS, MetadataCollector.defaultStatsMetadata());
        }
        else
        {
            try (DataInputStreamPlus in = new DataInputStreamPlus(new BufferedInputStream(new FileInputStream(statsFile))))
            {
                EstimatedHistogram partitionSizes = EstimatedHistogram.serializer.deserialize(in);
                EstimatedHistogram columnCounts = EstimatedHistogram.serializer.deserialize(in);
                ReplayPosition replayPosition = ReplayPosition.serializer.deserialize(in);
                long minTimestamp = in.readLong();
                long maxTimestamp = in.readLong();
                int maxLocalDeletionTime = in.readInt();
                double bloomFilterFPChance = in.readDouble();
                double compressionRatio = in.readDouble();
                String partitioner = in.readUTF();
                int nbAncestors = in.readInt(); //skip compaction ancestors
                in.skipBytes(nbAncestors * TypeSizes.sizeof(nbAncestors));
                StreamingHistogram tombstoneHistogram = StreamingHistogram.serializer.deserialize(in);
                int sstableLevel = 0;
                if (in.available() > 0)
                    sstableLevel = in.readInt();

                int colCount = in.readInt();
                List<ByteBuffer> minColumnNames = new ArrayList<>(colCount);
                for (int i = 0; i < colCount; i++)
                    minColumnNames.add(ByteBufferUtil.readWithShortLength(in));

                colCount = in.readInt();
                List<ByteBuffer> maxColumnNames = new ArrayList<>(colCount);
                for (int i = 0; i < colCount; i++)
                    maxColumnNames.add(ByteBufferUtil.readWithShortLength(in));

                if (types.contains(MetadataType.VALIDATION))
                    components.put(MetadataType.VALIDATION,
                                   new ValidationMetadata(partitioner, bloomFilterFPChance));
                if (types.contains(MetadataType.STATS))
                    components.put(MetadataType.STATS,
                                   new StatsMetadata(partitionSizes,
                                                     columnCounts,
                                                     replayPosition,
                                                     minTimestamp,
                                                     maxTimestamp,
                                                     Integer.MAX_VALUE,
                                                     maxLocalDeletionTime,
                                                     0,
                                                     Integer.MAX_VALUE,
                                                     compressionRatio,
                                                     tombstoneHistogram,
                                                     sstableLevel,
                                                     minColumnNames,
                                                     maxColumnNames,
                                                     true,
                                                     ActiveRepairService.UNREPAIRED_SSTABLE,
                                                     -1,
                                                     -1));
                if (types.contains(MetadataType.COMPACTION))
                    components.put(MetadataType.COMPACTION,
                                   new CompactionMetadata(null));
            }
        }
        return components;
+++++++++++++++++++++++
    public Map<MetadataType, MetadataComponent> deserialize(Descriptor descriptor, EnumSet<MetadataType> types) throws IOException
    {
        Map<MetadataType, MetadataComponent> components = Maps.newHashMap();

        File statsFile = new File(descriptor.filenameFor(Component.STATS));
        if (!statsFile.exists() && types.contains(MetadataType.STATS))
        {
            components.put(MetadataType.STATS, MetadataCollector.defaultStatsMetadata());
        }
        else
        {
            try (DataInputStreamPlus in = new DataInputStreamPlus(new BufferedInputStream(new FileInputStream(statsFile))))
            {
                EstimatedHistogram partitionSizes = EstimatedHistogram.serializer.deserialize(in);
                EstimatedHistogram columnCounts = EstimatedHistogram.serializer.deserialize(in);
                ReplayPosition commitLogLowerBound = ReplayPosition.NONE;
                ReplayPosition commitLogUpperBound = ReplayPosition.serializer.deserialize(in);
                long minTimestamp = in.readLong();
                long maxTimestamp = in.readLong();
                int maxLocalDeletionTime = in.readInt();
                double bloomFilterFPChance = in.readDouble();
                double compressionRatio = in.readDouble();
                String partitioner = in.readUTF();
                int nbAncestors = in.readInt(); //skip compaction ancestors
                in.skipBytes(nbAncestors * TypeSizes.sizeof(nbAncestors));
                StreamingHistogram tombstoneHistogram = StreamingHistogram.serializer.deserialize(in);
                int sstableLevel = 0;
                if (in.available() > 0)
                    sstableLevel = in.readInt();

                int colCount = in.readInt();
                List<ByteBuffer> minColumnNames = new ArrayList<>(colCount);
                for (int i = 0; i < colCount; i++)
                    minColumnNames.add(ByteBufferUtil.readWithShortLength(in));

                colCount = in.readInt();
                List<ByteBuffer> maxColumnNames = new ArrayList<>(colCount);
                for (int i = 0; i < colCount; i++)
                    maxColumnNames.add(ByteBufferUtil.readWithShortLength(in));

                if (descriptor.version.hasCommitLogLowerBound())
                    commitLogLowerBound = ReplayPosition.serializer.deserialize(in);

                if (types.contains(MetadataType.VALIDATION))
                    components.put(MetadataType.VALIDATION,
                                   new ValidationMetadata(partitioner, bloomFilterFPChance));
                if (types.contains(MetadataType.STATS))
                    components.put(MetadataType.STATS,
                                   new StatsMetadata(partitionSizes,
                                                     columnCounts,
                                                     commitLogLowerBound,
                                                     commitLogUpperBound,
                                                     minTimestamp,
                                                     maxTimestamp,
                                                     Integer.MAX_VALUE,
                                                     maxLocalDeletionTime,
                                                     0,
                                                     Integer.MAX_VALUE,
                                                     compressionRatio,
                                                     tombstoneHistogram,
                                                     sstableLevel,
                                                     minColumnNames,
                                                     maxColumnNames,
                                                     true,
                                                     ActiveRepairService.UNREPAIRED_SSTABLE,
                                                     -1,
                                                     -1));
                if (types.contains(MetadataType.COMPACTION))
                    components.put(MetadataType.COMPACTION,
                                   new CompactionMetadata(null));
            }
        }
        return components;
add: 8, delete: 1, change: 110, unhandled: 1 size_exceptions: 0 size_serialize: 3
-----------cassandra-3.0.7 vs cassandra-3.0.8-----------
add: 4, delete: 0, change: 105, unhandled: 0 size_exceptions: 2 size_serialize: 0
-----------cassandra-3.0.8 vs cassandra-3.0.9-----------
    private long serializedRowBodySize(Row row, SerializationHeader header, long previousUnfilteredSize, int version)
    {
        long size = 0;

        if (header.isForSSTable())
            size += TypeSizes.sizeofUnsignedVInt(previousUnfilteredSize);

        boolean isStatic = row.isStatic();
        Columns headerColumns = header.columns(isStatic);
        LivenessInfo pkLiveness = row.primaryKeyLivenessInfo();
        Row.Deletion deletion = row.deletion();
        boolean hasComplexDeletion = row.hasComplexDeletion();
        boolean hasAllColumns = (row.size() == headerColumns.size());

        if (!pkLiveness.isEmpty())
            size += header.timestampSerializedSize(pkLiveness.timestamp());
        if (pkLiveness.isExpiring())
        {
            size += header.ttlSerializedSize(pkLiveness.ttl());
            size += header.localDeletionTimeSerializedSize(pkLiveness.localExpirationTime());
        }
        if (!deletion.isLive())
            size += header.deletionTimeSerializedSize(deletion.time());

        if (!hasAllColumns)
            size += Columns.serializer.serializedSubsetSize(Collections2.transform(row, ColumnData::column), header.columns(isStatic));

        for (ColumnData data : row)
        {
            if (data.column.isSimple())
                size += Cell.serializer.serializedSize((Cell) data, pkLiveness, header);
            else
                size += sizeOfComplexColumn((ComplexColumnData) data, hasComplexDeletion, pkLiveness, header);
        }

        return size;
    }

+++++++++++++++++++++++
    private long serializedRowBodySize(Row row, SerializationHeader header, long previousUnfilteredSize, int version)
    {
        long size = 0;

        if (header.isForSSTable())
            size += TypeSizes.sizeofUnsignedVInt(previousUnfilteredSize);

        boolean isStatic = row.isStatic();
        Columns headerColumns = header.columns(isStatic);
        LivenessInfo pkLiveness = row.primaryKeyLivenessInfo();
        Row.Deletion deletion = row.deletion();
        boolean hasComplexDeletion = row.hasComplexDeletion();
        boolean hasAllColumns = (row.size() == headerColumns.size());

        if (!pkLiveness.isEmpty())
            size += header.timestampSerializedSize(pkLiveness.timestamp());
        if (pkLiveness.isExpiring())
        {
            size += header.ttlSerializedSize(pkLiveness.ttl());
            size += header.localDeletionTimeSerializedSize(pkLiveness.localExpirationTime());
        }
        if (!deletion.isLive())
            size += header.deletionTimeSerializedSize(deletion.time());

        if (!hasAllColumns)
            size += Columns.serializer.serializedSubsetSize(Collections2.transform(row, ColumnData::column), header.columns(isStatic));

        SearchIterator<ColumnDefinition, ColumnDefinition> si = headerColumns.iterator();
        for (ColumnData data : row)
        {
            ColumnDefinition column = si.next(data.column());
            assert column != null;

            if (data.column.isSimple())
                size += Cell.serializer.serializedSize((Cell) data, column, pkLiveness, header);
            else
                size += sizeOfComplexColumn((ComplexColumnData) data, column, hasComplexDeletion, pkLiveness, header);
        }

        return size;
    }

    public void serialize(Map<MetadataType, MetadataComponent> components, DataOutputPlus out, Version version) throws IOException
    {
        ValidationMetadata validation = (ValidationMetadata) components.get(MetadataType.VALIDATION);
        StatsMetadata stats = (StatsMetadata) components.get(MetadataType.STATS);
        CompactionMetadata compaction = (CompactionMetadata) components.get(MetadataType.COMPACTION);

        assert validation != null && stats != null && compaction != null && validation.partitioner != null;

        EstimatedHistogram.serializer.serialize(stats.estimatedPartitionSize, out);
        EstimatedHistogram.serializer.serialize(stats.estimatedColumnCount, out);
        ReplayPosition.serializer.serialize(stats.commitLogUpperBound, out);
        out.writeLong(stats.minTimestamp);
        out.writeLong(stats.maxTimestamp);
        out.writeInt(stats.maxLocalDeletionTime);
        out.writeDouble(validation.bloomFilterFPChance);
        out.writeDouble(stats.compressionRatio);
        out.writeUTF(validation.partitioner);
        out.writeInt(0); // compaction ancestors
        StreamingHistogram.serializer.serialize(stats.estimatedTombstoneDropTime, out);
        out.writeInt(stats.sstableLevel);
        out.writeInt(stats.minClusteringValues.size());
        for (ByteBuffer value : stats.minClusteringValues)
            ByteBufferUtil.writeWithShortLength(value, out);
        out.writeInt(stats.maxClusteringValues.size());
        for (ByteBuffer value : stats.maxClusteringValues)
            ByteBufferUtil.writeWithShortLength(value, out);
        if (version.hasCommitLogLowerBound())
            ReplayPosition.serializer.serialize(stats.commitLogLowerBound, out);
    }

    /**
     * Legacy serializer deserialize all components no matter what types are specified.
     */
    @Override
+++++++++++++++++++++++
    public void serialize(Map<MetadataType, MetadataComponent> components, DataOutputPlus out, Version version) throws IOException
    {
        ValidationMetadata validation = (ValidationMetadata) components.get(MetadataType.VALIDATION);
        StatsMetadata stats = (StatsMetadata) components.get(MetadataType.STATS);
        CompactionMetadata compaction = (CompactionMetadata) components.get(MetadataType.COMPACTION);

        assert validation != null && stats != null && compaction != null && validation.partitioner != null;

        EstimatedHistogram.serializer.serialize(stats.estimatedPartitionSize, out);
        EstimatedHistogram.serializer.serialize(stats.estimatedColumnCount, out);
        ReplayPosition.serializer.serialize(stats.commitLogIntervals.upperBound().orElse(ReplayPosition.NONE), out);
        out.writeLong(stats.minTimestamp);
        out.writeLong(stats.maxTimestamp);
        out.writeInt(stats.maxLocalDeletionTime);
        out.writeDouble(validation.bloomFilterFPChance);
        out.writeDouble(stats.compressionRatio);
        out.writeUTF(validation.partitioner);
        out.writeInt(0); // compaction ancestors
        StreamingHistogram.serializer.serialize(stats.estimatedTombstoneDropTime, out);
        out.writeInt(stats.sstableLevel);
        out.writeInt(stats.minClusteringValues.size());
        for (ByteBuffer value : stats.minClusteringValues)
            ByteBufferUtil.writeWithShortLength(value, out);
        out.writeInt(stats.maxClusteringValues.size());
        for (ByteBuffer value : stats.maxClusteringValues)
            ByteBufferUtil.writeWithShortLength(value, out);
        if (version.hasCommitLogLowerBound())
            ReplayPosition.serializer.serialize(stats.commitLogIntervals.lowerBound().orElse(ReplayPosition.NONE), out);
        if (version.hasCommitLogIntervals())
            replayPositionSetSerializer.serialize(stats.commitLogIntervals, out);
    }

    /**
     * Legacy serializer deserialize all components no matter what types are specified.
     */
    @Override
    public Map<MetadataType, MetadataComponent> deserialize(Descriptor descriptor, EnumSet<MetadataType> types) throws IOException
    {
        Map<MetadataType, MetadataComponent> components = Maps.newHashMap();

        File statsFile = new File(descriptor.filenameFor(Component.STATS));
        if (!statsFile.exists() && types.contains(MetadataType.STATS))
        {
            components.put(MetadataType.STATS, MetadataCollector.defaultStatsMetadata());
        }
        else
        {
            try (DataInputStreamPlus in = new DataInputStreamPlus(new BufferedInputStream(new FileInputStream(statsFile))))
            {
                EstimatedHistogram partitionSizes = EstimatedHistogram.serializer.deserialize(in);
                EstimatedHistogram columnCounts = EstimatedHistogram.serializer.deserialize(in);
                ReplayPosition commitLogLowerBound = ReplayPosition.NONE;
                ReplayPosition commitLogUpperBound = ReplayPosition.serializer.deserialize(in);
                long minTimestamp = in.readLong();
                long maxTimestamp = in.readLong();
                int maxLocalDeletionTime = in.readInt();
                double bloomFilterFPChance = in.readDouble();
                double compressionRatio = in.readDouble();
                String partitioner = in.readUTF();
                int nbAncestors = in.readInt(); //skip compaction ancestors
                in.skipBytes(nbAncestors * TypeSizes.sizeof(nbAncestors));
                StreamingHistogram tombstoneHistogram = StreamingHistogram.serializer.deserialize(in);
                int sstableLevel = 0;
                if (in.available() > 0)
                    sstableLevel = in.readInt();

                int colCount = in.readInt();
                List<ByteBuffer> minColumnNames = new ArrayList<>(colCount);
                for (int i = 0; i < colCount; i++)
                    minColumnNames.add(ByteBufferUtil.readWithShortLength(in));

                colCount = in.readInt();
                List<ByteBuffer> maxColumnNames = new ArrayList<>(colCount);
                for (int i = 0; i < colCount; i++)
                    maxColumnNames.add(ByteBufferUtil.readWithShortLength(in));

                if (descriptor.version.hasCommitLogLowerBound())
                    commitLogLowerBound = ReplayPosition.serializer.deserialize(in);

                if (types.contains(MetadataType.VALIDATION))
                    components.put(MetadataType.VALIDATION,
                                   new ValidationMetadata(partitioner, bloomFilterFPChance));
                if (types.contains(MetadataType.STATS))
                    components.put(MetadataType.STATS,
                                   new StatsMetadata(partitionSizes,
                                                     columnCounts,
                                                     commitLogLowerBound,
                                                     commitLogUpperBound,
                                                     minTimestamp,
                                                     maxTimestamp,
                                                     Integer.MAX_VALUE,
                                                     maxLocalDeletionTime,
                                                     0,
                                                     Integer.MAX_VALUE,
                                                     compressionRatio,
                                                     tombstoneHistogram,
                                                     sstableLevel,
                                                     minColumnNames,
                                                     maxColumnNames,
                                                     true,
                                                     ActiveRepairService.UNREPAIRED_SSTABLE,
                                                     -1,
                                                     -1));
                if (types.contains(MetadataType.COMPACTION))
                    components.put(MetadataType.COMPACTION,
                                   new CompactionMetadata(null));
            }
        }
        return components;
+++++++++++++++++++++++
    public Map<MetadataType, MetadataComponent> deserialize(Descriptor descriptor, EnumSet<MetadataType> types) throws IOException
    {
        Map<MetadataType, MetadataComponent> components = Maps.newHashMap();

        File statsFile = new File(descriptor.filenameFor(Component.STATS));
        if (!statsFile.exists() && types.contains(MetadataType.STATS))
        {
            components.put(MetadataType.STATS, MetadataCollector.defaultStatsMetadata());
        }
        else
        {
            try (DataInputStreamPlus in = new DataInputStreamPlus(new BufferedInputStream(new FileInputStream(statsFile))))
            {
                EstimatedHistogram partitionSizes = EstimatedHistogram.serializer.deserialize(in);
                EstimatedHistogram columnCounts = EstimatedHistogram.serializer.deserialize(in);
                ReplayPosition commitLogLowerBound = ReplayPosition.NONE;
                ReplayPosition commitLogUpperBound = ReplayPosition.serializer.deserialize(in);
                long minTimestamp = in.readLong();
                long maxTimestamp = in.readLong();
                int maxLocalDeletionTime = in.readInt();
                double bloomFilterFPChance = in.readDouble();
                double compressionRatio = in.readDouble();
                String partitioner = in.readUTF();
                int nbAncestors = in.readInt(); //skip compaction ancestors
                in.skipBytes(nbAncestors * TypeSizes.sizeof(nbAncestors));
                StreamingHistogram tombstoneHistogram = StreamingHistogram.serializer.deserialize(in);
                int sstableLevel = 0;
                if (in.available() > 0)
                    sstableLevel = in.readInt();

                int colCount = in.readInt();
                List<ByteBuffer> minColumnNames = new ArrayList<>(colCount);
                for (int i = 0; i < colCount; i++)
                    minColumnNames.add(ByteBufferUtil.readWithShortLength(in));

                colCount = in.readInt();
                List<ByteBuffer> maxColumnNames = new ArrayList<>(colCount);
                for (int i = 0; i < colCount; i++)
                    maxColumnNames.add(ByteBufferUtil.readWithShortLength(in));

                if (descriptor.version.hasCommitLogLowerBound())
                    commitLogLowerBound = ReplayPosition.serializer.deserialize(in);
                IntervalSet<ReplayPosition> commitLogIntervals;
                if (descriptor.version.hasCommitLogIntervals())
                    commitLogIntervals = replayPositionSetSerializer.deserialize(in);
                else
                    commitLogIntervals = new IntervalSet<>(commitLogLowerBound, commitLogUpperBound);

                if (types.contains(MetadataType.VALIDATION))
                    components.put(MetadataType.VALIDATION,
                                   new ValidationMetadata(partitioner, bloomFilterFPChance));
                if (types.contains(MetadataType.STATS))
                    components.put(MetadataType.STATS,
                                   new StatsMetadata(partitionSizes,
                                                     columnCounts,
                                                     commitLogIntervals,
                                                     minTimestamp,
                                                     maxTimestamp,
                                                     Integer.MAX_VALUE,
                                                     maxLocalDeletionTime,
                                                     0,
                                                     Integer.MAX_VALUE,
                                                     compressionRatio,
                                                     tombstoneHistogram,
                                                     sstableLevel,
                                                     minColumnNames,
                                                     maxColumnNames,
                                                     true,
                                                     ActiveRepairService.UNREPAIRED_SSTABLE,
                                                     -1,
                                                     -1));
                if (types.contains(MetadataType.COMPACTION))
                    components.put(MetadataType.COMPACTION,
                                   new CompactionMetadata(null));
            }
        }
        return components;
    public void serialize(DataOutputPlus out, int version) throws IOException
    {
        CompactEndpointSerializationHelper.serialize(from, out);

        out.writeInt(verb.ordinal());
        out.writeInt(parameters.size());
        for (Map.Entry<String, byte[]> entry : parameters.entrySet())
        {
            out.writeUTF(entry.getKey());
            out.writeInt(entry.getValue().length);
            out.write(entry.getValue());
        }

        long longSize = payloadSize(version);
        assert longSize <= Integer.MAX_VALUE; // larger values are supported in sstables but not messages
        out.writeInt((int) longSize);
        if (payload != null)
            serializer.serialize(payload, out, version);
    }

+++++++++++++++++++++++
    public void serialize(DataOutputPlus out, int version) throws IOException
    {
        CompactEndpointSerializationHelper.serialize(from, out);

        out.writeInt(MessagingService.Verb.convertForMessagingServiceVersion(verb, version).ordinal());
        out.writeInt(parameters.size());
        for (Map.Entry<String, byte[]> entry : parameters.entrySet())
        {
            out.writeUTF(entry.getKey());
            out.writeInt(entry.getValue().length);
            out.write(entry.getValue());
        }

        long longSize = payloadSize(version);
        assert longSize <= Integer.MAX_VALUE; // larger values are supported in sstables but not messages
        out.writeInt((int) longSize);
        if (payload != null)
            serializer.serialize(payload, out, version);
    }

    private void serializePartition(UnfilteredRowIterator partition)
    {
        String key = metadata.getKeyValidator().getString(partition.partitionKey().getKey());
        try
        {
            json.writeStartObject();

            json.writeFieldName("partition");
            json.writeStartObject();
            json.writeFieldName("key");
            serializePartitionKey(partition.partitionKey());
            json.writeNumberField("position", this.currentScanner.getCurrentPosition());

            if (!partition.partitionLevelDeletion().isLive())
            {
                serializeDeletion(partition.partitionLevelDeletion());
            }
            else
            {
                json.writeEndObject();
                json.writeFieldName("rows");
                json.writeStartArray();
                updatePosition();
                if (!partition.staticRow().isEmpty())
                {
                    serializeRow(partition.staticRow());
                }
                Unfiltered unfiltered;
                updatePosition();
                while (partition.hasNext())
                {
                    unfiltered = partition.next();
                    if (unfiltered instanceof Row)
                    {
                        serializeRow((Row) unfiltered);
                    }
                    else if (unfiltered instanceof RangeTombstoneMarker)
                    {
                        serializeTombstone((RangeTombstoneMarker) unfiltered);
                    }
                    updatePosition();
                }
                json.writeEndArray();
            }

            json.writeEndObject();
        }
        catch (IOException e)
        {
            logger.error("Fatal error parsing partition: {}", key, e);
        }
    }

+++++++++++++++++++++++
    private void serializePartition(UnfilteredRowIterator partition)
    {
        String key = metadata.getKeyValidator().getString(partition.partitionKey().getKey());
        try
        {
            json.writeStartObject();

            json.writeFieldName("partition");
            json.writeStartObject();
            json.writeFieldName("key");
            serializePartitionKey(partition.partitionKey());
            json.writeNumberField("position", this.currentScanner.getCurrentPosition());

            if (!partition.partitionLevelDeletion().isLive())
            {
                serializeDeletion(partition.partitionLevelDeletion());
                json.writeEndObject();
            }
            else
            {
                json.writeEndObject();
                json.writeFieldName("rows");
                json.writeStartArray();
                updatePosition();
                if (!partition.staticRow().isEmpty())
                {
                    serializeRow(partition.staticRow());
                }
                Unfiltered unfiltered;
                updatePosition();
                while (partition.hasNext())
                {
                    unfiltered = partition.next();
                    if (unfiltered instanceof Row)
                    {
                        serializeRow((Row) unfiltered);
                    }
                    else if (unfiltered instanceof RangeTombstoneMarker)
                    {
                        serializeTombstone((RangeTombstoneMarker) unfiltered);
                    }
                    updatePosition();
                }
                json.writeEndArray();
            }

            json.writeEndObject();
        }
        catch (IOException e)
        {
            logger.error("Fatal error parsing partition: {}", key, e);
        }
    }

add: 10, delete: 1, change: 169, unhandled: 1 size_exceptions: 0 size_serialize: 5
-----------cassandra-3.0.9 vs cassandra-3.0.10-----------
    public Boolean deserialize(ByteBuffer bytes)
    {
        if (bytes.remaining() == 0)
            return null;

        byte value = bytes.get(bytes.position());
        return value != 0;
    }

+++++++++++++++++++++++
    public Boolean deserialize(ByteBuffer bytes)
    {
        if (bytes == null || bytes.remaining() == 0)
            return null;

        byte value = bytes.get(bytes.position());
        return value != 0;
    }

    public List<T> deserializeForNativeProtocol(ByteBuffer bytes, int version)
    {
        try
        {
            ByteBuffer input = bytes.duplicate();
            int n = readCollectionSize(input, version);
            List<T> l = new ArrayList<T>(n);
            for (int i = 0; i < n; i++)
            {
                // We can have nulls in lists that are used for IN values
                ByteBuffer databb = readValue(input, version);
                if (databb != null)
                {
                    elements.validate(databb);
                    l.add(elements.deserialize(databb));
                }
                else
                {
                    l.add(null);
                }
            }

            if (input.hasRemaining())
                throw new MarshalException("Unexpected extraneous bytes after list value");

            return l;
        }
        catch (BufferUnderflowException e)
        {
            throw new MarshalException("Not enough bytes to read a list");
        }
    }

    /**
     * Returns the element at the given index in a list.
     * @param serializedList a serialized list
     * @param index the index to get
     * @return the serialized element at the given index, or null if the index exceeds the list size
     */
+++++++++++++++++++++++
    public List<T> deserializeForNativeProtocol(ByteBuffer bytes, int version)
    {
        try
        {
            ByteBuffer input = bytes.duplicate();
            int n = readCollectionSize(input, version);

            if (n < 0)
                throw new MarshalException("The data cannot be deserialized as a list");

            // If the received bytes are not corresponding to a list, n might be a huge number.
            // In such a case we do not want to initialize the list with that size as it can result
            // in an OOM (see CASSANDRA-12618). On the other hand we do not want to have to resize the list
            // if we can avoid it, so we put a reasonable limit on the initialCapacity.
            List<T> l = new ArrayList<T>(Math.min(n, 256));
            for (int i = 0; i < n; i++)
            {
                // We can have nulls in lists that are used for IN values
                ByteBuffer databb = readValue(input, version);
                if (databb != null)
                {
                    elements.validate(databb);
                    l.add(elements.deserialize(databb));
                }
                else
                {
                    l.add(null);
                }
            }

            if (input.hasRemaining())
                throw new MarshalException("Unexpected extraneous bytes after list value");

            return l;
        }
        catch (BufferUnderflowException e)
        {
            throw new MarshalException("Not enough bytes to read a list");
        }
    }

    /**
     * Returns the element at the given index in a list.
     * @param serializedList a serialized list
     * @param index the index to get
     * @return the serialized element at the given index, or null if the index exceeds the list size
     */
    public Map<K, V> deserializeForNativeProtocol(ByteBuffer bytes, int version)
    {
        try
        {
            ByteBuffer input = bytes.duplicate();
            int n = readCollectionSize(input, version);
            Map<K, V> m = new LinkedHashMap<K, V>(n);
            for (int i = 0; i < n; i++)
            {
                ByteBuffer kbb = readValue(input, version);
                keys.validate(kbb);

                ByteBuffer vbb = readValue(input, version);
                values.validate(vbb);

                m.put(keys.deserialize(kbb), values.deserialize(vbb));
            }
            if (input.hasRemaining())
                throw new MarshalException("Unexpected extraneous bytes after map value");
            return m;
        }
        catch (BufferUnderflowException e)
        {
            throw new MarshalException("Not enough bytes to read a map");
        }
    }

    /**
     * Given a serialized map, gets the value associated with a given key.
     * @param serializedMap a serialized map
     * @param serializedKey a serialized key
     * @param keyType the key type for the map
     * @return the value associated with the key if one exists, null otherwise
     */
+++++++++++++++++++++++
    public Map<K, V> deserializeForNativeProtocol(ByteBuffer bytes, int version)
    {
        try
        {
            ByteBuffer input = bytes.duplicate();
            int n = readCollectionSize(input, version);

            if (n < 0)
                throw new MarshalException("The data cannot be deserialized as a map");

            // If the received bytes are not corresponding to a map, n might be a huge number.
            // In such a case we do not want to initialize the map with that initialCapacity as it can result
            // in an OOM when put is called (see CASSANDRA-12618). On the other hand we do not want to have to resize
            // the map if we can avoid it, so we put a reasonable limit on the initialCapacity.
            Map<K, V> m = new LinkedHashMap<K, V>(Math.min(n, 256));
            for (int i = 0; i < n; i++)
            {
                ByteBuffer kbb = readValue(input, version);
                keys.validate(kbb);

                ByteBuffer vbb = readValue(input, version);
                values.validate(vbb);

                m.put(keys.deserialize(kbb), values.deserialize(vbb));
            }
            if (input.hasRemaining())
                throw new MarshalException("Unexpected extraneous bytes after map value");
            return m;
        }
        catch (BufferUnderflowException e)
        {
            throw new MarshalException("Not enough bytes to read a map");
        }
    }

    /**
     * Given a serialized map, gets the value associated with a given key.
     * @param serializedMap a serialized map
     * @param serializedKey a serialized key
     * @param keyType the key type for the map
     * @return the value associated with the key if one exists, null otherwise
     */
    public Set<T> deserializeForNativeProtocol(ByteBuffer bytes, int version)
    {
        try
        {
            ByteBuffer input = bytes.duplicate();
            int n = readCollectionSize(input, version);
            Set<T> l = new LinkedHashSet<T>(n);
            for (int i = 0; i < n; i++)
            {
                ByteBuffer databb = readValue(input, version);
                elements.validate(databb);
                l.add(elements.deserialize(databb));
            }
            if (input.hasRemaining())
                throw new MarshalException("Unexpected extraneous bytes after set value");
            return l;
        }
        catch (BufferUnderflowException e)
        {
            throw new MarshalException("Not enough bytes to read a set");
        }
    }

+++++++++++++++++++++++
    public Set<T> deserializeForNativeProtocol(ByteBuffer bytes, int version)
    {
        try
        {
            ByteBuffer input = bytes.duplicate();
            int n = readCollectionSize(input, version);

            if (n < 0)
                throw new MarshalException("The data cannot be deserialized as a set");

            // If the received bytes are not corresponding to a set, n might be a huge number.
            // In such a case we do not want to initialize the set with that initialCapacity as it can result
            // in an OOM when add is called (see CASSANDRA-12618). On the other hand we do not want to have to resize
            // the set if we can avoid it, so we put a reasonable limit on the initialCapacity.
            Set<T> l = new LinkedHashSet<T>(Math.min(n, 256));

            for (int i = 0; i < n; i++)
            {
                ByteBuffer databb = readValue(input, version);
                elements.validate(databb);
                l.add(elements.deserialize(databb));
            }
            if (input.hasRemaining())
                throw new MarshalException("Unexpected extraneous bytes after set value");
            return l;
        }
        catch (BufferUnderflowException e)
        {
            throw new MarshalException("Not enough bytes to read a set");
        }
    }

add: 3, delete: 0, change: 93, unhandled: 0 size_exceptions: 1 size_serialize: 4
-----------cassandra-3.0.10 vs cassandra-3.0.11-----------
    private void serializePartition(UnfilteredRowIterator partition)
    {
        String key = metadata.getKeyValidator().getString(partition.partitionKey().getKey());
        try
        {
            json.writeStartObject();

            json.writeFieldName("partition");
            json.writeStartObject();
            json.writeFieldName("key");
            serializePartitionKey(partition.partitionKey());
            json.writeNumberField("position", this.currentScanner.getCurrentPosition());

            if (!partition.partitionLevelDeletion().isLive())
            {
                serializeDeletion(partition.partitionLevelDeletion());
                json.writeEndObject();
            }
            else
            {
                json.writeEndObject();
                json.writeFieldName("rows");
                json.writeStartArray();
                updatePosition();
                if (!partition.staticRow().isEmpty())
                {
                    serializeRow(partition.staticRow());
                }
                Unfiltered unfiltered;
                updatePosition();
                while (partition.hasNext())
                {
                    unfiltered = partition.next();
                    if (unfiltered instanceof Row)
                    {
                        serializeRow((Row) unfiltered);
                    }
                    else if (unfiltered instanceof RangeTombstoneMarker)
                    {
                        serializeTombstone((RangeTombstoneMarker) unfiltered);
                    }
                    updatePosition();
                }
                json.writeEndArray();
            }

            json.writeEndObject();
        }
        catch (IOException e)
        {
            logger.error("Fatal error parsing partition: {}", key, e);
        }
    }

+++++++++++++++++++++++
    private void serializePartition(UnfilteredRowIterator partition)
    {
        String key = metadata.getKeyValidator().getString(partition.partitionKey().getKey());
        try
        {
            json.writeStartObject();

            json.writeFieldName("partition");
            json.writeStartObject();
            json.writeFieldName("key");
            serializePartitionKey(partition.partitionKey());
            json.writeNumberField("position", this.currentScanner.getCurrentPosition());

            if (!partition.partitionLevelDeletion().isLive())
                serializeDeletion(partition.partitionLevelDeletion());

            json.writeEndObject();

            if (partition.hasNext() || partition.staticRow() != null)
            {
                json.writeFieldName("rows");
                json.writeStartArray();
                updatePosition();
                if (!partition.staticRow().isEmpty())
                    serializeRow(partition.staticRow());

                Unfiltered unfiltered;
                updatePosition();
                while (partition.hasNext())
                {
                    unfiltered = partition.next();
                    if (unfiltered instanceof Row)
                    {
                        serializeRow((Row) unfiltered);
                    }
                    else if (unfiltered instanceof RangeTombstoneMarker)
                    {
                        serializeTombstone((RangeTombstoneMarker) unfiltered);
                    }
                    updatePosition();
                }
                json.writeEndArray();

                json.writeEndObject();
            }
        }
        catch (IOException e)
        {
            logger.error("Fatal error parsing partition: {}", key, e);
        }
    }

add: 7, delete: 1, change: 158, unhandled: 1 size_exceptions: 0 size_serialize: 1
-----------cassandra-3.0.11 vs cassandra-3.0.12-----------
add: 6, delete: 0, change: 27, unhandled: 0 size_exceptions: 1 size_serialize: 0
-----------cassandra-3.0.12 vs cassandra-3.0.13-----------
    public Unfiltered deserialize(DataInputPlus in, SerializationHeader header, SerializationHelper helper, Row.Builder builder)
    throws IOException
    {
        // It wouldn't be wrong per-se to use an unsorted builder, but it would be inefficient so make sure we don't do it by mistake
        assert builder.isSorted();

        int flags = in.readUnsignedByte();
        if (isEndOfPartition(flags))
            return null;

        int extendedFlags = readExtendedFlags(in, flags);

        if (kind(flags) == Unfiltered.Kind.RANGE_TOMBSTONE_MARKER)
        {
            RangeTombstone.Bound bound = RangeTombstone.Bound.serializer.deserialize(in, helper.version, header.clusteringTypes());
            return deserializeMarkerBody(in, header, bound);
        }
        else
        {
            // deserializeStaticRow should be used for that.
            if (isStatic(extendedFlags))
                throw new IOException("Corrupt flags value for unfiltered partition (isStatic flag set): " + flags);

            builder.newRow(Clustering.serializer.deserialize(in, helper.version, header.clusteringTypes()));
            Row row = deserializeRowBody(in, header, helper, flags, extendedFlags, builder);
            // we do not write empty rows because Rows.collectStats(), called by BTW.applyToRow(), asserts that rows are not empty
            // if we don't throw here, then later the very same assertion in Rows.collectStats() will fail compactions
            // see BlackListingCompactionsTest and CASSANDRA-9530 for details
            if (row.isEmpty())
                throw new IOException("Corrupt empty row found in unfiltered partition");
            return row;
        }
    }

+++++++++++++++++++++++
    public Unfiltered deserialize(DataInputPlus in, SerializationHeader header, SerializationHelper helper, Row.Builder builder)
    throws IOException
    {
        while (true)
        {
            Unfiltered unfiltered = deserializeOne(in, header, helper, builder);
            if (unfiltered == null)
                return null;

            // Skip empty rows, see deserializeOne javadoc
            if (!unfiltered.isEmpty())
                return unfiltered;
        }
    }

    /**
     * Deserialize a single {@link Unfiltered} from the provided input.
     * <p>
     * <b>WARNING:</b> this can return an empty row because it's possible there is a row serialized, but that row only
     * contains data for dropped columns, see CASSANDRA-13337. But as most code expect rows to not be empty, this isn't
     * meant to be exposed publicly.
     *
     * But as {@link UnfilteredRowIterator} should not return empty
     * rows, this mean consumer of this method should make sure to skip said empty rows.
     */
add: 6, delete: 6, change: 77, unhandled: 1 size_exceptions: 0 size_serialize: 1
-----------cassandra-3.0.13 vs cassandra-3.0.14-----------
add: 4, delete: 0, change: 51, unhandled: 0 size_exceptions: 0 size_serialize: 0
-----------cassandra-3.0.14 vs cassandra-3.0.15-----------
    public static long serializedSizeAsLegacyPartition(ReadCommand command, UnfilteredRowIterator partition, int version)
    {
        assert version < MessagingService.VERSION_30;

        if (partition.isEmpty())
            return TypeSizes.sizeof(false);

        long size = TypeSizes.sizeof(true);

        LegacyLayout.LegacyUnfilteredPartition legacyPartition = LegacyLayout.fromUnfilteredRowIterator(command, partition);

        size += UUIDSerializer.serializer.serializedSize(partition.metadata().cfId, version);
        size += DeletionTime.serializer.serializedSize(legacyPartition.partitionDeletion);
        size += legacyPartition.rangeTombstones.serializedSize(partition.metadata());

        // begin cell serialization
        size += TypeSizes.sizeof(legacyPartition.cells.size());
        for (LegacyLayout.LegacyCell cell : legacyPartition.cells)
        {
            size += ByteBufferUtil.serializedSizeWithShortLength(cell.name.encode(partition.metadata()));
            size += 1;  // serialization flags
            if (cell.kind == LegacyLayout.LegacyCell.Kind.EXPIRING)
            {
                size += TypeSizes.sizeof(cell.ttl);
                size += TypeSizes.sizeof(cell.localDeletionTime);
            }
            else if (cell.kind == LegacyLayout.LegacyCell.Kind.DELETED)
            {
                size += TypeSizes.sizeof(cell.timestamp);
                // localDeletionTime replaces cell.value as the body
                size += TypeSizes.sizeof(TypeSizes.sizeof(cell.localDeletionTime));
                size += TypeSizes.sizeof(cell.localDeletionTime);
                continue;
            }
            else if (cell.kind == LegacyLayout.LegacyCell.Kind.COUNTER)
            {
                size += TypeSizes.sizeof(Long.MIN_VALUE);  // timestampOfLastDelete
            }

            size += TypeSizes.sizeof(cell.timestamp);
            size += ByteBufferUtil.serializedSizeWithLength(cell.value);
        }

        return size;
    }

    // For thrift sake
+++++++++++++++++++++++
    public static long serializedSizeAsLegacyPartition(ReadCommand command, UnfilteredRowIterator partition, int version)
    {
        assert version < MessagingService.VERSION_30;

        if (partition.isEmpty())
            return TypeSizes.sizeof(false);

        long size = TypeSizes.sizeof(true);

        LegacyLayout.LegacyUnfilteredPartition legacyPartition = LegacyLayout.fromUnfilteredRowIterator(command, partition);

        size += UUIDSerializer.serializer.serializedSize(partition.metadata().cfId, version);
        size += DeletionTime.serializer.serializedSize(legacyPartition.partitionDeletion);
        size += legacyPartition.rangeTombstones.serializedSize(partition.metadata());

        // begin cell serialization
        size += TypeSizes.sizeof(legacyPartition.cells.size());
        for (LegacyLayout.LegacyCell cell : legacyPartition.cells)
        {
            size += ByteBufferUtil.serializedSizeWithShortLength(cell.name.encode(partition.metadata()));
            size += 1;  // serialization flags
            if (cell.isExpiring())
            {
                size += TypeSizes.sizeof(cell.ttl);
                size += TypeSizes.sizeof(cell.localDeletionTime);
            }
            else if (cell.isTombstone())
            {
                size += TypeSizes.sizeof(cell.timestamp);
                // localDeletionTime replaces cell.value as the body
                size += TypeSizes.sizeof(TypeSizes.sizeof(cell.localDeletionTime));
                size += TypeSizes.sizeof(cell.localDeletionTime);
                continue;
            }
            else if (cell.isCounterUpdate())
            {
                size += TypeSizes.sizeof(cell.timestamp);
                long count = CounterContext.instance().getLocalCount(cell.value);
                size += ByteBufferUtil.serializedSizeWithLength(ByteBufferUtil.bytes(count));
                continue;
            }
            else if (cell.isCounter())
            {
                size += TypeSizes.sizeof(Long.MIN_VALUE);  // timestampOfLastDelete
            }

            size += TypeSizes.sizeof(cell.timestamp);
            size += ByteBufferUtil.serializedSizeWithLength(cell.value);
        }

        return size;
    }

    // For thrift sake
    private void serializeCell(Cell cell, LivenessInfo liveInfo)
    {
        try
        {
            json.writeStartObject();
            objectIndenter.setCompact(true);
            json.writeFieldName("name");
            AbstractType<?> type = cell.column().type;
            json.writeString(cell.column().name.toCQLString());

            if (cell.path() != null && cell.path().size() > 0)
            {
                CollectionType ct = (CollectionType) type;
                json.writeFieldName("path");
                arrayIndenter.setCompact(true);
                json.writeStartArray();
                for (int i = 0; i < cell.path().size(); i++)
                {
                    json.writeString(ct.nameComparator().getString(cell.path().get(i)));
                }
                json.writeEndArray();
                arrayIndenter.setCompact(false);
            }
            if (cell.isTombstone())
            {
                json.writeFieldName("deletion_info");
                objectIndenter.setCompact(true);
                json.writeStartObject();
                json.writeFieldName("local_delete_time");
                json.writeString(dateString(TimeUnit.SECONDS, cell.localDeletionTime()));
                json.writeEndObject();
                objectIndenter.setCompact(false);
            }
            else
            {
                json.writeFieldName("value");
                json.writeString(cell.column().cellValueType().getString(cell.value()));
            }
            if (liveInfo.isEmpty() || cell.timestamp() != liveInfo.timestamp())
            {
                json.writeFieldName("tstamp");
                json.writeString(dateString(TimeUnit.MICROSECONDS, cell.timestamp()));
            }
            if (cell.isExpiring() && (liveInfo.isEmpty() || cell.ttl() != liveInfo.ttl()))
            {
                json.writeFieldName("ttl");
                json.writeNumber(cell.ttl());
                json.writeFieldName("expires_at");
                json.writeString(dateString(TimeUnit.SECONDS, cell.localDeletionTime()));
                json.writeFieldName("expired");
                json.writeBoolean(!cell.isLive((int) (System.currentTimeMillis() / 1000)));
            }
            json.writeEndObject();
            objectIndenter.setCompact(false);
        }
        catch (IOException e)
        {
            logger.error("Failure parsing cell.", e);
        }
    }

+++++++++++++++++++++++
    private void serializeCell(Cell cell, LivenessInfo liveInfo)
    {
        try
        {
            json.writeStartObject();
            objectIndenter.setCompact(true);
            json.writeFieldName("name");
            AbstractType<?> type = cell.column().type;
            json.writeString(cell.column().name.toCQLString());

            if (type.isCollection() && type.isMultiCell()) // non-frozen collection
            {
                CollectionType ct = (CollectionType) type;
                json.writeFieldName("path");
                arrayIndenter.setCompact(true);
                json.writeStartArray();
                for (int i = 0; i < cell.path().size(); i++)
                {
                    json.writeString(ct.nameComparator().getString(cell.path().get(i)));
                }
                json.writeEndArray();
                arrayIndenter.setCompact(false);
            }
            if (cell.isTombstone())
            {
                json.writeFieldName("deletion_info");
                objectIndenter.setCompact(true);
                json.writeStartObject();
                json.writeFieldName("local_delete_time");
                json.writeString(dateString(TimeUnit.SECONDS, cell.localDeletionTime()));
                json.writeEndObject();
                objectIndenter.setCompact(false);
            }
            else
            {
                json.writeFieldName("value");
                json.writeRawValue(cell.column().cellValueType().toJSONString(cell.value(), Server.CURRENT_VERSION));
            }
            if (liveInfo.isEmpty() || cell.timestamp() != liveInfo.timestamp())
            {
                json.writeFieldName("tstamp");
                json.writeString(dateString(TimeUnit.MICROSECONDS, cell.timestamp()));
            }
            if (cell.isExpiring() && (liveInfo.isEmpty() || cell.ttl() != liveInfo.ttl()))
            {
                json.writeFieldName("ttl");
                json.writeNumber(cell.ttl());
                json.writeFieldName("expires_at");
                json.writeString(dateString(TimeUnit.SECONDS, cell.localDeletionTime()));
                json.writeFieldName("expired");
                json.writeBoolean(!cell.isLive((int) (System.currentTimeMillis() / 1000)));
            }
            json.writeEndObject();
            objectIndenter.setCompact(false);
        }
        catch (IOException e)
        {
            logger.error("Failure parsing cell.", e);
        }
    }

add: 17, delete: 0, change: 194, unhandled: 1 size_exceptions: 0 size_serialize: 2
-----------cassandra-3.0.15 vs cassandra-3.0.16-----------
    private void serializeClustering(ClusteringPrefix clustering) throws IOException
    {
        if (clustering.size() > 0)
        {
            json.writeFieldName("clustering");
            objectIndenter.setCompact(true);
            json.writeStartArray();
            arrayIndenter.setCompact(true);
            List<ColumnDefinition> clusteringColumns = metadata.clusteringColumns();
            for (int i = 0; i < clusteringColumns.size(); i++)
            {
                ColumnDefinition column = clusteringColumns.get(i);
                if (i >= clustering.size())
                {
                    json.writeString("*");
                }
                else
                {
                    json.writeString(column.cellValueType().getString(clustering.get(i)));
                }
            }
            json.writeEndArray();
            objectIndenter.setCompact(false);
            arrayIndenter.setCompact(false);
        }
    }

+++++++++++++++++++++++
    private void serializeClustering(ClusteringPrefix clustering) throws IOException
    {
        if (clustering.size() > 0)
        {
            json.writeFieldName("clustering");
            objectIndenter.setCompact(true);
            json.writeStartArray();
            arrayIndenter.setCompact(true);
            List<ColumnDefinition> clusteringColumns = metadata.clusteringColumns();
            for (int i = 0; i < clusteringColumns.size(); i++)
            {
                ColumnDefinition column = clusteringColumns.get(i);
                if (i >= clustering.size())
                {
                    json.writeString("*");
                }
                else
                {
                    json.writeRawValue(column.cellValueType().toJSONString(clustering.get(i), Server.CURRENT_VERSION));
                }
            }
            json.writeEndArray();
            objectIndenter.setCompact(false);
            arrayIndenter.setCompact(false);
        }
    }

add: 14, delete: 1, change: 97, unhandled: 0 size_exceptions: 1 size_serialize: 1
-----------cassandra-3.0.16 vs cassandra-3.0.17-----------
    public void multiFlushAndDeserializeTest(Class<? extends ICompressor> compressorClass) throws Exception
    {
        int hintNum = 0;
        int bufferSize = HintsWriteExecutor.WRITE_BUFFER_SIZE;
        List<Hint> hints = new LinkedList<>();

        UUID hostId = UUIDGen.getTimeUUID();
        long ts = System.currentTimeMillis();

        HintsDescriptor descriptor = new HintsDescriptor(hostId, ts, params(compressorClass));
        File dir = Files.createTempDir();
        try (HintsWriter writer = HintsWriter.create(dir, descriptor))
        {
            assert writer instanceof CompressedHintsWriter;

            ByteBuffer writeBuffer = ByteBuffer.allocateDirect(bufferSize);
            try (HintsWriter.Session session = writer.newSession(writeBuffer))
            {
                while (session.getBytesWritten() < bufferSize * 3)
                {
                    Hint hint = createHint(hintNum, ts+hintNum);
                    session.append(hint);
                    hints.add(hint);
                    hintNum++;
                }
            }
        }

        try (HintsReader reader = HintsReader.open(new File(dir, descriptor.fileName())))
        {
            List<Hint> deserialized = new ArrayList<>(hintNum);

            for (HintsReader.Page page: reader)
            {
                Iterator<Hint> iterator = page.hintsIterator();
                while (iterator.hasNext())
                {
                    deserialized.add(iterator.next());
                }
            }

            Assert.assertEquals(hints.size(), deserialized.size());
            hintNum = 0;
            for (Hint expected: hints)
            {
                HintsTestUtil.assertHintsEqual(expected, deserialized.get(hintNum));
                hintNum++;
            }
        }
    }

    @Test
+++++++++++++++++++++++
    public void multiFlushAndDeserializeTest(Class<? extends ICompressor> compressorClass) throws Exception
    {
        int hintNum = 0;
        int bufferSize = HintsWriteExecutor.WRITE_BUFFER_SIZE;
        List<Hint> hints = new LinkedList<>();

        UUID hostId = UUIDGen.getTimeUUID();
        long ts = System.currentTimeMillis();

        HintsDescriptor descriptor = new HintsDescriptor(hostId, ts, params(compressorClass));
        File dir = Files.createTempDir();
        try (HintsWriter writer = HintsWriter.create(dir, descriptor))
        {
            assert writer instanceof CompressedHintsWriter;

            ByteBuffer writeBuffer = ByteBuffer.allocateDirect(bufferSize);
            try (HintsWriter.Session session = writer.newSession(writeBuffer))
            {
                while (session.getBytesWritten() < bufferSize * 3)
                {
                    Hint hint = createHint(hintNum, ts+hintNum);
                    session.append(hint);
                    hints.add(hint);
                    hintNum++;
                }
            }
        }

        try (HintsReader reader = HintsReader.open(new File(dir, descriptor.fileName())))
        {
            List<Hint> deserialized = new ArrayList<>(hintNum);
            List<InputPosition> pagePositions = new ArrayList<>(hintNum);

            for (HintsReader.Page page: reader)
            {
                pagePositions.add(page.position);
                Iterator<Hint> iterator = page.hintsIterator();
                while (iterator.hasNext())
                {
                    deserialized.add(iterator.next());
                }
            }

            Assert.assertEquals(hints.size(), deserialized.size());
            hintNum = 0;
            for (Hint expected: hints)
            {
                HintsTestUtil.assertHintsEqual(expected, deserialized.get(hintNum));
                hintNum++;
            }

            // explicitely seek to each page by iterating collected page positions and check if hints still match as expected
            int hintOffset = 0;
            for (InputPosition pos : pagePositions)
            {
                reader.seek(pos);
                HintsReader.Page page = reader.iterator().next();
                Iterator<Hint> iterator = page.hintsIterator();
                while (iterator.hasNext())
                {
                    Hint seekedHint = iterator.next();
                    HintsTestUtil.assertHintsEqual(hints.get(hintOffset), seekedHint);
                    hintOffset++;
                }
            }
        }
    }

    @Test
add: 14, delete: 0, change: 77, unhandled: 0 size_exceptions: 1 size_serialize: 1
-----------cassandra-3.0.17 vs cassandra-3.0.18-----------
    public static long serializedSizeAsLegacyPartition(ReadCommand command, UnfilteredRowIterator partition, int version)
    {
        assert version < MessagingService.VERSION_30;

        if (partition.isEmpty())
            return TypeSizes.sizeof(false);

        long size = TypeSizes.sizeof(true);

        LegacyLayout.LegacyUnfilteredPartition legacyPartition = LegacyLayout.fromUnfilteredRowIterator(command, partition);

        size += UUIDSerializer.serializer.serializedSize(partition.metadata().cfId, version);
        size += DeletionTime.serializer.serializedSize(legacyPartition.partitionDeletion);
        size += legacyPartition.rangeTombstones.serializedSize(partition.metadata());

        // begin cell serialization
        size += TypeSizes.sizeof(legacyPartition.cells.size());
        for (LegacyLayout.LegacyCell cell : legacyPartition.cells)
        {
            size += ByteBufferUtil.serializedSizeWithShortLength(cell.name.encode(partition.metadata()));
            size += 1;  // serialization flags
            if (cell.isExpiring())
            {
                size += TypeSizes.sizeof(cell.ttl);
                size += TypeSizes.sizeof(cell.localDeletionTime);
            }
            else if (cell.isTombstone())
            {
                size += TypeSizes.sizeof(cell.timestamp);
                // localDeletionTime replaces cell.value as the body
                size += TypeSizes.sizeof(TypeSizes.sizeof(cell.localDeletionTime));
                size += TypeSizes.sizeof(cell.localDeletionTime);
                continue;
            }
            else if (cell.isCounterUpdate())
            {
                size += TypeSizes.sizeof(cell.timestamp);
                long count = CounterContext.instance().getLocalCount(cell.value);
                size += ByteBufferUtil.serializedSizeWithLength(ByteBufferUtil.bytes(count));
                continue;
            }
            else if (cell.isCounter())
            {
                size += TypeSizes.sizeof(Long.MIN_VALUE);  // timestampOfLastDelete
            }

            size += TypeSizes.sizeof(cell.timestamp);
            size += ByteBufferUtil.serializedSizeWithLength(cell.value);
        }

        return size;
    }

    // For thrift sake
+++++++++++++++++++++++
    public static long serializedSizeAsLegacyPartition(ReadCommand command, UnfilteredRowIterator partition, int version)
    {
        assert version < MessagingService.VERSION_30;

        if (partition.isEmpty())
            return TypeSizes.sizeof(false);

        long size = TypeSizes.sizeof(true);

        LegacyLayout.LegacyUnfilteredPartition legacyPartition = LegacyLayout.fromUnfilteredRowIterator(command, partition);

        size += UUIDSerializer.serializer.serializedSize(partition.metadata().cfId, version);
        size += DeletionTime.serializer.serializedSize(legacyPartition.partitionDeletion);
        size += legacyPartition.rangeTombstones.serializedSize(partition.metadata());

        // begin cell serialization
        size += TypeSizes.sizeof(legacyPartition.cells.size());
        for (LegacyLayout.LegacyCell cell : legacyPartition.cells)
        {
            size += ByteBufferUtil.serializedSizeWithShortLength(cell.name.encode(partition.metadata()));
            size += 1;  // serialization flags
            if (cell.isExpiring())
            {
                size += TypeSizes.sizeof(cell.ttl);
                size += TypeSizes.sizeof(cell.localDeletionTime);
            }
            else if (cell.isTombstone())
            {
                size += TypeSizes.sizeof(cell.timestamp);
                // localDeletionTime replaces cell.value as the body
                size += TypeSizes.sizeof(TypeSizes.sizeof(cell.localDeletionTime));
                size += TypeSizes.sizeof(cell.localDeletionTime);
                continue;
            }
            else if (cell.isCounterUpdate())
            {
                size += TypeSizes.sizeof(cell.timestamp);
                long count = CounterContext.instance().getUpdateCount(cell.value);
                size += ByteBufferUtil.serializedSizeWithLength(ByteBufferUtil.bytes(count));
                continue;
            }
            else if (cell.isCounter())
            {
                size += TypeSizes.sizeof(Long.MIN_VALUE);  // timestampOfLastDelete
            }

            size += TypeSizes.sizeof(cell.timestamp);
            size += ByteBufferUtil.serializedSizeWithLength(cell.value);
        }

        return size;
    }

    // For thrift sake
    public static void serializeAsLegacyPartition(ReadCommand command, UnfilteredRowIterator partition, DataOutputPlus out, int version) throws IOException
    {
        assert version < MessagingService.VERSION_30;

        out.writeBoolean(true);

        LegacyLayout.LegacyUnfilteredPartition legacyPartition = LegacyLayout.fromUnfilteredRowIterator(command, partition);

        UUIDSerializer.serializer.serialize(partition.metadata().cfId, out, version);
        DeletionTime.serializer.serialize(legacyPartition.partitionDeletion, out);

        legacyPartition.rangeTombstones.serialize(out, partition.metadata());

        // begin cell serialization
        out.writeInt(legacyPartition.cells.size());
        for (LegacyLayout.LegacyCell cell : legacyPartition.cells)
        {
            ByteBufferUtil.writeWithShortLength(cell.name.encode(partition.metadata()), out);
            out.writeByte(cell.serializationFlags());
            if (cell.isExpiring())
            {
                out.writeInt(cell.ttl);
                out.writeInt(cell.localDeletionTime);
            }
            else if (cell.isTombstone())
            {
                out.writeLong(cell.timestamp);
                out.writeInt(TypeSizes.sizeof(cell.localDeletionTime));
                out.writeInt(cell.localDeletionTime);
                continue;
            }
            else if (cell.isCounterUpdate())
            {
                out.writeLong(cell.timestamp);
                long count = CounterContext.instance().getLocalCount(cell.value);
                ByteBufferUtil.writeWithLength(ByteBufferUtil.bytes(count), out);
                continue;
            }
            else if (cell.isCounter())
            {
                out.writeLong(Long.MIN_VALUE);  // timestampOfLastDelete (not used, and MIN_VALUE is the default)
            }

            out.writeLong(cell.timestamp);
            ByteBufferUtil.writeWithLength(cell.value, out);
        }
    }

    // For the old wire format
    // Note: this can return null if an empty partition is serialized!
+++++++++++++++++++++++
    public static void serializeAsLegacyPartition(ReadCommand command, UnfilteredRowIterator partition, DataOutputPlus out, int version) throws IOException
    {
        assert version < MessagingService.VERSION_30;

        out.writeBoolean(true);

        LegacyLayout.LegacyUnfilteredPartition legacyPartition = LegacyLayout.fromUnfilteredRowIterator(command, partition);

        UUIDSerializer.serializer.serialize(partition.metadata().cfId, out, version);
        DeletionTime.serializer.serialize(legacyPartition.partitionDeletion, out);

        legacyPartition.rangeTombstones.serialize(out, partition.metadata());

        // begin cell serialization
        out.writeInt(legacyPartition.cells.size());
        for (LegacyLayout.LegacyCell cell : legacyPartition.cells)
        {
            ByteBufferUtil.writeWithShortLength(cell.name.encode(partition.metadata()), out);
            out.writeByte(cell.serializationFlags());
            if (cell.isExpiring())
            {
                out.writeInt(cell.ttl);
                out.writeInt(cell.localDeletionTime);
            }
            else if (cell.isTombstone())
            {
                out.writeLong(cell.timestamp);
                out.writeInt(TypeSizes.sizeof(cell.localDeletionTime));
                out.writeInt(cell.localDeletionTime);
                continue;
            }
            else if (cell.isCounterUpdate())
            {
                out.writeLong(cell.timestamp);
                long count = CounterContext.instance().getUpdateCount(cell.value);
                ByteBufferUtil.writeWithLength(ByteBufferUtil.bytes(count), out);
                continue;
            }
            else if (cell.isCounter())
            {
                out.writeLong(Long.MIN_VALUE);  // timestampOfLastDelete (not used, and MIN_VALUE is the default)
            }

            out.writeLong(cell.timestamp);
            ByteBufferUtil.writeWithLength(cell.value, out);
        }
    }

    // For the old wire format
    // Note: this can return null if an empty partition is serialized!
    private long serializedRowBodySize(Row row, SerializationHeader header, long previousUnfilteredSize, int version)
    {
        long size = 0;

        if (header.isForSSTable())
            size += TypeSizes.sizeofUnsignedVInt(previousUnfilteredSize);

        boolean isStatic = row.isStatic();
        Columns headerColumns = header.columns(isStatic);
        LivenessInfo pkLiveness = row.primaryKeyLivenessInfo();
        Row.Deletion deletion = row.deletion();
        boolean hasComplexDeletion = row.hasComplexDeletion();
        boolean hasAllColumns = (row.size() == headerColumns.size());

        if (!pkLiveness.isEmpty())
            size += header.timestampSerializedSize(pkLiveness.timestamp());
        if (pkLiveness.isExpiring())
        {
            size += header.ttlSerializedSize(pkLiveness.ttl());
            size += header.localDeletionTimeSerializedSize(pkLiveness.localExpirationTime());
        }
        if (!deletion.isLive())
            size += header.deletionTimeSerializedSize(deletion.time());

        if (!hasAllColumns)
            size += Columns.serializer.serializedSubsetSize(Collections2.transform(row, ColumnData::column), header.columns(isStatic));

        SearchIterator<ColumnDefinition, ColumnDefinition> si = headerColumns.iterator();
        for (ColumnData data : row)
        {
            ColumnDefinition column = si.next(data.column());
            assert column != null;

            if (data.column.isSimple())
                size += Cell.serializer.serializedSize((Cell) data, column, pkLiveness, header);
            else
                size += sizeOfComplexColumn((ComplexColumnData) data, column, hasComplexDeletion, pkLiveness, header);
        }

        return size;
    }

+++++++++++++++++++++++
    private long serializedRowBodySize(Row row, SerializationHeader header, long previousUnfilteredSize, int version)
    {
        long size = 0;

        if (header.isForSSTable())
            size += TypeSizes.sizeofUnsignedVInt(previousUnfilteredSize);

        boolean isStatic = row.isStatic();
        Columns headerColumns = header.columns(isStatic);
        LivenessInfo pkLiveness = row.primaryKeyLivenessInfo();
        Row.Deletion deletion = row.deletion();
        boolean hasComplexDeletion = row.hasComplexDeletion();
        boolean hasAllColumns = (row.columnCount() == headerColumns.size());

        if (!pkLiveness.isEmpty())
            size += header.timestampSerializedSize(pkLiveness.timestamp());
        if (pkLiveness.isExpiring())
        {
            size += header.ttlSerializedSize(pkLiveness.ttl());
            size += header.localDeletionTimeSerializedSize(pkLiveness.localExpirationTime());
        }
        if (!deletion.isLive())
            size += header.deletionTimeSerializedSize(deletion.time());

        if (!hasAllColumns)
            size += Columns.serializer.serializedSubsetSize(row.columns(), header.columns(isStatic));

        SearchIterator<ColumnDefinition, ColumnDefinition> si = headerColumns.iterator();
        for (ColumnData data : row)
        {
            ColumnDefinition column = si.next(data.column());
            assert column != null;

            if (data.column.isSimple())
                size += Cell.serializer.serializedSize((Cell) data, column, pkLiveness, header);
            else
                size += sizeOfComplexColumn((ComplexColumnData) data, column, hasComplexDeletion, pkLiveness, header);
        }

        return size;
    }

add: 23, delete: 0, change: 130, unhandled: 1 size_exceptions: 3 size_serialize: 3
-----------cassandra-3.0.18 vs cassandra-3.0.19-----------
    public ByteBuffer serialize(int protocolVersion)
    {
        assert rowMark == null || protocolVersion == rowMark.protocolVersion;
        try (DataOutputBuffer out = new DataOutputBufferFixed(serializedSize(protocolVersion)))
        {
            ByteBuffer pk = partitionKey == null ? ByteBufferUtil.EMPTY_BYTE_BUFFER : partitionKey;
            ByteBuffer mark = rowMark == null ? ByteBufferUtil.EMPTY_BYTE_BUFFER : rowMark.mark;
            if (protocolVersion <= Server.VERSION_3)
            {
                ByteBufferUtil.writeWithShortLength(pk, out);
                ByteBufferUtil.writeWithShortLength(mark, out);
                out.writeInt(remaining);
                out.writeInt(remainingInPartition);
            }
            else
            {
                ByteBufferUtil.writeWithVIntLength(pk, out);
                ByteBufferUtil.writeWithVIntLength(mark, out);
                out.writeUnsignedVInt(remaining);
                out.writeUnsignedVInt(remainingInPartition);
            }
            return out.buffer();
        }
        catch (IOException e)
        {
            throw new RuntimeException(e);
        }
    }

+++++++++++++++++++++++
    public ByteBuffer serialize(int protocolVersion)
    {
        assert rowMark == null || protocolVersion == rowMark.protocolVersion;
        try
        {
            return protocolVersion > VERSION_3 ? modernSerialize() : legacySerialize(true);
        }
        catch (IOException e)
        {
            throw new RuntimeException(e);
        }
    }

    public int serializedSize(int protocolVersion)
    {
        assert rowMark == null || protocolVersion == rowMark.protocolVersion;
        ByteBuffer pk = partitionKey == null ? ByteBufferUtil.EMPTY_BYTE_BUFFER : partitionKey;
        ByteBuffer mark = rowMark == null ? ByteBufferUtil.EMPTY_BYTE_BUFFER : rowMark.mark;
        if (protocolVersion <= Server.VERSION_3)
        {
            return ByteBufferUtil.serializedSizeWithShortLength(pk)
                 + ByteBufferUtil.serializedSizeWithShortLength(mark)
                 + 8; // remaining & remainingInPartition
        }
        else
        {
            return ByteBufferUtil.serializedSizeWithVIntLength(pk)
                 + ByteBufferUtil.serializedSizeWithVIntLength(mark)
                 + TypeSizes.sizeofUnsignedVInt(remaining)
                 + TypeSizes.sizeofUnsignedVInt(remainingInPartition);
        }
    }

    @Override
+++++++++++++++++++++++
    public int serializedSize(int protocolVersion)
    {
        assert rowMark == null || protocolVersion == rowMark.protocolVersion;

        return protocolVersion > VERSION_3 ? modernSerializedSize() : legacySerializedSize(true);
    }

    /**
     * It's possible to receive a V3 paging state on a V4 client session, and vice versa - so we cannot
     * blindly rely on the protocol version provided. We must verify first that the buffer indeed contains
     * a paging state that adheres to the protocol version provided, or, if not - see if it is in a different
     * version, in which case we try the other format.
     */
    public static PagingState deserialize(ByteBuffer bytes, int protocolVersion)
    {
        if (bytes == null)
            return null;

        try (DataInputBuffer in = new DataInputBuffer(bytes, true))
        {
            ByteBuffer pk;
            RowMark mark;
            int remaining, remainingInPartition;
            if (protocolVersion <= Server.VERSION_3)
            {
                pk = ByteBufferUtil.readWithShortLength(in);
                mark = new RowMark(ByteBufferUtil.readWithShortLength(in), protocolVersion);
                remaining = in.readInt();
                // Note that while 'in.available()' is theoretically an estimate of how many bytes are available
                // without blocking, we know that since we're reading a ByteBuffer it will be exactly how many
                // bytes remain to be read. And the reason we want to condition this is for backward compatility
                // as we used to not set this.
                remainingInPartition = in.available() > 0 ? in.readInt() : Integer.MAX_VALUE;
            }
            else
            {
                pk = ByteBufferUtil.readWithVIntLength(in);
                mark = new RowMark(ByteBufferUtil.readWithVIntLength(in), protocolVersion);
                remaining = (int)in.readUnsignedVInt();
                remainingInPartition = (int)in.readUnsignedVInt();
            }
            return new PagingState(pk.hasRemaining() ? pk : null,
                                   mark.mark.hasRemaining() ? mark : null,
                                   remaining,
                                   remainingInPartition);
        }
        catch (IOException e)
        {
            throw new ProtocolException("Invalid value for the paging state");
        }
    }

+++++++++++++++++++++++
    public static PagingState deserialize(ByteBuffer bytes, int protocolVersion)
    {
        if (bytes == null)
            return null;

        try
        {
            /*
             * We can't just attempt to deser twice, as we risk to misinterpet short/vint
             * lengths and allocate huge byte arrays for readWithVIntLength() or,
             * to a lesser extent, readWithShortLength()
             */

            if (protocolVersion > VERSION_3)
            {
                if (isModernSerialized(bytes)) return modernDeserialize(bytes, protocolVersion);
                if (isLegacySerialized(bytes)) return legacyDeserialize(bytes, VERSION_3);
            }

            if (protocolVersion < VERSION_4)
            {
                if (isLegacySerialized(bytes)) return legacyDeserialize(bytes, protocolVersion);
                if (isModernSerialized(bytes)) return modernDeserialize(bytes, VERSION_4);
            }
        }
        catch (IOException e)
        {
            throw new ProtocolException("Invalid value for the paging state");
        }

        throw new ProtocolException("Invalid value for the paging state");
    }

    /*
     * Modern serde (> VERSION_3)
     */

    @SuppressWarnings({ "resource", "RedundantSuppression" })
add: 53, delete: 14, change: 119, unhandled: 2 size_exceptions: 3 size_serialize: 3
-----------cassandra-3.0.19 vs cassandra-3.0.20-----------
    private void serializePartition(UnfilteredRowIterator partition)
    {
        String key = metadata.getKeyValidator().getString(partition.partitionKey().getKey());
        try
        {
            json.writeStartObject();

            json.writeFieldName("partition");
            json.writeStartObject();
            json.writeFieldName("key");
            serializePartitionKey(partition.partitionKey());
            json.writeNumberField("position", this.currentScanner.getCurrentPosition());

            if (!partition.partitionLevelDeletion().isLive())
                serializeDeletion(partition.partitionLevelDeletion());

            json.writeEndObject();

            if (partition.hasNext() || partition.staticRow() != null)
            {
                json.writeFieldName("rows");
                json.writeStartArray();
                updatePosition();
                if (!partition.staticRow().isEmpty())
                    serializeRow(partition.staticRow());

                Unfiltered unfiltered;
                updatePosition();
                while (partition.hasNext())
                {
                    unfiltered = partition.next();
                    if (unfiltered instanceof Row)
                    {
                        serializeRow((Row) unfiltered);
                    }
                    else if (unfiltered instanceof RangeTombstoneMarker)
                    {
                        serializeTombstone((RangeTombstoneMarker) unfiltered);
                    }
                    updatePosition();
                }
                json.writeEndArray();

                json.writeEndObject();
            }
        }
        catch (IOException e)
        {
            logger.error("Fatal error parsing partition: {}", key, e);
        }
    }

+++++++++++++++++++++++
    private void serializePartition(UnfilteredRowIterator partition)
    {
        String key = metadata.getKeyValidator().getString(partition.partitionKey().getKey());
        try
        {
            json.writeStartObject();

            json.writeFieldName("partition");
            json.writeStartObject();
            json.writeFieldName("key");
            serializePartitionKey(partition.partitionKey());
            json.writeNumberField("position", this.currentScanner.getCurrentPosition());

            if (!partition.partitionLevelDeletion().isLive())
                serializeDeletion(partition.partitionLevelDeletion());

            json.writeEndObject();

            json.writeFieldName("rows");
            json.writeStartArray();
            updatePosition();

            if (partition.staticRow() != null)
            {
                if (!partition.staticRow().isEmpty())
                    serializeRow(partition.staticRow());
                updatePosition();
            }

            Unfiltered unfiltered;
            while (partition.hasNext())
            {
                unfiltered = partition.next();
                if (unfiltered instanceof Row)
                {
                    serializeRow((Row) unfiltered);
                }
                else if (unfiltered instanceof RangeTombstoneMarker)
                {
                    serializeTombstone((RangeTombstoneMarker) unfiltered);
                }
                updatePosition();
            }

            json.writeEndArray();

            json.writeEndObject();
        }
        catch (IOException e)
        {
            logger.error("Fatal error parsing partition: {}", key, e);
        }
    }

add: 11, delete: 0, change: 93, unhandled: 0 size_exceptions: 0 size_serialize: 1
-----------cassandra-3.0.20 vs cassandra-3.1-----------
    public static long serializedSizeAsLegacyPartition(ReadCommand command, UnfilteredRowIterator partition, int version)
    {
        assert version < MessagingService.VERSION_30;

        if (partition.isEmpty())
            return TypeSizes.sizeof(false);

        long size = TypeSizes.sizeof(true);

        LegacyLayout.LegacyUnfilteredPartition legacyPartition = LegacyLayout.fromUnfilteredRowIterator(command, partition);

        size += UUIDSerializer.serializer.serializedSize(partition.metadata().cfId, version);
        size += DeletionTime.serializer.serializedSize(legacyPartition.partitionDeletion);
        size += legacyPartition.rangeTombstones.serializedSize(partition.metadata());

        // begin cell serialization
        size += TypeSizes.sizeof(legacyPartition.cells.size());
        for (LegacyLayout.LegacyCell cell : legacyPartition.cells)
        {
            size += ByteBufferUtil.serializedSizeWithShortLength(cell.name.encode(partition.metadata()));
            size += 1;  // serialization flags
            if (cell.isExpiring())
            {
                size += TypeSizes.sizeof(cell.ttl);
                size += TypeSizes.sizeof(cell.localDeletionTime);
            }
            else if (cell.isTombstone())
            {
                size += TypeSizes.sizeof(cell.timestamp);
                // localDeletionTime replaces cell.value as the body
                size += TypeSizes.sizeof(TypeSizes.sizeof(cell.localDeletionTime));
                size += TypeSizes.sizeof(cell.localDeletionTime);
                continue;
            }
            else if (cell.isCounterUpdate())
            {
                size += TypeSizes.sizeof(cell.timestamp);
                long count = CounterContext.instance().getUpdateCount(cell.value);
                size += ByteBufferUtil.serializedSizeWithLength(ByteBufferUtil.bytes(count));
                continue;
            }
            else if (cell.isCounter())
            {
                size += TypeSizes.sizeof(Long.MIN_VALUE);  // timestampOfLastDelete
            }

            size += TypeSizes.sizeof(cell.timestamp);
            size += ByteBufferUtil.serializedSizeWithLength(cell.value);
        }

        return size;
    }

    // For thrift sake
+++++++++++++++++++++++
    public static long serializedSizeAsLegacyPartition(UnfilteredRowIterator partition, int version)
    {
        assert version < MessagingService.VERSION_30;

        if (partition.isEmpty())
            return TypeSizes.sizeof(false);

        long size = TypeSizes.sizeof(true);

        LegacyLayout.LegacyUnfilteredPartition legacyPartition = LegacyLayout.fromUnfilteredRowIterator(partition);

        size += UUIDSerializer.serializer.serializedSize(partition.metadata().cfId, version);
        size += DeletionTime.serializer.serializedSize(legacyPartition.partitionDeletion);
        size += legacyPartition.rangeTombstones.serializedSize(partition.metadata());

        // begin cell serialization
        size += TypeSizes.sizeof(legacyPartition.cells.size());
        for (LegacyLayout.LegacyCell cell : legacyPartition.cells)
        {
            size += ByteBufferUtil.serializedSizeWithShortLength(cell.name.encode(partition.metadata()));
            size += 1;  // serialization flags
            if (cell.kind == LegacyLayout.LegacyCell.Kind.EXPIRING)
            {
                size += TypeSizes.sizeof(cell.ttl);
                size += TypeSizes.sizeof(cell.localDeletionTime);
            }
            else if (cell.kind == LegacyLayout.LegacyCell.Kind.DELETED)
            {
                size += TypeSizes.sizeof(cell.timestamp);
                // localDeletionTime replaces cell.value as the body
                size += TypeSizes.sizeof(TypeSizes.sizeof(cell.localDeletionTime));
                size += TypeSizes.sizeof(cell.localDeletionTime);
                continue;
            }
            else if (cell.kind == LegacyLayout.LegacyCell.Kind.COUNTER)
            {
                size += TypeSizes.sizeof(Long.MIN_VALUE);  // timestampOfLastDelete
            }

            size += TypeSizes.sizeof(cell.timestamp);
            size += ByteBufferUtil.serializedSizeWithLength(cell.value);
        }

        return size;
    }

    // For thrift sake
    public static void serializeAsLegacyPartition(ReadCommand command, UnfilteredRowIterator partition, DataOutputPlus out, int version) throws IOException
    {
        assert version < MessagingService.VERSION_30;

        out.writeBoolean(true);

        LegacyLayout.LegacyUnfilteredPartition legacyPartition = LegacyLayout.fromUnfilteredRowIterator(command, partition);

        UUIDSerializer.serializer.serialize(partition.metadata().cfId, out, version);
        DeletionTime.serializer.serialize(legacyPartition.partitionDeletion, out);

        legacyPartition.rangeTombstones.serialize(out, partition.metadata());

        // begin cell serialization
        out.writeInt(legacyPartition.cells.size());
        for (LegacyLayout.LegacyCell cell : legacyPartition.cells)
        {
            ByteBufferUtil.writeWithShortLength(cell.name.encode(partition.metadata()), out);
            out.writeByte(cell.serializationFlags());
            if (cell.isExpiring())
            {
                out.writeInt(cell.ttl);
                out.writeInt(cell.localDeletionTime);
            }
            else if (cell.isTombstone())
            {
                out.writeLong(cell.timestamp);
                out.writeInt(TypeSizes.sizeof(cell.localDeletionTime));
                out.writeInt(cell.localDeletionTime);
                continue;
            }
            else if (cell.isCounterUpdate())
            {
                out.writeLong(cell.timestamp);
                long count = CounterContext.instance().getUpdateCount(cell.value);
                ByteBufferUtil.writeWithLength(ByteBufferUtil.bytes(count), out);
                continue;
            }
            else if (cell.isCounter())
            {
                out.writeLong(Long.MIN_VALUE);  // timestampOfLastDelete (not used, and MIN_VALUE is the default)
            }

            out.writeLong(cell.timestamp);
            ByteBufferUtil.writeWithLength(cell.value, out);
        }
    }

    // For the old wire format
    // Note: this can return null if an empty partition is serialized!
+++++++++++++++++++++++
    public static void serializeAsLegacyPartition(UnfilteredRowIterator partition, DataOutputPlus out, int version) throws IOException
    {
        assert version < MessagingService.VERSION_30;

        out.writeBoolean(true);

        LegacyLayout.LegacyUnfilteredPartition legacyPartition = LegacyLayout.fromUnfilteredRowIterator(partition);

        UUIDSerializer.serializer.serialize(partition.metadata().cfId, out, version);
        DeletionTime.serializer.serialize(legacyPartition.partitionDeletion, out);

        legacyPartition.rangeTombstones.serialize(out, partition.metadata());

        // begin cell serialization
        out.writeInt(legacyPartition.cells.size());
        for (LegacyLayout.LegacyCell cell : legacyPartition.cells)
        {
            ByteBufferUtil.writeWithShortLength(cell.name.encode(partition.metadata()), out);
            out.writeByte(cell.serializationFlags());
            if (cell.isExpiring())
            {
                out.writeInt(cell.ttl);
                out.writeInt(cell.localDeletionTime);
            }
            else if (cell.isTombstone())
            {
                out.writeLong(cell.timestamp);
                out.writeInt(TypeSizes.sizeof(cell.localDeletionTime));
                out.writeInt(cell.localDeletionTime);
                continue;
            }
            else if (cell.isCounterUpdate())
            {
                out.writeLong(cell.timestamp);
                long count = CounterContext.instance().getLocalCount(cell.value);
                ByteBufferUtil.writeWithLength(ByteBufferUtil.bytes(count), out);
                continue;
            }
            else if (cell.isCounter())
            {
                out.writeLong(Long.MIN_VALUE);  // timestampOfLastDelete (not used, and MIN_VALUE is the default)
            }

            out.writeLong(cell.timestamp);
            ByteBufferUtil.writeWithLength(cell.value, out);
        }
    }

    // For the old wire format
    // Note: this can return null if an empty partition is serialized!
    private long serializedRowBodySize(Row row, SerializationHeader header, long previousUnfilteredSize, int version)
    {
        long size = 0;

        if (header.isForSSTable())
            size += TypeSizes.sizeofUnsignedVInt(previousUnfilteredSize);

        boolean isStatic = row.isStatic();
        Columns headerColumns = header.columns(isStatic);
        LivenessInfo pkLiveness = row.primaryKeyLivenessInfo();
        Row.Deletion deletion = row.deletion();
        boolean hasComplexDeletion = row.hasComplexDeletion();
        boolean hasAllColumns = (row.columnCount() == headerColumns.size());

        if (!pkLiveness.isEmpty())
            size += header.timestampSerializedSize(pkLiveness.timestamp());
        if (pkLiveness.isExpiring())
        {
            size += header.ttlSerializedSize(pkLiveness.ttl());
            size += header.localDeletionTimeSerializedSize(pkLiveness.localExpirationTime());
        }
        if (!deletion.isLive())
            size += header.deletionTimeSerializedSize(deletion.time());

        if (!hasAllColumns)
            size += Columns.serializer.serializedSubsetSize(row.columns(), header.columns(isStatic));

        SearchIterator<ColumnDefinition, ColumnDefinition> si = headerColumns.iterator();
        for (ColumnData data : row)
        {
            ColumnDefinition column = si.next(data.column());
            assert column != null;

            if (data.column.isSimple())
                size += Cell.serializer.serializedSize((Cell) data, column, pkLiveness, header);
            else
                size += sizeOfComplexColumn((ComplexColumnData) data, column, hasComplexDeletion, pkLiveness, header);
        }

        return size;
    }

+++++++++++++++++++++++
    private long serializedRowBodySize(Row row, SerializationHeader header, long previousUnfilteredSize, int version)
    {
        long size = 0;

        if (header.isForSSTable())
            size += TypeSizes.sizeofUnsignedVInt(previousUnfilteredSize);

        boolean isStatic = row.isStatic();
        Columns headerColumns = header.columns(isStatic);
        LivenessInfo pkLiveness = row.primaryKeyLivenessInfo();
        Row.Deletion deletion = row.deletion();
        boolean hasComplexDeletion = row.hasComplexDeletion();
        boolean hasAllColumns = (row.size() == headerColumns.size());

        if (!pkLiveness.isEmpty())
            size += header.timestampSerializedSize(pkLiveness.timestamp());
        if (pkLiveness.isExpiring())
        {
            size += header.ttlSerializedSize(pkLiveness.ttl());
            size += header.localDeletionTimeSerializedSize(pkLiveness.localExpirationTime());
        }
        if (!deletion.isLive())
            size += header.deletionTimeSerializedSize(deletion.time());

        if (!hasAllColumns)
            size += Columns.serializer.serializedSubsetSize(Collections2.transform(row, ColumnData::column), header.columns(isStatic));

        for (ColumnData data : row)
        {
            if (data.column.isSimple())
                size += Cell.serializer.serializedSize((Cell) data, pkLiveness, header);
            else
                size += sizeOfComplexColumn((ComplexColumnData) data, hasComplexDeletion, pkLiveness, header);
        }

        return size;
    }

    public Unfiltered deserialize(DataInputPlus in, SerializationHeader header, SerializationHelper helper, Row.Builder builder)
    throws IOException
    {
        while (true)
        {
            Unfiltered unfiltered = deserializeOne(in, header, helper, builder);
            if (unfiltered == null)
                return null;

            // Skip empty rows, see deserializeOne javadoc
            if (!unfiltered.isEmpty())
                return unfiltered;
        }
    }

    /**
     * Deserialize a single {@link Unfiltered} from the provided input.
     * <p>
     * <b>WARNING:</b> this can return an empty row because it's possible there is a row serialized, but that row only
     * contains data for dropped columns, see CASSANDRA-13337. But as most code expect rows to not be empty, this isn't
     * meant to be exposed publicly.
     *
     * But as {@link UnfilteredRowIterator} should not return empty
     * rows, this mean consumer of this method should make sure to skip said empty rows.
     */
+++++++++++++++++++++++
    public Unfiltered deserialize(DataInputPlus in, SerializationHeader header, SerializationHelper helper, Row.Builder builder)
    throws IOException
    {
        // It wouldn't be wrong per-se to use an unsorted builder, but it would be inefficient so make sure we don't do it by mistake
        assert builder.isSorted();

        int flags = in.readUnsignedByte();
        if (isEndOfPartition(flags))
            return null;

        int extendedFlags = readExtendedFlags(in, flags);

        if (kind(flags) == Unfiltered.Kind.RANGE_TOMBSTONE_MARKER)
        {
            RangeTombstone.Bound bound = RangeTombstone.Bound.serializer.deserialize(in, helper.version, header.clusteringTypes());
            return deserializeMarkerBody(in, header, bound);
        }
        else
        {
            assert !isStatic(extendedFlags); // deserializeStaticRow should be used for that.
            builder.newRow(Clustering.serializer.deserialize(in, helper.version, header.clusteringTypes()));
            return deserializeRowBody(in, header, helper, flags, extendedFlags, builder);
        }
    }

    public void serialize(Map<MetadataType, MetadataComponent> components, DataOutputPlus out, Version version) throws IOException
    {
        ValidationMetadata validation = (ValidationMetadata) components.get(MetadataType.VALIDATION);
        StatsMetadata stats = (StatsMetadata) components.get(MetadataType.STATS);
        CompactionMetadata compaction = (CompactionMetadata) components.get(MetadataType.COMPACTION);

        assert validation != null && stats != null && compaction != null && validation.partitioner != null;

        EstimatedHistogram.serializer.serialize(stats.estimatedPartitionSize, out);
        EstimatedHistogram.serializer.serialize(stats.estimatedColumnCount, out);
        ReplayPosition.serializer.serialize(stats.commitLogIntervals.upperBound().orElse(ReplayPosition.NONE), out);
        out.writeLong(stats.minTimestamp);
        out.writeLong(stats.maxTimestamp);
        out.writeInt(stats.maxLocalDeletionTime);
        out.writeDouble(validation.bloomFilterFPChance);
        out.writeDouble(stats.compressionRatio);
        out.writeUTF(validation.partitioner);
        out.writeInt(0); // compaction ancestors
        StreamingHistogram.serializer.serialize(stats.estimatedTombstoneDropTime, out);
        out.writeInt(stats.sstableLevel);
        out.writeInt(stats.minClusteringValues.size());
        for (ByteBuffer value : stats.minClusteringValues)
            ByteBufferUtil.writeWithShortLength(value, out);
        out.writeInt(stats.maxClusteringValues.size());
        for (ByteBuffer value : stats.maxClusteringValues)
            ByteBufferUtil.writeWithShortLength(value, out);
        if (version.hasCommitLogLowerBound())
            ReplayPosition.serializer.serialize(stats.commitLogIntervals.lowerBound().orElse(ReplayPosition.NONE), out);
        if (version.hasCommitLogIntervals())
            replayPositionSetSerializer.serialize(stats.commitLogIntervals, out);
    }

    /**
     * Legacy serializer deserialize all components no matter what types are specified.
     */
    @Override
+++++++++++++++++++++++
    public void serialize(Map<MetadataType, MetadataComponent> components, DataOutputPlus out, Version version) throws IOException
    {
        ValidationMetadata validation = (ValidationMetadata) components.get(MetadataType.VALIDATION);
        StatsMetadata stats = (StatsMetadata) components.get(MetadataType.STATS);
        CompactionMetadata compaction = (CompactionMetadata) components.get(MetadataType.COMPACTION);

        assert validation != null && stats != null && compaction != null && validation.partitioner != null;

        EstimatedHistogram.serializer.serialize(stats.estimatedPartitionSize, out);
        EstimatedHistogram.serializer.serialize(stats.estimatedColumnCount, out);
        ReplayPosition.serializer.serialize(stats.replayPosition, out);
        out.writeLong(stats.minTimestamp);
        out.writeLong(stats.maxTimestamp);
        out.writeInt(stats.maxLocalDeletionTime);
        out.writeDouble(validation.bloomFilterFPChance);
        out.writeDouble(stats.compressionRatio);
        out.writeUTF(validation.partitioner);
        out.writeInt(0); // compaction ancestors
        StreamingHistogram.serializer.serialize(stats.estimatedTombstoneDropTime, out);
        out.writeInt(stats.sstableLevel);
        out.writeInt(stats.minClusteringValues.size());
        for (ByteBuffer value : stats.minClusteringValues)
            ByteBufferUtil.writeWithShortLength(value, out);
        out.writeInt(stats.maxClusteringValues.size());
        for (ByteBuffer value : stats.maxClusteringValues)
            ByteBufferUtil.writeWithShortLength(value, out);
    }

    /**
     * Legacy serializer deserialize all components no matter what types are specified.
     */
    @Override
    public Map<MetadataType, MetadataComponent> deserialize(Descriptor descriptor, EnumSet<MetadataType> types) throws IOException
    {
        Map<MetadataType, MetadataComponent> components = Maps.newHashMap();

        File statsFile = new File(descriptor.filenameFor(Component.STATS));
        if (!statsFile.exists() && types.contains(MetadataType.STATS))
        {
            components.put(MetadataType.STATS, MetadataCollector.defaultStatsMetadata());
        }
        else
        {
            try (DataInputStreamPlus in = new DataInputStreamPlus(new BufferedInputStream(new FileInputStream(statsFile))))
            {
                EstimatedHistogram partitionSizes = EstimatedHistogram.serializer.deserialize(in);
                EstimatedHistogram columnCounts = EstimatedHistogram.serializer.deserialize(in);
                ReplayPosition commitLogLowerBound = ReplayPosition.NONE;
                ReplayPosition commitLogUpperBound = ReplayPosition.serializer.deserialize(in);
                long minTimestamp = in.readLong();
                long maxTimestamp = in.readLong();
                int maxLocalDeletionTime = in.readInt();
                double bloomFilterFPChance = in.readDouble();
                double compressionRatio = in.readDouble();
                String partitioner = in.readUTF();
                int nbAncestors = in.readInt(); //skip compaction ancestors
                in.skipBytes(nbAncestors * TypeSizes.sizeof(nbAncestors));
                StreamingHistogram tombstoneHistogram = StreamingHistogram.serializer.deserialize(in);
                int sstableLevel = 0;
                if (in.available() > 0)
                    sstableLevel = in.readInt();

                int colCount = in.readInt();
                List<ByteBuffer> minColumnNames = new ArrayList<>(colCount);
                for (int i = 0; i < colCount; i++)
                    minColumnNames.add(ByteBufferUtil.readWithShortLength(in));

                colCount = in.readInt();
                List<ByteBuffer> maxColumnNames = new ArrayList<>(colCount);
                for (int i = 0; i < colCount; i++)
                    maxColumnNames.add(ByteBufferUtil.readWithShortLength(in));

                if (descriptor.version.hasCommitLogLowerBound())
                    commitLogLowerBound = ReplayPosition.serializer.deserialize(in);
                IntervalSet<ReplayPosition> commitLogIntervals;
                if (descriptor.version.hasCommitLogIntervals())
                    commitLogIntervals = replayPositionSetSerializer.deserialize(in);
                else
                    commitLogIntervals = new IntervalSet<>(commitLogLowerBound, commitLogUpperBound);

                if (types.contains(MetadataType.VALIDATION))
                    components.put(MetadataType.VALIDATION,
                                   new ValidationMetadata(partitioner, bloomFilterFPChance));
                if (types.contains(MetadataType.STATS))
                    components.put(MetadataType.STATS,
                                   new StatsMetadata(partitionSizes,
                                                     columnCounts,
                                                     commitLogIntervals,
                                                     minTimestamp,
                                                     maxTimestamp,
                                                     Integer.MAX_VALUE,
                                                     maxLocalDeletionTime,
                                                     0,
                                                     Integer.MAX_VALUE,
                                                     compressionRatio,
                                                     tombstoneHistogram,
                                                     sstableLevel,
                                                     minColumnNames,
                                                     maxColumnNames,
                                                     true,
                                                     ActiveRepairService.UNREPAIRED_SSTABLE,
                                                     -1,
                                                     -1));
                if (types.contains(MetadataType.COMPACTION))
                    components.put(MetadataType.COMPACTION,
                                   new CompactionMetadata(null));
            }
        }
        return components;
+++++++++++++++++++++++
    public Map<MetadataType, MetadataComponent> deserialize(Descriptor descriptor, EnumSet<MetadataType> types) throws IOException
    {
        Map<MetadataType, MetadataComponent> components = Maps.newHashMap();

        File statsFile = new File(descriptor.filenameFor(Component.STATS));
        if (!statsFile.exists() && types.contains(MetadataType.STATS))
        {
            components.put(MetadataType.STATS, MetadataCollector.defaultStatsMetadata());
        }
        else
        {
            try (DataInputStreamPlus in = new DataInputStreamPlus(new BufferedInputStream(new FileInputStream(statsFile))))
            {
                EstimatedHistogram partitionSizes = EstimatedHistogram.serializer.deserialize(in);
                EstimatedHistogram columnCounts = EstimatedHistogram.serializer.deserialize(in);
                ReplayPosition replayPosition = ReplayPosition.serializer.deserialize(in);
                long minTimestamp = in.readLong();
                long maxTimestamp = in.readLong();
                int maxLocalDeletionTime = in.readInt();
                double bloomFilterFPChance = in.readDouble();
                double compressionRatio = in.readDouble();
                String partitioner = in.readUTF();
                int nbAncestors = in.readInt(); //skip compaction ancestors
                in.skipBytes(nbAncestors * TypeSizes.sizeof(nbAncestors));
                StreamingHistogram tombstoneHistogram = StreamingHistogram.serializer.deserialize(in);
                int sstableLevel = 0;
                if (in.available() > 0)
                    sstableLevel = in.readInt();

                int colCount = in.readInt();
                List<ByteBuffer> minColumnNames = new ArrayList<>(colCount);
                for (int i = 0; i < colCount; i++)
                    minColumnNames.add(ByteBufferUtil.readWithShortLength(in));

                colCount = in.readInt();
                List<ByteBuffer> maxColumnNames = new ArrayList<>(colCount);
                for (int i = 0; i < colCount; i++)
                    maxColumnNames.add(ByteBufferUtil.readWithShortLength(in));

                if (types.contains(MetadataType.VALIDATION))
                    components.put(MetadataType.VALIDATION,
                                   new ValidationMetadata(partitioner, bloomFilterFPChance));
                if (types.contains(MetadataType.STATS))
                    components.put(MetadataType.STATS,
                                   new StatsMetadata(partitionSizes,
                                                     columnCounts,
                                                     replayPosition,
                                                     minTimestamp,
                                                     maxTimestamp,
                                                     Integer.MAX_VALUE,
                                                     maxLocalDeletionTime,
                                                     0,
                                                     Integer.MAX_VALUE,
                                                     compressionRatio,
                                                     tombstoneHistogram,
                                                     sstableLevel,
                                                     minColumnNames,
                                                     maxColumnNames,
                                                     true,
                                                     ActiveRepairService.UNREPAIRED_SSTABLE,
                                                     -1,
                                                     -1));
                if (types.contains(MetadataType.COMPACTION))
                    components.put(MetadataType.COMPACTION,
                                   new CompactionMetadata(null));
            }
        }
        return components;
    public void serialize(DataOutputPlus out, int version) throws IOException
    {
        CompactEndpointSerializationHelper.serialize(from, out);

        out.writeInt(MessagingService.Verb.convertForMessagingServiceVersion(verb, version).ordinal());
        out.writeInt(parameters.size());
        for (Map.Entry<String, byte[]> entry : parameters.entrySet())
        {
            out.writeUTF(entry.getKey());
            out.writeInt(entry.getValue().length);
            out.write(entry.getValue());
        }

        long longSize = payloadSize(version);
        assert longSize <= Integer.MAX_VALUE; // larger values are supported in sstables but not messages
        out.writeInt((int) longSize);
        if (payload != null)
            serializer.serialize(payload, out, version);
    }

+++++++++++++++++++++++
    public void serialize(DataOutputPlus out, int version) throws IOException
    {
        CompactEndpointSerializationHelper.serialize(from, out);

        out.writeInt(verb.ordinal());
        out.writeInt(parameters.size());
        for (Map.Entry<String, byte[]> entry : parameters.entrySet())
        {
            out.writeUTF(entry.getKey());
            out.writeInt(entry.getValue().length);
            out.write(entry.getValue());
        }

        long longSize = payloadSize(version);
        assert longSize <= Integer.MAX_VALUE; // larger values are supported in sstables but not messages
        out.writeInt((int) longSize);
        if (payload != null)
            serializer.serialize(payload, out, version);
    }

    public Boolean deserialize(ByteBuffer bytes)
    {
        if (bytes == null || bytes.remaining() == 0)
            return null;

        byte value = bytes.get(bytes.position());
        return value != 0;
    }

+++++++++++++++++++++++
    public Boolean deserialize(ByteBuffer bytes)
    {
        if (bytes.remaining() == 0)
            return null;

        byte value = bytes.get(bytes.position());
        return value != 0;
    }

    public List<T> deserializeForNativeProtocol(ByteBuffer bytes, int version)
    {
        try
        {
            ByteBuffer input = bytes.duplicate();
            int n = readCollectionSize(input, version);

            if (n < 0)
                throw new MarshalException("The data cannot be deserialized as a list");

            // If the received bytes are not corresponding to a list, n might be a huge number.
            // In such a case we do not want to initialize the list with that size as it can result
            // in an OOM (see CASSANDRA-12618). On the other hand we do not want to have to resize the list
            // if we can avoid it, so we put a reasonable limit on the initialCapacity.
            List<T> l = new ArrayList<T>(Math.min(n, 256));
            for (int i = 0; i < n; i++)
            {
                // We can have nulls in lists that are used for IN values
                ByteBuffer databb = readValue(input, version);
                if (databb != null)
                {
                    elements.validate(databb);
                    l.add(elements.deserialize(databb));
                }
                else
                {
                    l.add(null);
                }
            }

            if (input.hasRemaining())
                throw new MarshalException("Unexpected extraneous bytes after list value");

            return l;
        }
        catch (BufferUnderflowException e)
        {
            throw new MarshalException("Not enough bytes to read a list");
        }
    }

    /**
     * Returns the element at the given index in a list.
     * @param serializedList a serialized list
     * @param index the index to get
     * @return the serialized element at the given index, or null if the index exceeds the list size
     */
+++++++++++++++++++++++
    public List<T> deserializeForNativeProtocol(ByteBuffer bytes, int version)
    {
        try
        {
            ByteBuffer input = bytes.duplicate();
            int n = readCollectionSize(input, version);
            List<T> l = new ArrayList<T>(n);
            for (int i = 0; i < n; i++)
            {
                // We can have nulls in lists that are used for IN values
                ByteBuffer databb = readValue(input, version);
                if (databb != null)
                {
                    elements.validate(databb);
                    l.add(elements.deserialize(databb));
                }
                else
                {
                    l.add(null);
                }
            }

            if (input.hasRemaining())
                throw new MarshalException("Unexpected extraneous bytes after list value");

            return l;
        }
        catch (BufferUnderflowException e)
        {
            throw new MarshalException("Not enough bytes to read a list");
        }
    }

    /**
     * Returns the element at the given index in a list.
     * @param serializedList a serialized list
     * @param index the index to get
     * @return the serialized element at the given index, or null if the index exceeds the list size
     */
    public Map<K, V> deserializeForNativeProtocol(ByteBuffer bytes, int version)
    {
        try
        {
            ByteBuffer input = bytes.duplicate();
            int n = readCollectionSize(input, version);

            if (n < 0)
                throw new MarshalException("The data cannot be deserialized as a map");

            // If the received bytes are not corresponding to a map, n might be a huge number.
            // In such a case we do not want to initialize the map with that initialCapacity as it can result
            // in an OOM when put is called (see CASSANDRA-12618). On the other hand we do not want to have to resize
            // the map if we can avoid it, so we put a reasonable limit on the initialCapacity.
            Map<K, V> m = new LinkedHashMap<K, V>(Math.min(n, 256));
            for (int i = 0; i < n; i++)
            {
                ByteBuffer kbb = readValue(input, version);
                keys.validate(kbb);

                ByteBuffer vbb = readValue(input, version);
                values.validate(vbb);

                m.put(keys.deserialize(kbb), values.deserialize(vbb));
            }
            if (input.hasRemaining())
                throw new MarshalException("Unexpected extraneous bytes after map value");
            return m;
        }
        catch (BufferUnderflowException e)
        {
            throw new MarshalException("Not enough bytes to read a map");
        }
    }

    /**
     * Given a serialized map, gets the value associated with a given key.
     * @param serializedMap a serialized map
     * @param serializedKey a serialized key
     * @param keyType the key type for the map
     * @return the value associated with the key if one exists, null otherwise
     */
+++++++++++++++++++++++
    public Map<K, V> deserializeForNativeProtocol(ByteBuffer bytes, int version)
    {
        try
        {
            ByteBuffer input = bytes.duplicate();
            int n = readCollectionSize(input, version);
            Map<K, V> m = new LinkedHashMap<K, V>(n);
            for (int i = 0; i < n; i++)
            {
                ByteBuffer kbb = readValue(input, version);
                keys.validate(kbb);

                ByteBuffer vbb = readValue(input, version);
                values.validate(vbb);

                m.put(keys.deserialize(kbb), values.deserialize(vbb));
            }
            if (input.hasRemaining())
                throw new MarshalException("Unexpected extraneous bytes after map value");
            return m;
        }
        catch (BufferUnderflowException e)
        {
            throw new MarshalException("Not enough bytes to read a map");
        }
    }

    /**
     * Given a serialized map, gets the value associated with a given key.
     * @param serializedMap a serialized map
     * @param serializedKey a serialized key
     * @param keyType the key type for the map
     * @return the value associated with the key if one exists, null otherwise
     */
    public Set<T> deserializeForNativeProtocol(ByteBuffer bytes, int version)
    {
        try
        {
            ByteBuffer input = bytes.duplicate();
            int n = readCollectionSize(input, version);

            if (n < 0)
                throw new MarshalException("The data cannot be deserialized as a set");

            // If the received bytes are not corresponding to a set, n might be a huge number.
            // In such a case we do not want to initialize the set with that initialCapacity as it can result
            // in an OOM when add is called (see CASSANDRA-12618). On the other hand we do not want to have to resize
            // the set if we can avoid it, so we put a reasonable limit on the initialCapacity.
            Set<T> l = new LinkedHashSet<T>(Math.min(n, 256));

            for (int i = 0; i < n; i++)
            {
                ByteBuffer databb = readValue(input, version);
                elements.validate(databb);
                l.add(elements.deserialize(databb));
            }
            if (input.hasRemaining())
                throw new MarshalException("Unexpected extraneous bytes after set value");
            return l;
        }
        catch (BufferUnderflowException e)
        {
            throw new MarshalException("Not enough bytes to read a set");
        }
    }

+++++++++++++++++++++++
    public Set<T> deserializeForNativeProtocol(ByteBuffer bytes, int version)
    {
        try
        {
            ByteBuffer input = bytes.duplicate();
            int n = readCollectionSize(input, version);
            Set<T> l = new LinkedHashSet<T>(n);
            for (int i = 0; i < n; i++)
            {
                ByteBuffer databb = readValue(input, version);
                elements.validate(databb);
                l.add(elements.deserialize(databb));
            }
            if (input.hasRemaining())
                throw new MarshalException("Unexpected extraneous bytes after set value");
            return l;
        }
        catch (BufferUnderflowException e)
        {
            throw new MarshalException("Not enough bytes to read a set");
        }
    }

    public ByteBuffer serialize(int protocolVersion)
    {
        assert rowMark == null || protocolVersion == rowMark.protocolVersion;
        try
        {
            return protocolVersion > VERSION_3 ? modernSerialize() : legacySerialize(true);
        }
        catch (IOException e)
        {
            throw new RuntimeException(e);
        }
    }

+++++++++++++++++++++++
    public ByteBuffer serialize(int protocolVersion)
    {
        assert rowMark == null || protocolVersion == rowMark.protocolVersion;
        try (DataOutputBuffer out = new DataOutputBufferFixed(serializedSize(protocolVersion)))
        {
            ByteBuffer pk = partitionKey == null ? ByteBufferUtil.EMPTY_BYTE_BUFFER : partitionKey;
            ByteBuffer mark = rowMark == null ? ByteBufferUtil.EMPTY_BYTE_BUFFER : rowMark.mark;
            if (protocolVersion <= Server.VERSION_3)
            {
                ByteBufferUtil.writeWithShortLength(pk, out);
                ByteBufferUtil.writeWithShortLength(mark, out);
                out.writeInt(remaining);
                out.writeInt(remainingInPartition);
            }
            else
            {
                ByteBufferUtil.writeWithVIntLength(pk, out);
                ByteBufferUtil.writeWithVIntLength(mark, out);
                out.writeUnsignedVInt(remaining);
                out.writeUnsignedVInt(remainingInPartition);
            }
            return out.buffer();
        }
        catch (IOException e)
        {
            throw new RuntimeException(e);
        }
    }

    public int serializedSize(int protocolVersion)
    {
        assert rowMark == null || protocolVersion == rowMark.protocolVersion;

        return protocolVersion > VERSION_3 ? modernSerializedSize() : legacySerializedSize(true);
    }

    /**
     * It's possible to receive a V3 paging state on a V4 client session, and vice versa - so we cannot
     * blindly rely on the protocol version provided. We must verify first that the buffer indeed contains
     * a paging state that adheres to the protocol version provided, or, if not - see if it is in a different
     * version, in which case we try the other format.
     */
+++++++++++++++++++++++
    public int serializedSize(int protocolVersion)
    {
        assert rowMark == null || protocolVersion == rowMark.protocolVersion;
        ByteBuffer pk = partitionKey == null ? ByteBufferUtil.EMPTY_BYTE_BUFFER : partitionKey;
        ByteBuffer mark = rowMark == null ? ByteBufferUtil.EMPTY_BYTE_BUFFER : rowMark.mark;
        if (protocolVersion <= Server.VERSION_3)
        {
            return ByteBufferUtil.serializedSizeWithShortLength(pk)
                 + ByteBufferUtil.serializedSizeWithShortLength(mark)
                 + 8; // remaining & remainingInPartition
        }
        else
        {
            return ByteBufferUtil.serializedSizeWithVIntLength(pk)
                 + ByteBufferUtil.serializedSizeWithVIntLength(mark)
                 + TypeSizes.sizeofUnsignedVInt(remaining)
                 + TypeSizes.sizeofUnsignedVInt(remainingInPartition);
        }
    }

    @Override
    public static PagingState deserialize(ByteBuffer bytes, int protocolVersion)
    {
        if (bytes == null)
            return null;

        try
        {
            /*
             * We can't just attempt to deser twice, as we risk to misinterpet short/vint
             * lengths and allocate huge byte arrays for readWithVIntLength() or,
             * to a lesser extent, readWithShortLength()
             */

            if (protocolVersion > VERSION_3)
            {
                if (isModernSerialized(bytes)) return modernDeserialize(bytes, protocolVersion);
                if (isLegacySerialized(bytes)) return legacyDeserialize(bytes, VERSION_3);
            }

            if (protocolVersion < VERSION_4)
            {
                if (isLegacySerialized(bytes)) return legacyDeserialize(bytes, protocolVersion);
                if (isModernSerialized(bytes)) return modernDeserialize(bytes, VERSION_4);
            }
        }
        catch (IOException e)
        {
            throw new ProtocolException("Invalid value for the paging state");
        }

        throw new ProtocolException("Invalid value for the paging state");
    }

    /*
     * Modern serde (> VERSION_3)
     */

    @SuppressWarnings({ "resource", "RedundantSuppression" })
+++++++++++++++++++++++
    public static PagingState deserialize(ByteBuffer bytes, int protocolVersion)
    {
        if (bytes == null)
            return null;

        try (DataInputBuffer in = new DataInputBuffer(bytes, true))
        {
            ByteBuffer pk;
            RowMark mark;
            int remaining, remainingInPartition;
            if (protocolVersion <= Server.VERSION_3)
            {
                pk = ByteBufferUtil.readWithShortLength(in);
                mark = new RowMark(ByteBufferUtil.readWithShortLength(in), protocolVersion);
                remaining = in.readInt();
                // Note that while 'in.available()' is theoretically an estimate of how many bytes are available
                // without blocking, we know that since we're reading a ByteBuffer it will be exactly how many
                // bytes remain to be read. And the reason we want to condition this is for backward compatility
                // as we used to not set this.
                remainingInPartition = in.available() > 0 ? in.readInt() : Integer.MAX_VALUE;
            }
            else
            {
                pk = ByteBufferUtil.readWithVIntLength(in);
                mark = new RowMark(ByteBufferUtil.readWithVIntLength(in), protocolVersion);
                remaining = (int)in.readUnsignedVInt();
                remainingInPartition = (int)in.readUnsignedVInt();
            }
            return new PagingState(pk.hasRemaining() ? pk : null,
                                   mark.mark.hasRemaining() ? mark : null,
                                   remaining,
                                   remainingInPartition);
        }
        catch (IOException e)
        {
            throw new ProtocolException("Invalid value for the paging state");
        }
    }

add: 12, delete: 208, change: 773, unhandled: 2 size_exceptions: 10 size_serialize: 14
-----------cassandra-3.1 vs cassandra-3.1.1-----------
add: 0, delete: 0, change: 2, unhandled: 0 size_exceptions: 0 size_serialize: 0
-----------cassandra-3.1.1 vs cassandra-3.2-----------
    public static void serialize(IPartitioner partitioner, Collection<Token> tokens, DataOutput out) throws IOException
    {
        for (Token token : tokens)
        {
            byte[] bintoken = partitioner.getTokenFactory().toByteArray(token).array();
            out.writeInt(bintoken.length);
            out.write(bintoken);
        }
        out.writeInt(0);
    }

+++++++++++++++++++++++
    public static void serialize(IPartitioner partitioner, Collection<Token> tokens, DataOutput out) throws IOException
    {
        for (Token token : tokens)
        {
            ByteBuffer tokenBuffer = partitioner.getTokenFactory().toByteArray(token);
            assert tokenBuffer.arrayOffset() == 0;
            ByteBufferUtil.writeWithLength(tokenBuffer.array(), out);
        }
        out.writeInt(0);
    }

    public Byte deserialize(ByteBuffer bytes)
    {
        return bytes.remaining() == 0 ? null : bytes.get(bytes.position());
    }

+++++++++++++++++++++++
    public Byte deserialize(ByteBuffer bytes)
    {
        return bytes == null || bytes.remaining() == 0 ? null : bytes.get(bytes.position());
    }

    public ByteBuffer serialize(UUID value)
    {
        return value == null ? ByteBufferUtil.EMPTY_BYTE_BUFFER : ByteBuffer.wrap(UUIDGen.decompose(value));
    }

+++++++++++++++++++++++
    public ByteBuffer serialize(UUID value)
    {
        return value == null ? ByteBufferUtil.EMPTY_BYTE_BUFFER : UUIDGen.toByteBuffer(value);
    }

add: 47, delete: 1, change: 299, unhandled: 1 size_exceptions: 2 size_serialize: 3
-----------cassandra-3.2 vs cassandra-3.2.1-----------
add: 0, delete: 0, change: 1, unhandled: 0 size_exceptions: 0 size_serialize: 0
-----------cassandra-3.2.1 vs cassandra-3.3-----------
    public static long serializedSizeAsLegacyPartition(UnfilteredRowIterator partition, int version)
    {
        assert version < MessagingService.VERSION_30;

        if (partition.isEmpty())
            return TypeSizes.sizeof(false);

        long size = TypeSizes.sizeof(true);

        LegacyLayout.LegacyUnfilteredPartition legacyPartition = LegacyLayout.fromUnfilteredRowIterator(partition);

        size += UUIDSerializer.serializer.serializedSize(partition.metadata().cfId, version);
        size += DeletionTime.serializer.serializedSize(legacyPartition.partitionDeletion);
        size += legacyPartition.rangeTombstones.serializedSize(partition.metadata());

        // begin cell serialization
        size += TypeSizes.sizeof(legacyPartition.cells.size());
        for (LegacyLayout.LegacyCell cell : legacyPartition.cells)
        {
            size += ByteBufferUtil.serializedSizeWithShortLength(cell.name.encode(partition.metadata()));
            size += 1;  // serialization flags
            if (cell.kind == LegacyLayout.LegacyCell.Kind.EXPIRING)
            {
                size += TypeSizes.sizeof(cell.ttl);
                size += TypeSizes.sizeof(cell.localDeletionTime);
            }
            else if (cell.kind == LegacyLayout.LegacyCell.Kind.DELETED)
            {
                size += TypeSizes.sizeof(cell.timestamp);
                // localDeletionTime replaces cell.value as the body
                size += TypeSizes.sizeof(TypeSizes.sizeof(cell.localDeletionTime));
                size += TypeSizes.sizeof(cell.localDeletionTime);
                continue;
            }
            else if (cell.kind == LegacyLayout.LegacyCell.Kind.COUNTER)
            {
                size += TypeSizes.sizeof(Long.MIN_VALUE);  // timestampOfLastDelete
            }

            size += TypeSizes.sizeof(cell.timestamp);
            size += ByteBufferUtil.serializedSizeWithLength(cell.value);
        }

        return size;
    }

    // For thrift sake
+++++++++++++++++++++++
    public static long serializedSizeAsLegacyPartition(ReadCommand command, UnfilteredRowIterator partition, int version)
    {
        assert version < MessagingService.VERSION_30;

        if (partition.isEmpty())
            return TypeSizes.sizeof(false);

        long size = TypeSizes.sizeof(true);

        LegacyLayout.LegacyUnfilteredPartition legacyPartition = LegacyLayout.fromUnfilteredRowIterator(command, partition);

        size += UUIDSerializer.serializer.serializedSize(partition.metadata().cfId, version);
        size += DeletionTime.serializer.serializedSize(legacyPartition.partitionDeletion);
        size += legacyPartition.rangeTombstones.serializedSize(partition.metadata());

        // begin cell serialization
        size += TypeSizes.sizeof(legacyPartition.cells.size());
        for (LegacyLayout.LegacyCell cell : legacyPartition.cells)
        {
            size += ByteBufferUtil.serializedSizeWithShortLength(cell.name.encode(partition.metadata()));
            size += 1;  // serialization flags
            if (cell.kind == LegacyLayout.LegacyCell.Kind.EXPIRING)
            {
                size += TypeSizes.sizeof(cell.ttl);
                size += TypeSizes.sizeof(cell.localDeletionTime);
            }
            else if (cell.kind == LegacyLayout.LegacyCell.Kind.DELETED)
            {
                size += TypeSizes.sizeof(cell.timestamp);
                // localDeletionTime replaces cell.value as the body
                size += TypeSizes.sizeof(TypeSizes.sizeof(cell.localDeletionTime));
                size += TypeSizes.sizeof(cell.localDeletionTime);
                continue;
            }
            else if (cell.kind == LegacyLayout.LegacyCell.Kind.COUNTER)
            {
                size += TypeSizes.sizeof(Long.MIN_VALUE);  // timestampOfLastDelete
            }

            size += TypeSizes.sizeof(cell.timestamp);
            size += ByteBufferUtil.serializedSizeWithLength(cell.value);
        }

        return size;
    }

    // For thrift sake
    public static void serializeAsLegacyPartition(UnfilteredRowIterator partition, DataOutputPlus out, int version) throws IOException
    {
        assert version < MessagingService.VERSION_30;

        out.writeBoolean(true);

        LegacyLayout.LegacyUnfilteredPartition legacyPartition = LegacyLayout.fromUnfilteredRowIterator(partition);

        UUIDSerializer.serializer.serialize(partition.metadata().cfId, out, version);
        DeletionTime.serializer.serialize(legacyPartition.partitionDeletion, out);

        legacyPartition.rangeTombstones.serialize(out, partition.metadata());

        // begin cell serialization
        out.writeInt(legacyPartition.cells.size());
        for (LegacyLayout.LegacyCell cell : legacyPartition.cells)
        {
            ByteBufferUtil.writeWithShortLength(cell.name.encode(partition.metadata()), out);
            out.writeByte(cell.serializationFlags());
            if (cell.isExpiring())
            {
                out.writeInt(cell.ttl);
                out.writeInt(cell.localDeletionTime);
            }
            else if (cell.isTombstone())
            {
                out.writeLong(cell.timestamp);
                out.writeInt(TypeSizes.sizeof(cell.localDeletionTime));
                out.writeInt(cell.localDeletionTime);
                continue;
            }
            else if (cell.isCounterUpdate())
            {
                out.writeLong(cell.timestamp);
                long count = CounterContext.instance().getLocalCount(cell.value);
                ByteBufferUtil.writeWithLength(ByteBufferUtil.bytes(count), out);
                continue;
            }
            else if (cell.isCounter())
            {
                out.writeLong(Long.MIN_VALUE);  // timestampOfLastDelete (not used, and MIN_VALUE is the default)
            }

            out.writeLong(cell.timestamp);
            ByteBufferUtil.writeWithLength(cell.value, out);
        }
    }

    // For the old wire format
    // Note: this can return null if an empty partition is serialized!
+++++++++++++++++++++++
    public static void serializeAsLegacyPartition(ReadCommand command, UnfilteredRowIterator partition, DataOutputPlus out, int version) throws IOException
    {
        assert version < MessagingService.VERSION_30;

        out.writeBoolean(true);

        LegacyLayout.LegacyUnfilteredPartition legacyPartition = LegacyLayout.fromUnfilteredRowIterator(command, partition);

        UUIDSerializer.serializer.serialize(partition.metadata().cfId, out, version);
        DeletionTime.serializer.serialize(legacyPartition.partitionDeletion, out);

        legacyPartition.rangeTombstones.serialize(out, partition.metadata());

        // begin cell serialization
        out.writeInt(legacyPartition.cells.size());
        for (LegacyLayout.LegacyCell cell : legacyPartition.cells)
        {
            ByteBufferUtil.writeWithShortLength(cell.name.encode(partition.metadata()), out);
            out.writeByte(cell.serializationFlags());
            if (cell.isExpiring())
            {
                out.writeInt(cell.ttl);
                out.writeInt(cell.localDeletionTime);
            }
            else if (cell.isTombstone())
            {
                out.writeLong(cell.timestamp);
                out.writeInt(TypeSizes.sizeof(cell.localDeletionTime));
                out.writeInt(cell.localDeletionTime);
                continue;
            }
            else if (cell.isCounterUpdate())
            {
                out.writeLong(cell.timestamp);
                long count = CounterContext.instance().getLocalCount(cell.value);
                ByteBufferUtil.writeWithLength(ByteBufferUtil.bytes(count), out);
                continue;
            }
            else if (cell.isCounter())
            {
                out.writeLong(Long.MIN_VALUE);  // timestampOfLastDelete (not used, and MIN_VALUE is the default)
            }

            out.writeLong(cell.timestamp);
            ByteBufferUtil.writeWithLength(cell.value, out);
        }
    }

    // For the old wire format
    // Note: this can return null if an empty partition is serialized!
add: 11, delete: 2, change: 107, unhandled: 0 size_exceptions: 1 size_serialize: 2
-----------cassandra-3.3 vs cassandra-3.4-----------
    private static Mutation deserializeLegacyMutation(UntypedResultSet.Row row)
    {
        try (DataInputBuffer dib = new DataInputBuffer(row.getBlob("mutation"), true))
        {
            Mutation mutation = Mutation.serializer.deserialize(dib,
                                                                row.getInt("message_version"));
            mutation.getPartitionUpdates().forEach(PartitionUpdate::validate);
            return mutation;
        }
        catch (IOException e)
        {
            logger.error("Failed to migrate a hint for {} from legacy {}.{} table: {}",
                         row.getUUID("target_id"),
                         SystemKeyspace.NAME,
                         SystemKeyspace.LEGACY_HINTS,
                         e);
            return null;
        }
        catch (MarshalException e)
        {
            logger.warn("Failed to validate a hint for {} (table id {}) from legacy {}.{} table - skipping: {})",
                        row.getUUID("target_id"),
                        SystemKeyspace.NAME,
                        SystemKeyspace.LEGACY_HINTS,
                        e);
            return null;
        }
    }

+++++++++++++++++++++++
    private static Mutation deserializeLegacyMutation(UntypedResultSet.Row row)
    {
        try (DataInputBuffer dib = new DataInputBuffer(row.getBlob("mutation"), true))
        {
            Mutation mutation = Mutation.serializer.deserialize(dib,
                                                                row.getInt("message_version"));
            mutation.getPartitionUpdates().forEach(PartitionUpdate::validate);
            return mutation;
        }
        catch (IOException e)
        {
            logger.error("Failed to migrate a hint for {} from legacy {}.{} table",
                         row.getUUID("target_id"),
                         SystemKeyspace.NAME,
                         SystemKeyspace.LEGACY_HINTS,
                         e);
            return null;
        }
        catch (MarshalException e)
        {
            logger.warn("Failed to validate a hint for {} from legacy {}.{} table - skipping",
                        row.getUUID("target_id"),
                        SystemKeyspace.NAME,
                        SystemKeyspace.LEGACY_HINTS,
                        e);
            return null;
        }
    }

add: 126, delete: 1, change: 312, unhandled: 1 size_exceptions: 2 size_serialize: 1
-----------cassandra-3.4 vs cassandra-3.5-----------
add: 17, delete: 3, change: 137, unhandled: 0 size_exceptions: 1 size_serialize: 0
-----------cassandra-3.5 vs cassandra-3.6-----------
    private RefCountedMemory serialize(V value)
    {
        long serializedSize = serializer.serializedSize(value);
        if (serializedSize > Integer.MAX_VALUE)
            throw new IllegalArgumentException("Unable to allocate " + serializedSize + " bytes");

        RefCountedMemory freeableMemory;
        try
        {
            freeableMemory = new RefCountedMemory(serializedSize);
        }
        catch (OutOfMemoryError e)
        {
            return null;
        }

        try
        {
            serializer.serialize(value, new WrappedDataOutputStreamPlus(new MemoryOutputStream(freeableMemory)));
        }
        catch (IOException e)
        {
            freeableMemory.unreference();
            throw new RuntimeException(e);
        }
        return freeableMemory;
    }

+++++++++++++++++++++++
    private RefCountedMemory serialize(V value)
    {
        long serializedSize = serializer.serializedSize(value);
        if (serializedSize > Integer.MAX_VALUE)
            throw new IllegalArgumentException(String.format("Unable to allocate %s", FBUtilities.prettyPrintMemory(serializedSize)));

        RefCountedMemory freeableMemory;
        try
        {
            freeableMemory = new RefCountedMemory(serializedSize);
        }
        catch (OutOfMemoryError e)
        {
            return null;
        }

        try
        {
            serializer.serialize(value, new WrappedDataOutputStreamPlus(new MemoryOutputStream(freeableMemory)));
        }
        catch (IOException e)
        {
            freeableMemory.unreference();
            throw new RuntimeException(e);
        }
        return freeableMemory;
    }

    public ByteBuffer serializeForNativeProtocol(ColumnDefinition def, Iterator<Cell> cells, int version)
    {
        assert isMultiCell();
        List<ByteBuffer> values = serializedValues(cells);
        int size = collectionSize(values);
        return CollectionSerializer.pack(values, size, version);
    }

    @Override
+++++++++++++++++++++++
    public ByteBuffer serializeForNativeProtocol(Iterator<Cell> cells, int version)
    {
        assert isMultiCell();
        List<ByteBuffer> values = serializedValues(cells);
        int size = collectionSize(values);
        return CollectionSerializer.pack(values, size, version);
    }

    @Override
    private long serializedSize(RangeTombstoneMarker marker, SerializationHeader header, long previousUnfilteredSize, int version)
    {
        assert !header.isForSSTable();
        return 1 // flags
             + RangeTombstone.Bound.serializer.serializedSize(marker.clustering(), version, header.clusteringTypes())
             + serializedMarkerBodySize(marker, header, previousUnfilteredSize, version);
    }

+++++++++++++++++++++++
    private long serializedSize(RangeTombstoneMarker marker, SerializationHeader header, long previousUnfilteredSize, int version)
    {
        assert !header.isForSSTable();
        return 1 // flags
             + ClusteringBoundOrBoundary.serializer.serializedSize(marker.clustering(), version, header.clusteringTypes())
             + serializedMarkerBodySize(marker, header, previousUnfilteredSize, version);
    }

    public Row deserializeRowBody(DataInputPlus in,
                                  SerializationHeader header,
                                  SerializationHelper helper,
                                  int flags,
                                  int extendedFlags,
                                  Row.Builder builder)
    throws IOException
    {
        try
        {
            boolean isStatic = isStatic(extendedFlags);
            boolean hasTimestamp = (flags & HAS_TIMESTAMP) != 0;
            boolean hasTTL = (flags & HAS_TTL) != 0;
            boolean hasDeletion = (flags & HAS_DELETION) != 0;
            boolean deletionIsShadowable = (extendedFlags & HAS_SHADOWABLE_DELETION) != 0;
            boolean hasComplexDeletion = (flags & HAS_COMPLEX_DELETION) != 0;
            boolean hasAllColumns = (flags & HAS_ALL_COLUMNS) != 0;
            Columns headerColumns = header.columns(isStatic);

            if (header.isForSSTable())
            {
                in.readUnsignedVInt(); // Skip row size
                in.readUnsignedVInt(); // previous unfiltered size
            }

            LivenessInfo rowLiveness = LivenessInfo.EMPTY;
            if (hasTimestamp)
            {
                long timestamp = header.readTimestamp(in);
                int ttl = hasTTL ? header.readTTL(in) : LivenessInfo.NO_TTL;
                int localDeletionTime = hasTTL ? header.readLocalDeletionTime(in) : LivenessInfo.NO_EXPIRATION_TIME;
                rowLiveness = LivenessInfo.create(timestamp, ttl, localDeletionTime);
            }

            builder.addPrimaryKeyLivenessInfo(rowLiveness);
            builder.addRowDeletion(hasDeletion ? new Row.Deletion(header.readDeletionTime(in), deletionIsShadowable) : Row.Deletion.LIVE);

            Columns columns = hasAllColumns ? headerColumns : Columns.serializer.deserializeSubset(headerColumns, in);
            for (ColumnDefinition column : columns)
            {
                if (column.isSimple())
                    readSimpleColumn(column, in, header, helper, builder, rowLiveness);
                else
                    readComplexColumn(column, in, header, helper, hasComplexDeletion, builder, rowLiveness);
            }

            return builder.build();
        }
        catch (RuntimeException | AssertionError e)
        {
            // Corrupted data could be such that it triggers an assertion in the row Builder, or break one of its assumption.
            // Of course, a bug in said builder could also trigger this, but it's impossible a priori to always make the distinction
            // between a real bug and data corrupted in just the bad way. Besides, re-throwing as an IOException doesn't hide the
            // exception, it just make we catch it properly and mark the sstable as corrupted.
            throw new IOException("Error building row with data deserialized from " + in, e);
        }
    }

+++++++++++++++++++++++
    public Row deserializeRowBody(DataInputPlus in,
                                  SerializationHeader header,
                                  SerializationHelper helper,
                                  int flags,
                                  int extendedFlags,
                                  Row.Builder builder)
    throws IOException
    {
        try
        {
            boolean isStatic = isStatic(extendedFlags);
            boolean hasTimestamp = (flags & HAS_TIMESTAMP) != 0;
            boolean hasTTL = (flags & HAS_TTL) != 0;
            boolean hasDeletion = (flags & HAS_DELETION) != 0;
            boolean deletionIsShadowable = (extendedFlags & HAS_SHADOWABLE_DELETION) != 0;
            boolean hasComplexDeletion = (flags & HAS_COMPLEX_DELETION) != 0;
            boolean hasAllColumns = (flags & HAS_ALL_COLUMNS) != 0;
            Columns headerColumns = header.columns(isStatic);

            if (header.isForSSTable())
            {
                in.readUnsignedVInt(); // Skip row size
                in.readUnsignedVInt(); // previous unfiltered size
            }

            LivenessInfo rowLiveness = LivenessInfo.EMPTY;
            if (hasTimestamp)
            {
                long timestamp = header.readTimestamp(in);
                int ttl = hasTTL ? header.readTTL(in) : LivenessInfo.NO_TTL;
                int localDeletionTime = hasTTL ? header.readLocalDeletionTime(in) : LivenessInfo.NO_EXPIRATION_TIME;
                rowLiveness = LivenessInfo.withExpirationTime(timestamp, ttl, localDeletionTime);
            }

            builder.addPrimaryKeyLivenessInfo(rowLiveness);
            builder.addRowDeletion(hasDeletion ? new Row.Deletion(header.readDeletionTime(in), deletionIsShadowable) : Row.Deletion.LIVE);

            Columns columns = hasAllColumns ? headerColumns : Columns.serializer.deserializeSubset(headerColumns, in);
            for (ColumnDefinition column : columns)
            {
                if (column.isSimple())
                    readSimpleColumn(column, in, header, helper, builder, rowLiveness);
                else
                    readComplexColumn(column, in, header, helper, hasComplexDeletion, builder, rowLiveness);
            }

            return builder.build();
        }
        catch (RuntimeException | AssertionError e)
        {
            // Corrupted data could be such that it triggers an assertion in the row Builder, or break one of its assumption.
            // Of course, a bug in said builder could also trigger this, but it's impossible a priori to always make the distinction
            // between a real bug and data corrupted in just the bad way. Besides, re-throwing as an IOException doesn't hide the
            // exception, it just make we catch it properly and mark the sstable as corrupted.
            throw new IOException("Error building row with data deserialized from " + in, e);
        }
    }

    public RangeTombstoneMarker deserializeMarkerBody(DataInputPlus in, SerializationHeader header, RangeTombstone.Bound bound)
    throws IOException
    {
        if (header.isForSSTable())
        {
            in.readUnsignedVInt(); // marker size
            in.readUnsignedVInt(); // previous unfiltered size
        }

        if (bound.isBoundary())
            return new RangeTombstoneBoundaryMarker(bound, header.readDeletionTime(in), header.readDeletionTime(in));
        else
            return new RangeTombstoneBoundMarker(bound, header.readDeletionTime(in));
    }

+++++++++++++++++++++++
    public RangeTombstoneMarker deserializeMarkerBody(DataInputPlus in, SerializationHeader header, ClusteringBoundOrBoundary bound)
    throws IOException
    {
        if (header.isForSSTable())
        {
            in.readUnsignedVInt(); // marker size
            in.readUnsignedVInt(); // previous unfiltered size
        }

        if (bound.isBoundary())
            return new RangeTombstoneBoundaryMarker((ClusteringBoundary) bound, header.readDeletionTime(in), header.readDeletionTime(in));
        else
            return new RangeTombstoneBoundMarker((ClusteringBound) bound, header.readDeletionTime(in));
    }

    private void serialize(RangeTombstoneMarker marker, SerializationHeader header, DataOutputPlus out, long previousUnfilteredSize, int version)
    throws IOException
    {
        out.writeByte((byte)IS_MARKER);
        RangeTombstone.Bound.serializer.serialize(marker.clustering(), out, version, header.clusteringTypes());

        if (header.isForSSTable())
        {
            out.writeUnsignedVInt(serializedMarkerBodySize(marker, header, previousUnfilteredSize, version));
            out.writeUnsignedVInt(previousUnfilteredSize);
        }

        if (marker.isBoundary())
        {
            RangeTombstoneBoundaryMarker bm = (RangeTombstoneBoundaryMarker)marker;
            header.writeDeletionTime(bm.endDeletionTime(), out);
            header.writeDeletionTime(bm.startDeletionTime(), out);
        }
        else
        {
            header.writeDeletionTime(((RangeTombstoneBoundMarker)marker).deletionTime(), out);
        }
    }

+++++++++++++++++++++++
    private void serialize(RangeTombstoneMarker marker, SerializationHeader header, DataOutputPlus out, long previousUnfilteredSize, int version)
    throws IOException
    {
        out.writeByte((byte)IS_MARKER);
        ClusteringBoundOrBoundary.serializer.serialize(marker.clustering(), out, version, header.clusteringTypes());

        if (header.isForSSTable())
        {
            out.writeUnsignedVInt(serializedMarkerBodySize(marker, header, previousUnfilteredSize, version));
            out.writeUnsignedVInt(previousUnfilteredSize);
        }

        if (marker.isBoundary())
        {
            RangeTombstoneBoundaryMarker bm = (RangeTombstoneBoundaryMarker)marker;
            header.writeDeletionTime(bm.endDeletionTime(), out);
            header.writeDeletionTime(bm.startDeletionTime(), out);
        }
        else
        {
            header.writeDeletionTime(((RangeTombstoneBoundMarker)marker).deletionTime(), out);
        }
    }

    public Unfiltered deserialize(DataInputPlus in, SerializationHeader header, SerializationHelper helper, Row.Builder builder)
    throws IOException
    {
        // It wouldn't be wrong per-se to use an unsorted builder, but it would be inefficient so make sure we don't do it by mistake
        assert builder.isSorted();

        int flags = in.readUnsignedByte();
        if (isEndOfPartition(flags))
            return null;

        int extendedFlags = readExtendedFlags(in, flags);

        if (kind(flags) == Unfiltered.Kind.RANGE_TOMBSTONE_MARKER)
        {
            RangeTombstone.Bound bound = RangeTombstone.Bound.serializer.deserialize(in, helper.version, header.clusteringTypes());
            return deserializeMarkerBody(in, header, bound);
        }
        else
        {
            assert !isStatic(extendedFlags); // deserializeStaticRow should be used for that.
            builder.newRow(Clustering.serializer.deserialize(in, helper.version, header.clusteringTypes()));
            return deserializeRowBody(in, header, helper, flags, extendedFlags, builder);
        }
    }

+++++++++++++++++++++++
    public Unfiltered deserialize(DataInputPlus in, SerializationHeader header, SerializationHelper helper, Row.Builder builder)
    throws IOException
    {
        // It wouldn't be wrong per-se to use an unsorted builder, but it would be inefficient so make sure we don't do it by mistake
        assert builder.isSorted();

        int flags = in.readUnsignedByte();
        if (isEndOfPartition(flags))
            return null;

        int extendedFlags = readExtendedFlags(in, flags);

        if (kind(flags) == Unfiltered.Kind.RANGE_TOMBSTONE_MARKER)
        {
            ClusteringBoundOrBoundary bound = ClusteringBoundOrBoundary.serializer.deserialize(in, helper.version, header.clusteringTypes());
            return deserializeMarkerBody(in, header, bound);
        }
        else
        {
            assert !isStatic(extendedFlags); // deserializeStaticRow should be used for that.
            builder.newRow(Clustering.serializer.deserialize(in, helper.version, header.clusteringTypes()));
            return deserializeRowBody(in, header, helper, flags, extendedFlags, builder);
        }
    }

    public static Collection<Token> deserialize(IPartitioner partitioner, DataInput in) throws IOException
    {
        Collection<Token> tokens = new ArrayList<Token>();
        while (true)
        {
            int size = in.readInt();
            if (size < 1)
                break;
            logger.trace("Reading token of {} bytes", size);
            byte[] bintoken = new byte[size];
            in.readFully(bintoken);
            tokens.add(partitioner.getTokenFactory().fromByteArray(ByteBuffer.wrap(bintoken)));
        }
        return tokens;
+++++++++++++++++++++++
    public static Collection<Token> deserialize(IPartitioner partitioner, DataInput in) throws IOException
    {
        Collection<Token> tokens = new ArrayList<Token>();
        while (true)
        {
            int size = in.readInt();
            if (size < 1)
                break;
            logger.trace("Reading token of {}", FBUtilities.prettyPrintMemory(size));
            byte[] bintoken = new byte[size];
            in.readFully(bintoken);
            tokens.add(partitioner.getTokenFactory().fromByteArray(ByteBuffer.wrap(bintoken)));
        }
        return tokens;
    private void serializeBound(RangeTombstone.Bound bound, DeletionTime deletionTime) throws IOException
    {
        json.writeFieldName(bound.isStart() ? "start" : "end");
        json.writeStartObject();
        json.writeFieldName("type");
        json.writeString(bound.isInclusive() ? "inclusive" : "exclusive");
        serializeClustering(bound.clustering());
        serializeDeletion(deletionTime);
        json.writeEndObject();
    }

+++++++++++++++++++++++
    private void serializeBound(ClusteringBound bound, DeletionTime deletionTime) throws IOException
    {
        json.writeFieldName(bound.isStart() ? "start" : "end");
        json.writeStartObject();
        json.writeFieldName("type");
        json.writeString(bound.isInclusive() ? "inclusive" : "exclusive");
        serializeClustering(bound.clustering());
        serializeDeletion(deletionTime);
        json.writeEndObject();
    }

    private void serializePartition(UnfilteredRowIterator partition)
    {
        String key = metadata.getKeyValidator().getString(partition.partitionKey().getKey());
        try
        {
            json.writeStartObject();

            json.writeFieldName("partition");
            json.writeStartObject();
            json.writeFieldName("key");
            serializePartitionKey(partition.partitionKey());
            json.writeNumberField("position", this.currentScanner.getCurrentPosition());

            if (!partition.partitionLevelDeletion().isLive())
            {
                json.writeFieldName("deletion_info");
                objectIndenter.setCompact(true);
                json.writeStartObject();
                json.writeFieldName("deletion_time");
                json.writeNumber(partition.partitionLevelDeletion().markedForDeleteAt());
                json.writeFieldName("tstamp");
                json.writeNumber(partition.partitionLevelDeletion().localDeletionTime());
                json.writeEndObject();
                objectIndenter.setCompact(false);
                json.writeEndObject();
            }
            else
            {
                json.writeEndObject();
                json.writeFieldName("rows");
                json.writeStartArray();
                updatePosition();
                if (!partition.staticRow().isEmpty())
                {
                    serializeRow(partition.staticRow());
                }
                Unfiltered unfiltered;
                updatePosition();
                while (partition.hasNext())
                {
                    unfiltered = partition.next();
                    if (unfiltered instanceof Row)
                    {
                        serializeRow((Row) unfiltered);
                    }
                    else if (unfiltered instanceof RangeTombstoneMarker)
                    {
                        serializeTombstone((RangeTombstoneMarker) unfiltered);
                    }
                    updatePosition();
                }
                json.writeEndArray();
            }

            json.writeEndObject();
        }
        catch (IOException e)
        {
            logger.error("Fatal error parsing partition: {}", key, e);
        }
    }

+++++++++++++++++++++++
    private void serializePartition(UnfilteredRowIterator partition)
    {
        String key = metadata.getKeyValidator().getString(partition.partitionKey().getKey());
        try
        {
            json.writeStartObject();

            json.writeFieldName("partition");
            json.writeStartObject();
            json.writeFieldName("key");
            serializePartitionKey(partition.partitionKey());
            json.writeNumberField("position", this.currentScanner.getCurrentPosition());

            if (!partition.partitionLevelDeletion().isLive())
            {
                serializeDeletion(partition.partitionLevelDeletion());
            }
            else
            {
                json.writeEndObject();
                json.writeFieldName("rows");
                json.writeStartArray();
                updatePosition();
                if (!partition.staticRow().isEmpty())
                {
                    serializeRow(partition.staticRow());
                }
                Unfiltered unfiltered;
                updatePosition();
                while (partition.hasNext())
                {
                    unfiltered = partition.next();
                    if (unfiltered instanceof Row)
                    {
                        serializeRow((Row) unfiltered);
                    }
                    else if (unfiltered instanceof RangeTombstoneMarker)
                    {
                        serializeTombstone((RangeTombstoneMarker) unfiltered);
                    }
                    updatePosition();
                }
                json.writeEndArray();
            }

            json.writeEndObject();
        }
        catch (IOException e)
        {
            logger.error("Fatal error parsing partition: {}", key, e);
        }
    }

    private void serializeRow(Row row)
    {
        try
        {
            json.writeStartObject();
            String rowType = row.isStatic() ? "static_block" : "row";
            json.writeFieldName("type");
            json.writeString(rowType);
            json.writeNumberField("position", this.currentPosition);

            // Only print clustering information for non-static rows.
            if (!row.isStatic())
            {
                serializeClustering(row.clustering());
            }

            LivenessInfo liveInfo = row.primaryKeyLivenessInfo();
            if (!liveInfo.isEmpty())
            {
                objectIndenter.setCompact(false);
                json.writeFieldName("liveness_info");
                objectIndenter.setCompact(true);
                json.writeStartObject();
                json.writeFieldName("tstamp");
                json.writeNumber(liveInfo.timestamp());
                if (liveInfo.isExpiring())
                {
                    json.writeFieldName("ttl");
                    json.writeNumber(liveInfo.ttl());
                    json.writeFieldName("expires_at");
                    json.writeNumber(liveInfo.localExpirationTime());
                    json.writeFieldName("expired");
                    json.writeBoolean(liveInfo.localExpirationTime() < (System.currentTimeMillis() / 1000));
                }
                json.writeEndObject();
                objectIndenter.setCompact(false);
            }

            // If this is a deletion, indicate that, otherwise write cells.
            if (!row.deletion().isLive())
            {
                json.writeFieldName("deletion_info");
                objectIndenter.setCompact(true);
                json.writeStartObject();
                json.writeFieldName("deletion_time");
                json.writeNumber(row.deletion().time().markedForDeleteAt());
                json.writeFieldName("tstamp");
                json.writeNumber(row.deletion().time().localDeletionTime());
                json.writeEndObject();
                objectIndenter.setCompact(false);
            }
            else
            {
                json.writeFieldName("cells");
                json.writeStartArray();
                row.cells().forEach(c -> serializeCell(c, liveInfo));
                json.writeEndArray();
            }
            json.writeEndObject();
        }
        catch (IOException e)
        {
            logger.error("Fatal error parsing row.", e);
        }
    }

+++++++++++++++++++++++
    private void serializeRow(Row row)
    {
        try
        {
            json.writeStartObject();
            String rowType = row.isStatic() ? "static_block" : "row";
            json.writeFieldName("type");
            json.writeString(rowType);
            json.writeNumberField("position", this.currentPosition);

            // Only print clustering information for non-static rows.
            if (!row.isStatic())
            {
                serializeClustering(row.clustering());
            }

            LivenessInfo liveInfo = row.primaryKeyLivenessInfo();
            if (!liveInfo.isEmpty())
            {
                objectIndenter.setCompact(false);
                json.writeFieldName("liveness_info");
                objectIndenter.setCompact(true);
                json.writeStartObject();
                json.writeFieldName("tstamp");
                json.writeString(dateString(TimeUnit.MICROSECONDS, liveInfo.timestamp()));
                if (liveInfo.isExpiring())
                {
                    json.writeNumberField("ttl", liveInfo.ttl());
                    json.writeFieldName("expires_at");
                    json.writeString(dateString(TimeUnit.SECONDS, liveInfo.localExpirationTime()));
                    json.writeFieldName("expired");
                    json.writeBoolean(liveInfo.localExpirationTime() < (System.currentTimeMillis() / 1000));
                }
                json.writeEndObject();
                objectIndenter.setCompact(false);
            }

            // If this is a deletion, indicate that, otherwise write cells.
            if (!row.deletion().isLive())
            {
                serializeDeletion(row.deletion().time());
            }
            json.writeFieldName("cells");
            json.writeStartArray();
            for (ColumnData cd : row)
            {
                serializeColumnData(cd, liveInfo);
            }
            json.writeEndArray();
            json.writeEndObject();
        }
        catch (IOException e)
        {
            logger.error("Fatal error parsing row.", e);
        }
    }

    private void serializeCell(Cell cell, LivenessInfo liveInfo)
    {
        try
        {
            json.writeStartObject();
            objectIndenter.setCompact(true);
            json.writeFieldName("name");
            AbstractType<?> type = cell.column().type;
            json.writeString(cell.column().name.toCQLString());

            if (cell.path() != null && cell.path().size() > 0)
            {
                CollectionType ct = (CollectionType) type;
                json.writeFieldName("path");
                arrayIndenter.setCompact(true);
                json.writeStartArray();
                for (int i = 0; i < cell.path().size(); i++)
                {
                    json.writeString(ct.nameComparator().getString(cell.path().get(i)));
                }
                json.writeEndArray();
                arrayIndenter.setCompact(false);
            }
            if (cell.isTombstone())
            {
                json.writeFieldName("deletion_time");
                json.writeNumber(cell.localDeletionTime());
            }
            else
            {
                json.writeFieldName("value");
                json.writeString(cell.column().cellValueType().getString(cell.value()));
            }
            if (liveInfo.isEmpty() || cell.timestamp() != liveInfo.timestamp())
            {
                json.writeFieldName("tstamp");
                json.writeNumber(cell.timestamp());
            }
            if (cell.isExpiring() && (liveInfo.isEmpty() || cell.ttl() != liveInfo.ttl()))
            {
                json.writeFieldName("ttl");
                json.writeNumber(cell.ttl());
                json.writeFieldName("expires_at");
                json.writeNumber(cell.localDeletionTime());
                json.writeFieldName("expired");
                json.writeBoolean(!cell.isLive((int) (System.currentTimeMillis() / 1000)));
            }
            json.writeEndObject();
            objectIndenter.setCompact(false);
        }
        catch (IOException e)
        {
            logger.error("Failure parsing cell.", e);
        }
    }

    /**
     * A specialized {@link Indenter} that enables a 'compact' mode which puts all subsequent json values on the same
     * line. This is manipulated via {@link CompactIndenter#setCompact(boolean)}
     */
+++++++++++++++++++++++
    private void serializeCell(Cell cell, LivenessInfo liveInfo)
    {
        try
        {
            json.writeStartObject();
            objectIndenter.setCompact(true);
            json.writeFieldName("name");
            AbstractType<?> type = cell.column().type;
            json.writeString(cell.column().name.toCQLString());

            if (cell.path() != null && cell.path().size() > 0)
            {
                CollectionType ct = (CollectionType) type;
                json.writeFieldName("path");
                arrayIndenter.setCompact(true);
                json.writeStartArray();
                for (int i = 0; i < cell.path().size(); i++)
                {
                    json.writeString(ct.nameComparator().getString(cell.path().get(i)));
                }
                json.writeEndArray();
                arrayIndenter.setCompact(false);
            }
            if (cell.isTombstone())
            {
                json.writeFieldName("deletion_info");
                objectIndenter.setCompact(true);
                json.writeStartObject();
                json.writeFieldName("local_delete_time");
                json.writeString(dateString(TimeUnit.SECONDS, cell.localDeletionTime()));
                json.writeEndObject();
                objectIndenter.setCompact(false);
            }
            else
            {
                json.writeFieldName("value");
                json.writeString(cell.column().cellValueType().getString(cell.value()));
            }
            if (liveInfo.isEmpty() || cell.timestamp() != liveInfo.timestamp())
            {
                json.writeFieldName("tstamp");
                json.writeString(dateString(TimeUnit.MICROSECONDS, cell.timestamp()));
            }
            if (cell.isExpiring() && (liveInfo.isEmpty() || cell.ttl() != liveInfo.ttl()))
            {
                json.writeFieldName("ttl");
                json.writeNumber(cell.ttl());
                json.writeFieldName("expires_at");
                json.writeString(dateString(TimeUnit.SECONDS, cell.localDeletionTime()));
                json.writeFieldName("expired");
                json.writeBoolean(!cell.isLive((int) (System.currentTimeMillis() / 1000)));
            }
            json.writeEndObject();
            objectIndenter.setCompact(false);
        }
        catch (IOException e)
        {
            logger.error("Failure parsing cell.", e);
        }
    }

    private void serializeDeletion(DeletionTime deletion) throws IOException
    {
        json.writeFieldName("deletion_info");
        objectIndenter.setCompact(true);
        json.writeStartObject();
        json.writeFieldName("deletion_time");
        json.writeNumber(deletion.markedForDeleteAt());
        json.writeFieldName("tstamp");
        json.writeNumber(deletion.localDeletionTime());
        json.writeEndObject();
        objectIndenter.setCompact(false);
    }

+++++++++++++++++++++++
    private void serializeDeletion(DeletionTime deletion) throws IOException
    {
        json.writeFieldName("deletion_info");
        objectIndenter.setCompact(true);
        json.writeStartObject();
        json.writeFieldName("marked_deleted");
        json.writeString(dateString(TimeUnit.MICROSECONDS, deletion.markedForDeleteAt()));
        json.writeFieldName("local_delete_time");
        json.writeString(dateString(TimeUnit.SECONDS, deletion.localDeletionTime()));
        json.writeEndObject();
        objectIndenter.setCompact(false);
    }

add: 53, delete: 12, change: 397, unhandled: 1 size_exceptions: 9 size_serialize: 13
-----------cassandra-3.6 vs cassandra-3.7-----------
    public Unfiltered deserialize(DataInputPlus in, SerializationHeader header, SerializationHelper helper, Row.Builder builder)
    throws IOException
    {
        // It wouldn't be wrong per-se to use an unsorted builder, but it would be inefficient so make sure we don't do it by mistake
        assert builder.isSorted();

        int flags = in.readUnsignedByte();
        if (isEndOfPartition(flags))
            return null;

        int extendedFlags = readExtendedFlags(in, flags);

        if (kind(flags) == Unfiltered.Kind.RANGE_TOMBSTONE_MARKER)
        {
            ClusteringBoundOrBoundary bound = ClusteringBoundOrBoundary.serializer.deserialize(in, helper.version, header.clusteringTypes());
            return deserializeMarkerBody(in, header, bound);
        }
        else
        {
            assert !isStatic(extendedFlags); // deserializeStaticRow should be used for that.
            builder.newRow(Clustering.serializer.deserialize(in, helper.version, header.clusteringTypes()));
            return deserializeRowBody(in, header, helper, flags, extendedFlags, builder);
        }
    }

+++++++++++++++++++++++
    public Unfiltered deserialize(DataInputPlus in, SerializationHeader header, SerializationHelper helper, Row.Builder builder)
    throws IOException
    {
        // It wouldn't be wrong per-se to use an unsorted builder, but it would be inefficient so make sure we don't do it by mistake
        assert builder.isSorted();

        int flags = in.readUnsignedByte();
        if (isEndOfPartition(flags))
            return null;

        int extendedFlags = readExtendedFlags(in, flags);

        if (kind(flags) == Unfiltered.Kind.RANGE_TOMBSTONE_MARKER)
        {
            ClusteringBoundOrBoundary bound = ClusteringBoundOrBoundary.serializer.deserialize(in, helper.version, header.clusteringTypes());
            return deserializeMarkerBody(in, header, bound);
        }
        else
        {
            // deserializeStaticRow should be used for that.
            if (isStatic(extendedFlags))
                throw new IOException("Corrupt flags value for unfiltered partition (isStatic flag set): " + flags);

            builder.newRow(Clustering.serializer.deserialize(in, helper.version, header.clusteringTypes()));
            Row row = deserializeRowBody(in, header, helper, flags, extendedFlags, builder);
            // we do not write empty rows because Rows.collectStats(), called by BTW.applyToRow(), asserts that rows are not empty
            // if we don't throw here, then later the very same assertion in Rows.collectStats() will fail compactions
            // see BlackListingCompactionsTest and CASSANDRA-9530 for details
            if (row.isEmpty())
                throw new IOException("Corrupt empty row found in unfiltered partition");
            return row;
        }
    }

    public void serialize(Map<MetadataType, MetadataComponent> components, DataOutputPlus out, Version version) throws IOException
    {
        ValidationMetadata validation = (ValidationMetadata) components.get(MetadataType.VALIDATION);
        StatsMetadata stats = (StatsMetadata) components.get(MetadataType.STATS);
        CompactionMetadata compaction = (CompactionMetadata) components.get(MetadataType.COMPACTION);

        assert validation != null && stats != null && compaction != null && validation.partitioner != null;

        EstimatedHistogram.serializer.serialize(stats.estimatedPartitionSize, out);
        EstimatedHistogram.serializer.serialize(stats.estimatedColumnCount, out);
        ReplayPosition.serializer.serialize(stats.replayPosition, out);
        out.writeLong(stats.minTimestamp);
        out.writeLong(stats.maxTimestamp);
        out.writeInt(stats.maxLocalDeletionTime);
        out.writeDouble(validation.bloomFilterFPChance);
        out.writeDouble(stats.compressionRatio);
        out.writeUTF(validation.partitioner);
        out.writeInt(0); // compaction ancestors
        StreamingHistogram.serializer.serialize(stats.estimatedTombstoneDropTime, out);
        out.writeInt(stats.sstableLevel);
        out.writeInt(stats.minClusteringValues.size());
        for (ByteBuffer value : stats.minClusteringValues)
            ByteBufferUtil.writeWithShortLength(value, out);
        out.writeInt(stats.maxClusteringValues.size());
        for (ByteBuffer value : stats.maxClusteringValues)
            ByteBufferUtil.writeWithShortLength(value, out);
    }

    /**
     * Legacy serializer deserialize all components no matter what types are specified.
     */
    @Override
+++++++++++++++++++++++
    public void serialize(Map<MetadataType, MetadataComponent> components, DataOutputPlus out, Version version) throws IOException
    {
        ValidationMetadata validation = (ValidationMetadata) components.get(MetadataType.VALIDATION);
        StatsMetadata stats = (StatsMetadata) components.get(MetadataType.STATS);
        CompactionMetadata compaction = (CompactionMetadata) components.get(MetadataType.COMPACTION);

        assert validation != null && stats != null && compaction != null && validation.partitioner != null;

        EstimatedHistogram.serializer.serialize(stats.estimatedPartitionSize, out);
        EstimatedHistogram.serializer.serialize(stats.estimatedColumnCount, out);
        ReplayPosition.serializer.serialize(stats.commitLogUpperBound, out);
        out.writeLong(stats.minTimestamp);
        out.writeLong(stats.maxTimestamp);
        out.writeInt(stats.maxLocalDeletionTime);
        out.writeDouble(validation.bloomFilterFPChance);
        out.writeDouble(stats.compressionRatio);
        out.writeUTF(validation.partitioner);
        out.writeInt(0); // compaction ancestors
        StreamingHistogram.serializer.serialize(stats.estimatedTombstoneDropTime, out);
        out.writeInt(stats.sstableLevel);
        out.writeInt(stats.minClusteringValues.size());
        for (ByteBuffer value : stats.minClusteringValues)
            ByteBufferUtil.writeWithShortLength(value, out);
        out.writeInt(stats.maxClusteringValues.size());
        for (ByteBuffer value : stats.maxClusteringValues)
            ByteBufferUtil.writeWithShortLength(value, out);
        if (version.hasCommitLogLowerBound())
            ReplayPosition.serializer.serialize(stats.commitLogLowerBound, out);
    }

    /**
     * Legacy serializer deserialize all components no matter what types are specified.
     */
    @Override
    public Map<MetadataType, MetadataComponent> deserialize(Descriptor descriptor, EnumSet<MetadataType> types) throws IOException
    {
        Map<MetadataType, MetadataComponent> components = Maps.newHashMap();

        File statsFile = new File(descriptor.filenameFor(Component.STATS));
        if (!statsFile.exists() && types.contains(MetadataType.STATS))
        {
            components.put(MetadataType.STATS, MetadataCollector.defaultStatsMetadata());
        }
        else
        {
            try (DataInputStreamPlus in = new DataInputStreamPlus(new BufferedInputStream(new FileInputStream(statsFile))))
            {
                EstimatedHistogram partitionSizes = EstimatedHistogram.serializer.deserialize(in);
                EstimatedHistogram columnCounts = EstimatedHistogram.serializer.deserialize(in);
                ReplayPosition replayPosition = ReplayPosition.serializer.deserialize(in);
                long minTimestamp = in.readLong();
                long maxTimestamp = in.readLong();
                int maxLocalDeletionTime = in.readInt();
                double bloomFilterFPChance = in.readDouble();
                double compressionRatio = in.readDouble();
                String partitioner = in.readUTF();
                int nbAncestors = in.readInt(); //skip compaction ancestors
                in.skipBytes(nbAncestors * TypeSizes.sizeof(nbAncestors));
                StreamingHistogram tombstoneHistogram = StreamingHistogram.serializer.deserialize(in);
                int sstableLevel = 0;
                if (in.available() > 0)
                    sstableLevel = in.readInt();

                int colCount = in.readInt();
                List<ByteBuffer> minColumnNames = new ArrayList<>(colCount);
                for (int i = 0; i < colCount; i++)
                    minColumnNames.add(ByteBufferUtil.readWithShortLength(in));

                colCount = in.readInt();
                List<ByteBuffer> maxColumnNames = new ArrayList<>(colCount);
                for (int i = 0; i < colCount; i++)
                    maxColumnNames.add(ByteBufferUtil.readWithShortLength(in));

                if (types.contains(MetadataType.VALIDATION))
                    components.put(MetadataType.VALIDATION,
                                   new ValidationMetadata(partitioner, bloomFilterFPChance));
                if (types.contains(MetadataType.STATS))
                    components.put(MetadataType.STATS,
                                   new StatsMetadata(partitionSizes,
                                                     columnCounts,
                                                     replayPosition,
                                                     minTimestamp,
                                                     maxTimestamp,
                                                     Integer.MAX_VALUE,
                                                     maxLocalDeletionTime,
                                                     0,
                                                     Integer.MAX_VALUE,
                                                     compressionRatio,
                                                     tombstoneHistogram,
                                                     sstableLevel,
                                                     minColumnNames,
                                                     maxColumnNames,
                                                     true,
                                                     ActiveRepairService.UNREPAIRED_SSTABLE,
                                                     -1,
                                                     -1));
                if (types.contains(MetadataType.COMPACTION))
                    components.put(MetadataType.COMPACTION,
                                   new CompactionMetadata(null));
            }
        }
        return components;
+++++++++++++++++++++++
    public Map<MetadataType, MetadataComponent> deserialize(Descriptor descriptor, EnumSet<MetadataType> types) throws IOException
    {
        Map<MetadataType, MetadataComponent> components = Maps.newHashMap();

        File statsFile = new File(descriptor.filenameFor(Component.STATS));
        if (!statsFile.exists() && types.contains(MetadataType.STATS))
        {
            components.put(MetadataType.STATS, MetadataCollector.defaultStatsMetadata());
        }
        else
        {
            try (DataInputStreamPlus in = new DataInputStreamPlus(new BufferedInputStream(new FileInputStream(statsFile))))
            {
                EstimatedHistogram partitionSizes = EstimatedHistogram.serializer.deserialize(in);
                EstimatedHistogram columnCounts = EstimatedHistogram.serializer.deserialize(in);
                ReplayPosition commitLogLowerBound = ReplayPosition.NONE;
                ReplayPosition commitLogUpperBound = ReplayPosition.serializer.deserialize(in);
                long minTimestamp = in.readLong();
                long maxTimestamp = in.readLong();
                int maxLocalDeletionTime = in.readInt();
                double bloomFilterFPChance = in.readDouble();
                double compressionRatio = in.readDouble();
                String partitioner = in.readUTF();
                int nbAncestors = in.readInt(); //skip compaction ancestors
                in.skipBytes(nbAncestors * TypeSizes.sizeof(nbAncestors));
                StreamingHistogram tombstoneHistogram = StreamingHistogram.serializer.deserialize(in);
                int sstableLevel = 0;
                if (in.available() > 0)
                    sstableLevel = in.readInt();

                int colCount = in.readInt();
                List<ByteBuffer> minColumnNames = new ArrayList<>(colCount);
                for (int i = 0; i < colCount; i++)
                    minColumnNames.add(ByteBufferUtil.readWithShortLength(in));

                colCount = in.readInt();
                List<ByteBuffer> maxColumnNames = new ArrayList<>(colCount);
                for (int i = 0; i < colCount; i++)
                    maxColumnNames.add(ByteBufferUtil.readWithShortLength(in));

                if (descriptor.version.hasCommitLogLowerBound())
                    commitLogLowerBound = ReplayPosition.serializer.deserialize(in);

                if (types.contains(MetadataType.VALIDATION))
                    components.put(MetadataType.VALIDATION,
                                   new ValidationMetadata(partitioner, bloomFilterFPChance));
                if (types.contains(MetadataType.STATS))
                    components.put(MetadataType.STATS,
                                   new StatsMetadata(partitionSizes,
                                                     columnCounts,
                                                     commitLogLowerBound,
                                                     commitLogUpperBound,
                                                     minTimestamp,
                                                     maxTimestamp,
                                                     Integer.MAX_VALUE,
                                                     maxLocalDeletionTime,
                                                     0,
                                                     Integer.MAX_VALUE,
                                                     compressionRatio,
                                                     tombstoneHistogram,
                                                     sstableLevel,
                                                     minColumnNames,
                                                     maxColumnNames,
                                                     true,
                                                     ActiveRepairService.UNREPAIRED_SSTABLE,
                                                     -1,
                                                     -1));
                if (types.contains(MetadataType.COMPACTION))
                    components.put(MetadataType.COMPACTION,
                                   new CompactionMetadata(null));
            }
        }
        return components;
add: 5, delete: 1, change: 119, unhandled: 1 size_exceptions: 0 size_serialize: 3
-----------cassandra-3.7 vs cassandra-3.8-----------
    private long serializedRowBodySize(Row row, SerializationHeader header, long previousUnfilteredSize, int version)
    {
        long size = 0;

        if (header.isForSSTable())
            size += TypeSizes.sizeofUnsignedVInt(previousUnfilteredSize);

        boolean isStatic = row.isStatic();
        Columns headerColumns = header.columns(isStatic);
        LivenessInfo pkLiveness = row.primaryKeyLivenessInfo();
        Row.Deletion deletion = row.deletion();
        boolean hasComplexDeletion = row.hasComplexDeletion();
        boolean hasAllColumns = (row.size() == headerColumns.size());

        if (!pkLiveness.isEmpty())
            size += header.timestampSerializedSize(pkLiveness.timestamp());
        if (pkLiveness.isExpiring())
        {
            size += header.ttlSerializedSize(pkLiveness.ttl());
            size += header.localDeletionTimeSerializedSize(pkLiveness.localExpirationTime());
        }
        if (!deletion.isLive())
            size += header.deletionTimeSerializedSize(deletion.time());

        if (!hasAllColumns)
            size += Columns.serializer.serializedSubsetSize(Collections2.transform(row, ColumnData::column), header.columns(isStatic));

        for (ColumnData data : row)
        {
            if (data.column.isSimple())
                size += Cell.serializer.serializedSize((Cell) data, pkLiveness, header);
            else
                size += sizeOfComplexColumn((ComplexColumnData) data, hasComplexDeletion, pkLiveness, header);
        }

        return size;
    }

+++++++++++++++++++++++
    private long serializedRowBodySize(Row row, SerializationHeader header, long previousUnfilteredSize, int version)
    {
        long size = 0;

        if (header.isForSSTable())
            size += TypeSizes.sizeofUnsignedVInt(previousUnfilteredSize);

        boolean isStatic = row.isStatic();
        Columns headerColumns = header.columns(isStatic);
        LivenessInfo pkLiveness = row.primaryKeyLivenessInfo();
        Row.Deletion deletion = row.deletion();
        boolean hasComplexDeletion = row.hasComplexDeletion();
        boolean hasAllColumns = (row.size() == headerColumns.size());

        if (!pkLiveness.isEmpty())
            size += header.timestampSerializedSize(pkLiveness.timestamp());
        if (pkLiveness.isExpiring())
        {
            size += header.ttlSerializedSize(pkLiveness.ttl());
            size += header.localDeletionTimeSerializedSize(pkLiveness.localExpirationTime());
        }
        if (!deletion.isLive())
            size += header.deletionTimeSerializedSize(deletion.time());

        if (!hasAllColumns)
            size += Columns.serializer.serializedSubsetSize(Collections2.transform(row, ColumnData::column), header.columns(isStatic));

        SearchIterator<ColumnDefinition, ColumnDefinition> si = headerColumns.iterator();
        for (ColumnData data : row)
        {
            ColumnDefinition column = si.next(data.column());
            assert column != null;

            if (data.column.isSimple())
                size += Cell.serializer.serializedSize((Cell) data, column, pkLiveness, header);
            else
                size += sizeOfComplexColumn((ComplexColumnData) data, column, hasComplexDeletion, pkLiveness, header);
        }

        return size;
    }

    public void serialize(Map<MetadataType, MetadataComponent> components, DataOutputPlus out, Version version) throws IOException
    {
        ValidationMetadata validation = (ValidationMetadata) components.get(MetadataType.VALIDATION);
        StatsMetadata stats = (StatsMetadata) components.get(MetadataType.STATS);
        CompactionMetadata compaction = (CompactionMetadata) components.get(MetadataType.COMPACTION);

        assert validation != null && stats != null && compaction != null && validation.partitioner != null;

        EstimatedHistogram.serializer.serialize(stats.estimatedPartitionSize, out);
        EstimatedHistogram.serializer.serialize(stats.estimatedColumnCount, out);
        ReplayPosition.serializer.serialize(stats.commitLogUpperBound, out);
        out.writeLong(stats.minTimestamp);
        out.writeLong(stats.maxTimestamp);
        out.writeInt(stats.maxLocalDeletionTime);
        out.writeDouble(validation.bloomFilterFPChance);
        out.writeDouble(stats.compressionRatio);
        out.writeUTF(validation.partitioner);
        out.writeInt(0); // compaction ancestors
        StreamingHistogram.serializer.serialize(stats.estimatedTombstoneDropTime, out);
        out.writeInt(stats.sstableLevel);
        out.writeInt(stats.minClusteringValues.size());
        for (ByteBuffer value : stats.minClusteringValues)
            ByteBufferUtil.writeWithShortLength(value, out);
        out.writeInt(stats.maxClusteringValues.size());
        for (ByteBuffer value : stats.maxClusteringValues)
            ByteBufferUtil.writeWithShortLength(value, out);
        if (version.hasCommitLogLowerBound())
            ReplayPosition.serializer.serialize(stats.commitLogLowerBound, out);
    }

    /**
     * Legacy serializer deserialize all components no matter what types are specified.
     */
    @Override
+++++++++++++++++++++++
    public void serialize(Map<MetadataType, MetadataComponent> components, DataOutputPlus out, Version version) throws IOException
    {
        ValidationMetadata validation = (ValidationMetadata) components.get(MetadataType.VALIDATION);
        StatsMetadata stats = (StatsMetadata) components.get(MetadataType.STATS);
        CompactionMetadata compaction = (CompactionMetadata) components.get(MetadataType.COMPACTION);

        assert validation != null && stats != null && compaction != null && validation.partitioner != null;

        EstimatedHistogram.serializer.serialize(stats.estimatedPartitionSize, out);
        EstimatedHistogram.serializer.serialize(stats.estimatedColumnCount, out);
        CommitLogPosition.serializer.serialize(stats.commitLogIntervals.upperBound().orElse(CommitLogPosition.NONE), out);
        out.writeLong(stats.minTimestamp);
        out.writeLong(stats.maxTimestamp);
        out.writeInt(stats.maxLocalDeletionTime);
        out.writeDouble(validation.bloomFilterFPChance);
        out.writeDouble(stats.compressionRatio);
        out.writeUTF(validation.partitioner);
        out.writeInt(0); // compaction ancestors
        StreamingHistogram.serializer.serialize(stats.estimatedTombstoneDropTime, out);
        out.writeInt(stats.sstableLevel);
        out.writeInt(stats.minClusteringValues.size());
        for (ByteBuffer value : stats.minClusteringValues)
            ByteBufferUtil.writeWithShortLength(value, out);
        out.writeInt(stats.maxClusteringValues.size());
        for (ByteBuffer value : stats.maxClusteringValues)
            ByteBufferUtil.writeWithShortLength(value, out);
        if (version.hasCommitLogLowerBound())
            CommitLogPosition.serializer.serialize(stats.commitLogIntervals.lowerBound().orElse(CommitLogPosition.NONE), out);
        if (version.hasCommitLogIntervals())
            commitLogPositionSetSerializer.serialize(stats.commitLogIntervals, out);
    }

    /**
     * Legacy serializer deserialize all components no matter what types are specified.
     */
    @Override
    public Map<MetadataType, MetadataComponent> deserialize(Descriptor descriptor, EnumSet<MetadataType> types) throws IOException
    {
        Map<MetadataType, MetadataComponent> components = Maps.newHashMap();

        File statsFile = new File(descriptor.filenameFor(Component.STATS));
        if (!statsFile.exists() && types.contains(MetadataType.STATS))
        {
            components.put(MetadataType.STATS, MetadataCollector.defaultStatsMetadata());
        }
        else
        {
            try (DataInputStreamPlus in = new DataInputStreamPlus(new BufferedInputStream(new FileInputStream(statsFile))))
            {
                EstimatedHistogram partitionSizes = EstimatedHistogram.serializer.deserialize(in);
                EstimatedHistogram columnCounts = EstimatedHistogram.serializer.deserialize(in);
                ReplayPosition commitLogLowerBound = ReplayPosition.NONE;
                ReplayPosition commitLogUpperBound = ReplayPosition.serializer.deserialize(in);
                long minTimestamp = in.readLong();
                long maxTimestamp = in.readLong();
                int maxLocalDeletionTime = in.readInt();
                double bloomFilterFPChance = in.readDouble();
                double compressionRatio = in.readDouble();
                String partitioner = in.readUTF();
                int nbAncestors = in.readInt(); //skip compaction ancestors
                in.skipBytes(nbAncestors * TypeSizes.sizeof(nbAncestors));
                StreamingHistogram tombstoneHistogram = StreamingHistogram.serializer.deserialize(in);
                int sstableLevel = 0;
                if (in.available() > 0)
                    sstableLevel = in.readInt();

                int colCount = in.readInt();
                List<ByteBuffer> minColumnNames = new ArrayList<>(colCount);
                for (int i = 0; i < colCount; i++)
                    minColumnNames.add(ByteBufferUtil.readWithShortLength(in));

                colCount = in.readInt();
                List<ByteBuffer> maxColumnNames = new ArrayList<>(colCount);
                for (int i = 0; i < colCount; i++)
                    maxColumnNames.add(ByteBufferUtil.readWithShortLength(in));

                if (descriptor.version.hasCommitLogLowerBound())
                    commitLogLowerBound = ReplayPosition.serializer.deserialize(in);

                if (types.contains(MetadataType.VALIDATION))
                    components.put(MetadataType.VALIDATION,
                                   new ValidationMetadata(partitioner, bloomFilterFPChance));
                if (types.contains(MetadataType.STATS))
                    components.put(MetadataType.STATS,
                                   new StatsMetadata(partitionSizes,
                                                     columnCounts,
                                                     commitLogLowerBound,
                                                     commitLogUpperBound,
                                                     minTimestamp,
                                                     maxTimestamp,
                                                     Integer.MAX_VALUE,
                                                     maxLocalDeletionTime,
                                                     0,
                                                     Integer.MAX_VALUE,
                                                     compressionRatio,
                                                     tombstoneHistogram,
                                                     sstableLevel,
                                                     minColumnNames,
                                                     maxColumnNames,
                                                     true,
                                                     ActiveRepairService.UNREPAIRED_SSTABLE,
                                                     -1,
                                                     -1));
                if (types.contains(MetadataType.COMPACTION))
                    components.put(MetadataType.COMPACTION,
                                   new CompactionMetadata(null));
            }
        }
        return components;
+++++++++++++++++++++++
    public Map<MetadataType, MetadataComponent> deserialize(Descriptor descriptor, EnumSet<MetadataType> types) throws IOException
    {
        Map<MetadataType, MetadataComponent> components = Maps.newHashMap();

        File statsFile = new File(descriptor.filenameFor(Component.STATS));
        if (!statsFile.exists() && types.contains(MetadataType.STATS))
        {
            components.put(MetadataType.STATS, MetadataCollector.defaultStatsMetadata());
        }
        else
        {
            try (DataInputStreamPlus in = new DataInputStreamPlus(new BufferedInputStream(new FileInputStream(statsFile))))
            {
                EstimatedHistogram partitionSizes = EstimatedHistogram.serializer.deserialize(in);
                EstimatedHistogram columnCounts = EstimatedHistogram.serializer.deserialize(in);
                CommitLogPosition commitLogLowerBound = CommitLogPosition.NONE;
                CommitLogPosition commitLogUpperBound = CommitLogPosition.serializer.deserialize(in);
                long minTimestamp = in.readLong();
                long maxTimestamp = in.readLong();
                int maxLocalDeletionTime = in.readInt();
                double bloomFilterFPChance = in.readDouble();
                double compressionRatio = in.readDouble();
                String partitioner = in.readUTF();
                int nbAncestors = in.readInt(); //skip compaction ancestors
                in.skipBytes(nbAncestors * TypeSizes.sizeof(nbAncestors));
                StreamingHistogram tombstoneHistogram = StreamingHistogram.serializer.deserialize(in);
                int sstableLevel = 0;
                if (in.available() > 0)
                    sstableLevel = in.readInt();

                int colCount = in.readInt();
                List<ByteBuffer> minColumnNames = new ArrayList<>(colCount);
                for (int i = 0; i < colCount; i++)
                    minColumnNames.add(ByteBufferUtil.readWithShortLength(in));

                colCount = in.readInt();
                List<ByteBuffer> maxColumnNames = new ArrayList<>(colCount);
                for (int i = 0; i < colCount; i++)
                    maxColumnNames.add(ByteBufferUtil.readWithShortLength(in));

                if (descriptor.version.hasCommitLogLowerBound())
                    commitLogLowerBound = CommitLogPosition.serializer.deserialize(in);
                IntervalSet<CommitLogPosition> commitLogIntervals;
                if (descriptor.version.hasCommitLogIntervals())
                    commitLogIntervals = commitLogPositionSetSerializer.deserialize(in);
                else
                    commitLogIntervals = new IntervalSet<>(commitLogLowerBound, commitLogUpperBound);

                if (types.contains(MetadataType.VALIDATION))
                    components.put(MetadataType.VALIDATION,
                                   new ValidationMetadata(partitioner, bloomFilterFPChance));
                if (types.contains(MetadataType.STATS))
                    components.put(MetadataType.STATS,
                                   new StatsMetadata(partitionSizes,
                                                     columnCounts,
                                                     commitLogIntervals,
                                                     minTimestamp,
                                                     maxTimestamp,
                                                     Integer.MAX_VALUE,
                                                     maxLocalDeletionTime,
                                                     0,
                                                     Integer.MAX_VALUE,
                                                     compressionRatio,
                                                     tombstoneHistogram,
                                                     sstableLevel,
                                                     minColumnNames,
                                                     maxColumnNames,
                                                     true,
                                                     ActiveRepairService.UNREPAIRED_SSTABLE,
                                                     -1,
                                                     -1));
                if (types.contains(MetadataType.COMPACTION))
                    components.put(MetadataType.COMPACTION,
                                   new CompactionMetadata(null));
            }
        }
        return components;
    public void serialize(DataOutputPlus out, int version) throws IOException
    {
        CompactEndpointSerializationHelper.serialize(from, out);

        out.writeInt(verb.ordinal());
        out.writeInt(parameters.size());
        for (Map.Entry<String, byte[]> entry : parameters.entrySet())
        {
            out.writeUTF(entry.getKey());
            out.writeInt(entry.getValue().length);
            out.write(entry.getValue());
        }

        long longSize = payloadSize(version);
        assert longSize <= Integer.MAX_VALUE; // larger values are supported in sstables but not messages
        out.writeInt((int) longSize);
        if (payload != null)
            serializer.serialize(payload, out, version);
    }

+++++++++++++++++++++++
    public void serialize(DataOutputPlus out, int version) throws IOException
    {
        CompactEndpointSerializationHelper.serialize(from, out);

        out.writeInt(MessagingService.Verb.convertForMessagingServiceVersion(verb, version).ordinal());
        out.writeInt(parameters.size());
        for (Map.Entry<String, byte[]> entry : parameters.entrySet())
        {
            out.writeUTF(entry.getKey());
            out.writeInt(entry.getValue().length);
            out.write(entry.getValue());
        }

        long longSize = payloadSize(version);
        assert longSize <= Integer.MAX_VALUE; // larger values are supported in sstables but not messages
        out.writeInt((int) longSize);
        if (payload != null)
            serializer.serialize(payload, out, version);
    }

add: 36, delete: 9, change: 476, unhandled: 1 size_exceptions: 9 size_serialize: 4
-----------cassandra-3.8 vs cassandra-3.9-----------
add: 0, delete: 0, change: 0, unhandled: 0 size_exceptions: 0 size_serialize: 0
-----------cassandra-3.9 vs cassandra-3.10-----------
    public static Iterator<LegacyCell> deserializeCells(final CFMetaData metadata,
                                                        final DataInput in,
                                                        final SerializationHelper.Flag flag,
                                                        final int size)
    {
        return new AbstractIterator<LegacyCell>()
        {
            private int i = 0;

            protected LegacyCell computeNext()
            {
                if (i >= size)
                    return endOfData();

                ++i;
                try
                {
                    return readLegacyCell(metadata, in, flag);
                }
                catch (UnknownColumnException e)
                {
                    // We can get there if we read a cell for a dropped column, and if that is the case,
                    // then simply ignore the cell is fine. But also not that we ignore if it's the
                    // system keyspace because for those table we actually remove columns without registering
                    // them in the dropped columns
                    if (metadata.ksName.equals(SystemKeyspace.NAME) || metadata.getDroppedColumnDefinition(e.columnName) != null)
                        return computeNext();
                    else
                        throw new IOError(e);
                }
                catch (IOException e)
                {
                    throw new IOError(e);
                }
            }
        };
    }

+++++++++++++++++++++++
    public static Iterator<LegacyCell> deserializeCells(final CFMetaData metadata,
                                                        final DataInput in,
                                                        final SerializationHelper.Flag flag,
                                                        final int size)
    {
        return new AbstractIterator<LegacyCell>()
        {
            private int i = 0;

            protected LegacyCell computeNext()
            {
                if (i >= size)
                    return endOfData();

                ++i;
                try
                {
                    return readLegacyCell(metadata, in, flag);
                }
                catch (UnknownColumnException e)
                {
                    // We can get there if we read a cell for a dropped column, and if that is the case,
                    // then simply ignore the cell is fine. But also not that we ignore if it's the
                    // system keyspace because for those table we actually remove columns without registering
                    // them in the dropped columns
                    if (metadata.ksName.equals(SchemaConstants.SYSTEM_KEYSPACE_NAME) || metadata.getDroppedColumnDefinition(e.columnName) != null)
                        return computeNext();
                    else
                        throw new IOError(e);
                }
                catch (IOException e)
                {
                    throw new IOError(e);
                }
            }
        };
    }

    public ByteBuffer serializeForNativeProtocol(Iterator<Cell> cells, int version)
    {
        assert isMultiCell();
        List<ByteBuffer> values = serializedValues(cells);
        int size = collectionSize(values);
        return CollectionSerializer.pack(values, size, version);
    }

    @Override
+++++++++++++++++++++++
    public ByteBuffer serializeForNativeProtocol(Iterator<Cell> cells, ProtocolVersion version)
    {
        assert isMultiCell();
        List<ByteBuffer> values = serializedValues(cells);
        int size = collectionSize(values);
        return CollectionSerializer.pack(values, size, version);
    }

    @Override
    public ByteBuffer serializeForNativeProtocol(Iterator<Cell> cells, int protocolVersion)
    {
        assert isMultiCell;

        ByteBuffer[] components = new ByteBuffer[size()];
        short fieldPosition = 0;
        while (cells.hasNext())
        {
            Cell cell = cells.next();

            // handle null fields that aren't at the end
            short fieldPositionOfCell = ByteBufferUtil.toShort(cell.path().get(0));
            while (fieldPosition < fieldPositionOfCell)
                components[fieldPosition++] = null;

            components[fieldPosition++] = cell.value();
        }

        // append trailing nulls for missing cells
        while (fieldPosition < size())
            components[fieldPosition++] = null;

        return TupleType.buildValue(components);
    }

    // Note: the only reason we override this is to provide nicer error message, but since that's not that much code...
    @Override
+++++++++++++++++++++++
    public ByteBuffer serializeForNativeProtocol(Iterator<Cell> cells, ProtocolVersion protocolVersion)
    {
        assert isMultiCell;

        ByteBuffer[] components = new ByteBuffer[size()];
        short fieldPosition = 0;
        while (cells.hasNext())
        {
            Cell cell = cells.next();

            // handle null fields that aren't at the end
            short fieldPositionOfCell = ByteBufferUtil.toShort(cell.path().get(0));
            while (fieldPosition < fieldPositionOfCell)
                components[fieldPosition++] = null;

            components[fieldPosition++] = cell.value();
        }

        // append trailing nulls for missing cells
        while (fieldPosition < size())
            components[fieldPosition++] = null;

        return TupleType.buildValue(components);
    }

    public Row deserializeRowBody(DataInputPlus in,
                                  SerializationHeader header,
                                  SerializationHelper helper,
                                  int flags,
                                  int extendedFlags,
                                  Row.Builder builder)
    throws IOException
    {
        try
        {
            boolean isStatic = isStatic(extendedFlags);
            boolean hasTimestamp = (flags & HAS_TIMESTAMP) != 0;
            boolean hasTTL = (flags & HAS_TTL) != 0;
            boolean hasDeletion = (flags & HAS_DELETION) != 0;
            boolean deletionIsShadowable = (extendedFlags & HAS_SHADOWABLE_DELETION) != 0;
            boolean hasComplexDeletion = (flags & HAS_COMPLEX_DELETION) != 0;
            boolean hasAllColumns = (flags & HAS_ALL_COLUMNS) != 0;
            Columns headerColumns = header.columns(isStatic);

            if (header.isForSSTable())
            {
                in.readUnsignedVInt(); // Skip row size
                in.readUnsignedVInt(); // previous unfiltered size
            }

            LivenessInfo rowLiveness = LivenessInfo.EMPTY;
            if (hasTimestamp)
            {
                long timestamp = header.readTimestamp(in);
                int ttl = hasTTL ? header.readTTL(in) : LivenessInfo.NO_TTL;
                int localDeletionTime = hasTTL ? header.readLocalDeletionTime(in) : LivenessInfo.NO_EXPIRATION_TIME;
                rowLiveness = LivenessInfo.withExpirationTime(timestamp, ttl, localDeletionTime);
            }

            builder.addPrimaryKeyLivenessInfo(rowLiveness);
            builder.addRowDeletion(hasDeletion ? new Row.Deletion(header.readDeletionTime(in), deletionIsShadowable) : Row.Deletion.LIVE);

            Columns columns = hasAllColumns ? headerColumns : Columns.serializer.deserializeSubset(headerColumns, in);
            for (ColumnDefinition column : columns)
            {
                if (column.isSimple())
                    readSimpleColumn(column, in, header, helper, builder, rowLiveness);
                else
                    readComplexColumn(column, in, header, helper, hasComplexDeletion, builder, rowLiveness);
            }

            return builder.build();
        }
        catch (RuntimeException | AssertionError e)
        {
            // Corrupted data could be such that it triggers an assertion in the row Builder, or break one of its assumption.
            // Of course, a bug in said builder could also trigger this, but it's impossible a priori to always make the distinction
            // between a real bug and data corrupted in just the bad way. Besides, re-throwing as an IOException doesn't hide the
            // exception, it just make we catch it properly and mark the sstable as corrupted.
            throw new IOException("Error building row with data deserialized from " + in, e);
        }
    }

+++++++++++++++++++++++
    public Row deserializeRowBody(DataInputPlus in,
                                  SerializationHeader header,
                                  SerializationHelper helper,
                                  int flags,
                                  int extendedFlags,
                                  Row.Builder builder)
    throws IOException
    {
        try
        {
            boolean isStatic = isStatic(extendedFlags);
            boolean hasTimestamp = (flags & HAS_TIMESTAMP) != 0;
            boolean hasTTL = (flags & HAS_TTL) != 0;
            boolean hasDeletion = (flags & HAS_DELETION) != 0;
            boolean deletionIsShadowable = (extendedFlags & HAS_SHADOWABLE_DELETION) != 0;
            boolean hasComplexDeletion = (flags & HAS_COMPLEX_DELETION) != 0;
            boolean hasAllColumns = (flags & HAS_ALL_COLUMNS) != 0;
            Columns headerColumns = header.columns(isStatic);

            if (header.isForSSTable())
            {
                in.readUnsignedVInt(); // Skip row size
                in.readUnsignedVInt(); // previous unfiltered size
            }

            LivenessInfo rowLiveness = LivenessInfo.EMPTY;
            if (hasTimestamp)
            {
                long timestamp = header.readTimestamp(in);
                int ttl = hasTTL ? header.readTTL(in) : LivenessInfo.NO_TTL;
                int localDeletionTime = hasTTL ? header.readLocalDeletionTime(in) : LivenessInfo.NO_EXPIRATION_TIME;
                rowLiveness = LivenessInfo.withExpirationTime(timestamp, ttl, localDeletionTime);
            }

            builder.addPrimaryKeyLivenessInfo(rowLiveness);
            builder.addRowDeletion(hasDeletion ? new Row.Deletion(header.readDeletionTime(in), deletionIsShadowable) : Row.Deletion.LIVE);

            Columns columns = hasAllColumns ? headerColumns : Columns.serializer.deserializeSubset(headerColumns, in);

            final LivenessInfo livenessInfo = rowLiveness;

            try
            {
                columns.apply(column -> {
                    try
                    {
                        if (column.isSimple())
                            readSimpleColumn(column, in, header, helper, builder, livenessInfo);
                        else
                            readComplexColumn(column, in, header, helper, hasComplexDeletion, builder, livenessInfo);
                    }
                    catch (IOException e)
                    {
                        throw new WrappedException(e);
                    }
                }, false);
            }
            catch (WrappedException e)
            {
                if (e.getCause() instanceof IOException)
                    throw (IOException) e.getCause();

                throw e;
            }

            return builder.build();
        }
        catch (RuntimeException | AssertionError e)
        {
            // Corrupted data could be such that it triggers an assertion in the row Builder, or break one of its assumption.
            // Of course, a bug in said builder could also trigger this, but it's impossible a priori to always make the distinction
            // between a real bug and data corrupted in just the bad way. Besides, re-throwing as an IOException doesn't hide the
            // exception, it just make we catch it properly and mark the sstable as corrupted.
            throw new IOException("Error building row with data deserialized from " + in, e);
        }
    }

    private void serializeRowBody(Row row, int flags, SerializationHeader header, DataOutputPlus out)
    throws IOException
    {
        boolean isStatic = row.isStatic();

        Columns headerColumns = header.columns(isStatic);
        LivenessInfo pkLiveness = row.primaryKeyLivenessInfo();
        Row.Deletion deletion = row.deletion();

        if ((flags & HAS_TIMESTAMP) != 0)
            header.writeTimestamp(pkLiveness.timestamp(), out);
        if ((flags & HAS_TTL) != 0)
        {
            header.writeTTL(pkLiveness.ttl(), out);
            header.writeLocalDeletionTime(pkLiveness.localExpirationTime(), out);
        }
        if ((flags & HAS_DELETION) != 0)
            header.writeDeletionTime(deletion.time(), out);

        if ((flags & HAS_ALL_COLUMNS) == 0)
            Columns.serializer.serializeSubset(Collections2.transform(row, ColumnData::column), headerColumns, out);

        SearchIterator<ColumnDefinition, ColumnDefinition> si = headerColumns.iterator();
        for (ColumnData data : row)
        {
            // We can obtain the column for data directly from data.column(). However, if the cell/complex data
            // originates from a sstable, the column we'll get will have the type used when the sstable was serialized,
            // and if that type have been recently altered, that may not be the type we want to serialize the column
            // with. So we use the ColumnDefinition from the "header" which is "current". Also see #11810 for what
            // happens if we don't do that.
            ColumnDefinition column = si.next(data.column());
            assert column != null;

            if (data.column.isSimple())
                Cell.serializer.serialize((Cell) data, column, out, pkLiveness, header);
            else
                writeComplexColumn((ComplexColumnData) data, column, (flags & HAS_COMPLEX_DELETION) != 0, pkLiveness, header, out);
        }
    }

+++++++++++++++++++++++
    private void serializeRowBody(Row row, int flags, SerializationHeader header, DataOutputPlus out)
    throws IOException
    {
        boolean isStatic = row.isStatic();

        Columns headerColumns = header.columns(isStatic);
        LivenessInfo pkLiveness = row.primaryKeyLivenessInfo();
        Row.Deletion deletion = row.deletion();

        if ((flags & HAS_TIMESTAMP) != 0)
            header.writeTimestamp(pkLiveness.timestamp(), out);
        if ((flags & HAS_TTL) != 0)
        {
            header.writeTTL(pkLiveness.ttl(), out);
            header.writeLocalDeletionTime(pkLiveness.localExpirationTime(), out);
        }
        if ((flags & HAS_DELETION) != 0)
            header.writeDeletionTime(deletion.time(), out);

        if ((flags & HAS_ALL_COLUMNS) == 0)
            Columns.serializer.serializeSubset(Collections2.transform(row, ColumnData::column), headerColumns, out);

        SearchIterator<ColumnDefinition, ColumnDefinition> si = headerColumns.iterator();

        try
        {
            row.apply(cd -> {
                // We can obtain the column for data directly from data.column(). However, if the cell/complex data
                // originates from a sstable, the column we'll get will have the type used when the sstable was serialized,
                // and if that type have been recently altered, that may not be the type we want to serialize the column
                // with. So we use the ColumnDefinition from the "header" which is "current". Also see #11810 for what
                // happens if we don't do that.
                ColumnDefinition column = si.next(cd.column());
                assert column != null : cd.column.toString();

                try
                {
                    if (cd.column.isSimple())
                        Cell.serializer.serialize((Cell) cd, column, out, pkLiveness, header);
                    else
                        writeComplexColumn((ComplexColumnData) cd, column, (flags & HAS_COMPLEX_DELETION) != 0, pkLiveness, header, out);
                }
                catch (IOException e)
                {
                    throw new WrappedException(e);
                }
            }, false);
        }
        catch (WrappedException e)
        {
            if (e.getCause() instanceof IOException)
                throw (IOException) e.getCause();

            throw e;
        }
    }

    private static Mutation deserializeLegacyMutation(UntypedResultSet.Row row)
    {
        try (DataInputBuffer dib = new DataInputBuffer(row.getBlob("mutation"), true))
        {
            Mutation mutation = Mutation.serializer.deserialize(dib,
                                                                row.getInt("message_version"));
            mutation.getPartitionUpdates().forEach(PartitionUpdate::validate);
            return mutation;
        }
        catch (IOException e)
        {
            logger.error("Failed to migrate a hint for {} from legacy {}.{} table",
                         row.getUUID("target_id"),
                         SystemKeyspace.NAME,
                         SystemKeyspace.LEGACY_HINTS,
                         e);
            return null;
        }
        catch (MarshalException e)
        {
            logger.warn("Failed to validate a hint for {} from legacy {}.{} table - skipping",
                        row.getUUID("target_id"),
                        SystemKeyspace.NAME,
                        SystemKeyspace.LEGACY_HINTS,
                        e);
            return null;
        }
    }

+++++++++++++++++++++++
    private static Mutation deserializeLegacyMutation(UntypedResultSet.Row row)
    {
        try (DataInputBuffer dib = new DataInputBuffer(row.getBlob("mutation"), true))
        {
            Mutation mutation = Mutation.serializer.deserialize(dib,
                                                                row.getInt("message_version"));
            mutation.getPartitionUpdates().forEach(PartitionUpdate::validate);
            return mutation;
        }
        catch (IOException e)
        {
            logger.error("Failed to migrate a hint for {} from legacy {}.{} table",
                         row.getUUID("target_id"),
                         SchemaConstants.SYSTEM_KEYSPACE_NAME,
                         SystemKeyspace.LEGACY_HINTS,
                         e);
            return null;
        }
        catch (MarshalException e)
        {
            logger.warn("Failed to validate a hint for {} from legacy {}.{} table - skipping",
                        row.getUUID("target_id"),
                        SchemaConstants.SYSTEM_KEYSPACE_NAME,
                        SystemKeyspace.LEGACY_HINTS,
                        e);
            return null;
        }
    }

    public Map<MetadataType, MetadataComponent> deserialize(Descriptor descriptor, EnumSet<MetadataType> types) throws IOException
    {
        Map<MetadataType, MetadataComponent> components = Maps.newHashMap();

        File statsFile = new File(descriptor.filenameFor(Component.STATS));
        if (!statsFile.exists() && types.contains(MetadataType.STATS))
        {
            components.put(MetadataType.STATS, MetadataCollector.defaultStatsMetadata());
        }
        else
        {
            try (DataInputStreamPlus in = new DataInputStreamPlus(new BufferedInputStream(new FileInputStream(statsFile))))
            {
                EstimatedHistogram partitionSizes = EstimatedHistogram.serializer.deserialize(in);
                EstimatedHistogram columnCounts = EstimatedHistogram.serializer.deserialize(in);
                CommitLogPosition commitLogLowerBound = CommitLogPosition.NONE;
                CommitLogPosition commitLogUpperBound = CommitLogPosition.serializer.deserialize(in);
                long minTimestamp = in.readLong();
                long maxTimestamp = in.readLong();
                int maxLocalDeletionTime = in.readInt();
                double bloomFilterFPChance = in.readDouble();
                double compressionRatio = in.readDouble();
                String partitioner = in.readUTF();
                int nbAncestors = in.readInt(); //skip compaction ancestors
                in.skipBytes(nbAncestors * TypeSizes.sizeof(nbAncestors));
                StreamingHistogram tombstoneHistogram = StreamingHistogram.serializer.deserialize(in);
                int sstableLevel = 0;
                if (in.available() > 0)
                    sstableLevel = in.readInt();

                int colCount = in.readInt();
                List<ByteBuffer> minColumnNames = new ArrayList<>(colCount);
                for (int i = 0; i < colCount; i++)
                    minColumnNames.add(ByteBufferUtil.readWithShortLength(in));

                colCount = in.readInt();
                List<ByteBuffer> maxColumnNames = new ArrayList<>(colCount);
                for (int i = 0; i < colCount; i++)
                    maxColumnNames.add(ByteBufferUtil.readWithShortLength(in));

                if (descriptor.version.hasCommitLogLowerBound())
                    commitLogLowerBound = CommitLogPosition.serializer.deserialize(in);
                IntervalSet<CommitLogPosition> commitLogIntervals;
                if (descriptor.version.hasCommitLogIntervals())
                    commitLogIntervals = commitLogPositionSetSerializer.deserialize(in);
                else
                    commitLogIntervals = new IntervalSet<>(commitLogLowerBound, commitLogUpperBound);

                if (types.contains(MetadataType.VALIDATION))
                    components.put(MetadataType.VALIDATION,
                                   new ValidationMetadata(partitioner, bloomFilterFPChance));
                if (types.contains(MetadataType.STATS))
                    components.put(MetadataType.STATS,
                                   new StatsMetadata(partitionSizes,
                                                     columnCounts,
                                                     commitLogIntervals,
                                                     minTimestamp,
                                                     maxTimestamp,
                                                     Integer.MAX_VALUE,
                                                     maxLocalDeletionTime,
                                                     0,
                                                     Integer.MAX_VALUE,
                                                     compressionRatio,
                                                     tombstoneHistogram,
                                                     sstableLevel,
                                                     minColumnNames,
                                                     maxColumnNames,
                                                     true,
                                                     ActiveRepairService.UNREPAIRED_SSTABLE,
                                                     -1,
                                                     -1));
                if (types.contains(MetadataType.COMPACTION))
                    components.put(MetadataType.COMPACTION,
                                   new CompactionMetadata(null));
            }
        }
        return components;
+++++++++++++++++++++++
    public Map<MetadataType, MetadataComponent> deserialize(Descriptor descriptor, EnumSet<MetadataType> types) throws IOException
    {
        Map<MetadataType, MetadataComponent> components = new EnumMap<>(MetadataType.class);

        File statsFile = new File(descriptor.filenameFor(Component.STATS));
        if (!statsFile.exists() && types.contains(MetadataType.STATS))
        {
            components.put(MetadataType.STATS, MetadataCollector.defaultStatsMetadata());
        }
        else
        {
            try (DataInputStreamPlus in = new DataInputStreamPlus(new BufferedInputStream(new FileInputStream(statsFile))))
            {
                EstimatedHistogram partitionSizes = EstimatedHistogram.serializer.deserialize(in);
                EstimatedHistogram columnCounts = EstimatedHistogram.serializer.deserialize(in);
                CommitLogPosition commitLogLowerBound = CommitLogPosition.NONE;
                CommitLogPosition commitLogUpperBound = CommitLogPosition.serializer.deserialize(in);
                long minTimestamp = in.readLong();
                long maxTimestamp = in.readLong();
                int maxLocalDeletionTime = in.readInt();
                double bloomFilterFPChance = in.readDouble();
                double compressionRatio = in.readDouble();
                String partitioner = in.readUTF();
                int nbAncestors = in.readInt(); //skip compaction ancestors
                in.skipBytes(nbAncestors * TypeSizes.sizeof(nbAncestors));
                StreamingHistogram tombstoneHistogram = StreamingHistogram.serializer.deserialize(in);
                int sstableLevel = 0;
                if (in.available() > 0)
                    sstableLevel = in.readInt();

                int colCount = in.readInt();
                List<ByteBuffer> minColumnNames = new ArrayList<>(colCount);
                for (int i = 0; i < colCount; i++)
                    minColumnNames.add(ByteBufferUtil.readWithShortLength(in));

                colCount = in.readInt();
                List<ByteBuffer> maxColumnNames = new ArrayList<>(colCount);
                for (int i = 0; i < colCount; i++)
                    maxColumnNames.add(ByteBufferUtil.readWithShortLength(in));

                if (descriptor.version.hasCommitLogLowerBound())
                    commitLogLowerBound = CommitLogPosition.serializer.deserialize(in);
                IntervalSet<CommitLogPosition> commitLogIntervals;
                if (descriptor.version.hasCommitLogIntervals())
                    commitLogIntervals = commitLogPositionSetSerializer.deserialize(in);
                else
                    commitLogIntervals = new IntervalSet<>(commitLogLowerBound, commitLogUpperBound);

                if (types.contains(MetadataType.VALIDATION))
                    components.put(MetadataType.VALIDATION,
                                   new ValidationMetadata(partitioner, bloomFilterFPChance));
                if (types.contains(MetadataType.STATS))
                    components.put(MetadataType.STATS,
                                   new StatsMetadata(partitionSizes,
                                                     columnCounts,
                                                     commitLogIntervals,
                                                     minTimestamp,
                                                     maxTimestamp,
                                                     Integer.MAX_VALUE,
                                                     maxLocalDeletionTime,
                                                     0,
                                                     Integer.MAX_VALUE,
                                                     compressionRatio,
                                                     tombstoneHistogram,
                                                     sstableLevel,
                                                     minColumnNames,
                                                     maxColumnNames,
                                                     true,
                                                     ActiveRepairService.UNREPAIRED_SSTABLE,
                                                     -1,
                                                     -1));
                if (types.contains(MetadataType.COMPACTION))
                    components.put(MetadataType.COMPACTION,
                                   new CompactionMetadata(null));
            }
        }
        return components;
    public Map<MetadataType, MetadataComponent> deserialize(Descriptor descriptor, FileDataInput in, EnumSet<MetadataType> types) throws IOException
    {
        Map<MetadataType, MetadataComponent> components = Maps.newHashMap();
        // read number of components
        int numComponents = in.readInt();
        // read toc
        Map<MetadataType, Integer> toc = new HashMap<>(numComponents);
        MetadataType[] values = MetadataType.values();
        for (int i = 0; i < numComponents; i++)
        {
            toc.put(values[in.readInt()], in.readInt());
        }
        for (MetadataType type : types)
        {
            Integer offset = toc.get(type);
            if (offset != null)
            {
                in.seek(offset);
                MetadataComponent component = type.serializer.deserialize(descriptor.version, in);
                components.put(type, component);
            }
        }
        return components;
    }

+++++++++++++++++++++++
    public Map<MetadataType, MetadataComponent> deserialize(Descriptor descriptor, FileDataInput in, EnumSet<MetadataType> types) throws IOException
    {
        Map<MetadataType, MetadataComponent> components = new EnumMap<>(MetadataType.class);
        // read number of components
        int numComponents = in.readInt();
        // read toc
        Map<MetadataType, Integer> toc = new EnumMap<>(MetadataType.class);
        MetadataType[] values = MetadataType.values();
        for (int i = 0; i < numComponents; i++)
        {
            toc.put(values[in.readInt()], in.readInt());
        }
        for (MetadataType type : types)
        {
            Integer offset = toc.get(type);
            if (offset != null)
            {
                in.seek(offset);
                MetadataComponent component = type.serializer.deserialize(descriptor.version, in);
                components.put(type, component);
            }
        }
        return components;
    }

    public void serialize(DataOutputPlus out, int version) throws IOException
    {
        CompactEndpointSerializationHelper.serialize(from, out);

        out.writeInt(MessagingService.Verb.convertForMessagingServiceVersion(verb, version).ordinal());
        out.writeInt(parameters.size());
        for (Map.Entry<String, byte[]> entry : parameters.entrySet())
        {
            out.writeUTF(entry.getKey());
            out.writeInt(entry.getValue().length);
            out.write(entry.getValue());
        }

        long longSize = payloadSize(version);
        assert longSize <= Integer.MAX_VALUE; // larger values are supported in sstables but not messages
        out.writeInt((int) longSize);
        if (payload != null)
            serializer.serialize(payload, out, version);
    }

+++++++++++++++++++++++
    public void serialize(DataOutputPlus out, int version) throws IOException
    {
        CompactEndpointSerializationHelper.serialize(from, out);

        out.writeInt(MessagingService.Verb.convertForMessagingServiceVersion(verb, version).ordinal());
        out.writeInt(parameters.size());
        for (Map.Entry<String, byte[]> entry : parameters.entrySet())
        {
            out.writeUTF(entry.getKey());
            out.writeInt(entry.getValue().length);
            out.write(entry.getValue());
        }

        if (payload != null)
        {
            try(DataOutputBuffer dob = DataOutputBuffer.scratchBuffer.get())
            {
                serializer.serialize(payload, dob, version);

                int size = dob.getLength();
                out.writeInt(size);
                out.write(dob.getData(), 0, size);
            }
        }
        else
        {
            out.writeInt(0);
        }
    }

    public Boolean deserialize(ByteBuffer bytes)
    {
        if (bytes.remaining() == 0)
            return null;

        byte value = bytes.get(bytes.position());
        return value != 0;
    }

+++++++++++++++++++++++
    public Boolean deserialize(ByteBuffer bytes)
    {
        if (bytes == null || bytes.remaining() == 0)
            return null;

        byte value = bytes.get(bytes.position());
        return value != 0;
    }

    public ByteBuffer serialize(T value)
    {
        List<ByteBuffer> values = serializeValues(value);
        // See deserialize() for why using the protocol v3 variant is the right thing to do.
        return pack(values, getElementCount(value), Server.VERSION_3);
    }

+++++++++++++++++++++++
    public ByteBuffer serialize(T value)
    {
        List<ByteBuffer> values = serializeValues(value);
        // See deserialize() for why using the protocol v3 variant is the right thing to do.
        return pack(values, getElementCount(value), ProtocolVersion.V3);
    }

    public T deserialize(ByteBuffer bytes)
    {
        // The only cases we serialize/deserialize collections internally (i.e. not for the protocol sake),
        // is:
        //  1) when collections are frozen
        //  2) for internal calls.
        // In both case, using the protocol 3 version variant is the right thing to do.
        return deserializeForNativeProtocol(bytes, Server.VERSION_3);
    }

+++++++++++++++++++++++
    public T deserialize(ByteBuffer bytes)
    {
        // The only cases we serialize/deserialize collections internally (i.e. not for the protocol sake),
        // is:
        //  1) when collections are frozen
        //  2) for internal calls.
        // In both case, using the protocol 3 version variant is the right thing to do.
        return deserializeForNativeProtocol(bytes, ProtocolVersion.V3);
    }

    public abstract T deserializeForNativeProtocol(ByteBuffer buffer, int version);
+++++++++++++++++++++++
    public abstract T deserializeForNativeProtocol(ByteBuffer buffer, ProtocolVersion version);
    public List<T> deserializeForNativeProtocol(ByteBuffer bytes, int version)
    {
        try
        {
            ByteBuffer input = bytes.duplicate();
            int n = readCollectionSize(input, version);
            List<T> l = new ArrayList<T>(n);
            for (int i = 0; i < n; i++)
            {
                // We can have nulls in lists that are used for IN values
                ByteBuffer databb = readValue(input, version);
                if (databb != null)
                {
                    elements.validate(databb);
                    l.add(elements.deserialize(databb));
                }
                else
                {
                    l.add(null);
                }
            }

            if (input.hasRemaining())
                throw new MarshalException("Unexpected extraneous bytes after list value");

            return l;
        }
        catch (BufferUnderflowException e)
        {
            throw new MarshalException("Not enough bytes to read a list");
        }
    }

    /**
     * Returns the element at the given index in a list.
     * @param serializedList a serialized list
     * @param index the index to get
     * @return the serialized element at the given index, or null if the index exceeds the list size
     */
+++++++++++++++++++++++
    public List<T> deserializeForNativeProtocol(ByteBuffer bytes, ProtocolVersion version)
    {
        try
        {
            ByteBuffer input = bytes.duplicate();
            int n = readCollectionSize(input, version);

            if (n < 0)
                throw new MarshalException("The data cannot be deserialized as a list");

            // If the received bytes are not corresponding to a list, n might be a huge number.
            // In such a case we do not want to initialize the list with that size as it can result
            // in an OOM (see CASSANDRA-12618). On the other hand we do not want to have to resize the list
            // if we can avoid it, so we put a reasonable limit on the initialCapacity.
            List<T> l = new ArrayList<T>(Math.min(n, 256));
            for (int i = 0; i < n; i++)
            {
                // We can have nulls in lists that are used for IN values
                ByteBuffer databb = readValue(input, version);
                if (databb != null)
                {
                    elements.validate(databb);
                    l.add(elements.deserialize(databb));
                }
                else
                {
                    l.add(null);
                }
            }

            if (input.hasRemaining())
                throw new MarshalException("Unexpected extraneous bytes after list value");

            return l;
        }
        catch (BufferUnderflowException e)
        {
            throw new MarshalException("Not enough bytes to read a list");
        }
    }

    /**
     * Returns the element at the given index in a list.
     * @param serializedList a serialized list
     * @param index the index to get
     * @return the serialized element at the given index, or null if the index exceeds the list size
     */
    public Map<K, V> deserializeForNativeProtocol(ByteBuffer bytes, int version)
    {
        try
        {
            ByteBuffer input = bytes.duplicate();
            int n = readCollectionSize(input, version);
            Map<K, V> m = new LinkedHashMap<K, V>(n);
            for (int i = 0; i < n; i++)
            {
                ByteBuffer kbb = readValue(input, version);
                keys.validate(kbb);

                ByteBuffer vbb = readValue(input, version);
                values.validate(vbb);

                m.put(keys.deserialize(kbb), values.deserialize(vbb));
            }
            if (input.hasRemaining())
                throw new MarshalException("Unexpected extraneous bytes after map value");
            return m;
        }
        catch (BufferUnderflowException e)
        {
            throw new MarshalException("Not enough bytes to read a map");
        }
    }

    /**
     * Given a serialized map, gets the value associated with a given key.
     * @param serializedMap a serialized map
     * @param serializedKey a serialized key
     * @param keyType the key type for the map
     * @return the value associated with the key if one exists, null otherwise
     */
+++++++++++++++++++++++
    public Map<K, V> deserializeForNativeProtocol(ByteBuffer bytes, ProtocolVersion version)
    {
        try
        {
            ByteBuffer input = bytes.duplicate();
            int n = readCollectionSize(input, version);

            if (n < 0)
                throw new MarshalException("The data cannot be deserialized as a map");

            // If the received bytes are not corresponding to a map, n might be a huge number.
            // In such a case we do not want to initialize the map with that initialCapacity as it can result
            // in an OOM when put is called (see CASSANDRA-12618). On the other hand we do not want to have to resize
            // the map if we can avoid it, so we put a reasonable limit on the initialCapacity.
            Map<K, V> m = new LinkedHashMap<K, V>(Math.min(n, 256));
            for (int i = 0; i < n; i++)
            {
                ByteBuffer kbb = readValue(input, version);
                keys.validate(kbb);

                ByteBuffer vbb = readValue(input, version);
                values.validate(vbb);

                m.put(keys.deserialize(kbb), values.deserialize(vbb));
            }
            if (input.hasRemaining())
                throw new MarshalException("Unexpected extraneous bytes after map value");
            return m;
        }
        catch (BufferUnderflowException e)
        {
            throw new MarshalException("Not enough bytes to read a map");
        }
    }

    /**
     * Given a serialized map, gets the value associated with a given key.
     * @param serializedMap a serialized map
     * @param serializedKey a serialized key
     * @param keyType the key type for the map
     * @return the value associated with the key if one exists, null otherwise
     */
    public Set<T> deserializeForNativeProtocol(ByteBuffer bytes, int version)
    {
        try
        {
            ByteBuffer input = bytes.duplicate();
            int n = readCollectionSize(input, version);
            Set<T> l = new LinkedHashSet<T>(n);
            for (int i = 0; i < n; i++)
            {
                ByteBuffer databb = readValue(input, version);
                elements.validate(databb);
                l.add(elements.deserialize(databb));
            }
            if (input.hasRemaining())
                throw new MarshalException("Unexpected extraneous bytes after set value");
            return l;
        }
        catch (BufferUnderflowException e)
        {
            throw new MarshalException("Not enough bytes to read a set");
        }
    }

+++++++++++++++++++++++
    public Set<T> deserializeForNativeProtocol(ByteBuffer bytes, ProtocolVersion version)
    {
        try
        {
            ByteBuffer input = bytes.duplicate();
            int n = readCollectionSize(input, version);

            if (n < 0)
                throw new MarshalException("The data cannot be deserialized as a set");

            // If the received bytes are not corresponding to a set, n might be a huge number.
            // In such a case we do not want to initialize the set with that initialCapacity as it can result
            // in an OOM when add is called (see CASSANDRA-12618). On the other hand we do not want to have to resize
            // the set if we can avoid it, so we put a reasonable limit on the initialCapacity.
            Set<T> l = new LinkedHashSet<T>(Math.min(n, 256));

            for (int i = 0; i < n; i++)
            {
                ByteBuffer databb = readValue(input, version);
                elements.validate(databb);
                l.add(elements.deserialize(databb));
            }
            if (input.hasRemaining())
                throw new MarshalException("Unexpected extraneous bytes after set value");
            return l;
        }
        catch (BufferUnderflowException e)
        {
            throw new MarshalException("Not enough bytes to read a set");
        }
    }

    public ByteBuffer serialize(int protocolVersion)
    {
        assert rowMark == null || protocolVersion == rowMark.protocolVersion;
        try (DataOutputBuffer out = new DataOutputBufferFixed(serializedSize(protocolVersion)))
        {
            ByteBuffer pk = partitionKey == null ? ByteBufferUtil.EMPTY_BYTE_BUFFER : partitionKey;
            ByteBuffer mark = rowMark == null ? ByteBufferUtil.EMPTY_BYTE_BUFFER : rowMark.mark;
            if (protocolVersion <= Server.VERSION_3)
            {
                ByteBufferUtil.writeWithShortLength(pk, out);
                ByteBufferUtil.writeWithShortLength(mark, out);
                out.writeInt(remaining);
                out.writeInt(remainingInPartition);
            }
            else
            {
                ByteBufferUtil.writeWithVIntLength(pk, out);
                ByteBufferUtil.writeWithVIntLength(mark, out);
                out.writeUnsignedVInt(remaining);
                out.writeUnsignedVInt(remainingInPartition);
            }
            return out.buffer();
        }
        catch (IOException e)
        {
            throw new RuntimeException(e);
        }
    }

+++++++++++++++++++++++
    public ByteBuffer serialize(ProtocolVersion protocolVersion)
    {
        assert rowMark == null || protocolVersion == rowMark.protocolVersion;
        try (DataOutputBuffer out = new DataOutputBufferFixed(serializedSize(protocolVersion)))
        {
            ByteBuffer pk = partitionKey == null ? ByteBufferUtil.EMPTY_BYTE_BUFFER : partitionKey;
            ByteBuffer mark = rowMark == null ? ByteBufferUtil.EMPTY_BYTE_BUFFER : rowMark.mark;
            if (protocolVersion.isSmallerOrEqualTo(ProtocolVersion.V3))
            {
                ByteBufferUtil.writeWithShortLength(pk, out);
                ByteBufferUtil.writeWithShortLength(mark, out);
                out.writeInt(remaining);
                out.writeInt(remainingInPartition);
            }
            else
            {
                ByteBufferUtil.writeWithVIntLength(pk, out);
                ByteBufferUtil.writeWithVIntLength(mark, out);
                out.writeUnsignedVInt(remaining);
                out.writeUnsignedVInt(remainingInPartition);
            }
            return out.buffer();
        }
        catch (IOException e)
        {
            throw new RuntimeException(e);
        }
    }

    public int serializedSize(int protocolVersion)
    {
        assert rowMark == null || protocolVersion == rowMark.protocolVersion;
        ByteBuffer pk = partitionKey == null ? ByteBufferUtil.EMPTY_BYTE_BUFFER : partitionKey;
        ByteBuffer mark = rowMark == null ? ByteBufferUtil.EMPTY_BYTE_BUFFER : rowMark.mark;
        if (protocolVersion <= Server.VERSION_3)
        {
            return ByteBufferUtil.serializedSizeWithShortLength(pk)
                 + ByteBufferUtil.serializedSizeWithShortLength(mark)
                 + 8; // remaining & remainingInPartition
        }
        else
        {
            return ByteBufferUtil.serializedSizeWithVIntLength(pk)
                 + ByteBufferUtil.serializedSizeWithVIntLength(mark)
                 + TypeSizes.sizeofUnsignedVInt(remaining)
                 + TypeSizes.sizeofUnsignedVInt(remainingInPartition);
        }
    }

    @Override
+++++++++++++++++++++++
    public int serializedSize(ProtocolVersion protocolVersion)
    {
        assert rowMark == null || protocolVersion == rowMark.protocolVersion;
        ByteBuffer pk = partitionKey == null ? ByteBufferUtil.EMPTY_BYTE_BUFFER : partitionKey;
        ByteBuffer mark = rowMark == null ? ByteBufferUtil.EMPTY_BYTE_BUFFER : rowMark.mark;
        if (protocolVersion.isSmallerOrEqualTo(ProtocolVersion.V3))
        {
            return ByteBufferUtil.serializedSizeWithShortLength(pk)
                 + ByteBufferUtil.serializedSizeWithShortLength(mark)
                 + 8; // remaining & remainingInPartition
        }
        else
        {
            return ByteBufferUtil.serializedSizeWithVIntLength(pk)
                 + ByteBufferUtil.serializedSizeWithVIntLength(mark)
                 + TypeSizes.sizeofUnsignedVInt(remaining)
                 + TypeSizes.sizeofUnsignedVInt(remainingInPartition);
        }
    }

    @Override
    public static PagingState deserialize(ByteBuffer bytes, int protocolVersion)
    {
        if (bytes == null)
            return null;

        try (DataInputBuffer in = new DataInputBuffer(bytes, true))
        {
            ByteBuffer pk;
            RowMark mark;
            int remaining, remainingInPartition;
            if (protocolVersion <= Server.VERSION_3)
            {
                pk = ByteBufferUtil.readWithShortLength(in);
                mark = new RowMark(ByteBufferUtil.readWithShortLength(in), protocolVersion);
                remaining = in.readInt();
                // Note that while 'in.available()' is theoretically an estimate of how many bytes are available
                // without blocking, we know that since we're reading a ByteBuffer it will be exactly how many
                // bytes remain to be read. And the reason we want to condition this is for backward compatility
                // as we used to not set this.
                remainingInPartition = in.available() > 0 ? in.readInt() : Integer.MAX_VALUE;
            }
            else
            {
                pk = ByteBufferUtil.readWithVIntLength(in);
                mark = new RowMark(ByteBufferUtil.readWithVIntLength(in), protocolVersion);
                remaining = (int)in.readUnsignedVInt();
                remainingInPartition = (int)in.readUnsignedVInt();
            }
            return new PagingState(pk.hasRemaining() ? pk : null,
                                   mark.mark.hasRemaining() ? mark : null,
                                   remaining,
                                   remainingInPartition);
        }
        catch (IOException e)
        {
            throw new ProtocolException("Invalid value for the paging state");
        }
    }

+++++++++++++++++++++++
    public static PagingState deserialize(ByteBuffer bytes, ProtocolVersion protocolVersion)
    {
        if (bytes == null)
            return null;

        try (DataInputBuffer in = new DataInputBuffer(bytes, true))
        {
            ByteBuffer pk;
            RowMark mark;
            int remaining, remainingInPartition;
            if (protocolVersion.isSmallerOrEqualTo(ProtocolVersion.V3))
            {
                pk = ByteBufferUtil.readWithShortLength(in);
                mark = new RowMark(ByteBufferUtil.readWithShortLength(in), protocolVersion);
                remaining = in.readInt();
                // Note that while 'in.available()' is theoretically an estimate of how many bytes are available
                // without blocking, we know that since we're reading a ByteBuffer it will be exactly how many
                // bytes remain to be read. And the reason we want to condition this is for backward compatility
                // as we used to not set this.
                remainingInPartition = in.available() > 0 ? in.readInt() : Integer.MAX_VALUE;
            }
            else
            {
                pk = ByteBufferUtil.readWithVIntLength(in);
                mark = new RowMark(ByteBufferUtil.readWithVIntLength(in), protocolVersion);
                remaining = (int)in.readUnsignedVInt();
                remainingInPartition = (int)in.readUnsignedVInt();
            }
            return new PagingState(pk.hasRemaining() ? pk : null,
                                   mark.mark.hasRemaining() ? mark : null,
                                   remaining,
                                   remainingInPartition);
        }
        catch (IOException e)
        {
            throw new ProtocolException("Invalid value for the paging state");
        }
    }

    private void serializePartition(UnfilteredRowIterator partition)
    {
        String key = metadata.getKeyValidator().getString(partition.partitionKey().getKey());
        try
        {
            json.writeStartObject();

            json.writeFieldName("partition");
            json.writeStartObject();
            json.writeFieldName("key");
            serializePartitionKey(partition.partitionKey());
            json.writeNumberField("position", this.currentScanner.getCurrentPosition());

            if (!partition.partitionLevelDeletion().isLive())
            {
                serializeDeletion(partition.partitionLevelDeletion());
            }
            else
            {
                json.writeEndObject();
                json.writeFieldName("rows");
                json.writeStartArray();
                updatePosition();
                if (!partition.staticRow().isEmpty())
                {
                    serializeRow(partition.staticRow());
                }
                Unfiltered unfiltered;
                updatePosition();
                while (partition.hasNext())
                {
                    unfiltered = partition.next();
                    if (unfiltered instanceof Row)
                    {
                        serializeRow((Row) unfiltered);
                    }
                    else if (unfiltered instanceof RangeTombstoneMarker)
                    {
                        serializeTombstone((RangeTombstoneMarker) unfiltered);
                    }
                    updatePosition();
                }
                json.writeEndArray();
            }

            json.writeEndObject();
        }
        catch (IOException e)
        {
            logger.error("Fatal error parsing partition: {}", key, e);
        }
    }

+++++++++++++++++++++++
    private void serializePartition(UnfilteredRowIterator partition)
    {
        try
        {
            json.writeStartObject();

            json.writeFieldName("partition");
            json.writeStartObject();
            json.writeFieldName("key");
            serializePartitionKey(partition.partitionKey());
            json.writeNumberField("position", this.currentScanner.getCurrentPosition());

            if (!partition.partitionLevelDeletion().isLive())
            {
                serializeDeletion(partition.partitionLevelDeletion());
                json.writeEndObject();
            }
            else
            {
                json.writeEndObject();
                json.writeFieldName("rows");
                json.writeStartArray();
                updatePosition();
                if (!partition.staticRow().isEmpty())
                {
                    serializeRow(partition.staticRow());
                }
                Unfiltered unfiltered;
                updatePosition();
                while (partition.hasNext())
                {
                    unfiltered = partition.next();
                    if (unfiltered instanceof Row)
                    {
                        serializeRow((Row) unfiltered);
                    }
                    else if (unfiltered instanceof RangeTombstoneMarker)
                    {
                        serializeTombstone((RangeTombstoneMarker) unfiltered);
                    }
                    updatePosition();
                }
                json.writeEndArray();
            }

            json.writeEndObject();
        }
        catch (IOException e)
        {
            String key = metadata.getKeyValidator().getString(partition.partitionKey().getKey());
            logger.error("Fatal error parsing partition: {}", key, e);
        }
    }

    public void serialize(ByteBuf dest, int version)
    {
        if (type.minimumVersion > version)
            throw new ProtocolException("Event " + type.name() + " not valid for protocol version " + version);
        CBUtil.writeEnumValue(type, dest);
        serializeEvent(dest, version);
    }

+++++++++++++++++++++++
    public void serialize(ByteBuf dest, ProtocolVersion version)
    {
        if (type.minimumVersion.isGreaterThan(version))
            throw new ProtocolException("Event " + type.name() + " not valid for protocol version " + version);
        CBUtil.writeEnumValue(type, dest);
        serializeEvent(dest, version);
    }

    protected abstract void serializeEvent(ByteBuf dest, int version);
+++++++++++++++++++++++
    protected abstract void serializeEvent(ByteBuf dest, ProtocolVersion version);
    public int serializedSize(int version)
    {
        return CBUtil.sizeOfEnumValue(type) + eventSerializedSize(version);
    }

+++++++++++++++++++++++
    public int serializedSize(ProtocolVersion version)
    {
        return CBUtil.sizeOfEnumValue(type) + eventSerializedSize(version);
    }

    public static Event deserialize(ByteBuf cb, int version)
    {
        Type eventType = CBUtil.readEnumValue(Type.class, cb);
        if (eventType.minimumVersion > version)
            throw new ProtocolException("Event " + eventType.name() + " not valid for protocol version " + version);
        switch (eventType)
        {
            case TOPOLOGY_CHANGE:
                return TopologyChange.deserializeEvent(cb, version);
            case STATUS_CHANGE:
                return StatusChange.deserializeEvent(cb, version);
            case SCHEMA_CHANGE:
                return SchemaChange.deserializeEvent(cb, version);
        }
        throw new AssertionError();
    }

+++++++++++++++++++++++
    public static Event deserialize(ByteBuf cb, ProtocolVersion version)
    {
        Type eventType = CBUtil.readEnumValue(Type.class, cb);
        if (eventType.minimumVersion.isGreaterThan(version))
            throw new ProtocolException("Event " + eventType.name() + " not valid for protocol version " + version);
        switch (eventType)
        {
            case TOPOLOGY_CHANGE:
                return TopologyChange.deserializeEvent(cb, version);
            case STATUS_CHANGE:
                return StatusChange.deserializeEvent(cb, version);
            case SCHEMA_CHANGE:
                return SchemaChange.deserializeEvent(cb, version);
        }
        throw new AssertionError();
    }

    public void multiFlushAndDeserializeTest() throws Exception
    {
        int hintNum = 0;
        int bufferSize = HintsWriteExecutor.WRITE_BUFFER_SIZE;
        List<Hint> hints = new LinkedList<>();

        UUID hostId = UUIDGen.getTimeUUID();
        long ts = System.currentTimeMillis();

        HintsDescriptor descriptor = new HintsDescriptor(hostId, ts, params());
        File dir = Files.createTempDir();
        try (HintsWriter writer = HintsWriter.create(dir, descriptor))
        {
            Assert.assertTrue(looksLegit(writer));

            ByteBuffer writeBuffer = ByteBuffer.allocateDirect(bufferSize);
            try (HintsWriter.Session session = writer.newSession(writeBuffer))
            {
                while (session.getBytesWritten() < bufferSize * 3)
                {
                    Hint hint = createHint(hintNum, ts+hintNum);
                    session.append(hint);
                    hints.add(hint);
                    hintNum++;
                }
            }
        }

        try (HintsReader reader = HintsReader.open(new File(dir, descriptor.fileName())))
        {
            Assert.assertTrue(looksLegit(reader.getInput()));
            List<Hint> deserialized = new ArrayList<>(hintNum);

            for (HintsReader.Page page: reader)
            {
                Iterator<Hint> iterator = page.hintsIterator();
                while (iterator.hasNext())
                {
                    deserialized.add(iterator.next());
                }
            }

            Assert.assertEquals(hints.size(), deserialized.size());
            hintNum = 0;
            for (Hint expected: hints)
            {
                HintsTestUtil.assertHintsEqual(expected, deserialized.get(hintNum));
                hintNum++;
            }
        }
+++++++++++++++++++++++
    public void multiFlushAndDeserializeTest() throws Exception
    {
        int hintNum = 0;
        int bufferSize = HintsWriteExecutor.WRITE_BUFFER_SIZE;
        List<Hint> hints = new LinkedList<>();

        UUID hostId = UUIDGen.getTimeUUID();
        long ts = System.currentTimeMillis();

        HintsDescriptor descriptor = new HintsDescriptor(hostId, ts, params());
        File dir = Files.createTempDir();
        try (HintsWriter writer = HintsWriter.create(dir, descriptor))
        {
            Assert.assertTrue(looksLegit(writer));

            ByteBuffer writeBuffer = ByteBuffer.allocateDirect(bufferSize);
            try (HintsWriter.Session session = writer.newSession(writeBuffer))
            {
                while (session.getBytesWritten() < bufferSize * 3)
                {
                    Hint hint = createHint(hintNum, ts+hintNum);
                    session.append(hint);
                    hints.add(hint);
                    hintNum++;
                }
            }
        }

        try (HintsReader reader = HintsReader.open(new File(dir, descriptor.fileName())))
        {
            Assert.assertTrue(looksLegit(reader.getInput()));
            List<Hint> deserialized = new ArrayList<>(hintNum);
            List<InputPosition> pagePositions = new ArrayList<>(hintNum);

            for (HintsReader.Page page: reader)
            {
                pagePositions.add(page.position);
                Iterator<Hint> iterator = page.hintsIterator();
                while (iterator.hasNext())
                {
                    deserialized.add(iterator.next());
                }
            }

            Assert.assertEquals(hints.size(), deserialized.size());
            hintNum = 0;
            for (Hint expected: hints)
            {
                HintsTestUtil.assertHintsEqual(expected, deserialized.get(hintNum));
                hintNum++;
            }

            // explicitely seek to each page by iterating collected page positions and check if hints still match as expected
            int hintOffset = 0;
            for (InputPosition pos : pagePositions)
            {
                reader.seek(pos);
                HintsReader.Page page = reader.iterator().next();
                Iterator<Hint> iterator = page.hintsIterator();
                while (iterator.hasNext())
                {
                    Hint seekedHint = iterator.next();
                    HintsTestUtil.assertHintsEqual(hints.get(hintOffset), seekedHint);
                    hintOffset++;
                }
            }
        }
    public File serialize(Map<MetadataType, MetadataComponent> metadata, MetadataSerializer serializer, Version version)
            throws IOException, FileNotFoundException
    {
        // Serialize to tmp file
        File statsFile = File.createTempFile(Component.STATS.name, null);
        try (DataOutputStreamPlus out = new BufferedDataOutputStreamPlus(new FileOutputStream(statsFile)))
        {
            serializer.serialize(metadata, out, version);
        }
        return statsFile;
    }

+++++++++++++++++++++++
    public File serialize(Map<MetadataType, MetadataComponent> metadata, MetadataSerializer serializer, Version version)
            throws IOException
    {
        // Serialize to tmp file
        File statsFile = File.createTempFile(Component.STATS.name, null);
        try (DataOutputStreamPlus out = new BufferedDataOutputStreamPlus(new FileOutputStream(statsFile)))
        {
            serializer.serialize(metadata, out, version);
        }
        return statsFile;
    }

    public void testSerializeDeserializeV3()
    {
        PagingState state = makeSomePagingState(Server.VERSION_3);
        ByteBuffer serialized = state.serialize(Server.VERSION_3);
        assertEquals(serialized.remaining(), state.serializedSize(Server.VERSION_3));
        assertEquals(state, PagingState.deserialize(serialized, Server.VERSION_3));
    }

    @Test
+++++++++++++++++++++++
    public void testSerializeDeserializeV3()
    {
        PagingState state = Util.makeSomePagingState(ProtocolVersion.V3);
        ByteBuffer serialized = state.serialize(ProtocolVersion.V3);
        assertEquals(serialized.remaining(), state.serializedSize(ProtocolVersion.V3));
        assertEquals(state, PagingState.deserialize(serialized, ProtocolVersion.V3));
    }

    @Test
    public void testSerializeDeserializeV4()
    {
        PagingState state = makeSomePagingState(Server.VERSION_4);
        ByteBuffer serialized = state.serialize(Server.VERSION_4);
        assertEquals(serialized.remaining(), state.serializedSize(Server.VERSION_4));
        assertEquals(state, PagingState.deserialize(serialized, Server.VERSION_4));
+++++++++++++++++++++++
    public void testSerializeDeserializeV4()
    {
        PagingState state = Util.makeSomePagingState(ProtocolVersion.V4);
        ByteBuffer serialized = state.serialize(ProtocolVersion.V4);
        assertEquals(serialized.remaining(), state.serializedSize(ProtocolVersion.V4));
        assertEquals(state, PagingState.deserialize(serialized, ProtocolVersion.V4));
add: 123, delete: 17, change: 959, unhandled: 1 size_exceptions: 8 size_serialize: 28
-----------cassandra-3.10 vs cassandra-3.11.0-----------
    public Unfiltered deserialize(DataInputPlus in, SerializationHeader header, SerializationHelper helper, Row.Builder builder)
    throws IOException
    {
        // It wouldn't be wrong per-se to use an unsorted builder, but it would be inefficient so make sure we don't do it by mistake
        assert builder.isSorted();

        int flags = in.readUnsignedByte();
        if (isEndOfPartition(flags))
            return null;

        int extendedFlags = readExtendedFlags(in, flags);

        if (kind(flags) == Unfiltered.Kind.RANGE_TOMBSTONE_MARKER)
        {
            ClusteringBoundOrBoundary bound = ClusteringBoundOrBoundary.serializer.deserialize(in, helper.version, header.clusteringTypes());
            return deserializeMarkerBody(in, header, bound);
        }
        else
        {
            // deserializeStaticRow should be used for that.
            if (isStatic(extendedFlags))
                throw new IOException("Corrupt flags value for unfiltered partition (isStatic flag set): " + flags);

            builder.newRow(Clustering.serializer.deserialize(in, helper.version, header.clusteringTypes()));
            Row row = deserializeRowBody(in, header, helper, flags, extendedFlags, builder);
            // we do not write empty rows because Rows.collectStats(), called by BTW.applyToRow(), asserts that rows are not empty
            // if we don't throw here, then later the very same assertion in Rows.collectStats() will fail compactions
            // see BlackListingCompactionsTest and CASSANDRA-9530 for details
            if (row.isEmpty())
                throw new IOException("Corrupt empty row found in unfiltered partition");
            return row;
        }
    }

+++++++++++++++++++++++
    public Unfiltered deserialize(DataInputPlus in, SerializationHeader header, SerializationHelper helper, Row.Builder builder)
    throws IOException
    {
        while (true)
        {
            Unfiltered unfiltered = deserializeOne(in, header, helper, builder);
            if (unfiltered == null)
                return null;

            // Skip empty rows, see deserializeOne javadoc
            if (!unfiltered.isEmpty())
                return unfiltered;
        }
    }

    /**
     * Deserialize a single {@link Unfiltered} from the provided input.
     * <p>
     * <b>WARNING:</b> this can return an empty row because it's possible there is a row serialized, but that row only
     * contains data for dropped columns, see CASSANDRA-13337. But as most code expect rows to not be empty, this isn't
     * meant to be exposed publicly.
     *
     * But as {@link UnfilteredRowIterator} should not return empty
     * rows, this mean consumer of this method should make sure to skip said empty rows.
     */
    private void serializePartition(UnfilteredRowIterator partition)
    {
        try
        {
            json.writeStartObject();

            json.writeFieldName("partition");
            json.writeStartObject();
            json.writeFieldName("key");
            serializePartitionKey(partition.partitionKey());
            json.writeNumberField("position", this.currentScanner.getCurrentPosition());

            if (!partition.partitionLevelDeletion().isLive())
            {
                serializeDeletion(partition.partitionLevelDeletion());
                json.writeEndObject();
            }
            else
            {
                json.writeEndObject();
                json.writeFieldName("rows");
                json.writeStartArray();
                updatePosition();
                if (!partition.staticRow().isEmpty())
                {
                    serializeRow(partition.staticRow());
                }
                Unfiltered unfiltered;
                updatePosition();
                while (partition.hasNext())
                {
                    unfiltered = partition.next();
                    if (unfiltered instanceof Row)
                    {
                        serializeRow((Row) unfiltered);
                    }
                    else if (unfiltered instanceof RangeTombstoneMarker)
                    {
                        serializeTombstone((RangeTombstoneMarker) unfiltered);
                    }
                    updatePosition();
                }
                json.writeEndArray();
            }

            json.writeEndObject();
        }
        catch (IOException e)
        {
            String key = metadata.getKeyValidator().getString(partition.partitionKey().getKey());
            logger.error("Fatal error parsing partition: {}", key, e);
        }
    }

+++++++++++++++++++++++
    private void serializePartition(UnfilteredRowIterator partition)
    {
        try
        {
            json.writeStartObject();

            json.writeFieldName("partition");
            json.writeStartObject();
            json.writeFieldName("key");
            serializePartitionKey(partition.partitionKey());
            json.writeNumberField("position", this.currentScanner.getCurrentPosition());

            if (!partition.partitionLevelDeletion().isLive())
                serializeDeletion(partition.partitionLevelDeletion());

            json.writeEndObject();

            if (partition.hasNext() || partition.staticRow() != null)
            {
                json.writeFieldName("rows");
                json.writeStartArray();
                updatePosition();
                if (!partition.staticRow().isEmpty())
                    serializeRow(partition.staticRow());

                Unfiltered unfiltered;
                updatePosition();
                while (partition.hasNext())
                {
                    unfiltered = partition.next();
                    if (unfiltered instanceof Row)
                    {
                        serializeRow((Row) unfiltered);
                    }
                    else if (unfiltered instanceof RangeTombstoneMarker)
                    {
                        serializeTombstone((RangeTombstoneMarker) unfiltered);
                    }
                    updatePosition();
                }
                json.writeEndArray();

                json.writeEndObject();
            }
        }
        catch (IOException e)
        {
            String key = metadata.getKeyValidator().getString(partition.partitionKey().getKey());
            logger.error("Fatal error parsing partition: {}", key, e);
        }
    }

add: 17, delete: 2, change: 232, unhandled: 0 size_exceptions: 1 size_serialize: 2
-----------cassandra-3.11.0 vs cassandra-3.11.1-----------
    public static long serializedSizeAsLegacyPartition(ReadCommand command, UnfilteredRowIterator partition, int version)
    {
        assert version < MessagingService.VERSION_30;

        if (partition.isEmpty())
            return TypeSizes.sizeof(false);

        long size = TypeSizes.sizeof(true);

        LegacyLayout.LegacyUnfilteredPartition legacyPartition = LegacyLayout.fromUnfilteredRowIterator(command, partition);

        size += UUIDSerializer.serializer.serializedSize(partition.metadata().cfId, version);
        size += DeletionTime.serializer.serializedSize(legacyPartition.partitionDeletion);
        size += legacyPartition.rangeTombstones.serializedSize(partition.metadata());

        // begin cell serialization
        size += TypeSizes.sizeof(legacyPartition.cells.size());
        for (LegacyLayout.LegacyCell cell : legacyPartition.cells)
        {
            size += ByteBufferUtil.serializedSizeWithShortLength(cell.name.encode(partition.metadata()));
            size += 1;  // serialization flags
            if (cell.kind == LegacyLayout.LegacyCell.Kind.EXPIRING)
            {
                size += TypeSizes.sizeof(cell.ttl);
                size += TypeSizes.sizeof(cell.localDeletionTime);
            }
            else if (cell.kind == LegacyLayout.LegacyCell.Kind.DELETED)
            {
                size += TypeSizes.sizeof(cell.timestamp);
                // localDeletionTime replaces cell.value as the body
                size += TypeSizes.sizeof(TypeSizes.sizeof(cell.localDeletionTime));
                size += TypeSizes.sizeof(cell.localDeletionTime);
                continue;
            }
            else if (cell.kind == LegacyLayout.LegacyCell.Kind.COUNTER)
            {
                size += TypeSizes.sizeof(Long.MIN_VALUE);  // timestampOfLastDelete
            }

            size += TypeSizes.sizeof(cell.timestamp);
            size += ByteBufferUtil.serializedSizeWithLength(cell.value);
        }

        return size;
    }

    // For thrift sake
+++++++++++++++++++++++
    public static long serializedSizeAsLegacyPartition(ReadCommand command, UnfilteredRowIterator partition, int version)
    {
        assert version < MessagingService.VERSION_30;

        if (partition.isEmpty())
            return TypeSizes.sizeof(false);

        long size = TypeSizes.sizeof(true);

        LegacyLayout.LegacyUnfilteredPartition legacyPartition = LegacyLayout.fromUnfilteredRowIterator(command, partition);

        size += UUIDSerializer.serializer.serializedSize(partition.metadata().cfId, version);
        size += DeletionTime.serializer.serializedSize(legacyPartition.partitionDeletion);
        size += legacyPartition.rangeTombstones.serializedSize(partition.metadata());

        // begin cell serialization
        size += TypeSizes.sizeof(legacyPartition.cells.size());
        for (LegacyLayout.LegacyCell cell : legacyPartition.cells)
        {
            size += ByteBufferUtil.serializedSizeWithShortLength(cell.name.encode(partition.metadata()));
            size += 1;  // serialization flags
            if (cell.isExpiring())
            {
                size += TypeSizes.sizeof(cell.ttl);
                size += TypeSizes.sizeof(cell.localDeletionTime);
            }
            else if (cell.isTombstone())
            {
                size += TypeSizes.sizeof(cell.timestamp);
                // localDeletionTime replaces cell.value as the body
                size += TypeSizes.sizeof(TypeSizes.sizeof(cell.localDeletionTime));
                size += TypeSizes.sizeof(cell.localDeletionTime);
                continue;
            }
            else if (cell.isCounterUpdate())
            {
                size += TypeSizes.sizeof(cell.timestamp);
                long count = CounterContext.instance().getLocalCount(cell.value);
                size += ByteBufferUtil.serializedSizeWithLength(ByteBufferUtil.bytes(count));
                continue;
            }
            else if (cell.isCounter())
            {
                size += TypeSizes.sizeof(Long.MIN_VALUE);  // timestampOfLastDelete
            }

            size += TypeSizes.sizeof(cell.timestamp);
            size += ByteBufferUtil.serializedSizeWithLength(cell.value);
        }

        return size;
    }

    // For thrift sake
    public int serializedSize()
    {
        if (numBlocks == 1)
            return (BLOCK_HEADER_BYTES + ((int) tokenCount * 16));
        else
            return numBlocks * BLOCK_BYTES;
    }

+++++++++++++++++++++++
    public int serializedSize()
    {
        if (numBlocks == 1)
            return BLOCK_HEADER_BYTES +
                   ((int) tokenCount * BLOCK_ENTRY_BYTES) +
                   (((Leaf) root).overflowCollisionCount() * OVERFLOW_ENTRY_BYTES);
        else
            return numBlocks * BLOCK_BYTES;
    }

    private void serializeCell(Cell cell, LivenessInfo liveInfo)
    {
        try
        {
            json.writeStartObject();
            objectIndenter.setCompact(true);
            json.writeFieldName("name");
            AbstractType<?> type = cell.column().type;
            json.writeString(cell.column().name.toCQLString());

            if (cell.path() != null && cell.path().size() > 0)
            {
                CollectionType ct = (CollectionType) type;
                json.writeFieldName("path");
                arrayIndenter.setCompact(true);
                json.writeStartArray();
                for (int i = 0; i < cell.path().size(); i++)
                {
                    json.writeString(ct.nameComparator().getString(cell.path().get(i)));
                }
                json.writeEndArray();
                arrayIndenter.setCompact(false);
            }
            if (cell.isTombstone())
            {
                json.writeFieldName("deletion_info");
                objectIndenter.setCompact(true);
                json.writeStartObject();
                json.writeFieldName("local_delete_time");
                json.writeString(dateString(TimeUnit.SECONDS, cell.localDeletionTime()));
                json.writeEndObject();
                objectIndenter.setCompact(false);
            }
            else
            {
                json.writeFieldName("value");
                json.writeString(cell.column().cellValueType().getString(cell.value()));
            }
            if (liveInfo.isEmpty() || cell.timestamp() != liveInfo.timestamp())
            {
                json.writeFieldName("tstamp");
                json.writeString(dateString(TimeUnit.MICROSECONDS, cell.timestamp()));
            }
            if (cell.isExpiring() && (liveInfo.isEmpty() || cell.ttl() != liveInfo.ttl()))
            {
                json.writeFieldName("ttl");
                json.writeNumber(cell.ttl());
                json.writeFieldName("expires_at");
                json.writeString(dateString(TimeUnit.SECONDS, cell.localDeletionTime()));
                json.writeFieldName("expired");
                json.writeBoolean(!cell.isLive((int) (System.currentTimeMillis() / 1000)));
            }
            json.writeEndObject();
            objectIndenter.setCompact(false);
        }
        catch (IOException e)
        {
            logger.error("Failure parsing cell.", e);
        }
    }

+++++++++++++++++++++++
    private void serializeCell(Cell cell, LivenessInfo liveInfo)
    {
        try
        {
            json.writeStartObject();
            objectIndenter.setCompact(true);
            json.writeFieldName("name");
            AbstractType<?> type = cell.column().type;
            AbstractType<?> cellType = null;
            json.writeString(cell.column().name.toCQLString());

            if (type.isCollection() && type.isMultiCell()) // non-frozen collection
            {
                CollectionType ct = (CollectionType) type;
                json.writeFieldName("path");
                arrayIndenter.setCompact(true);
                json.writeStartArray();
                for (int i = 0; i < cell.path().size(); i++)
                {
                    json.writeString(ct.nameComparator().getString(cell.path().get(i)));
                }
                json.writeEndArray();
                arrayIndenter.setCompact(false);

                cellType = cell.column().cellValueType();
            }
            else if (type.isUDT() && type.isMultiCell()) // non-frozen udt
            {
                UserType ut = (UserType) type;
                json.writeFieldName("path");
                arrayIndenter.setCompact(true);
                json.writeStartArray();
                for (int i = 0; i < cell.path().size(); i++)
                {
                    Short fieldPosition = ut.nameComparator().compose(cell.path().get(i));
                    json.writeString(ut.fieldNameAsString(fieldPosition));
                }
                json.writeEndArray();
                arrayIndenter.setCompact(false);

                // cellType of udt
                Short fieldPosition = ((UserType) type).nameComparator().compose(cell.path().get(0));
                cellType = ((UserType) type).fieldType(fieldPosition);
            }
            else
            {
                cellType = cell.column().cellValueType();
            }
            if (cell.isTombstone())
            {
                json.writeFieldName("deletion_info");
                objectIndenter.setCompact(true);
                json.writeStartObject();
                json.writeFieldName("local_delete_time");
                json.writeString(dateString(TimeUnit.SECONDS, cell.localDeletionTime()));
                json.writeEndObject();
                objectIndenter.setCompact(false);
            }
            else
            {
                json.writeFieldName("value");
                json.writeRawValue(cellType.toJSONString(cell.value(), ProtocolVersion.CURRENT));
            }
            if (liveInfo.isEmpty() || cell.timestamp() != liveInfo.timestamp())
            {
                json.writeFieldName("tstamp");
                json.writeString(dateString(TimeUnit.MICROSECONDS, cell.timestamp()));
            }
            if (cell.isExpiring() && (liveInfo.isEmpty() || cell.ttl() != liveInfo.ttl()))
            {
                json.writeFieldName("ttl");
                json.writeNumber(cell.ttl());
                json.writeFieldName("expires_at");
                json.writeString(dateString(TimeUnit.SECONDS, cell.localDeletionTime()));
                json.writeFieldName("expired");
                json.writeBoolean(!cell.isLive((int) (System.currentTimeMillis() / 1000)));
            }
            json.writeEndObject();
            objectIndenter.setCompact(false);
        }
        catch (IOException e)
        {
            logger.error("Failure parsing cell.", e);
        }
    }

add: 19, delete: 0, change: 219, unhandled: 1 size_exceptions: 0 size_serialize: 3
-----------cassandra-3.11.1 vs cassandra-3.11.2-----------
    private void serializeClustering(ClusteringPrefix clustering) throws IOException
    {
        if (clustering.size() > 0)
        {
            json.writeFieldName("clustering");
            objectIndenter.setCompact(true);
            json.writeStartArray();
            arrayIndenter.setCompact(true);
            List<ColumnDefinition> clusteringColumns = metadata.clusteringColumns();
            for (int i = 0; i < clusteringColumns.size(); i++)
            {
                ColumnDefinition column = clusteringColumns.get(i);
                if (i >= clustering.size())
                {
                    json.writeString("*");
                }
                else
                {
                    json.writeString(column.cellValueType().getString(clustering.get(i)));
                }
            }
            json.writeEndArray();
            objectIndenter.setCompact(false);
            arrayIndenter.setCompact(false);
        }
    }

+++++++++++++++++++++++
    private void serializeClustering(ClusteringPrefix clustering) throws IOException
    {
        if (clustering.size() > 0)
        {
            json.writeFieldName("clustering");
            objectIndenter.setCompact(true);
            json.writeStartArray();
            arrayIndenter.setCompact(true);
            List<ColumnDefinition> clusteringColumns = metadata.clusteringColumns();
            for (int i = 0; i < clusteringColumns.size(); i++)
            {
                ColumnDefinition column = clusteringColumns.get(i);
                if (i >= clustering.size())
                {
                    json.writeString("*");
                }
                else
                {
                    json.writeRawValue(column.cellValueType().toJSONString(clustering.get(i), ProtocolVersion.CURRENT));
                }
            }
            json.writeEndArray();
            objectIndenter.setCompact(false);
            arrayIndenter.setCompact(false);
        }
    }

add: 19, delete: 1, change: 151, unhandled: 0 size_exceptions: 1 size_serialize: 1
-----------cassandra-3.11.2 vs cassandra-3.11.3-----------
add: 17, delete: 2, change: 88, unhandled: 0 size_exceptions: 0 size_serialize: 0
-----------cassandra-3.11.3 vs cassandra-3.11.4-----------
    public static long serializedSizeAsLegacyPartition(ReadCommand command, UnfilteredRowIterator partition, int version)
    {
        assert version < MessagingService.VERSION_30;

        if (partition.isEmpty())
            return TypeSizes.sizeof(false);

        long size = TypeSizes.sizeof(true);

        LegacyLayout.LegacyUnfilteredPartition legacyPartition = LegacyLayout.fromUnfilteredRowIterator(command, partition);

        size += UUIDSerializer.serializer.serializedSize(partition.metadata().cfId, version);
        size += DeletionTime.serializer.serializedSize(legacyPartition.partitionDeletion);
        size += legacyPartition.rangeTombstones.serializedSize(partition.metadata());

        // begin cell serialization
        size += TypeSizes.sizeof(legacyPartition.cells.size());
        for (LegacyLayout.LegacyCell cell : legacyPartition.cells)
        {
            size += ByteBufferUtil.serializedSizeWithShortLength(cell.name.encode(partition.metadata()));
            size += 1;  // serialization flags
            if (cell.isExpiring())
            {
                size += TypeSizes.sizeof(cell.ttl);
                size += TypeSizes.sizeof(cell.localDeletionTime);
            }
            else if (cell.isTombstone())
            {
                size += TypeSizes.sizeof(cell.timestamp);
                // localDeletionTime replaces cell.value as the body
                size += TypeSizes.sizeof(TypeSizes.sizeof(cell.localDeletionTime));
                size += TypeSizes.sizeof(cell.localDeletionTime);
                continue;
            }
            else if (cell.isCounterUpdate())
            {
                size += TypeSizes.sizeof(cell.timestamp);
                long count = CounterContext.instance().getLocalCount(cell.value);
                size += ByteBufferUtil.serializedSizeWithLength(ByteBufferUtil.bytes(count));
                continue;
            }
            else if (cell.isCounter())
            {
                size += TypeSizes.sizeof(Long.MIN_VALUE);  // timestampOfLastDelete
            }

            size += TypeSizes.sizeof(cell.timestamp);
            size += ByteBufferUtil.serializedSizeWithLength(cell.value);
        }

        return size;
    }

    // For thrift sake
+++++++++++++++++++++++
    public static long serializedSizeAsLegacyPartition(ReadCommand command, UnfilteredRowIterator partition, int version)
    {
        assert version < MessagingService.VERSION_30;

        if (partition.isEmpty())
            return TypeSizes.sizeof(false);

        long size = TypeSizes.sizeof(true);

        LegacyLayout.LegacyUnfilteredPartition legacyPartition = LegacyLayout.fromUnfilteredRowIterator(command, partition);

        size += UUIDSerializer.serializer.serializedSize(partition.metadata().cfId, version);
        size += DeletionTime.serializer.serializedSize(legacyPartition.partitionDeletion);
        size += legacyPartition.rangeTombstones.serializedSize(partition.metadata());

        // begin cell serialization
        size += TypeSizes.sizeof(legacyPartition.cells.size());
        for (LegacyLayout.LegacyCell cell : legacyPartition.cells)
        {
            size += ByteBufferUtil.serializedSizeWithShortLength(cell.name.encode(partition.metadata()));
            size += 1;  // serialization flags
            if (cell.isExpiring())
            {
                size += TypeSizes.sizeof(cell.ttl);
                size += TypeSizes.sizeof(cell.localDeletionTime);
            }
            else if (cell.isTombstone())
            {
                size += TypeSizes.sizeof(cell.timestamp);
                // localDeletionTime replaces cell.value as the body
                size += TypeSizes.sizeof(TypeSizes.sizeof(cell.localDeletionTime));
                size += TypeSizes.sizeof(cell.localDeletionTime);
                continue;
            }
            else if (cell.isCounterUpdate())
            {
                size += TypeSizes.sizeof(cell.timestamp);
                long count = CounterContext.instance().getUpdateCount(cell.value);
                size += ByteBufferUtil.serializedSizeWithLength(ByteBufferUtil.bytes(count));
                continue;
            }
            else if (cell.isCounter())
            {
                size += TypeSizes.sizeof(Long.MIN_VALUE);  // timestampOfLastDelete
            }

            size += TypeSizes.sizeof(cell.timestamp);
            size += ByteBufferUtil.serializedSizeWithLength(cell.value);
        }

        return size;
    }

    // For thrift sake
    public static void serializeAsLegacyPartition(ReadCommand command, UnfilteredRowIterator partition, DataOutputPlus out, int version) throws IOException
    {
        assert version < MessagingService.VERSION_30;

        out.writeBoolean(true);

        LegacyLayout.LegacyUnfilteredPartition legacyPartition = LegacyLayout.fromUnfilteredRowIterator(command, partition);

        UUIDSerializer.serializer.serialize(partition.metadata().cfId, out, version);
        DeletionTime.serializer.serialize(legacyPartition.partitionDeletion, out);

        legacyPartition.rangeTombstones.serialize(out, partition.metadata());

        // begin cell serialization
        out.writeInt(legacyPartition.cells.size());
        for (LegacyLayout.LegacyCell cell : legacyPartition.cells)
        {
            ByteBufferUtil.writeWithShortLength(cell.name.encode(partition.metadata()), out);
            out.writeByte(cell.serializationFlags());
            if (cell.isExpiring())
            {
                out.writeInt(cell.ttl);
                out.writeInt(cell.localDeletionTime);
            }
            else if (cell.isTombstone())
            {
                out.writeLong(cell.timestamp);
                out.writeInt(TypeSizes.sizeof(cell.localDeletionTime));
                out.writeInt(cell.localDeletionTime);
                continue;
            }
            else if (cell.isCounterUpdate())
            {
                out.writeLong(cell.timestamp);
                long count = CounterContext.instance().getLocalCount(cell.value);
                ByteBufferUtil.writeWithLength(ByteBufferUtil.bytes(count), out);
                continue;
            }
            else if (cell.isCounter())
            {
                out.writeLong(Long.MIN_VALUE);  // timestampOfLastDelete (not used, and MIN_VALUE is the default)
            }

            out.writeLong(cell.timestamp);
            ByteBufferUtil.writeWithLength(cell.value, out);
        }
    }

    // For the old wire format
    // Note: this can return null if an empty partition is serialized!
+++++++++++++++++++++++
    public static void serializeAsLegacyPartition(ReadCommand command, UnfilteredRowIterator partition, DataOutputPlus out, int version) throws IOException
    {
        assert version < MessagingService.VERSION_30;

        out.writeBoolean(true);

        LegacyLayout.LegacyUnfilteredPartition legacyPartition = LegacyLayout.fromUnfilteredRowIterator(command, partition);

        UUIDSerializer.serializer.serialize(partition.metadata().cfId, out, version);
        DeletionTime.serializer.serialize(legacyPartition.partitionDeletion, out);

        legacyPartition.rangeTombstones.serialize(out, partition.metadata());

        // begin cell serialization
        out.writeInt(legacyPartition.cells.size());
        for (LegacyLayout.LegacyCell cell : legacyPartition.cells)
        {
            ByteBufferUtil.writeWithShortLength(cell.name.encode(partition.metadata()), out);
            out.writeByte(cell.serializationFlags());
            if (cell.isExpiring())
            {
                out.writeInt(cell.ttl);
                out.writeInt(cell.localDeletionTime);
            }
            else if (cell.isTombstone())
            {
                out.writeLong(cell.timestamp);
                out.writeInt(TypeSizes.sizeof(cell.localDeletionTime));
                out.writeInt(cell.localDeletionTime);
                continue;
            }
            else if (cell.isCounterUpdate())
            {
                out.writeLong(cell.timestamp);
                long count = CounterContext.instance().getUpdateCount(cell.value);
                ByteBufferUtil.writeWithLength(ByteBufferUtil.bytes(count), out);
                continue;
            }
            else if (cell.isCounter())
            {
                out.writeLong(Long.MIN_VALUE);  // timestampOfLastDelete (not used, and MIN_VALUE is the default)
            }

            out.writeLong(cell.timestamp);
            ByteBufferUtil.writeWithLength(cell.value, out);
        }
    }

    // For the old wire format
    // Note: this can return null if an empty partition is serialized!
    private void serializeRowBody(Row row, int flags, SerializationHeader header, DataOutputPlus out)
    throws IOException
    {
        boolean isStatic = row.isStatic();

        Columns headerColumns = header.columns(isStatic);
        LivenessInfo pkLiveness = row.primaryKeyLivenessInfo();
        Row.Deletion deletion = row.deletion();

        if ((flags & HAS_TIMESTAMP) != 0)
            header.writeTimestamp(pkLiveness.timestamp(), out);
        if ((flags & HAS_TTL) != 0)
        {
            header.writeTTL(pkLiveness.ttl(), out);
            header.writeLocalDeletionTime(pkLiveness.localExpirationTime(), out);
        }
        if ((flags & HAS_DELETION) != 0)
            header.writeDeletionTime(deletion.time(), out);

        if ((flags & HAS_ALL_COLUMNS) == 0)
            Columns.serializer.serializeSubset(Collections2.transform(row, ColumnData::column), headerColumns, out);

        SearchIterator<ColumnDefinition, ColumnDefinition> si = headerColumns.iterator();

        try
        {
            row.apply(cd -> {
                // We can obtain the column for data directly from data.column(). However, if the cell/complex data
                // originates from a sstable, the column we'll get will have the type used when the sstable was serialized,
                // and if that type have been recently altered, that may not be the type we want to serialize the column
                // with. So we use the ColumnDefinition from the "header" which is "current". Also see #11810 for what
                // happens if we don't do that.
                ColumnDefinition column = si.next(cd.column());
                assert column != null : cd.column.toString();

                try
                {
                    if (cd.column.isSimple())
                        Cell.serializer.serialize((Cell) cd, column, out, pkLiveness, header);
                    else
                        writeComplexColumn((ComplexColumnData) cd, column, (flags & HAS_COMPLEX_DELETION) != 0, pkLiveness, header, out);
                }
                catch (IOException e)
                {
                    throw new WrappedException(e);
                }
            }, false);
        }
        catch (WrappedException e)
        {
            if (e.getCause() instanceof IOException)
                throw (IOException) e.getCause();

            throw e;
        }
    }

+++++++++++++++++++++++
    private void serializeRowBody(Row row, int flags, SerializationHeader header, DataOutputPlus out)
    throws IOException
    {
        boolean isStatic = row.isStatic();

        Columns headerColumns = header.columns(isStatic);
        LivenessInfo pkLiveness = row.primaryKeyLivenessInfo();
        Row.Deletion deletion = row.deletion();

        if ((flags & HAS_TIMESTAMP) != 0)
            header.writeTimestamp(pkLiveness.timestamp(), out);
        if ((flags & HAS_TTL) != 0)
        {
            header.writeTTL(pkLiveness.ttl(), out);
            header.writeLocalDeletionTime(pkLiveness.localExpirationTime(), out);
        }
        if ((flags & HAS_DELETION) != 0)
            header.writeDeletionTime(deletion.time(), out);

        if ((flags & HAS_ALL_COLUMNS) == 0)
            Columns.serializer.serializeSubset(row.columns(), headerColumns, out);

        SearchIterator<ColumnDefinition, ColumnDefinition> si = headerColumns.iterator();

        try
        {
            row.apply(cd -> {
                // We can obtain the column for data directly from data.column(). However, if the cell/complex data
                // originates from a sstable, the column we'll get will have the type used when the sstable was serialized,
                // and if that type have been recently altered, that may not be the type we want to serialize the column
                // with. So we use the ColumnDefinition from the "header" which is "current". Also see #11810 for what
                // happens if we don't do that.
                ColumnDefinition column = si.next(cd.column());
                assert column != null : cd.column.toString();

                try
                {
                    if (cd.column.isSimple())
                        Cell.serializer.serialize((Cell) cd, column, out, pkLiveness, header);
                    else
                        writeComplexColumn((ComplexColumnData) cd, column, (flags & HAS_COMPLEX_DELETION) != 0, pkLiveness, header, out);
                }
                catch (IOException e)
                {
                    throw new WrappedException(e);
                }
            }, false);
        }
        catch (WrappedException e)
        {
            if (e.getCause() instanceof IOException)
                throw (IOException) e.getCause();

            throw e;
        }
    }

    private long serializedRowBodySize(Row row, SerializationHeader header, long previousUnfilteredSize, int version)
    {
        long size = 0;

        if (header.isForSSTable())
            size += TypeSizes.sizeofUnsignedVInt(previousUnfilteredSize);

        boolean isStatic = row.isStatic();
        Columns headerColumns = header.columns(isStatic);
        LivenessInfo pkLiveness = row.primaryKeyLivenessInfo();
        Row.Deletion deletion = row.deletion();
        boolean hasComplexDeletion = row.hasComplexDeletion();
        boolean hasAllColumns = (row.size() == headerColumns.size());

        if (!pkLiveness.isEmpty())
            size += header.timestampSerializedSize(pkLiveness.timestamp());
        if (pkLiveness.isExpiring())
        {
            size += header.ttlSerializedSize(pkLiveness.ttl());
            size += header.localDeletionTimeSerializedSize(pkLiveness.localExpirationTime());
        }
        if (!deletion.isLive())
            size += header.deletionTimeSerializedSize(deletion.time());

        if (!hasAllColumns)
            size += Columns.serializer.serializedSubsetSize(Collections2.transform(row, ColumnData::column), header.columns(isStatic));

        SearchIterator<ColumnDefinition, ColumnDefinition> si = headerColumns.iterator();
        for (ColumnData data : row)
        {
            ColumnDefinition column = si.next(data.column());
            assert column != null;

            if (data.column.isSimple())
                size += Cell.serializer.serializedSize((Cell) data, column, pkLiveness, header);
            else
                size += sizeOfComplexColumn((ComplexColumnData) data, column, hasComplexDeletion, pkLiveness, header);
        }

        return size;
    }

+++++++++++++++++++++++
    private long serializedRowBodySize(Row row, SerializationHeader header, long previousUnfilteredSize, int version)
    {
        long size = 0;

        if (header.isForSSTable())
            size += TypeSizes.sizeofUnsignedVInt(previousUnfilteredSize);

        boolean isStatic = row.isStatic();
        Columns headerColumns = header.columns(isStatic);
        LivenessInfo pkLiveness = row.primaryKeyLivenessInfo();
        Row.Deletion deletion = row.deletion();
        boolean hasComplexDeletion = row.hasComplexDeletion();
        boolean hasAllColumns = (row.columnCount() == headerColumns.size());

        if (!pkLiveness.isEmpty())
            size += header.timestampSerializedSize(pkLiveness.timestamp());
        if (pkLiveness.isExpiring())
        {
            size += header.ttlSerializedSize(pkLiveness.ttl());
            size += header.localDeletionTimeSerializedSize(pkLiveness.localExpirationTime());
        }
        if (!deletion.isLive())
            size += header.deletionTimeSerializedSize(deletion.time());

        if (!hasAllColumns)
            size += Columns.serializer.serializedSubsetSize(row.columns(), header.columns(isStatic));

        SearchIterator<ColumnDefinition, ColumnDefinition> si = headerColumns.iterator();
        for (ColumnData data : row)
        {
            ColumnDefinition column = si.next(data.column());
            assert column != null;

            if (data.column.isSimple())
                size += Cell.serializer.serializedSize((Cell) data, column, pkLiveness, header);
            else
                size += sizeOfComplexColumn((ComplexColumnData) data, column, hasComplexDeletion, pkLiveness, header);
        }

        return size;
    }

add: 23, delete: 0, change: 131, unhandled: 1 size_exceptions: 4 size_serialize: 4
-----------cassandra-3.11.4 vs cassandra-3.11.5-----------
    public ByteBuffer serialize(ProtocolVersion protocolVersion)
    {
        assert rowMark == null || protocolVersion == rowMark.protocolVersion;
        try (DataOutputBuffer out = new DataOutputBufferFixed(serializedSize(protocolVersion)))
        {
            ByteBuffer pk = partitionKey == null ? ByteBufferUtil.EMPTY_BYTE_BUFFER : partitionKey;
            ByteBuffer mark = rowMark == null ? ByteBufferUtil.EMPTY_BYTE_BUFFER : rowMark.mark;
            if (protocolVersion.isSmallerOrEqualTo(ProtocolVersion.V3))
            {
                ByteBufferUtil.writeWithShortLength(pk, out);
                ByteBufferUtil.writeWithShortLength(mark, out);
                out.writeInt(remaining);
                out.writeInt(remainingInPartition);
            }
            else
            {
                ByteBufferUtil.writeWithVIntLength(pk, out);
                ByteBufferUtil.writeWithVIntLength(mark, out);
                out.writeUnsignedVInt(remaining);
                out.writeUnsignedVInt(remainingInPartition);
            }
            return out.buffer();
        }
        catch (IOException e)
        {
            throw new RuntimeException(e);
        }
    }

+++++++++++++++++++++++
    public ByteBuffer serialize(ProtocolVersion protocolVersion)
    {
        assert rowMark == null || protocolVersion == rowMark.protocolVersion;
        try
        {
            return protocolVersion.isGreaterThan(ProtocolVersion.V3) ? modernSerialize() : legacySerialize(true);
        }
        catch (IOException e)
        {
            throw new RuntimeException(e);
        }
    }

    public int serializedSize(ProtocolVersion protocolVersion)
    {
        assert rowMark == null || protocolVersion == rowMark.protocolVersion;
        ByteBuffer pk = partitionKey == null ? ByteBufferUtil.EMPTY_BYTE_BUFFER : partitionKey;
        ByteBuffer mark = rowMark == null ? ByteBufferUtil.EMPTY_BYTE_BUFFER : rowMark.mark;
        if (protocolVersion.isSmallerOrEqualTo(ProtocolVersion.V3))
        {
            return ByteBufferUtil.serializedSizeWithShortLength(pk)
                 + ByteBufferUtil.serializedSizeWithShortLength(mark)
                 + 8; // remaining & remainingInPartition
        }
        else
        {
            return ByteBufferUtil.serializedSizeWithVIntLength(pk)
                 + ByteBufferUtil.serializedSizeWithVIntLength(mark)
                 + TypeSizes.sizeofUnsignedVInt(remaining)
                 + TypeSizes.sizeofUnsignedVInt(remainingInPartition);
        }
    }

    @Override
+++++++++++++++++++++++
    public int serializedSize(ProtocolVersion protocolVersion)
    {
        assert rowMark == null || protocolVersion == rowMark.protocolVersion;

        return protocolVersion.isGreaterThan(ProtocolVersion.V3) ? modernSerializedSize() : legacySerializedSize(true);
    }

    /**
     * It's possible to receive a V3 paging state on a V4 client session, and vice versa - so we cannot
     * blindly rely on the protocol version provided. We must verify first that the buffer indeed contains
     * a paging state that adheres to the protocol version provided, or, if not - see if it is in a different
     * version, in which case we try the other format.
     */
    public static PagingState deserialize(ByteBuffer bytes, ProtocolVersion protocolVersion)
    {
        if (bytes == null)
            return null;

        try (DataInputBuffer in = new DataInputBuffer(bytes, true))
        {
            ByteBuffer pk;
            RowMark mark;
            int remaining, remainingInPartition;
            if (protocolVersion.isSmallerOrEqualTo(ProtocolVersion.V3))
            {
                pk = ByteBufferUtil.readWithShortLength(in);
                mark = new RowMark(ByteBufferUtil.readWithShortLength(in), protocolVersion);
                remaining = in.readInt();
                // Note that while 'in.available()' is theoretically an estimate of how many bytes are available
                // without blocking, we know that since we're reading a ByteBuffer it will be exactly how many
                // bytes remain to be read. And the reason we want to condition this is for backward compatility
                // as we used to not set this.
                remainingInPartition = in.available() > 0 ? in.readInt() : Integer.MAX_VALUE;
            }
            else
            {
                pk = ByteBufferUtil.readWithVIntLength(in);
                mark = new RowMark(ByteBufferUtil.readWithVIntLength(in), protocolVersion);
                remaining = (int)in.readUnsignedVInt();
                remainingInPartition = (int)in.readUnsignedVInt();
            }
            return new PagingState(pk.hasRemaining() ? pk : null,
                                   mark.mark.hasRemaining() ? mark : null,
                                   remaining,
                                   remainingInPartition);
        }
        catch (IOException e)
        {
            throw new ProtocolException("Invalid value for the paging state");
        }
    }

+++++++++++++++++++++++
    public static PagingState deserialize(ByteBuffer bytes, ProtocolVersion protocolVersion)
    {
        if (bytes == null)
            return null;

        try
        {
            /*
             * We can't just attempt to deser twice, as we risk to misinterpet short/vint
             * lengths and allocate huge byte arrays for readWithVIntLength() or,
             * to a lesser extent, readWithShortLength()
             */

            if (protocolVersion.isGreaterThan(ProtocolVersion.V3))
            {
                if (isModernSerialized(bytes)) return modernDeserialize(bytes, protocolVersion);
                if (isLegacySerialized(bytes)) return legacyDeserialize(bytes, ProtocolVersion.V3);
            }

            if (protocolVersion.isSmallerThan(ProtocolVersion.V4))
            {
                if (isLegacySerialized(bytes)) return legacyDeserialize(bytes, protocolVersion);
                if (isModernSerialized(bytes)) return modernDeserialize(bytes, ProtocolVersion.V4);
            }
        }
        catch (IOException e)
        {
            throw new ProtocolException("Invalid value for the paging state");
        }

        throw new ProtocolException("Invalid value for the paging state");
    }

    /*
     * Modern serde (> VERSION_3)
     */

    @SuppressWarnings({ "resource", "RedundantSuppression" })
add: 54, delete: 14, change: 132, unhandled: 2 size_exceptions: 6 size_serialize: 3
-----------cassandra-3.11.5 vs cassandra-3.11.6-----------
    private void serializePartition(UnfilteredRowIterator partition)
    {
        try
        {
            json.writeStartObject();

            json.writeFieldName("partition");
            json.writeStartObject();
            json.writeFieldName("key");
            serializePartitionKey(partition.partitionKey());
            json.writeNumberField("position", this.currentScanner.getCurrentPosition());

            if (!partition.partitionLevelDeletion().isLive())
                serializeDeletion(partition.partitionLevelDeletion());

            json.writeEndObject();

            if (partition.hasNext() || partition.staticRow() != null)
            {
                json.writeFieldName("rows");
                json.writeStartArray();
                updatePosition();
                if (!partition.staticRow().isEmpty())
                    serializeRow(partition.staticRow());

                Unfiltered unfiltered;
                updatePosition();
                while (partition.hasNext())
                {
                    unfiltered = partition.next();
                    if (unfiltered instanceof Row)
                    {
                        serializeRow((Row) unfiltered);
                    }
                    else if (unfiltered instanceof RangeTombstoneMarker)
                    {
                        serializeTombstone((RangeTombstoneMarker) unfiltered);
                    }
                    updatePosition();
                }
                json.writeEndArray();

                json.writeEndObject();
            }
        }
        catch (IOException e)
        {
            String key = metadata.getKeyValidator().getString(partition.partitionKey().getKey());
            logger.error("Fatal error parsing partition: {}", key, e);
        }
    }

+++++++++++++++++++++++
    private void serializePartition(UnfilteredRowIterator partition)
    {
        try
        {
            json.writeStartObject();

            json.writeFieldName("partition");
            json.writeStartObject();
            json.writeFieldName("key");
            serializePartitionKey(partition.partitionKey());
            json.writeNumberField("position", this.currentScanner.getCurrentPosition());

            if (!partition.partitionLevelDeletion().isLive())
                serializeDeletion(partition.partitionLevelDeletion());

            json.writeEndObject();

            json.writeFieldName("rows");
            json.writeStartArray();
            updatePosition();

            if (partition.staticRow() != null)
            {
                if (!partition.staticRow().isEmpty())
                    serializeRow(partition.staticRow());
                updatePosition();
            }

            Unfiltered unfiltered;
            while (partition.hasNext())
            {
                unfiltered = partition.next();
                if (unfiltered instanceof Row)
                {
                    serializeRow((Row) unfiltered);
                }
                else if (unfiltered instanceof RangeTombstoneMarker)
                {
                    serializeTombstone((RangeTombstoneMarker) unfiltered);
                }
                updatePosition();
            }

            json.writeEndArray();

            json.writeEndObject();
        }
        catch (IOException e)
        {
            String key = metadata.getKeyValidator().getString(partition.partitionKey().getKey());
            logger.error("Fatal error parsing partition: {}", key, e);
        }
    }

add: 17, delete: 2, change: 114, unhandled: 0 size_exceptions: 0 size_serialize: 1
-----------cassandra-3.11.6 vs cassandra-4.0-alpha1-----------
    private V deserialize(RefCountedMemory mem)
    {
        try
        {
            return serializer.deserialize(new MemoryInputStream(mem));
        }
        catch (IOException e)
        {
            logger.trace("Cannot fetch in memory data, we will fallback to read from disk ", e);
            return null;
        }
    }

+++++++++++++++++++++++
    private V deserialize(RefCountedMemory mem)
    {
        try
        {
            return serializer.deserialize(new MemoryInputStream(mem));
        }
        catch (IOException e)
        {
            logger.trace("Cannot fetch in memory data, we will fallback to read from disk ", e);
            return null;
        }
    }

    @SuppressWarnings("resource")
    protected void serializeSelection(DataOutputPlus out, int version) throws IOException
    {
        metadata().getKeyValidator().writeValue(partitionKey().getKey(), out);
        ClusteringIndexFilter.serializer.serialize(clusteringIndexFilter(), out, version);
    }

+++++++++++++++++++++++
    protected void serializeSelection(DataOutputPlus out, int version) throws IOException
    {
        metadata().partitionKeyType.writeValue(partitionKey().getKey(), out);
        ClusteringIndexFilter.serializer.serialize(clusteringIndexFilter(), out, version);
    }

    public Header deserializeHeader(CFMetaData metadata, ColumnFilter selection, DataInputPlus in, int version, SerializationHelper.Flag flag) throws IOException
    {
        DecoratedKey key = metadata.decorateKey(ByteBufferUtil.readWithVIntLength(in));
        int flags = in.readUnsignedByte();
        boolean isReversed = (flags & IS_REVERSED) != 0;
        if ((flags & IS_EMPTY) != 0)
        {
            SerializationHeader sh = new SerializationHeader(false, metadata, PartitionColumns.NONE, EncodingStats.NO_STATS);
            return new Header(sh, key, isReversed, true, null, null, 0);
        }

        boolean hasPartitionDeletion = (flags & HAS_PARTITION_DELETION) != 0;
        boolean hasStatic = (flags & HAS_STATIC_ROW) != 0;
        boolean hasRowEstimate = (flags & HAS_ROW_ESTIMATE) != 0;

        SerializationHeader header = SerializationHeader.serializer.deserializeForMessaging(in, metadata, selection, hasStatic);

        DeletionTime partitionDeletion = hasPartitionDeletion ? header.readDeletionTime(in) : DeletionTime.LIVE;

        Row staticRow = Rows.EMPTY_STATIC_ROW;
        if (hasStatic)
            staticRow = UnfilteredSerializer.serializer.deserializeStaticRow(in, header, new SerializationHelper(metadata, version, flag));

        int rowEstimate = hasRowEstimate ? (int)in.readUnsignedVInt() : -1;
        return new Header(header, key, isReversed, false, partitionDeletion, staticRow, rowEstimate);
    }

+++++++++++++++++++++++
    public Header deserializeHeader(TableMetadata metadata, ColumnFilter selection, DataInputPlus in, int version, SerializationHelper.Flag flag) throws IOException
    {
        DecoratedKey key = metadata.partitioner.decorateKey(ByteBufferUtil.readWithVIntLength(in));
        int flags = in.readUnsignedByte();
        boolean isReversed = (flags & IS_REVERSED) != 0;
        if ((flags & IS_EMPTY) != 0)
        {
            SerializationHeader sh = new SerializationHeader(false, metadata, RegularAndStaticColumns.NONE, EncodingStats.NO_STATS);
            return new Header(sh, key, isReversed, true, null, null, 0);
        }

        boolean hasPartitionDeletion = (flags & HAS_PARTITION_DELETION) != 0;
        boolean hasStatic = (flags & HAS_STATIC_ROW) != 0;
        boolean hasRowEstimate = (flags & HAS_ROW_ESTIMATE) != 0;

        SerializationHeader header = SerializationHeader.serializer.deserializeForMessaging(in, metadata, selection, hasStatic);

        DeletionTime partitionDeletion = hasPartitionDeletion ? header.readDeletionTime(in) : DeletionTime.LIVE;

        Row staticRow = Rows.EMPTY_STATIC_ROW;
        if (hasStatic)
            staticRow = UnfilteredSerializer.serializer.deserializeStaticRow(in, header, new SerializationHelper(metadata, version, flag));

        int rowEstimate = hasRowEstimate ? (int)in.readUnsignedVInt() : -1;
        return new Header(header, key, isReversed, false, partitionDeletion, staticRow, rowEstimate);
    }

    public UnfilteredRowIterator deserialize(DataInputPlus in, int version, CFMetaData metadata, ColumnFilter selection, SerializationHelper.Flag flag) throws IOException
    {
        return deserialize(in, version, metadata, flag, deserializeHeader(metadata, selection, in, version, flag));
    }

+++++++++++++++++++++++
    public UnfilteredRowIterator deserialize(DataInputPlus in, int version, TableMetadata metadata, ColumnFilter selection, SerializationHelper.Flag flag) throws IOException
    {
        return deserialize(in, version, metadata, flag, deserializeHeader(metadata, selection, in, version, flag));
    }

    private void serializeRowBody(Row row, int flags, SerializationHeader header, DataOutputPlus out)
    throws IOException
    {
        boolean isStatic = row.isStatic();

        Columns headerColumns = header.columns(isStatic);
        LivenessInfo pkLiveness = row.primaryKeyLivenessInfo();
        Row.Deletion deletion = row.deletion();

        if ((flags & HAS_TIMESTAMP) != 0)
            header.writeTimestamp(pkLiveness.timestamp(), out);
        if ((flags & HAS_TTL) != 0)
        {
            header.writeTTL(pkLiveness.ttl(), out);
            header.writeLocalDeletionTime(pkLiveness.localExpirationTime(), out);
        }
        if ((flags & HAS_DELETION) != 0)
            header.writeDeletionTime(deletion.time(), out);

        if ((flags & HAS_ALL_COLUMNS) == 0)
            Columns.serializer.serializeSubset(row.columns(), headerColumns, out);

        SearchIterator<ColumnDefinition, ColumnDefinition> si = headerColumns.iterator();

        try
        {
            row.apply(cd -> {
                // We can obtain the column for data directly from data.column(). However, if the cell/complex data
                // originates from a sstable, the column we'll get will have the type used when the sstable was serialized,
                // and if that type have been recently altered, that may not be the type we want to serialize the column
                // with. So we use the ColumnDefinition from the "header" which is "current". Also see #11810 for what
                // happens if we don't do that.
                ColumnDefinition column = si.next(cd.column());
                assert column != null : cd.column.toString();

                try
                {
                    if (cd.column.isSimple())
                        Cell.serializer.serialize((Cell) cd, column, out, pkLiveness, header);
                    else
                        writeComplexColumn((ComplexColumnData) cd, column, (flags & HAS_COMPLEX_DELETION) != 0, pkLiveness, header, out);
                }
                catch (IOException e)
                {
                    throw new WrappedException(e);
                }
            }, false);
        }
        catch (WrappedException e)
        {
            if (e.getCause() instanceof IOException)
                throw (IOException) e.getCause();

            throw e;
        }
    }

+++++++++++++++++++++++
    private void serializeRowBody(Row row, int flags, SerializationHeader header, DataOutputPlus out)
    throws IOException
    {
        boolean isStatic = row.isStatic();

        Columns headerColumns = header.columns(isStatic);
        LivenessInfo pkLiveness = row.primaryKeyLivenessInfo();
        Row.Deletion deletion = row.deletion();

        if ((flags & HAS_TIMESTAMP) != 0)
            header.writeTimestamp(pkLiveness.timestamp(), out);
        if ((flags & HAS_TTL) != 0)
        {
            header.writeTTL(pkLiveness.ttl(), out);
            header.writeLocalDeletionTime(pkLiveness.localExpirationTime(), out);
        }
        if ((flags & HAS_DELETION) != 0)
            header.writeDeletionTime(deletion.time(), out);

        if ((flags & HAS_ALL_COLUMNS) == 0)
            Columns.serializer.serializeSubset(row.columns(), headerColumns, out);

        SearchIterator<ColumnMetadata, ColumnMetadata> si = headerColumns.iterator();

        try
        {
            row.apply(cd -> {
                // We can obtain the column for data directly from data.column(). However, if the cell/complex data
                // originates from a sstable, the column we'll get will have the type used when the sstable was serialized,
                // and if that type have been recently altered, that may not be the type we want to serialize the column
                // with. So we use the ColumnMetadata from the "header" which is "current". Also see #11810 for what
                // happens if we don't do that.
                ColumnMetadata column = si.next(cd.column());
                assert column != null : cd.column.toString();

                try
                {
                    if (cd.column.isSimple())
                        Cell.serializer.serialize((Cell) cd, column, out, pkLiveness, header);
                    else
                        writeComplexColumn((ComplexColumnData) cd, column, (flags & HAS_COMPLEX_DELETION) != 0, pkLiveness, header, out);
                }
                catch (IOException e)
                {
                    throw new WrappedException(e);
                }
            }, false);
        }
        catch (WrappedException e)
        {
            if (e.getCause() instanceof IOException)
                throw (IOException) e.getCause();

            throw e;
        }
    }

    private long serializedRowBodySize(Row row, SerializationHeader header, long previousUnfilteredSize, int version)
    {
        long size = 0;

        if (header.isForSSTable())
            size += TypeSizes.sizeofUnsignedVInt(previousUnfilteredSize);

        boolean isStatic = row.isStatic();
        Columns headerColumns = header.columns(isStatic);
        LivenessInfo pkLiveness = row.primaryKeyLivenessInfo();
        Row.Deletion deletion = row.deletion();
        boolean hasComplexDeletion = row.hasComplexDeletion();
        boolean hasAllColumns = (row.columnCount() == headerColumns.size());

        if (!pkLiveness.isEmpty())
            size += header.timestampSerializedSize(pkLiveness.timestamp());
        if (pkLiveness.isExpiring())
        {
            size += header.ttlSerializedSize(pkLiveness.ttl());
            size += header.localDeletionTimeSerializedSize(pkLiveness.localExpirationTime());
        }
        if (!deletion.isLive())
            size += header.deletionTimeSerializedSize(deletion.time());

        if (!hasAllColumns)
            size += Columns.serializer.serializedSubsetSize(row.columns(), header.columns(isStatic));

        SearchIterator<ColumnDefinition, ColumnDefinition> si = headerColumns.iterator();
        for (ColumnData data : row)
        {
            ColumnDefinition column = si.next(data.column());
            assert column != null;

            if (data.column.isSimple())
                size += Cell.serializer.serializedSize((Cell) data, column, pkLiveness, header);
            else
                size += sizeOfComplexColumn((ComplexColumnData) data, column, hasComplexDeletion, pkLiveness, header);
        }

        return size;
    }

+++++++++++++++++++++++
    private long serializedRowBodySize(Row row, SerializationHeader header, long previousUnfilteredSize, int version)
    {
        long size = 0;

        if (header.isForSSTable())
            size += TypeSizes.sizeofUnsignedVInt(previousUnfilteredSize);

        boolean isStatic = row.isStatic();
        Columns headerColumns = header.columns(isStatic);
        LivenessInfo pkLiveness = row.primaryKeyLivenessInfo();
        Row.Deletion deletion = row.deletion();
        boolean hasComplexDeletion = row.hasComplexDeletion();
        boolean hasAllColumns = (row.columnCount() == headerColumns.size());

        if (!pkLiveness.isEmpty())
            size += header.timestampSerializedSize(pkLiveness.timestamp());
        if (pkLiveness.isExpiring())
        {
            size += header.ttlSerializedSize(pkLiveness.ttl());
            size += header.localDeletionTimeSerializedSize(pkLiveness.localExpirationTime());
        }
        if (!deletion.isLive())
            size += header.deletionTimeSerializedSize(deletion.time());

        if (!hasAllColumns)
            size += Columns.serializer.serializedSubsetSize(row.columns(), header.columns(isStatic));

        SearchIterator<ColumnMetadata, ColumnMetadata> si = headerColumns.iterator();
        for (ColumnData data : row)
        {
            ColumnMetadata column = si.next(data.column());
            assert column != null;

            if (data.column.isSimple())
                size += Cell.serializer.serializedSize((Cell) data, column, pkLiveness, header);
            else
                size += sizeOfComplexColumn((ComplexColumnData) data, column, hasComplexDeletion, pkLiveness, header);
        }

        return size;
    }

    public void serialize(GossipDigest gDigest, DataOutputPlus out, int version) throws IOException
    {
        CompactEndpointSerializationHelper.serialize(gDigest.endpoint, out);
        out.writeInt(gDigest.generation);
        out.writeInt(gDigest.maxVersion);
    }

+++++++++++++++++++++++
    public void serialize(GossipDigest gDigest, DataOutputPlus out, int version) throws IOException
    {
        inetAddressAndPortSerializer.serialize(gDigest.endpoint, out, version);
        out.writeInt(gDigest.generation);
        out.writeInt(gDigest.maxVersion);
    }

    public long serializedSize(GossipDigest gDigest, int version)
    {
        long size = CompactEndpointSerializationHelper.serializedSize(gDigest.endpoint);
        size += TypeSizes.sizeof(gDigest.generation);
        size += TypeSizes.sizeof(gDigest.maxVersion);
        return size;
+++++++++++++++++++++++
    public long serializedSize(GossipDigest gDigest, int version)
    {
        long size = inetAddressAndPortSerializer.serializedSize(gDigest.endpoint, version);
        size += TypeSizes.sizeof(gDigest.generation);
        size += TypeSizes.sizeof(gDigest.maxVersion);
        return size;
    public GossipDigest deserialize(DataInputPlus in, int version) throws IOException
    {
        InetAddress endpoint = CompactEndpointSerializationHelper.deserialize(in);
        int generation = in.readInt();
        int maxVersion = in.readInt();
        return new GossipDigest(endpoint, generation, maxVersion);
    }

+++++++++++++++++++++++
    public GossipDigest deserialize(DataInputPlus in, int version) throws IOException
    {
        InetAddressAndPort endpoint = inetAddressAndPortSerializer.deserialize(in, version);
        int generation = in.readInt();
        int maxVersion = in.readInt();
        return new GossipDigest(endpoint, generation, maxVersion);
    }

    public void serialize(GossipDigestAck gDigestAckMessage, DataOutputPlus out, int version) throws IOException
    {
        GossipDigestSerializationHelper.serialize(gDigestAckMessage.gDigestList, out, version);
        out.writeInt(gDigestAckMessage.epStateMap.size());
        for (Map.Entry<InetAddress, EndpointState> entry : gDigestAckMessage.epStateMap.entrySet())
        {
            InetAddress ep = entry.getKey();
            CompactEndpointSerializationHelper.serialize(ep, out);
            EndpointState.serializer.serialize(entry.getValue(), out, version);
        }
    }

+++++++++++++++++++++++
    public void serialize(GossipDigestAck gDigestAckMessage, DataOutputPlus out, int version) throws IOException
    {
        GossipDigestSerializationHelper.serialize(gDigestAckMessage.gDigestList, out, version);
        out.writeInt(gDigestAckMessage.epStateMap.size());
        for (Map.Entry<InetAddressAndPort, EndpointState> entry : gDigestAckMessage.epStateMap.entrySet())
        {
            InetAddressAndPort ep = entry.getKey();
            inetAddressAndPortSerializer.serialize(ep, out, version);
            EndpointState.serializer.serialize(entry.getValue(), out, version);
        }
    }

    public long serializedSize(GossipDigestAck ack, int version)
    {
        int size = GossipDigestSerializationHelper.serializedSize(ack.gDigestList, version);
        size += TypeSizes.sizeof(ack.epStateMap.size());
        for (Map.Entry<InetAddress, EndpointState> entry : ack.epStateMap.entrySet())
            size += CompactEndpointSerializationHelper.serializedSize(entry.getKey())
                    + EndpointState.serializer.serializedSize(entry.getValue(), version);
        return size;
+++++++++++++++++++++++
    public long serializedSize(GossipDigestAck ack, int version)
    {
        int size = GossipDigestSerializationHelper.serializedSize(ack.gDigestList, version);
        size += TypeSizes.sizeof(ack.epStateMap.size());
        for (Map.Entry<InetAddressAndPort, EndpointState> entry : ack.epStateMap.entrySet())
            size += inetAddressAndPortSerializer.serializedSize(entry.getKey(), version)
                    + EndpointState.serializer.serializedSize(entry.getValue(), version);
        return size;
    public GossipDigestAck deserialize(DataInputPlus in, int version) throws IOException
    {
        List<GossipDigest> gDigestList = GossipDigestSerializationHelper.deserialize(in, version);
        int size = in.readInt();
        Map<InetAddress, EndpointState> epStateMap = new HashMap<InetAddress, EndpointState>(size);

        for (int i = 0; i < size; ++i)
        {
            InetAddress ep = CompactEndpointSerializationHelper.deserialize(in);
            EndpointState epState = EndpointState.serializer.deserialize(in, version);
            epStateMap.put(ep, epState);
        }
        return new GossipDigestAck(gDigestList, epStateMap);
    }

+++++++++++++++++++++++
    public GossipDigestAck deserialize(DataInputPlus in, int version) throws IOException
    {
        List<GossipDigest> gDigestList = GossipDigestSerializationHelper.deserialize(in, version);
        int size = in.readInt();
        Map<InetAddressAndPort, EndpointState> epStateMap = new HashMap<InetAddressAndPort, EndpointState>(size);

        for (int i = 0; i < size; ++i)
        {
            InetAddressAndPort ep = inetAddressAndPortSerializer.deserialize(in, version);
            EndpointState epState = EndpointState.serializer.deserialize(in, version);
            epStateMap.put(ep, epState);
        }
        return new GossipDigestAck(gDigestList, epStateMap);
    }

    public void serialize(GossipDigestAck2 ack2, DataOutputPlus out, int version) throws IOException
    {
        out.writeInt(ack2.epStateMap.size());
        for (Map.Entry<InetAddress, EndpointState> entry : ack2.epStateMap.entrySet())
        {
            InetAddress ep = entry.getKey();
            CompactEndpointSerializationHelper.serialize(ep, out);
            EndpointState.serializer.serialize(entry.getValue(), out, version);
        }
    }

+++++++++++++++++++++++
    public void serialize(GossipDigestAck2 ack2, DataOutputPlus out, int version) throws IOException
    {
        out.writeInt(ack2.epStateMap.size());
        for (Map.Entry<InetAddressAndPort, EndpointState> entry : ack2.epStateMap.entrySet())
        {
            InetAddressAndPort ep = entry.getKey();
            inetAddressAndPortSerializer.serialize(ep, out, version);
            EndpointState.serializer.serialize(entry.getValue(), out, version);
        }
    }

    public long serializedSize(GossipDigestAck2 ack2, int version)
    {
        long size = TypeSizes.sizeof(ack2.epStateMap.size());
        for (Map.Entry<InetAddress, EndpointState> entry : ack2.epStateMap.entrySet())
            size += CompactEndpointSerializationHelper.serializedSize(entry.getKey())
                    + EndpointState.serializer.serializedSize(entry.getValue(), version);
        return size;
+++++++++++++++++++++++
    public long serializedSize(GossipDigestAck2 ack2, int version)
    {
        long size = TypeSizes.sizeof(ack2.epStateMap.size());
        for (Map.Entry<InetAddressAndPort, EndpointState> entry : ack2.epStateMap.entrySet())
            size += inetAddressAndPortSerializer.serializedSize(entry.getKey(), version)
                    + EndpointState.serializer.serializedSize(entry.getValue(), version);
        return size;
    public GossipDigestAck2 deserialize(DataInputPlus in, int version) throws IOException
    {
        int size = in.readInt();
        Map<InetAddress, EndpointState> epStateMap = new HashMap<InetAddress, EndpointState>(size);

        for (int i = 0; i < size; ++i)
        {
            InetAddress ep = CompactEndpointSerializationHelper.deserialize(in);
            EndpointState epState = EndpointState.serializer.deserialize(in, version);
            epStateMap.put(ep, epState);
        }
        return new GossipDigestAck2(epStateMap);
    }

+++++++++++++++++++++++
    public GossipDigestAck2 deserialize(DataInputPlus in, int version) throws IOException
    {
        int size = in.readInt();
        Map<InetAddressAndPort, EndpointState> epStateMap = new HashMap<>(size);

        for (int i = 0; i < size; ++i)
        {
            InetAddressAndPort ep = inetAddressAndPortSerializer.deserialize(in, version);
            EndpointState epState = EndpointState.serializer.deserialize(in, version);
            epStateMap.put(ep, epState);
        }
        return new GossipDigestAck2(epStateMap);
    }

    public static Collection<Token> deserialize(IPartitioner partitioner, DataInput in) throws IOException
    {
        Collection<Token> tokens = new ArrayList<Token>();
        while (true)
        {
            int size = in.readInt();
            if (size < 1)
                break;
            logger.trace("Reading token of {}", FBUtilities.prettyPrintMemory(size));
            byte[] bintoken = new byte[size];
            in.readFully(bintoken);
            tokens.add(partitioner.getTokenFactory().fromByteArray(ByteBuffer.wrap(bintoken)));
        }
        return tokens;
+++++++++++++++++++++++
    public static Collection<Token> deserialize(IPartitioner partitioner, DataInput in) throws IOException
    {
        Collection<Token> tokens = new ArrayList<Token>();
        while (true)
        {
            int size = in.readInt();
            if (size < 1)
                break;
            if (logger.isTraceEnabled())
                logger.trace("Reading token of {}", FBUtilities.prettyPrintMemory(size));
            byte[] bintoken = new byte[size];
            in.readFully(bintoken);
            tokens.add(partitioner.getTokenFactory().fromByteArray(ByteBuffer.wrap(bintoken)));
        }
        return tokens;
    public void serialize(Map<MetadataType, MetadataComponent> components, DataOutputPlus out, Version version) throws IOException
    {
        // sort components by type
        List<MetadataComponent> sortedComponents = Lists.newArrayList(components.values());
        Collections.sort(sortedComponents);

        // write number of component
        out.writeInt(components.size());
        // build and write toc
        int lastPosition = 4 + (8 * sortedComponents.size());
        for (MetadataComponent component : sortedComponents)
        {
            MetadataType type = component.getType();
            // serialize type
            out.writeInt(type.ordinal());
            // serialize position
            out.writeInt(lastPosition);
            lastPosition += type.serializer.serializedSize(version, component);
        }
        // serialize components
        for (MetadataComponent component : sortedComponents)
        {
            component.getType().serializer.serialize(version, component, out);
        }
    }

+++++++++++++++++++++++
    public void serialize(Map<MetadataType, MetadataComponent> components, DataOutputPlus out, Version version) throws IOException
    {
        boolean checksum = version.hasMetadataChecksum();
        CRC32 crc = new CRC32();
        // sort components by type
        List<MetadataComponent> sortedComponents = Lists.newArrayList(components.values());
        Collections.sort(sortedComponents);

        // write number of component
        out.writeInt(components.size());
        updateChecksumInt(crc, components.size());
        maybeWriteChecksum(crc, out, version);

        // build and write toc
        int lastPosition = 4 + (8 * sortedComponents.size()) + (checksum ? 2 * CHECKSUM_LENGTH : 0);
        Map<MetadataType, Integer> sizes = new EnumMap<>(MetadataType.class);
        for (MetadataComponent component : sortedComponents)
        {
            MetadataType type = component.getType();
            // serialize type
            out.writeInt(type.ordinal());
            updateChecksumInt(crc, type.ordinal());
            // serialize position
            out.writeInt(lastPosition);
            updateChecksumInt(crc, lastPosition);
            int size = type.serializer.serializedSize(version, component);
            lastPosition += size + (checksum ? CHECKSUM_LENGTH : 0);
            sizes.put(type, size);
        }
        maybeWriteChecksum(crc, out, version);

        // serialize components
        for (MetadataComponent component : sortedComponents)
        {
            byte[] bytes;
            try (DataOutputBuffer dob = new DataOutputBuffer(sizes.get(component.getType())))
            {
                component.getType().serializer.serialize(version, component, dob);
                bytes = dob.getData();
            }
            out.write(bytes);

            crc.reset(); crc.update(bytes);
            maybeWriteChecksum(crc, out, version);
        }
    }

    public Map<MetadataType, MetadataComponent> deserialize(Descriptor descriptor, FileDataInput in, EnumSet<MetadataType> types) throws IOException
    {
        Map<MetadataType, MetadataComponent> components = new EnumMap<>(MetadataType.class);
        // read number of components
        int numComponents = in.readInt();
        // read toc
        Map<MetadataType, Integer> toc = new EnumMap<>(MetadataType.class);
        MetadataType[] values = MetadataType.values();
        for (int i = 0; i < numComponents; i++)
        {
            toc.put(values[in.readInt()], in.readInt());
        }
        for (MetadataType type : types)
        {
            Integer offset = toc.get(type);
            if (offset != null)
            {
                in.seek(offset);
                MetadataComponent component = type.serializer.deserialize(descriptor.version, in);
                components.put(type, component);
            }
        }
        return components;
    }

+++++++++++++++++++++++
    public Map<MetadataType, MetadataComponent> deserialize(Descriptor descriptor,
                                                            FileDataInput in,
                                                            EnumSet<MetadataType> selectedTypes)
    throws IOException
    {
        boolean isChecksummed = descriptor.version.hasMetadataChecksum();
        CRC32 crc = new CRC32();

        /*
         * Read TOC
         */

        int length = (int) in.bytesRemaining();

        int count = in.readInt();
        updateChecksumInt(crc, count);
        maybeValidateChecksum(crc, in, descriptor);

        int[] ordinals = new int[count];
        int[]  offsets = new int[count];
        int[]  lengths = new int[count];

        for (int i = 0; i < count; i++)
        {
            ordinals[i] = in.readInt();
            updateChecksumInt(crc, ordinals[i]);

            offsets[i] = in.readInt();
            updateChecksumInt(crc, offsets[i]);
        }
        maybeValidateChecksum(crc, in, descriptor);

        lengths[count - 1] = length - offsets[count - 1];
        for (int i = 0; i < count - 1; i++)
            lengths[i] = offsets[i + 1] - offsets[i];

        /*
         * Read components
         */

        MetadataType[] allMetadataTypes = MetadataType.values();

        Map<MetadataType, MetadataComponent> components = new EnumMap<>(MetadataType.class);

        for (int i = 0; i < count; i++)
        {
            MetadataType type = allMetadataTypes[ordinals[i]];

            if (!selectedTypes.contains(type))
            {
                in.skipBytes(lengths[i]);
                continue;
            }

            byte[] buffer = new byte[isChecksummed ? lengths[i] - CHECKSUM_LENGTH : lengths[i]];
            in.readFully(buffer);

            crc.reset(); crc.update(buffer);
            maybeValidateChecksum(crc, in, descriptor);
            try (DataInputBuffer dataInputBuffer = new DataInputBuffer(buffer))
            {
                components.put(type, type.serializer.deserialize(descriptor.version, dataInputBuffer));
            }
        }

        return components;
    }

    public ByteBuffer serialize(Byte value)
    {
        return value == null ? ByteBufferUtil.EMPTY_BYTE_BUFFER : ByteBuffer.allocate(1).put(0, value);
    }

+++++++++++++++++++++++
    public ByteBuffer serialize(Byte value)
    {
        return value == null ? ByteBufferUtil.EMPTY_BYTE_BUFFER : ByteBufferUtil.bytes(value);
    }

    public Byte deserialize(ByteBuffer bytes)
    {
        return bytes == null || bytes.remaining() == 0 ? null : bytes.get(bytes.position());
    }

+++++++++++++++++++++++
    public Byte deserialize(ByteBuffer bytes)
    {
        return bytes == null || bytes.remaining() == 0 ? null : ByteBufferUtil.toByte(bytes);
    }

    public static void serialize(StreamMessage message, DataOutputStreamPlus out, int version, StreamSession session) throws IOException
    {
        ByteBuffer buff = ByteBuffer.allocate(1);
        // message type
        buff.put(message.type.type);
        buff.flip();
        out.write(buff);
        message.type.outSerializer.serialize(message, out, version, session);
    }

+++++++++++++++++++++++
    public static void serialize(StreamMessage message, DataOutputStreamPlus out, int version, StreamSession session) throws IOException
    {
        out.writeByte(message.type.id);
        message.type.outSerializer.serialize(message, out, version, session);
    }

    public static StreamMessage deserialize(ReadableByteChannel in, int version, StreamSession session) throws IOException
    {
        ByteBuffer buff = ByteBuffer.allocate(1);
        int readBytes = in.read(buff);
        if (readBytes > 0)
        {
            buff.flip();
            Type type = Type.get(buff.get());
            return type.inSerializer.deserialize(in, version, session);
        }
        else if (readBytes == 0)
        {
            // input socket buffer was not filled yet
            return null;
        }
        else
        {
            // possibly socket gets closed
            throw new SocketException("End-of-stream reached");
        }
    }

+++++++++++++++++++++++
    public static StreamMessage deserialize(DataInputPlus in, int version, StreamSession session) throws IOException
    {
        Type type = Type.lookupById(in.readByte());
        return type.inSerializer.deserialize(in, version, session);
    }

    /** StreamMessage serializer */
    private void serializeClustering(ClusteringPrefix clustering) throws IOException
    {
        if (clustering.size() > 0)
        {
            json.writeFieldName("clustering");
            objectIndenter.setCompact(true);
            json.writeStartArray();
            arrayIndenter.setCompact(true);
            List<ColumnDefinition> clusteringColumns = metadata.clusteringColumns();
            for (int i = 0; i < clusteringColumns.size(); i++)
            {
                ColumnDefinition column = clusteringColumns.get(i);
                if (i >= clustering.size())
                {
                    json.writeString("*");
                }
                else
                {
                    json.writeRawValue(column.cellValueType().toJSONString(clustering.get(i), ProtocolVersion.CURRENT));
                }
            }
            json.writeEndArray();
            objectIndenter.setCompact(false);
            arrayIndenter.setCompact(false);
        }
    }

+++++++++++++++++++++++
    private void serializeClustering(ClusteringPrefix clustering) throws IOException
    {
        if (clustering.size() > 0)
        {
            json.writeFieldName("clustering");
            objectIndenter.setCompact(true);
            json.writeStartArray();
            arrayIndenter.setCompact(true);
            List<ColumnMetadata> clusteringColumns = metadata.clusteringColumns();
            for (int i = 0; i < clusteringColumns.size(); i++)
            {
                ColumnMetadata column = clusteringColumns.get(i);
                if (i >= clustering.size())
                {
                    json.writeString("*");
                }
                else
                {
                    json.writeRawValue(column.cellValueType().toJSONString(clustering.get(i), ProtocolVersion.CURRENT));
                }
            }
            json.writeEndArray();
            objectIndenter.setCompact(false);
            arrayIndenter.setCompact(false);
        }
    }

    private void serializePartitionKey(DecoratedKey key)
    {
        AbstractType<?> keyValidator = metadata.getKeyValidator();
        objectIndenter.setCompact(true);
        try
        {
            arrayIndenter.setCompact(true);
            json.writeStartArray();
            if (keyValidator instanceof CompositeType)
            {
                // if a composite type, the partition has multiple keys.
                CompositeType compositeType = (CompositeType) keyValidator;
                ByteBuffer keyBytes = key.getKey().duplicate();
                // Skip static data if it exists.
                if (keyBytes.remaining() >= 2)
                {
                    int header = ByteBufferUtil.getShortLength(keyBytes, keyBytes.position());
                    if ((header & 0xFFFF) == 0xFFFF)
                    {
                        ByteBufferUtil.readShortLength(keyBytes);
                    }
                }

                int i = 0;
                while (keyBytes.remaining() > 0 && i < compositeType.getComponents().size())
                {
                    AbstractType<?> colType = compositeType.getComponents().get(i);

                    ByteBuffer value = ByteBufferUtil.readBytesWithShortLength(keyBytes);
                    String colValue = colType.getString(value);

                    json.writeString(colValue);

                    byte b = keyBytes.get();
                    if (b != 0)
                    {
                        break;
                    }
                    ++i;
                }
            }
            else
            {
                // if not a composite type, assume a single column partition key.
                assert metadata.partitionKeyColumns().size() == 1;
                json.writeString(keyValidator.getString(key.getKey()));
            }
            json.writeEndArray();
            objectIndenter.setCompact(false);
            arrayIndenter.setCompact(false);
        }
        catch (IOException e)
        {
            logger.error("Failure serializing partition key.", e);
        }
    }

+++++++++++++++++++++++
    private void serializePartitionKey(DecoratedKey key)
    {
        AbstractType<?> keyValidator = metadata.partitionKeyType;
        objectIndenter.setCompact(true);
        try
        {
            arrayIndenter.setCompact(true);
            json.writeStartArray();
            if (keyValidator instanceof CompositeType)
            {
                // if a composite type, the partition has multiple keys.
                CompositeType compositeType = (CompositeType) keyValidator;
                ByteBuffer keyBytes = key.getKey().duplicate();
                // Skip static data if it exists.
                if (keyBytes.remaining() >= 2)
                {
                    int header = ByteBufferUtil.getShortLength(keyBytes, keyBytes.position());
                    if ((header & 0xFFFF) == 0xFFFF)
                    {
                        ByteBufferUtil.readShortLength(keyBytes);
                    }
                }

                int i = 0;
                while (keyBytes.remaining() > 0 && i < compositeType.getComponents().size())
                {
                    AbstractType<?> colType = compositeType.getComponents().get(i);

                    ByteBuffer value = ByteBufferUtil.readBytesWithShortLength(keyBytes);
                    String colValue = colType.getString(value);

                    json.writeString(colValue);

                    byte b = keyBytes.get();
                    if (b != 0)
                    {
                        break;
                    }
                    ++i;
                }
            }
            else
            {
                // if not a composite type, assume a single column partition key.
                assert metadata.partitionKeyColumns().size() == 1;
                json.writeString(keyValidator.getString(key.getKey()));
            }
            json.writeEndArray();
            objectIndenter.setCompact(false);
            arrayIndenter.setCompact(false);
        }
        catch (IOException e)
        {
            logger.error("Failure serializing partition key.", e);
        }
    }

    private void serializePartition(UnfilteredRowIterator partition)
    {
        try
        {
            json.writeStartObject();

            json.writeFieldName("partition");
            json.writeStartObject();
            json.writeFieldName("key");
            serializePartitionKey(partition.partitionKey());
            json.writeNumberField("position", this.currentScanner.getCurrentPosition());

            if (!partition.partitionLevelDeletion().isLive())
                serializeDeletion(partition.partitionLevelDeletion());

            json.writeEndObject();

            json.writeFieldName("rows");
            json.writeStartArray();
            updatePosition();

            if (partition.staticRow() != null)
            {
                if (!partition.staticRow().isEmpty())
                    serializeRow(partition.staticRow());
                updatePosition();
            }

            Unfiltered unfiltered;
            while (partition.hasNext())
            {
                unfiltered = partition.next();
                if (unfiltered instanceof Row)
                {
                    serializeRow((Row) unfiltered);
                }
                else if (unfiltered instanceof RangeTombstoneMarker)
                {
                    serializeTombstone((RangeTombstoneMarker) unfiltered);
                }
                updatePosition();
            }

            json.writeEndArray();

            json.writeEndObject();
        }
        catch (IOException e)
        {
            String key = metadata.getKeyValidator().getString(partition.partitionKey().getKey());
            logger.error("Fatal error parsing partition: {}", key, e);
        }
    }

+++++++++++++++++++++++
    private void serializePartition(UnfilteredRowIterator partition)
    {
        try
        {
            json.writeStartObject();

            json.writeFieldName("partition");
            json.writeStartObject();
            json.writeFieldName("key");
            serializePartitionKey(partition.partitionKey());
            json.writeNumberField("position", this.currentScanner.getCurrentPosition());

            if (!partition.partitionLevelDeletion().isLive())
                serializeDeletion(partition.partitionLevelDeletion());

            json.writeEndObject();

            if (partition.hasNext() || partition.staticRow() != null)
            {
                json.writeFieldName("rows");
                json.writeStartArray();
                updatePosition();
                if (!partition.staticRow().isEmpty())
                    serializeRow(partition.staticRow());

                Unfiltered unfiltered;
                updatePosition();
                while (partition.hasNext())
                {
                    unfiltered = partition.next();
                    if (unfiltered instanceof Row)
                    {
                        serializeRow((Row) unfiltered);
                    }
                    else if (unfiltered instanceof RangeTombstoneMarker)
                    {
                        serializeTombstone((RangeTombstoneMarker) unfiltered);
                    }
                    updatePosition();
                }
                json.writeEndArray();

                json.writeEndObject();
            }
        }

        catch (IOException e)
        {
            String key = metadata.partitionKeyType.getString(partition.partitionKey().getKey());
            logger.error("Fatal error parsing partition: {}", key, e);
        }
    }

    public static BloomFilter deserialize(DataInput in, boolean offheap, boolean oldBfHashOrder) throws IOException
    {
        int hashes = in.readInt();
        IBitSet bs = offheap ? OffHeapBitSet.deserialize(in) : OpenBitSet.deserialize(in);

        return new BloomFilter(hashes, bs, oldBfHashOrder);
    }

    /**
     * Calculates a serialized size of the given Bloom Filter
     * @param bf Bloom filter to calculate serialized size
     * @see org.apache.cassandra.io.ISerializer#serialize(Object, org.apache.cassandra.io.util.DataOutputPlus)
     *
     * @return serialized size of the given bloom filter
     */
+++++++++++++++++++++++
    public static BloomFilter deserialize(DataInputStream in, boolean oldBfFormat) throws IOException
    {
        int hashes = in.readInt();
        IBitSet bs = OffHeapBitSet.deserialize(in, oldBfFormat);

        return new BloomFilter(hashes, bs);
    }

    /**
     * Calculates a serialized size of the given Bloom Filter
     * @param bf Bloom filter to calculate serialized size
     * @see org.apache.cassandra.io.ISerializer#serialize(Object, org.apache.cassandra.io.util.DataOutputPlus)
     *
     * @return serialized size of the given bloom filter
     */
    public static OffHeapBitSet deserialize(DataInput in) throws IOException
    {
        long byteCount = in.readInt() * 8L;
        Memory memory = Memory.allocate(byteCount);
        for (long i = 0; i < byteCount;)
        {
            long v = in.readLong();
            memory.setByte(i++, (byte) (v >>> 0));
            memory.setByte(i++, (byte) (v >>> 8));
            memory.setByte(i++, (byte) (v >>> 16));
            memory.setByte(i++, (byte) (v >>> 24));
            memory.setByte(i++, (byte) (v >>> 32));
            memory.setByte(i++, (byte) (v >>> 40));
            memory.setByte(i++, (byte) (v >>> 48));
            memory.setByte(i++, (byte) (v >>> 56));
        }
        return new OffHeapBitSet(memory);
    }

+++++++++++++++++++++++
    public static OffHeapBitSet deserialize(DataInputStream in, boolean oldBfFormat) throws IOException
    {
        long byteCount = in.readInt() * 8L;
        Memory memory = Memory.allocate(byteCount);
        if (oldBfFormat)
        {
            for (long i = 0; i < byteCount; )
            {
                long v = in.readLong();
                memory.setByte(i++, (byte) (v >>> 0));
                memory.setByte(i++, (byte) (v >>> 8));
                memory.setByte(i++, (byte) (v >>> 16));
                memory.setByte(i++, (byte) (v >>> 24));
                memory.setByte(i++, (byte) (v >>> 32));
                memory.setByte(i++, (byte) (v >>> 40));
                memory.setByte(i++, (byte) (v >>> 48));
                memory.setByte(i++, (byte) (v >>> 56));
            }
        }
        else
        {
            FBUtilities.copy(in, new MemoryOutputStream(memory), byteCount);
        }
        return new OffHeapBitSet(memory);
    }

    public void serialize(DataOutput out) throws IOException
    {
        out.writeInt((int) (bytes.size() / 8));
        for (long i = 0; i < bytes.size();)
        {
            long value = ((bytes.getByte(i++) & 0xff) << 0)
                       + ((bytes.getByte(i++) & 0xff) << 8)
                       + ((bytes.getByte(i++) & 0xff) << 16)
                       + ((long) (bytes.getByte(i++) & 0xff) << 24)
                       + ((long) (bytes.getByte(i++) & 0xff) << 32)
                       + ((long) (bytes.getByte(i++) & 0xff) << 40)
                       + ((long) (bytes.getByte(i++) & 0xff) << 48)
                       + ((long) bytes.getByte(i++) << 56);
            out.writeLong(value);
        }
    }

+++++++++++++++++++++++
    public void serialize(DataOutputPlus out) throws IOException
    {
        out.writeInt((int) (bytes.size() / 8));
        out.write(bytes, 0, bytes.size());
    }

    @VisibleForTesting
    public File serialize(Map<MetadataType, MetadataComponent> metadata, MetadataSerializer serializer, Version version)
            throws IOException
    {
        // Serialize to tmp file
        File statsFile = File.createTempFile(Component.STATS.name, null);
        try (DataOutputStreamPlus out = new BufferedDataOutputStreamPlus(new FileOutputStream(statsFile)))
        {
            serializer.serialize(metadata, out, version);
        }
        return statsFile;
    }

+++++++++++++++++++++++
    public File serialize(Map<MetadataType, MetadataComponent> metadata, MetadataSerializer serializer, Version version)
            throws IOException
    {
        // Serialize to tmp file
        File statsFile = FileUtils.createTempFile(Component.STATS.name, null);
        try (DataOutputStreamPlus out = new BufferedDataOutputStreamPlus(new FileOutputStream(statsFile)))
        {
            serializer.serialize(metadata, out, version);
        }
        return statsFile;
    }

add: 650, delete: 282, change: 1284, unhandled: 3 size_exceptions: 96 size_serialize: 29
-----------cassandra-4.0-alpha1 vs cassandra-4.0-alpha2-----------
add: 11, delete: 2, change: 85, unhandled: 2 size_exceptions: 0 size_serialize: 0
-----------cassandra-4.0-alpha2 vs cassandra-4.0-alpha3-----------
    private void serializePartition(UnfilteredRowIterator partition)
    {
        try
        {
            json.writeStartObject();

            json.writeFieldName("partition");
            json.writeStartObject();
            json.writeFieldName("key");
            serializePartitionKey(partition.partitionKey());
            json.writeNumberField("position", this.currentScanner.getCurrentPosition());

            if (!partition.partitionLevelDeletion().isLive())
                serializeDeletion(partition.partitionLevelDeletion());

            json.writeEndObject();

            if (partition.hasNext() || partition.staticRow() != null)
            {
                json.writeFieldName("rows");
                json.writeStartArray();
                updatePosition();
                if (!partition.staticRow().isEmpty())
                    serializeRow(partition.staticRow());

                Unfiltered unfiltered;
                updatePosition();
                while (partition.hasNext())
                {
                    unfiltered = partition.next();
                    if (unfiltered instanceof Row)
                    {
                        serializeRow((Row) unfiltered);
                    }
                    else if (unfiltered instanceof RangeTombstoneMarker)
                    {
                        serializeTombstone((RangeTombstoneMarker) unfiltered);
                    }
                    updatePosition();
                }
                json.writeEndArray();

                json.writeEndObject();
            }
        }

        catch (IOException e)
        {
            String key = metadata.partitionKeyType.getString(partition.partitionKey().getKey());
            logger.error("Fatal error parsing partition: {}", key, e);
        }
    }

+++++++++++++++++++++++
    private void serializePartition(UnfilteredRowIterator partition)
    {
        try
        {
            json.writeStartObject();

            json.writeFieldName("partition");
            json.writeStartObject();
            json.writeFieldName("key");
            serializePartitionKey(partition.partitionKey());
            json.writeNumberField("position", this.currentScanner.getCurrentPosition());

            if (!partition.partitionLevelDeletion().isLive())
                serializeDeletion(partition.partitionLevelDeletion());

            json.writeEndObject();

            json.writeFieldName("rows");
            json.writeStartArray();
            updatePosition();

            if (partition.staticRow() != null)
            {
                if (!partition.staticRow().isEmpty())
                    serializeRow(partition.staticRow());
                updatePosition();
            }

            Unfiltered unfiltered;
            while (partition.hasNext())
            {
                unfiltered = partition.next();
                if (unfiltered instanceof Row)
                {
                    serializeRow((Row) unfiltered);
                }
                else if (unfiltered instanceof RangeTombstoneMarker)
                {
                    serializeTombstone((RangeTombstoneMarker) unfiltered);
                }
                updatePosition();
            }

            json.writeEndArray();

            json.writeEndObject();
        }

        catch (IOException e)
        {
            String key = metadata.partitionKeyType.getString(partition.partitionKey().getKey());
            logger.error("Fatal error parsing partition: {}", key, e);
        }
    }

add: 22, delete: 7, change: 253, unhandled: 2 size_exceptions: 0 size_serialize: 1
